{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVF3KEKOY_hm",
    "outputId": "5799b70b-6111-4cab-ec60-d8dca40d2f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed row 190149, saved to /content/cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = '/content/prepared_data.csv'\n",
    "output_file = '/content/cleaned_data.csv'\n",
    "\n",
    "# Count rows from 0, so row 190149 is line 190150\n",
    "row_to_remove = 190149\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "    for i, line in enumerate(infile):\n",
    "        if i != row_to_remove:\n",
    "            outfile.write(line)\n",
    "\n",
    "print(f\"Removed row {row_to_remove}, saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fLuG76Wybdtc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Try using the CSV module directly for more control\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "with open('/content/cleaned_data.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        if i == 2938:  # Skip or fix the problematic row\n",
    "            continue  # or manually fix this row\n",
    "        data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data[1:], columns=data[0])  # Assuming first row is header\n",
    "\n",
    "# Convert to dictionary for easier processing\n",
    "df = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SJXqgHrfbq1j"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Input, Attention, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1Xic2dBJcalr"
   },
   "outputs": [],
   "source": [
    "# Assuming 'df' already exists somehow (you didn't show its loading part)\n",
    "# If it's a list of dicts, we convert it\n",
    "df = pd.DataFrame(df)  # Convert the list of dicts into a dataframe\n",
    "\n",
    "# Make sure columns are string type\n",
    "texts = df['content'].astype(str).tolist()\n",
    "summaries = df['content'].astype(str).tolist()  # Replace with real summaries when ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rt3dgT6mh4KB"
   },
   "outputs": [],
   "source": [
    "# Tokenization and padding function\n",
    "def preprocess(texts, max_words=10000, max_len=200):\n",
    "    tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\", filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "    return padded, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HPF51FZyh8bH"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "max_len_text = 200\n",
    "max_len_summary = 50\n",
    "\n",
    "X, text_tokenizer = preprocess(texts, max_len=max_len_text)\n",
    "y, summary_tokenizer = preprocess(summaries, max_len=max_len_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bowPpllHiAnZ"
   },
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_val = X[:split], X[split:]\n",
    "y_train, y_val = y[:split], y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4DXLhZa9l6IT"
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "gru_units = 256\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(max_len_text,))\n",
    "enc_emb = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_gru = GRU(gru_units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, encoder_state = encoder_gru(enc_emb)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(max_len_summary - 1,))\n",
    "dec_emb = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_gru = GRU(gru_units, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _ = decoder_gru(dec_emb, initial_state=encoder_state)\n",
    "\n",
    "# Attention\n",
    "attention = Attention()([decoder_outputs, encoder_outputs])\n",
    "decoder_concat = Concatenate()([decoder_outputs, attention])\n",
    "\n",
    "# Output Layer\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "output = decoder_dense(decoder_concat)\n",
    "\n",
    "# Create the model\n",
    "model = Model([encoder_inputs, decoder_inputs], output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R-P3XShrl-E2",
    "outputId": "2b3fbfc6-ffce-476b-e7b6-7d01efc8e344"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m3909/3909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12704s\u001b[0m 3s/step - loss: 4.1318 - val_loss: 0.0679\n",
      "Epoch 2/3\n",
      "\u001b[1m3909/3909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12809s\u001b[0m 3s/step - loss: 0.0304 - val_loss: 0.0130\n",
      "Epoch 3/3\n",
      "\u001b[1m3909/3909\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12841s\u001b[0m 3s/step - loss: 0.0083 - val_loss: 0.0099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d494633f590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(\n",
    "    [X_train, y_train[:, :-1]],\n",
    "    y_train[:, 1:],\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    validation_data=([X_val, y_val[:, :-1]], y_val[:, 1:])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "si3c7T7qmB8c"
   },
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    # 1) Text → sequence → padded\n",
    "    seq = text_tokenizer.texts_to_sequences([text])\n",
    "    if not seq or not seq[0]:\n",
    "        return \"\"\n",
    "    padded = pad_sequences(seq, maxlen=max_len_text, padding='post')\n",
    "\n",
    "    # 2) Initialize target_seq with the exact decoder_seq_len\n",
    "    target_seq = np.zeros((1, decoder_seq_len))\n",
    "    if '<start>' in summary_tokenizer.word_index:\n",
    "        target_seq[0, 0] = summary_tokenizer.word_index['<start>']\n",
    "\n",
    "    # 3) Iteratively predict next token\n",
    "    for i in range(decoder_seq_len - 1):\n",
    "        preds = model.predict([padded, target_seq], verbose=0)\n",
    "        sampled_idx = np.argmax(preds[0, i, :])\n",
    "        target_seq[0, i + 1] = sampled_idx\n",
    "        if sampled_idx == summary_tokenizer.word_index.get('<end>', 0):\n",
    "            break\n",
    "\n",
    "    # 4) Convert token indices → words\n",
    "    summary = []\n",
    "    for idx in target_seq[0]:\n",
    "        idx = int(idx)\n",
    "        if idx > 0:\n",
    "            w = summary_tokenizer.index_word.get(idx, '')\n",
    "            if w == '<end>':\n",
    "                break\n",
    "            summary.append(w)\n",
    "    return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJvfB9qtCpAN",
    "outputId": "429bf108-3df3-4c73-dcb6-f1e50132999c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input length (max summary tokens): 49\n"
     ]
    }
   ],
   "source": [
    "# decoder_seq_len will be 49 in your case\n",
    "decoder_seq_len = model.input_shape[1][1]\n",
    "print(f\"Decoder input length (max summary tokens): {decoder_seq_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "pvwsq40rCrgj",
    "outputId": "f67810fe-4069-4ee5-cdf9-25bfad879933"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eb6b9f0d54d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_head_10\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_head_10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gru_summary'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_head_10\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_head_10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gru_summarized_head_10_fixed.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fixed summarization on first 10 articles complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_head_10 = df.head(10).copy()\n",
    "df_head_10['gru_summary'] = df_head_10['content'].astype(str).apply(summarize)\n",
    "df_head_10.to_csv('gru_summarized_head_10_fixed.csv', index=False)\n",
    "print(\"Fixed summarization on first 10 articles complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CPHGaRDDTS7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
