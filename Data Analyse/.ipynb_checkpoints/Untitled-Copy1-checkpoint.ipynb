{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4eb9bd5-3a83-4b4d-ad1b-062bc6abde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('prepared_data.csv')\n",
    "\n",
    "# Convert to dictionary for easier processing\n",
    "data = df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b36dcd-c8f8-43a3-85de-5a0c0650a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained T5-small model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ad0f70-f001-4436-bf87-e738755b2a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Create a custom dataset class\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_input_len=512, max_output_len=150):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_output_len = max_output_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        \n",
    "        # Combine title and content for better context\n",
    "        input_text = f\"summarize: {item['title']}. {item['content']}\"\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            max_length=self.max_input_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets (using content as target for self-supervised learning)\n",
    "        # In a real scenario, you'd have human-written summaries as targets\n",
    "        targets = self.tokenizer(\n",
    "            item['content'][:500],  # Using first 500 chars as pseudo-summary\n",
    "            max_length=self.max_output_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].flatten(),\n",
    "            \"labels\": targets[\"input_ids\"].flatten()\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SummaryDataset(data, tokenizer)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=3e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c29fff3b-257d-43ba-88d9-e37e7712b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 07:00:14.615317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745128814.642653  377345 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745128814.651333  377345 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745128814.673677  377345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745128814.673706  377345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745128814.673709  377345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745128814.673712  377345 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-20 07:00:14.679544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8/192392\n",
      "Processed 16/192392\n",
      "Processed 24/192392\n",
      "Processed 32/192392\n",
      "Processed 40/192392\n",
      "Processed 48/192392\n",
      "Processed 56/192392\n",
      "Processed 64/192392\n",
      "Processed 72/192392\n",
      "Processed 80/192392\n",
      "Processed 88/192392\n",
      "Processed 96/192392\n",
      "Processed 104/192392\n",
      "Processed 112/192392\n",
      "Processed 120/192392\n",
      "Processed 128/192392\n",
      "Processed 136/192392\n",
      "Processed 144/192392\n",
      "Processed 152/192392\n",
      "Processed 160/192392\n",
      "Processed 168/192392\n",
      "Processed 176/192392\n",
      "Processed 184/192392\n",
      "Processed 192/192392\n",
      "Processed 200/192392\n",
      "Processed 208/192392\n",
      "Processed 216/192392\n",
      "Processed 224/192392\n",
      "Processed 232/192392\n",
      "Processed 240/192392\n",
      "Processed 248/192392\n",
      "Processed 256/192392\n",
      "Processed 264/192392\n",
      "Processed 272/192392\n",
      "Processed 280/192392\n",
      "Processed 288/192392\n",
      "Processed 296/192392\n",
      "Processed 304/192392\n",
      "Processed 312/192392\n",
      "Processed 320/192392\n",
      "Processed 328/192392\n",
      "Processed 336/192392\n",
      "Processed 344/192392\n",
      "Processed 352/192392\n",
      "Processed 360/192392\n",
      "Processed 368/192392\n",
      "Processed 376/192392\n",
      "Processed 384/192392\n",
      "Processed 392/192392\n",
      "Processed 400/192392\n",
      "Processed 408/192392\n",
      "Processed 416/192392\n",
      "Processed 424/192392\n",
      "Processed 432/192392\n",
      "Processed 440/192392\n",
      "Processed 448/192392\n",
      "Processed 456/192392\n",
      "Processed 464/192392\n",
      "Processed 472/192392\n",
      "Processed 480/192392\n",
      "Processed 488/192392\n",
      "Processed 496/192392\n",
      "Processed 504/192392\n",
      "Processed 512/192392\n",
      "Processed 520/192392\n",
      "Processed 528/192392\n",
      "Processed 536/192392\n",
      "Processed 544/192392\n",
      "Processed 552/192392\n",
      "Processed 560/192392\n",
      "Processed 568/192392\n",
      "Processed 576/192392\n",
      "Processed 584/192392\n",
      "Processed 592/192392\n",
      "Processed 600/192392\n",
      "Processed 608/192392\n",
      "Processed 616/192392\n",
      "Processed 624/192392\n",
      "Processed 632/192392\n",
      "Processed 640/192392\n",
      "Processed 648/192392\n",
      "Processed 656/192392\n",
      "Processed 664/192392\n",
      "Processed 672/192392\n",
      "Processed 680/192392\n",
      "Processed 688/192392\n",
      "Processed 696/192392\n",
      "Processed 704/192392\n",
      "Processed 712/192392\n",
      "Processed 720/192392\n",
      "Processed 728/192392\n",
      "Processed 736/192392\n",
      "Processed 744/192392\n",
      "Processed 752/192392\n",
      "Processed 760/192392\n",
      "Processed 768/192392\n",
      "Processed 776/192392\n",
      "Processed 784/192392\n",
      "Processed 792/192392\n",
      "Processed 800/192392\n",
      "Processed 808/192392\n",
      "Processed 816/192392\n",
      "Processed 824/192392\n",
      "Processed 832/192392\n",
      "Processed 840/192392\n",
      "Processed 848/192392\n",
      "Processed 856/192392\n",
      "Processed 864/192392\n",
      "Processed 872/192392\n",
      "Processed 880/192392\n",
      "Processed 888/192392\n",
      "Processed 896/192392\n",
      "Processed 904/192392\n",
      "Processed 912/192392\n",
      "Processed 920/192392\n",
      "Processed 928/192392\n",
      "Processed 936/192392\n",
      "Processed 944/192392\n",
      "Processed 952/192392\n",
      "Processed 960/192392\n",
      "Processed 968/192392\n",
      "Processed 976/192392\n",
      "Processed 984/192392\n",
      "Processed 992/192392\n",
      "Processed 1000/192392\n",
      "Processed 1008/192392\n",
      "Processed 1016/192392\n",
      "Processed 1024/192392\n",
      "Processed 1032/192392\n",
      "Processed 1040/192392\n",
      "Processed 1048/192392\n",
      "Processed 1056/192392\n",
      "Processed 1064/192392\n",
      "Processed 1072/192392\n",
      "Processed 1080/192392\n",
      "Processed 1088/192392\n",
      "Processed 1096/192392\n",
      "Processed 1104/192392\n",
      "Processed 1112/192392\n",
      "Processed 1120/192392\n",
      "Processed 1128/192392\n",
      "Processed 1136/192392\n",
      "Processed 1144/192392\n",
      "Processed 1152/192392\n",
      "Processed 1160/192392\n",
      "Processed 1168/192392\n",
      "Processed 1176/192392\n",
      "Processed 1184/192392\n",
      "Processed 1192/192392\n",
      "Processed 1200/192392\n",
      "Processed 1208/192392\n",
      "Processed 1216/192392\n",
      "Processed 1224/192392\n",
      "Processed 1232/192392\n",
      "Processed 1240/192392\n",
      "Processed 1248/192392\n",
      "Processed 1256/192392\n",
      "Processed 1264/192392\n",
      "Processed 1272/192392\n",
      "Processed 1280/192392\n",
      "Processed 1288/192392\n",
      "Processed 1296/192392\n",
      "Processed 1304/192392\n",
      "Processed 1312/192392\n",
      "Processed 1320/192392\n",
      "Processed 1328/192392\n",
      "Processed 1336/192392\n",
      "Processed 1344/192392\n",
      "Processed 1352/192392\n",
      "Processed 1360/192392\n",
      "Processed 1368/192392\n",
      "Processed 1376/192392\n",
      "Processed 1384/192392\n",
      "Processed 1392/192392\n",
      "Processed 1400/192392\n",
      "Processed 1408/192392\n",
      "Processed 1416/192392\n",
      "Processed 1424/192392\n",
      "Processed 1432/192392\n",
      "Processed 1440/192392\n",
      "Processed 1448/192392\n",
      "Processed 1456/192392\n",
      "Processed 1464/192392\n",
      "Processed 1472/192392\n",
      "Processed 1480/192392\n",
      "Processed 1488/192392\n",
      "Processed 1496/192392\n",
      "Processed 1504/192392\n",
      "Processed 1512/192392\n",
      "Processed 1520/192392\n",
      "Processed 1528/192392\n",
      "Processed 1536/192392\n",
      "Processed 1544/192392\n",
      "Processed 1552/192392\n",
      "Processed 1560/192392\n",
      "Processed 1568/192392\n",
      "Processed 1576/192392\n",
      "Processed 1584/192392\n",
      "Processed 1592/192392\n",
      "Processed 1600/192392\n",
      "Processed 1608/192392\n",
      "Processed 1616/192392\n",
      "Processed 1624/192392\n",
      "Processed 1632/192392\n",
      "Processed 1640/192392\n",
      "Processed 1648/192392\n",
      "Processed 1656/192392\n",
      "Processed 1664/192392\n",
      "Processed 1672/192392\n",
      "Processed 1680/192392\n",
      "Processed 1688/192392\n",
      "Processed 1696/192392\n",
      "Processed 1704/192392\n",
      "Processed 1712/192392\n",
      "Processed 1720/192392\n",
      "Processed 1728/192392\n",
      "Processed 1736/192392\n",
      "Processed 1744/192392\n",
      "Processed 1752/192392\n",
      "Processed 1760/192392\n",
      "Processed 1768/192392\n",
      "Processed 1776/192392\n",
      "Processed 1784/192392\n",
      "Processed 1792/192392\n",
      "Processed 1800/192392\n",
      "Processed 1808/192392\n",
      "Processed 1816/192392\n",
      "Processed 1824/192392\n",
      "Processed 1832/192392\n",
      "Processed 1840/192392\n",
      "Processed 1848/192392\n",
      "Processed 1856/192392\n",
      "Processed 1864/192392\n",
      "Processed 1872/192392\n",
      "Processed 1880/192392\n",
      "Processed 1888/192392\n",
      "Processed 1896/192392\n",
      "Processed 1904/192392\n",
      "Processed 1912/192392\n",
      "Processed 1920/192392\n",
      "Processed 1928/192392\n",
      "Processed 1936/192392\n",
      "Processed 1944/192392\n",
      "Processed 1952/192392\n",
      "Processed 1960/192392\n",
      "Processed 1968/192392\n",
      "Processed 1976/192392\n",
      "Processed 1984/192392\n",
      "Processed 1992/192392\n",
      "Processed 2000/192392\n",
      "Processed 2008/192392\n",
      "Processed 2016/192392\n",
      "Processed 2024/192392\n",
      "Processed 2032/192392\n",
      "Processed 2040/192392\n",
      "Processed 2048/192392\n",
      "Processed 2056/192392\n",
      "Processed 2064/192392\n",
      "Processed 2072/192392\n",
      "Processed 2080/192392\n",
      "Processed 2088/192392\n",
      "Processed 2096/192392\n",
      "Processed 2104/192392\n",
      "Processed 2112/192392\n",
      "Processed 2120/192392\n",
      "Processed 2128/192392\n",
      "Processed 2136/192392\n",
      "Processed 2144/192392\n",
      "Processed 2152/192392\n",
      "Processed 2160/192392\n",
      "Processed 2168/192392\n",
      "Processed 2176/192392\n",
      "Processed 2184/192392\n",
      "Processed 2192/192392\n",
      "Processed 2200/192392\n",
      "Processed 2208/192392\n",
      "Processed 2216/192392\n",
      "Processed 2224/192392\n",
      "Processed 2232/192392\n",
      "Processed 2240/192392\n",
      "Processed 2248/192392\n",
      "Processed 2256/192392\n",
      "Processed 2264/192392\n",
      "Processed 2272/192392\n",
      "Processed 2280/192392\n",
      "Processed 2288/192392\n",
      "Processed 2296/192392\n",
      "Processed 2304/192392\n",
      "Processed 2312/192392\n",
      "Processed 2320/192392\n",
      "Processed 2328/192392\n",
      "Processed 2336/192392\n",
      "Processed 2344/192392\n",
      "Processed 2352/192392\n",
      "Processed 2360/192392\n",
      "Processed 2368/192392\n",
      "Processed 2376/192392\n",
      "Processed 2384/192392\n",
      "Processed 2392/192392\n",
      "Processed 2400/192392\n",
      "Processed 2408/192392\n",
      "Processed 2416/192392\n",
      "Processed 2424/192392\n",
      "Processed 2432/192392\n",
      "Processed 2440/192392\n",
      "Processed 2448/192392\n",
      "Processed 2456/192392\n",
      "Processed 2464/192392\n",
      "Processed 2472/192392\n",
      "Processed 2480/192392\n",
      "Processed 2488/192392\n",
      "Processed 2496/192392\n",
      "Processed 2504/192392\n",
      "Processed 2512/192392\n",
      "Processed 2520/192392\n",
      "Processed 2528/192392\n",
      "Processed 2536/192392\n",
      "Processed 2544/192392\n",
      "Processed 2552/192392\n",
      "Processed 2560/192392\n",
      "Processed 2568/192392\n",
      "Processed 2576/192392\n",
      "Processed 2584/192392\n",
      "Processed 2592/192392\n",
      "Processed 2600/192392\n",
      "Processed 2608/192392\n",
      "Processed 2616/192392\n",
      "Processed 2624/192392\n",
      "Processed 2632/192392\n",
      "Processed 2640/192392\n",
      "Processed 2648/192392\n",
      "Processed 2656/192392\n",
      "Processed 2664/192392\n",
      "Processed 2672/192392\n",
      "Processed 2680/192392\n",
      "Processed 2688/192392\n",
      "Processed 2696/192392\n",
      "Processed 2704/192392\n",
      "Processed 2712/192392\n",
      "Processed 2720/192392\n",
      "Processed 2728/192392\n",
      "Processed 2736/192392\n",
      "Processed 2744/192392\n",
      "Processed 2752/192392\n",
      "Processed 2760/192392\n",
      "Processed 2768/192392\n",
      "Processed 2776/192392\n",
      "Processed 2784/192392\n",
      "Processed 2792/192392\n",
      "Processed 2800/192392\n",
      "Processed 2808/192392\n",
      "Processed 2816/192392\n",
      "Processed 2824/192392\n",
      "Processed 2832/192392\n",
      "Processed 2840/192392\n",
      "Processed 2848/192392\n",
      "Processed 2856/192392\n",
      "Processed 2864/192392\n",
      "Processed 2872/192392\n",
      "Processed 2880/192392\n",
      "Processed 2888/192392\n",
      "Processed 2896/192392\n",
      "Processed 2904/192392\n",
      "Processed 2912/192392\n",
      "Processed 2920/192392\n",
      "Processed 2928/192392\n",
      "Processed 2936/192392\n",
      "Processed 2944/192392\n",
      "Processed 2952/192392\n",
      "Processed 2960/192392\n",
      "Processed 2968/192392\n",
      "Processed 2976/192392\n",
      "Processed 2984/192392\n",
      "Processed 2992/192392\n",
      "Processed 3000/192392\n",
      "Processed 3008/192392\n",
      "Processed 3016/192392\n",
      "Processed 3024/192392\n",
      "Processed 3032/192392\n",
      "Processed 3040/192392\n",
      "Processed 3048/192392\n",
      "Processed 3056/192392\n",
      "Processed 3064/192392\n",
      "Processed 3072/192392\n",
      "Processed 3080/192392\n",
      "Processed 3088/192392\n",
      "Processed 3096/192392\n",
      "Processed 3104/192392\n",
      "Processed 3112/192392\n",
      "Processed 3120/192392\n",
      "Processed 3128/192392\n",
      "Processed 3136/192392\n",
      "Processed 3144/192392\n",
      "Processed 3152/192392\n",
      "Processed 3160/192392\n",
      "Processed 3168/192392\n",
      "Processed 3176/192392\n",
      "Processed 3184/192392\n",
      "Processed 3192/192392\n",
      "Processed 3200/192392\n",
      "Processed 3208/192392\n",
      "Processed 3216/192392\n",
      "Processed 3224/192392\n",
      "Processed 3232/192392\n",
      "Processed 3240/192392\n",
      "Processed 3248/192392\n",
      "Processed 3256/192392\n",
      "Processed 3264/192392\n",
      "Processed 3272/192392\n",
      "Processed 3280/192392\n",
      "Processed 3288/192392\n",
      "Processed 3296/192392\n",
      "Processed 3304/192392\n",
      "Processed 3312/192392\n",
      "Processed 3320/192392\n",
      "Processed 3328/192392\n",
      "Processed 3336/192392\n",
      "Processed 3344/192392\n",
      "Processed 3352/192392\n",
      "Processed 3360/192392\n",
      "Processed 3368/192392\n",
      "Processed 3376/192392\n",
      "Processed 3384/192392\n",
      "Processed 3392/192392\n",
      "Processed 3400/192392\n",
      "Processed 3408/192392\n",
      "Processed 3416/192392\n",
      "Processed 3424/192392\n",
      "Processed 3432/192392\n",
      "Processed 3440/192392\n",
      "Processed 3448/192392\n",
      "Processed 3456/192392\n",
      "Processed 3464/192392\n",
      "Processed 3472/192392\n",
      "Processed 3480/192392\n",
      "Processed 3488/192392\n",
      "Processed 3496/192392\n",
      "Processed 3504/192392\n",
      "Processed 3512/192392\n",
      "Processed 3520/192392\n",
      "Processed 3528/192392\n",
      "Processed 3536/192392\n",
      "Processed 3544/192392\n",
      "Processed 3552/192392\n",
      "Processed 3560/192392\n",
      "Processed 3568/192392\n",
      "Processed 3576/192392\n",
      "Processed 3584/192392\n",
      "Processed 3592/192392\n",
      "Processed 3600/192392\n",
      "Processed 3608/192392\n",
      "Processed 3616/192392\n",
      "Processed 3624/192392\n",
      "Processed 3632/192392\n",
      "Processed 3640/192392\n",
      "Processed 3648/192392\n",
      "Processed 3656/192392\n",
      "Processed 3664/192392\n",
      "Processed 3672/192392\n",
      "Processed 3680/192392\n",
      "Processed 3688/192392\n",
      "Processed 3696/192392\n",
      "Processed 3704/192392\n",
      "Processed 3712/192392\n",
      "Processed 3720/192392\n",
      "Processed 3728/192392\n",
      "Processed 3736/192392\n",
      "Processed 3744/192392\n",
      "Processed 3752/192392\n",
      "Processed 3760/192392\n",
      "Processed 3768/192392\n",
      "Processed 3776/192392\n",
      "Processed 3784/192392\n",
      "Processed 3792/192392\n",
      "Processed 3800/192392\n",
      "Processed 3808/192392\n",
      "Processed 3816/192392\n",
      "Processed 3824/192392\n",
      "Processed 3832/192392\n",
      "Processed 3840/192392\n",
      "Processed 3848/192392\n",
      "Processed 3856/192392\n",
      "Processed 3864/192392\n",
      "Processed 3872/192392\n",
      "Processed 3880/192392\n",
      "Processed 3888/192392\n",
      "Processed 3896/192392\n",
      "Processed 3904/192392\n",
      "Processed 3912/192392\n",
      "Processed 3920/192392\n",
      "Processed 3928/192392\n",
      "Processed 3936/192392\n",
      "Processed 3944/192392\n",
      "Processed 3952/192392\n",
      "Processed 3960/192392\n",
      "Processed 3968/192392\n",
      "Processed 3976/192392\n",
      "Processed 3984/192392\n",
      "Processed 3992/192392\n",
      "Processed 4000/192392\n",
      "Processed 4008/192392\n",
      "Processed 4016/192392\n",
      "Processed 4024/192392\n",
      "Processed 4032/192392\n",
      "Processed 4040/192392\n",
      "Processed 4048/192392\n",
      "Processed 4056/192392\n",
      "Processed 4064/192392\n",
      "Processed 4072/192392\n",
      "Processed 4080/192392\n",
      "Processed 4088/192392\n",
      "Processed 4096/192392\n",
      "Processed 4104/192392\n",
      "Processed 4112/192392\n",
      "Processed 4120/192392\n",
      "Processed 4128/192392\n",
      "Processed 4136/192392\n",
      "Processed 4144/192392\n",
      "Processed 4152/192392\n",
      "Processed 4160/192392\n",
      "Processed 4168/192392\n",
      "Processed 4176/192392\n",
      "Processed 4184/192392\n",
      "Processed 4192/192392\n",
      "Processed 4200/192392\n",
      "Processed 4208/192392\n",
      "Processed 4216/192392\n",
      "Processed 4224/192392\n",
      "Processed 4232/192392\n",
      "Processed 4240/192392\n",
      "Processed 4248/192392\n",
      "Processed 4256/192392\n",
      "Processed 4264/192392\n",
      "Processed 4272/192392\n",
      "Processed 4280/192392\n",
      "Processed 4288/192392\n",
      "Processed 4296/192392\n",
      "Processed 4304/192392\n",
      "Processed 4312/192392\n",
      "Processed 4320/192392\n",
      "Processed 4328/192392\n",
      "Processed 4336/192392\n",
      "Processed 4344/192392\n",
      "Processed 4352/192392\n",
      "Processed 4360/192392\n",
      "Processed 4368/192392\n",
      "Processed 4376/192392\n",
      "Processed 4384/192392\n",
      "Processed 4392/192392\n",
      "Processed 4400/192392\n",
      "Processed 4408/192392\n",
      "Processed 4416/192392\n",
      "Processed 4424/192392\n",
      "Processed 4432/192392\n",
      "Processed 4440/192392\n",
      "Processed 4448/192392\n",
      "Processed 4456/192392\n",
      "Processed 4464/192392\n",
      "Processed 4472/192392\n",
      "Processed 4480/192392\n",
      "Processed 4488/192392\n",
      "Processed 4496/192392\n",
      "Processed 4504/192392\n",
      "Processed 4512/192392\n",
      "Processed 4520/192392\n",
      "Processed 4528/192392\n",
      "Processed 4536/192392\n",
      "Processed 4544/192392\n",
      "Processed 4552/192392\n",
      "Processed 4560/192392\n",
      "Processed 4568/192392\n",
      "Processed 4576/192392\n",
      "Processed 4584/192392\n",
      "Processed 4592/192392\n",
      "Processed 4600/192392\n",
      "Processed 4608/192392\n",
      "Processed 4616/192392\n",
      "Processed 4624/192392\n",
      "Processed 4632/192392\n",
      "Processed 4640/192392\n",
      "Processed 4648/192392\n",
      "Processed 4656/192392\n",
      "Processed 4664/192392\n",
      "Processed 4672/192392\n",
      "Processed 4680/192392\n",
      "Processed 4688/192392\n",
      "Processed 4696/192392\n",
      "Processed 4704/192392\n",
      "Processed 4712/192392\n",
      "Processed 4720/192392\n",
      "Processed 4728/192392\n",
      "Processed 4736/192392\n",
      "Processed 4744/192392\n",
      "Processed 4752/192392\n",
      "Processed 4760/192392\n",
      "Processed 4768/192392\n",
      "Processed 4776/192392\n",
      "Processed 4784/192392\n",
      "Processed 4792/192392\n",
      "Processed 4800/192392\n",
      "Processed 4808/192392\n",
      "Processed 4816/192392\n",
      "Processed 4824/192392\n",
      "Processed 4832/192392\n",
      "Processed 4840/192392\n",
      "Processed 4848/192392\n",
      "Processed 4856/192392\n",
      "Processed 4864/192392\n",
      "Processed 4872/192392\n",
      "Processed 4880/192392\n",
      "Processed 4888/192392\n",
      "Processed 4896/192392\n",
      "Processed 4904/192392\n",
      "Processed 4912/192392\n",
      "Processed 4920/192392\n",
      "Processed 4928/192392\n",
      "Processed 4936/192392\n",
      "Processed 4944/192392\n",
      "Processed 4952/192392\n",
      "Processed 4960/192392\n",
      "Processed 4968/192392\n",
      "Processed 4976/192392\n",
      "Processed 4984/192392\n",
      "Processed 4992/192392\n",
      "Processed 5000/192392\n",
      "Processed 5008/192392\n",
      "Processed 5016/192392\n",
      "Processed 5024/192392\n",
      "Processed 5032/192392\n",
      "Processed 5040/192392\n",
      "Processed 5048/192392\n",
      "Processed 5056/192392\n",
      "Processed 5064/192392\n",
      "Processed 5072/192392\n",
      "Processed 5080/192392\n",
      "Processed 5088/192392\n",
      "Processed 5096/192392\n",
      "Processed 5104/192392\n",
      "Processed 5112/192392\n",
      "Processed 5120/192392\n",
      "Processed 5128/192392\n",
      "Processed 5136/192392\n",
      "Processed 5144/192392\n",
      "Processed 5152/192392\n",
      "Processed 5160/192392\n",
      "Processed 5168/192392\n",
      "Processed 5176/192392\n",
      "Processed 5184/192392\n",
      "Processed 5192/192392\n",
      "Processed 5200/192392\n",
      "Processed 5208/192392\n",
      "Processed 5216/192392\n",
      "Processed 5224/192392\n",
      "Processed 5232/192392\n",
      "Processed 5240/192392\n",
      "Processed 5248/192392\n",
      "Processed 5256/192392\n",
      "Processed 5264/192392\n",
      "Processed 5272/192392\n",
      "Processed 5280/192392\n",
      "Processed 5288/192392\n",
      "Processed 5296/192392\n",
      "Processed 5304/192392\n",
      "Processed 5312/192392\n",
      "Processed 5320/192392\n",
      "Processed 5328/192392\n",
      "Processed 5336/192392\n",
      "Processed 5344/192392\n",
      "Processed 5352/192392\n",
      "Processed 5360/192392\n",
      "Processed 5368/192392\n",
      "Processed 5376/192392\n",
      "Processed 5384/192392\n",
      "Processed 5392/192392\n",
      "Processed 5400/192392\n",
      "Processed 5408/192392\n",
      "Processed 5416/192392\n",
      "Processed 5424/192392\n",
      "Processed 5432/192392\n",
      "Processed 5440/192392\n",
      "Processed 5448/192392\n",
      "Processed 5456/192392\n",
      "Processed 5464/192392\n",
      "Processed 5472/192392\n",
      "Processed 5480/192392\n",
      "Processed 5488/192392\n",
      "Processed 5496/192392\n",
      "Processed 5504/192392\n",
      "Processed 5512/192392\n",
      "Processed 5520/192392\n",
      "Processed 5528/192392\n",
      "Processed 5536/192392\n",
      "Processed 5544/192392\n",
      "Processed 5552/192392\n",
      "Processed 5560/192392\n",
      "Processed 5568/192392\n",
      "Processed 5576/192392\n",
      "Processed 5584/192392\n",
      "Processed 5592/192392\n",
      "Processed 5600/192392\n",
      "Processed 5608/192392\n",
      "Processed 5616/192392\n",
      "Processed 5624/192392\n",
      "Processed 5632/192392\n",
      "Processed 5640/192392\n",
      "Processed 5648/192392\n",
      "Processed 5656/192392\n",
      "Processed 5664/192392\n",
      "Processed 5672/192392\n",
      "Processed 5680/192392\n",
      "Processed 5688/192392\n",
      "Processed 5696/192392\n",
      "Processed 5704/192392\n",
      "Processed 5712/192392\n",
      "Processed 5720/192392\n",
      "Processed 5728/192392\n",
      "Processed 5736/192392\n",
      "Processed 5744/192392\n",
      "Processed 5752/192392\n",
      "Processed 5760/192392\n",
      "Processed 5768/192392\n",
      "Processed 5776/192392\n",
      "Processed 5784/192392\n",
      "Processed 5792/192392\n",
      "Processed 5800/192392\n",
      "Processed 5808/192392\n",
      "Processed 5816/192392\n",
      "Processed 5824/192392\n",
      "Processed 5832/192392\n",
      "Processed 5840/192392\n",
      "Processed 5848/192392\n",
      "Processed 5856/192392\n",
      "Processed 5864/192392\n",
      "Processed 5872/192392\n",
      "Processed 5880/192392\n",
      "Processed 5888/192392\n",
      "Processed 5896/192392\n",
      "Processed 5904/192392\n",
      "Processed 5912/192392\n",
      "Processed 5920/192392\n",
      "Processed 5928/192392\n",
      "Processed 5936/192392\n",
      "Processed 5944/192392\n",
      "Processed 5952/192392\n",
      "Processed 5960/192392\n",
      "Processed 5968/192392\n",
      "Processed 5976/192392\n",
      "Processed 5984/192392\n",
      "Processed 5992/192392\n",
      "Processed 6000/192392\n",
      "Processed 6008/192392\n",
      "Processed 6016/192392\n",
      "Processed 6024/192392\n",
      "Processed 6032/192392\n",
      "Processed 6040/192392\n",
      "Processed 6048/192392\n",
      "Processed 6056/192392\n",
      "Processed 6064/192392\n",
      "Processed 6072/192392\n",
      "Processed 6080/192392\n",
      "Processed 6088/192392\n",
      "Processed 6096/192392\n",
      "Processed 6104/192392\n",
      "Processed 6112/192392\n",
      "Processed 6120/192392\n",
      "Processed 6128/192392\n",
      "Processed 6136/192392\n",
      "Processed 6144/192392\n",
      "Processed 6152/192392\n",
      "Processed 6160/192392\n",
      "Processed 6168/192392\n",
      "Processed 6176/192392\n",
      "Processed 6184/192392\n",
      "Processed 6192/192392\n",
      "Processed 6200/192392\n",
      "Processed 6208/192392\n",
      "Processed 6216/192392\n",
      "Processed 6224/192392\n",
      "Processed 6232/192392\n",
      "Processed 6240/192392\n",
      "Processed 6248/192392\n",
      "Processed 6256/192392\n",
      "Processed 6264/192392\n",
      "Processed 6272/192392\n",
      "Processed 6280/192392\n",
      "Processed 6288/192392\n",
      "Processed 6296/192392\n",
      "Processed 6304/192392\n",
      "Processed 6312/192392\n",
      "Processed 6320/192392\n",
      "Processed 6328/192392\n",
      "Processed 6336/192392\n",
      "Processed 6344/192392\n",
      "Processed 6352/192392\n",
      "Processed 6360/192392\n",
      "Processed 6368/192392\n",
      "Processed 6376/192392\n",
      "Processed 6384/192392\n",
      "Processed 6392/192392\n",
      "Processed 6400/192392\n",
      "Processed 6408/192392\n",
      "Processed 6416/192392\n",
      "Processed 6424/192392\n",
      "Processed 6432/192392\n",
      "Processed 6440/192392\n",
      "Processed 6448/192392\n",
      "Processed 6456/192392\n",
      "Processed 6464/192392\n",
      "Processed 6472/192392\n",
      "Processed 6480/192392\n",
      "Processed 6488/192392\n",
      "Processed 6496/192392\n",
      "Processed 6504/192392\n",
      "Processed 6512/192392\n",
      "Processed 6520/192392\n",
      "Processed 6528/192392\n",
      "Processed 6536/192392\n",
      "Processed 6544/192392\n",
      "Processed 6552/192392\n",
      "Processed 6560/192392\n",
      "Processed 6568/192392\n",
      "Processed 6576/192392\n",
      "Processed 6584/192392\n",
      "Processed 6592/192392\n",
      "Processed 6600/192392\n",
      "Processed 6608/192392\n",
      "Processed 6616/192392\n",
      "Processed 6624/192392\n",
      "Processed 6632/192392\n",
      "Processed 6640/192392\n",
      "Processed 6648/192392\n",
      "Processed 6656/192392\n",
      "Processed 6664/192392\n",
      "Processed 6672/192392\n",
      "Processed 6680/192392\n",
      "Processed 6688/192392\n",
      "Processed 6696/192392\n",
      "Processed 6704/192392\n",
      "Processed 6712/192392\n",
      "Processed 6720/192392\n",
      "Processed 6728/192392\n",
      "Processed 6736/192392\n",
      "Processed 6744/192392\n",
      "Processed 6752/192392\n",
      "Processed 6760/192392\n",
      "Processed 6768/192392\n",
      "Processed 6776/192392\n",
      "Processed 6784/192392\n",
      "Processed 6792/192392\n",
      "Processed 6800/192392\n",
      "Processed 6808/192392\n",
      "Processed 6816/192392\n",
      "Processed 6824/192392\n",
      "Processed 6832/192392\n",
      "Processed 6840/192392\n",
      "Processed 6848/192392\n",
      "Processed 6856/192392\n",
      "Processed 6864/192392\n",
      "Processed 6872/192392\n",
      "Processed 6880/192392\n",
      "Processed 6888/192392\n",
      "Processed 6896/192392\n",
      "Processed 6904/192392\n",
      "Processed 6912/192392\n",
      "Processed 6920/192392\n",
      "Processed 6928/192392\n",
      "Processed 6936/192392\n",
      "Processed 6944/192392\n",
      "Processed 6952/192392\n",
      "Processed 6960/192392\n",
      "Processed 6968/192392\n",
      "Processed 6976/192392\n",
      "Processed 6984/192392\n",
      "Processed 6992/192392\n",
      "Processed 7000/192392\n",
      "Processed 7008/192392\n",
      "Processed 7016/192392\n",
      "Processed 7024/192392\n",
      "Processed 7032/192392\n",
      "Processed 7040/192392\n",
      "Processed 7048/192392\n",
      "Processed 7056/192392\n",
      "Processed 7064/192392\n",
      "Processed 7072/192392\n",
      "Processed 7080/192392\n",
      "Processed 7088/192392\n",
      "Processed 7096/192392\n",
      "Processed 7104/192392\n",
      "Processed 7112/192392\n",
      "Processed 7120/192392\n",
      "Processed 7128/192392\n",
      "Processed 7136/192392\n",
      "Processed 7144/192392\n",
      "Processed 7152/192392\n",
      "Processed 7160/192392\n",
      "Processed 7168/192392\n",
      "Processed 7176/192392\n",
      "Processed 7184/192392\n",
      "Processed 7192/192392\n",
      "Processed 7200/192392\n",
      "Processed 7208/192392\n",
      "Processed 7216/192392\n",
      "Processed 7224/192392\n",
      "Processed 7232/192392\n",
      "Processed 7240/192392\n",
      "Processed 7248/192392\n",
      "Processed 7256/192392\n",
      "Processed 7264/192392\n",
      "Processed 7272/192392\n",
      "Processed 7280/192392\n",
      "Processed 7288/192392\n",
      "Processed 7296/192392\n",
      "Processed 7304/192392\n",
      "Processed 7312/192392\n",
      "Processed 7320/192392\n",
      "Processed 7328/192392\n",
      "Processed 7336/192392\n",
      "Processed 7344/192392\n",
      "Processed 7352/192392\n",
      "Processed 7360/192392\n",
      "Processed 7368/192392\n",
      "Processed 7376/192392\n",
      "Processed 7384/192392\n",
      "Processed 7392/192392\n",
      "Processed 7400/192392\n",
      "Processed 7408/192392\n",
      "Processed 7416/192392\n",
      "Processed 7424/192392\n",
      "Processed 7432/192392\n",
      "Processed 7440/192392\n",
      "Processed 7448/192392\n",
      "Processed 7456/192392\n",
      "Processed 7464/192392\n",
      "Processed 7472/192392\n",
      "Processed 7480/192392\n",
      "Processed 7488/192392\n",
      "Processed 7496/192392\n",
      "Processed 7504/192392\n",
      "Processed 7512/192392\n",
      "Processed 7520/192392\n",
      "Processed 7528/192392\n",
      "Processed 7536/192392\n",
      "Processed 7544/192392\n",
      "Processed 7552/192392\n",
      "Processed 7560/192392\n",
      "Processed 7568/192392\n",
      "Processed 7576/192392\n",
      "Processed 7584/192392\n",
      "Processed 7592/192392\n",
      "Processed 7600/192392\n",
      "Processed 7608/192392\n",
      "Processed 7616/192392\n",
      "Processed 7624/192392\n",
      "Processed 7632/192392\n",
      "Processed 7640/192392\n",
      "Processed 7648/192392\n",
      "Processed 7656/192392\n",
      "Processed 7664/192392\n",
      "Processed 7672/192392\n",
      "Processed 7680/192392\n",
      "Processed 7688/192392\n",
      "Processed 7696/192392\n",
      "Processed 7704/192392\n",
      "Processed 7712/192392\n",
      "Processed 7720/192392\n",
      "Processed 7728/192392\n",
      "Processed 7736/192392\n",
      "Processed 7744/192392\n",
      "Processed 7752/192392\n",
      "Processed 7760/192392\n",
      "Processed 7768/192392\n",
      "Processed 7776/192392\n",
      "Processed 7784/192392\n",
      "Processed 7792/192392\n",
      "Processed 7800/192392\n",
      "Processed 7808/192392\n",
      "Processed 7816/192392\n",
      "Processed 7824/192392\n",
      "Processed 7832/192392\n",
      "Processed 7840/192392\n",
      "Processed 7848/192392\n",
      "Processed 7856/192392\n",
      "Processed 7864/192392\n",
      "Processed 7872/192392\n",
      "Processed 7880/192392\n",
      "Processed 7888/192392\n",
      "Processed 7896/192392\n",
      "Processed 7904/192392\n",
      "Processed 7912/192392\n",
      "Processed 7920/192392\n",
      "Processed 7928/192392\n",
      "Processed 7936/192392\n",
      "Processed 7944/192392\n",
      "Processed 7952/192392\n",
      "Processed 7960/192392\n",
      "Processed 7968/192392\n",
      "Processed 7976/192392\n",
      "Processed 7984/192392\n",
      "Processed 7992/192392\n",
      "Processed 8000/192392\n",
      "Processed 8008/192392\n",
      "Processed 8016/192392\n",
      "Processed 8024/192392\n",
      "Processed 8032/192392\n",
      "Processed 8040/192392\n",
      "Processed 8048/192392\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df), batch_size):\n\u001b[1;32m     44\u001b[0m     batch \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 45\u001b[0m     batch_summaries \u001b[38;5;241m=\u001b[39m [t5_summarize(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m batch]\n\u001b[1;32m     46\u001b[0m     summaries\u001b[38;5;241m.\u001b[39mextend(batch_summaries)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(i\u001b[38;5;241m+\u001b[39mbatch_size,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(df))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m, in \u001b[0;36mt5_summarize\u001b[0;34m(text, max_length)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Tokenize and generate\u001b[39;00m\n\u001b[1;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m     20\u001b[0m     input_text,\n\u001b[1;32m     21\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     23\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     27\u001b[0m     inputs,\n\u001b[1;32m     28\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[1;32m     29\u001b[0m     min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[1;32m     30\u001b[0m     length_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.0\u001b[39m,\n\u001b[1;32m     31\u001b[0m     num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     32\u001b[0m     early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:2484\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2477\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2478\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2479\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2480\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2481\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2482\u001b[0m     )\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;66;03m# 12. run beam sample\u001b[39;00m\n\u001b[0;32m-> 2484\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_beam_search(\n\u001b[1;32m   2485\u001b[0m         input_ids,\n\u001b[1;32m   2486\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2487\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2488\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2489\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2490\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2491\u001b[0m     )\n\u001b[1;32m   2493\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   2494\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2495\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2496\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2497\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2503\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2504\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/generation/utils.py:3904\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3901\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_attentions} \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3902\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[0;32m-> 3904\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   3906\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3907\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3908\u001b[0m     model_outputs,\n\u001b[1;32m   3909\u001b[0m     model_kwargs,\n\u001b[1;32m   3910\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3911\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1905\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1902\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1904\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1905\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\n\u001b[1;32m   1906\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39mdecoder_input_ids,\n\u001b[1;32m   1907\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mdecoder_attention_mask,\n\u001b[1;32m   1908\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39mdecoder_inputs_embeds,\n\u001b[1;32m   1909\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1910\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m   1911\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1912\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mdecoder_head_mask,\n\u001b[1;32m   1913\u001b[0m     cross_attn_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_head_mask,\n\u001b[1;32m   1914\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1915\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1916\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1917\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1918\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1919\u001b[0m )\n\u001b[1;32m   1921\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:1131\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1115\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1116\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         cache_position,\n\u001b[1;32m   1129\u001b[0m     )\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m   1132\u001b[0m         hidden_states,\n\u001b[1;32m   1133\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1134\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m   1135\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1136\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1137\u001b[0m         encoder_decoder_position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m   1138\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m   1139\u001b[0m         cross_attn_layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m   1140\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1141\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1142\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1143\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1144\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1145\u001b[0m     )\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:706\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    704\u001b[0m do_cross_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[0;32m--> 706\u001b[0m     cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer[\u001b[38;5;241m1\u001b[39m](\n\u001b[1;32m    707\u001b[0m         hidden_states,\n\u001b[1;32m    708\u001b[0m         key_value_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    709\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m    710\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mencoder_decoder_position_bias,\n\u001b[1;32m    711\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mcross_attn_layer_head_mask,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    713\u001b[0m         query_length\u001b[38;5;241m=\u001b[39mcache_position[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    715\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    716\u001b[0m     )\n\u001b[1;32m    717\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:636\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    634\u001b[0m ):\n\u001b[1;32m    635\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 636\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[1;32m    637\u001b[0m         normed_hidden_states,\n\u001b[1;32m    638\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    639\u001b[0m         key_value_states\u001b[38;5;241m=\u001b[39mkey_value_states,\n\u001b[1;32m    640\u001b[0m         position_bias\u001b[38;5;241m=\u001b[39mposition_bias,\n\u001b[1;32m    641\u001b[0m         layer_head_mask\u001b[38;5;241m=\u001b[39mlayer_head_mask,\n\u001b[1;32m    642\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[1;32m    643\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    644\u001b[0m         query_length\u001b[38;5;241m=\u001b[39mquery_length,\n\u001b[1;32m    645\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    646\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[1;32m    648\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    649\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py:560\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    559\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(scores\u001b[38;5;241m.\u001b[39mfloat(), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(scores)\n\u001b[0;32m--> 560\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28minput\u001b[39m, p, training)\n\u001b[1;32m   1426\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize the pretrained model\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set device (use CPU if GPU not available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def t5_summarize(text, max_length=150):\n",
    "    # Prepare input text with task prefix\n",
    "    input_text = f\"summarize: {text}\"\n",
    "    \n",
    "    # Tokenize and generate\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=max_length,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('prepared_data.csv')\n",
    "\n",
    "# Apply summarization in batches to avoid memory issues\n",
    "batch_size = 8  # Adjust based on your RAM\n",
    "summaries = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch = df['content'].iloc[i:i+batch_size].astype(str).tolist()\n",
    "    batch_summaries = [t5_summarize(text) for text in batch]\n",
    "    summaries.extend(batch_summaries)\n",
    "    print(f\"Processed {min(i+batch_size, len(df))}/{len(df)}\")\n",
    "\n",
    "df['t5_summary'] = summaries\n",
    "df.to_csv('t5_summaries_no_finetuning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb49f3a-f785-4767-b8fa-0da66a41ce85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
