title,content
Repetition in Songs: A Python Tutorial,"Repetition in Songs: A Python Tutorial

Credit: Unsplash

Everyone has heard a song or knows what a song sounds like. I can carelessly say everyone can define a song …in their own words. Just for the benefit of the doubt, a song (according to Wikipedia) is a single work of music that is typically intended to be sung by the human voice with distinct and fixed pitches and patterns using sound and silence and a variety of forms that often include the repetition of sections.

In his journal article called “The complexity of Songs”, computer scientist Donald Knuth capitalized on the tendency of popular songs to devolve from long and content-rich ballads to highly repetitive texts. As some may waste no time agreeing with his notion, it does raise some questions like: Does repetitiveness really help songs become a hit? Is music really becoming more repetitive over time?

In an attempt to teach some basic python code in the form of a case study, I am going to test this hypothesis (Are popular songs really repetitive?)with one of my favorite songs. One way to test this hypothesis is to figure out the unique words and calculate the fraction of those words to the total number of words in a song.

In this tutorial, we’ll cover:

Variables and data types

Lists and Dictionaries

Basic Arithmetic operations

Built-in Functions and Loops

Prerequisite Knowledge

To get the most out of this tutorial, you can follow along by running the codes yourself.

The music we will be using is entitled ‘Perfect’ by Ed Sheeran. You can copy the lyrics here. However, the lyrics I am using in this analysis was cleaned out to get a conclusive result. For example, I changed words like “we’ll” to “we will” etc. You can get my version of the lyrics here The editor used was Jupiter NoteBook. Here is a quick tutorial on how to install and use it.

For the purpose of this case study, we will streamline our hypothesis by asking two major questions:

How many unique words were used compared to the whole lyrics of our case study song — Perfect by Ed Sheeran?

What are the most repetitive words used and how many times were they used throughout the song?

Let's get started analyzing already

The Basic

1. A String is a list of characters. A character is anything you can type on the keyboard in one keystroke, like a letter, a number, or a backslash. However, Python recognizes strings as anything that is delimited by quotation marks either a double quote (“ “) or a single quote (‘ ‘) at the beginning and end of a character or text. For example: ‘Hello world’

For this case study, a string is our lyrics as seen below

2. Variables are typically descriptive names, words or symbols used to assign or store values. In other words, they are storage placeholders for any datatype. It is quite handy in order to refer to a value at any time. A variable is always assigned with an equal sign, followed by the value of the

variable. (A way to view your code output is to use a print function. As you may already know with Jupyter notebook, an output can be viewed without a print function)

To store the lyrics, we will assign it a variable named perfect_lyrics .

3. Lists can be created simply by putting different comma-separated values between square brackets. It can have any number of items and they may be of different types (integer, float, string etc.). It can even have another list as an item. For example:

list1 = [1,'mouse', 3.5, [2,'is',5.0]]

#3.5 is a float

Now that we have gotten a sense of what a list looks like. Let go back to our data.

Since one of our aims is to figure out the number of unique words used, it means we will need to do a bit of counting i.e to count each word. In order to achieve these, we will not only have to put our string into a list but will have to separate each word using a .split() method. Therefore our dataset will look like this

Input

Output"
Deep Dive into SF Crime,"Deep Dive into SF Crime

San Francisco is famous for many things: its vibrant tech environment, the iconic Golden Gate, charming cable cars and (arguably) the world’s best restaurants. It is also the heart of LGBT and hipster culture which makes it an extremely attractive tourist and migration destination. However, with thriving tourism, rising wealth inequality, and thousands of homeless people there is no scarcity of crime in the city. In this post, I invite you to dive into the San Francisco Crime data to get some insights into the SF crime environment and engineer features for your own crime classification model.

Exploratory Analysis

You can download San Francisco Crime classification data from Kaggle. The dataset contains nearly 800,000 observations of crime reports from all the city’s neighborhoods in 2003–2015. It includes the following variables:

San Francisco Crime Rates Dataset Variable Description

Let’s first explore our target variable and find out which 10 crime types are the most common in San Francisco. We will sort the categories by the number of incidences and then use the horizontal bar chart to present our findings:

# Get 10 most common crimes

most_common_cat = train['Category'].value_counts()[0:9].sort_values()

most_common_cat.values





categs = most_common_cat.index

y_pos = np.arange(len(categs))

counts = most_common_cat.values



plt.barh(y_pos, counts, align='center', alpha=0.5)

plt.barh(y_pos, counts, align='center', alpha=0.5)

plt.yticks(y_pos, categs)

plt.xlabel('Number of Incidences')

plt.show()

Most Common Crime Types

It’s quite comforting to know that violent crimes are not at the top of the crime incidence list. However, property crimes seem to be quite common. Let’s now explore which districts have the highest number of registered crimes.

To do so, we will use Folium which is an easy to use tool that creates interactive maps. To run the chunk of code below, you’d need to install Folium by running pip install folium in your terminal or directly inside the notebook by adding “!” in front of the command. Then you’d need to download the JSON file as specified in this cell:

by_zone = train.apply(pd.Series.value_counts).reset_index()



# Load SF data

!wget --quiet https://cocl.us/sanfran_geojson -O sf_neighborhoods.json

sf_zones = r'sf_neighborhoods.json'



SF_COORDINATES = (37.76, -122.45)





# Create an empty map zoomed in on San Francisco

sf_crime_map = folium.Map(location=SF_COORDINATES, zoom_start=12)





sf_crime_map.choropleth(

geo_data=sf_zones,

data=by_zone,

columns=['index', 'PdDistrict'],

key_on='feature.properties.DISTRICT',

fill_color='YlOrRd',

fill_opacity=0.7,

line_opacity=0.2,

legend_name='San Fransisco Crime by Neighborhood'

)



sf_crime_map

The output of this cell is an interactive map with the number of crime incidences sorted by police department districts:

San Francisco Crime Rate by Neighborhood

Feel free to experiment with this plot by plotting individual crime categories or checking the change in the distribution over time.

Next, let’s look at the distribution of crimes over weekdays. I first built a crosstable to get the crime counts per weekdays. After that, I normalized the counts and visualized them using a heatmap from the seaborn library:

# Extract the most common crimes from the data

most_commons = train[train['Category'].apply(lambda x: x in categs)]



# Build a cross table to get the number of each crime type per day of week

cat_per_week_common = pd.crosstab(most_commons['Category'], most_commons['DayOfWeek'])

# Calculate percentages of crimes

cat_per_week_common = cat_per_week_common.div(cat_per_week_common.sum(axis=1), axis=0)

# Rearrange columns

cat_per_week_common = cat_per_week_common[['Monday',

'Tuesday', 'Wednesday',

'Thursday', 'Friday',

'Saturday','Sunday']]

# Transform into a heat map

fig = plt.figure(figsize=(10,10))

ax = sns.heatmap(cat_per_week_common,

cmap=""BuPu"", linewidths=.5)

plt.xticks(fontsize=12,rotation=45,ha='right')

plt.yticks(fontsize=12)

plt.xlabel("""")

plt.ylabel("""")

Most Common Crimes per Day of Week

As the graph above illustrates, most crime categories such as assaults and acts of vandalism happen during the weekend when people go out. Some other crime types are more often registered during weekdays which might be associated with police working hours.

Finally, let’s see the success rate of the SF police department. Specifically, how many violent crimes are getting resolved. Let’s first subset violent crimes from the list of categories. I picked the ones that I consider interesting, but feel free to explore other categories as well! After that, we can create the arrest variable which will divide all the possible resolutions into two classes: prosecuted or not. I’m making an assumption that ‘NOT PROSECUTED’ and ‘NONE’ are the only two resolutions that correspond to the negative class. We will calculate the proportion of solved cases and plot them using a horizontal plot. This time, we will plot proportions, not absolute counts:

# Pick crime types of interest

violent = train[train.Category.isin(['ASSAULT', 'BURGLARY',

'KIDNAPPING', 'ROBBERY',

'SEX OFFENSES FORCIBLE'])].copy()

# Create Arrest variable

violent['Arrest'] = np.where(violent['Resolution'].isin(['NONE', 'NOT PROSECUTED']), 0,1)



# Calculate counts

arrest_counts = violent['Category'][violent.Arrest==1].value_counts()[0:9]

total_counts = violent['Category'].value_counts()[0:9]

arrest_counts = arrest_counts/(total_counts).sort_index()

total_counts = total_counts/(total_counts).sort_index()



# Plot values

total_counts.plot.barh(color='crimson', label= 'Unsolved')

arrest_counts.plot.barh(color='mediumseagreen', label='Solved')

plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

plt.xlabel('Proportion')

plt.show()

Solved and unsolved violent crimes in SF

We can see that the police success rate is below 50% for all the violent crimes we picked. In the case of burglary, it is just 17%.

Feature Engineering

Our dataset has many observations but only a limited number of features. In this section, we will create two sets of features: temporal and spatial. Temporal features can be extracted from the Dates variable. Apart from the obvious features such as month, day, and hour, I extracted business hours, weekends, and national holidays. To get access to all the US holidays, simply import USFederalHolidayCalendar from pandas.tseries.holiday. Here’s the function that will help you to extract all the features:

def time_engineer(data):

'''

Extract temporal features from dates.

'''

# Turn strings into timestamp objects

data.Dates = pd.to_datetime(data.Dates)



# Extract years, months, times of the day, and weeks of year

data['Year'] = data['Dates'].dt.year

data['Month'] = data['Dates'].dt.month

data['Day'] = data['Dates'].dt.day

data['Hour'] = data['Dates'].dt.hour

data['WeekOfYear'] = data['Dates'].dt.weekofyear



# Add a dummy for public holidays

cal = calendar()

holidays = cal.holidays(start=data['Dates'].min(), end=data['Dates'].max())

data['Holiday'] = data['Dates'].dt.date.astype('datetime64').isin(holidays).astype('bool')



# Add times of a day

data['Night'] = np.where((data['Hour']< 6), 1, 0)

data['Morning'] = np.where((data['Hour']>=6) & (data['Hour']<12), 1, 0)

data['Afternoon'] = np.where((data['Hour']>= 12) & (data['Hour']<18), 1, 0)

data['Evening'] = np.where((data['Hour']>= 18) & (data['Hour']<24), 1, 0)

data['BusinessHour'] = np.where((data['Hour']>= 8) & (data['Hour']<18), 1, 0)



# Add seasons

data['Spring'] = np.where((data['Month']>=3) & (data['Month']<6), 1, 0)

data['Summer'] = np.where((data['Month']>=6) & (data['Month']<9), 1, 0)

data['Autumn'] = np.where((data['Month']>=9) & (data['Month']<12), 1, 0)

data['Winter'] = np.where((data['Month']<=2) | (data['Month']==12), 1, 0)



# Encode weekdays

data_dummies = pd.get_dummies(data['DayOfWeek'])

data = pd.concat([data, data_dummies], axis=1)



# Create a dummy for weekends

data['Weekend'] = np.where((data['DayOfWeek']=='Saturday') & (data['DayOfWeek']=='Sunday'), 1, 0)



# Encode districts

data_dummies = pd.get_dummies(data['PdDistrict'])

data = pd.concat([data, data_dummies], axis=1)

data = data.drop(columns=['PdDistrict'])

# Drop categorical variables and variables that are not in test set

# School valiables contain too many NaNs

data.drop(columns=(['Address', 'Dates', 'Descript', 'DayOfWeek',

'Resolution', 'Enrolled In Public School',

'Enrolled In Private School', 'Not Enrolled In School']))

return data

Word Cloud of Temporal Features

Extracting spatial features is a bit more tricky since they are based on the uszipcode database that can be loaded as a package. In the notebook, you will find all the functions that are necessary to clean up the zipcode data and extract relevant demographics features. Be aware that the feature engineering process takes a lot of time (a couple of hours), mostly because there is a need to impute zip codes for every latitude and longitude.

Your final dataset should include 89 variables that contain information about the temporal and spatial aspects of crime incidences. Now, feel free to play around with this data and train your own model that will score at the top of the Kaggle leaderboard! To get you started, here’s a simple function that trains a model, makes predictions for the test set and calculates the logloss as specified in the challenge:

# Try out different models

models = [LogisticRegression, RandomForestClassifier, KNeighborsClassifier] def run_model(model, X_train, y_train, X_test, y_test):

model = model()

model.fit(X_train, y_train)

y_preds = model.predict(X_test)

return (log_loss(y_test, y_preds))



results = [run_model(model) for model in models]

Conclusion

In this post, we looked into the San Francisco Crime Classification dataset and learned how to produce data visualizations to explore different aspects of the data. We also used geographical location and dates to engineer spatial and temporal features. In a different post (hopefully) we will explore model and feature selection techniques, hyperparameter tuning, and some popular dimensionality reduction methods! Stay tuned :)

Full notebook: https://github.com/ritakurban/Practical-Data-Science/blob/master/SF_crime.ipynb"
"The path to being the best data analyst: Help, Build, then Do.","The core competency of a data analyst is “Speed to Insight”.

A data team often consists of many people, with many skills, using potentially overlapping techniques. This focus on speed distinguishes this role from data scientists or statisticians.

Today I’m focused on answering questions about the business or about how users behave. I’ll refer to these types of questions as mostly in the realm of data analysts, though some organizations call these folks data scientists, too.

A good data analyst should be able to interface directly with folks in the business unit that they’re working with. They need to have a solid understanding of business fundamentals in addition to data chops. A junior analyst may rely on business people asking smart questions, and answering the questions that they’re asked, quickly. While this is clearly helpful, it’s not the highest-leverage opportunity for an analyst.

The best analysts don’t only answer the questions that they’re asked. Actually doing analysis is often the easy part. It’s other skills that separate an average analyst from the best.

Help teammates rephrase their question.

Ask the “next 3 questions” that they should know

When you’re asked a particular question, it can be tempting to think “sure, I can answer that”. While that might be the first step, it’s important to get at the root reason for the question. If someone asks for the signup conversion rate across a section of your website, it’s the analyst’s job to dig in.

Why are you wondering about signup conversion? Would we rather measure conversion to active users? Are you interested in a particular segment? Does the signup rate vary across paid, direct, organic, and social traffic?

It’s unusual that a PM wants a metric for the sake of a metric. They’re really trying to learn something about the nature of your product or your audience. It’s your job to know enough about your data sources and about the business itself to answer the next three questions that they didn’t ask. Short circuiting the back and forth will help your team move…"
Download Course Materials with A Simple Python Crawler,"Recently I am taking the famous CS 61A from UC Berkeley. I want to download the slides and print them all. But there are 36 slides totally. As a lazy programmer, I won’t waste my precious time to click the downloading button 36 times. So I write a simple script to automatically download all the slides.

In this post, I will give a simple introduction about what you need to know to build a simple downloader.

Find the pattern of the download button

First, open the download page. The download button is shown as “8pp” (see below figure), which means one page contains 8 slides.

Ok, next we open the DevTools in Chrome. You can find the DevTools in other browsers too. Clicking the arrow icon in the upper left corner in the DevTools, then move your cursor to the “8pp” button, which will highlight the HTML element in the DevTools.

We can get the below HTML lines. The download links are “assets/slides/01-Functions_8pp.pdf” and “assets/slides/02-Names_8pp.pdf”. These links are in the href attribute in the a tag.

<li><a href=""assets/slides/01-Functions_full.pdf"" class=""label label-outline"">full</a></li>

<li><a href=""assets/slides/01-Functions_1pp.pdf"" class=""label label-outline"">1pp</a></li>

<li><a href=""assets/slides/01-Functions_8pp.pdf"" class=""label label-outline"">8pp</a></li>

<li><a href=""assets/slides/01.py"" class=""label label-outline"">01.py</a></li>

...

<li><a href=""assets/slides/02-Names_full.pdf"" class=""label label-outline"">full</a></li>

<li><a href=""assets/slides/02-Names_1pp.pdf"" class=""label label-outline"">1pp</a></li>

<li><a href=""assets/slides/02-Names_8pp.pdf"" class=""label label-outline"">8pp</a></li>

<li><a href=""assets/slides/02.py"" class=""label label-outline"">02.py</a></li>"
Face Detection in 2 Minutes using OpenCV & Python,"Face Detection in 2 Minutes using OpenCV & Python

In this quick post I wanted to share a very popular and easy way of detecting faces using Haar cascades in OpenCV and Python. Adarsh Menon · Follow Published in Towards Data Science · 2 min read · Apr 22, 2019 -- 5 Listen Share

The video version for those who prefer that !

First of all make sure you have OpenCV installed. You can install it using pip:

pip install opencv-python

Face detection using Haar cascades is a machine learning based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, smiles, etc.. Today we will be using the face classifier. You can experiment with other classifiers as well.

You need to download the trained classifier XML file (haarcascade_frontalface_default.xml), which is available in OpenCv’s GitHub repository. Save it to your working location.

To detect faces in images:

A few things to note:

The detection works only on grayscale images. So it is important to convert the color image to grayscale. (line 8)

detectMultiScale function (line 10) is used to detect the faces. It takes 3 arguments — the input image, scaleFactor and minNeighbours. scaleFactor specifies how much the image size is reduced with each scale. minNeighbours specifies how many neighbors each candidate rectangle should have to retain it. You can read about it in detail here. You may have to tweak these values to get the best results.

function (line 10) is used to detect the faces. It takes 3 arguments — the input image, scaleFactor and minNeighbours. scaleFactor specifies how much the image size is reduced with each scale. minNeighbours specifies how many neighbors each candidate rectangle should have to retain it. You can read about it in detail here. You may have to tweak these values to get the best results. faces contains a list of coordinates for the rectangular regions where faces were found. We use these coordinates to draw the rectangles in our image.

Results:

Similarly, we can detect faces in videos. As you know videos are basically made up of frames, which are still images. So we perform the face detection for each frame in a video. Here is the code:

The only difference here is that we use an infinite loop to loop through each frame in the video. We use cap.read() to read each frame. The first value returned is a flag that indicates if the frame was read correctly or not. We don’t need it. The second value returned is the still frame on which we will be performing the detection.

Find the code here: https://github.com/adarsh1021/facedetection"
What if Your Colleague is a Robot,"Throughout history, we have seen how organisations across entire industries have embraced robotic technology, and how today, it is almost impossible for some of these organisations to operate without it. Every day, we are witnesses of how technology is integrating into nearly every aspect of our life at work, and in many cases, we depend on it to perform most of our daily tasks.

Over many decades, organisations have been using robots to automate daily processes. In fact, every day we are surrounded by them, whether at home, in the street or at the office — from ATMs and vending machines to more sophisticated surgical robots and self-driving cars. These robots come in different forms and shapes, and perhaps they don’t look like humans, but the enterprise loves them, and they surely are a key driver for massive productivity.

Now, imagine a regular day — wake up in the morning, go to your workplace, grab a coffee, and start a conversation with your colleague… Everything seems normal, you engage in the conversation, exchange thoughts and new ideas, then finish and move on to your desk to start working. An hour later, your manager asks you to analyse and compare five long reports. You don’t have time for that, it would take too many hours of work and tons of energy; plus there are other things that you have to do. So, you ask your colleague for help and amazingly get the results back within an hour.

This sounds very unrealistic, right? No normal colleague would do this work for you or even finish it so fast. But what if I tell you that your colleague is a ROBOT? A robot that looks and acts the same as any other human being. It can talk like you, think like you, and almost reason like you do. But it is still a robot. How would you feel?

SOURCE: © ADOBE STOCK

Artificial Intelligence, Machine Learning & Robotics:

Many robots are not artificially intelligent, they are programmed to perform repetitive tasks or movements to manipulate objects in the real world. However, some experts suggest that what constitutes a robot is its ability to think and make decisions, but doesn’t it imply the use of AI?

On the other hand, Artificial Intelligence (AI) aims to replicate human intelligence behaviour by addressing skills like problem-solving, learning, perception, and reasoning. It can involve some level of Machine learning (ML), which is based on the idea of granting machines access to data that will allow them to learn by themselves.

AI together with ML and Robotics intend to create a man-made machine/robot with human intellectual capacities that can be able to formulate original ideas by itself. We have not achieved this yet, but we have made a lot of progress.

You have probably already heard about Sophia, the social robot that looks like a human. If you know what I’m talking about, think about a more sophisticated version of Sophia working and collaborating with humans.

Other big developments include the so-called “Robotic Process Automation” (RPA). These are software robots that help businesses and employees do simple jobs by replicating human interaction. It is only a software and not a physical AI Robot, but this is definitely a significant breakthrough.

The real challenge is to make AI understand how natural intelligence works because we know how our brain functions, and we can teach AI how to think and learn, but we still don’t know exactly how all these connections can lead AI to the use of abstract reasoning or “common sense”.

SOURCE: © ADOBE STOCK

My Colleague is a Robot:

Going back to where we started — robots designed to share a workspace with humans by carrying out physical and intellectual tasks, building up ideas with humans, learning from our environment, questioning decisions, and finding solutions together, will reduce many risks and exponentially increase productivity.

Robots can already do many things much better than humans, but it still takes humans to interpret their work and apply the results in strategic and creative ways. For this reason, we need to make sure that robots are seen as complementary rather than competitive, and assign them the work that no one wants to do, the one that is intense and repetitive; leaving the part of the work that involves judgement and expertise, to humans.

Imagine one more time, the same scenario where you engage in a conversation with your colleague, but now you know for sure and from the beginning that it is a robot. Would it feel weird?

Probably, and I know it sounds scary as well, but if we give this deeper thought and think about the benefits and not the drawbacks of having an artificially intelligent coworker, it might just change our perspective. Some of these benefits include:

Safety: Tasks that involve the use of heavy machinery, sharp objects, very high or low temperatures, chemicals, and others, will be performed by robots. This will protect workers in dangerous and unhealthy working conditions.

Speed and Consistency: AI robots work fast and without any distractions, they have no need for vacations and are available 24/7.

No Errors: Robots have almost no room for mistakes, they are precise and deliver quality.

Happiness & Productivity: Most importantly, all these perks are intended to increase both, employee happiness and productivity. As mentioned before, these robots will take over those tasks that we don’t enjoy. From dangerous, tedious, and repetitive basic tasks to more complicated ones that require highly analytical skills.

SOURCE: © ADOBE STOCK

Examples of Work That Can Be Complemented By AI Robots:

Data Journalists: This type of journalists are those who are focused on analysing data. AI robots could perfectly perform these work much faster and efficiently.

Secretaries: Administrative tasks like answering phone calls, sending emails, scheduling meetings, and others (including physical, manual tasks), can be done by AI robots.

Document-Review (Attorneys): Many attorneys have to search through thousands of documents looking for specific information. AI robots can filter information in a flash, they can also analyse it and generate reports. This work applies to other related fields as well.

Pharmacists: When you go to a pharmacy, an AI robot could scan your prescription and get your medicine. For over-the-counter medicine, you could indicate your symptoms and the robot will suggest a recommendation. Also, these robots could potentially have access to data from hospitals and your health records to make suggestions more accurate.

AI Police and Intelligence Assitant: AI Robots could potentially assist the police and agencies like the CIA by collecting, storing, sorting through, and highlighting key data that is necessary for investigations. They could also perform some physical duties like patrolling, arresting, and even directing traffic.

Botenders: Robots can be taught how to mix and serve drinks. Anything from beers to signature cocktails. They can make hundreds of them within minutes.

There are many concerns about robots replacing people and eliminating jobs, but these robots could potentially work alongside humans, collaborate and complement our work rather than taking over jobs. In fact, technology will create more jobs than it will eliminate. Many jobs will change, and the new ones will require a new set of skills that we must acquire through advanced education and training systems boosted by AI.

If we are going to share a workspace with robots and see them as partners rather than adversaries, they must first experience the world as a human, meaning that they would need to be able to learn about us. This will make the interaction between humans and robots much easier and can also allow them to keep learning much faster.

Morality and AI Coworkers

What do we expect from artificially intelligent coworkers in terms of morality?

SOURCE: © ADOBE STOCK

AI and robotics will have a huge impact on society, values, and human rights.

If we want machines to operate autonomously, at one point, they will require to collect a lot of data. But how much of these data do we want to share with robots? If a robot causes an accident, who would be responsible? Are we willing to give up our privacy to interact with robots? Can we even trust a robot?

More questions will continue to arise as technology develops, and it is up to us to answer those questions by working together to implement a structure that regulates and protects the contour of these innovations.

There are always two sides to every story, times of uncertainty will come and technology if gone wrong, can become dangerous. Therefore, we have to manage robots carefully, in the end, it is humans who will take charge of controlling, checking, and running the bots.

AI and robotics should no longer be feared, but rather be seen as a tool for collaboration."
Classic Cryptography Systems,"Written communication is found in every instance of our history — dating back to our earliest days as a species. Prior to our discovery of agriculture & permanent settlements, nomadic tribes famously left behind stories of triumphs & hardships in primitive language structures known as proto-writing(s). Once we began settling, however, the utility of written communication rightly evolved. Previously, nomadic tribes feared nature most in the form of predatory animals & cataclysmic climate events; post-settlement, however, other tribes took the mantle as the largest threats.

Interactions among early settlements forced the birth of diplomacy, which, in turn, pushed communication & language to evolve. Specifically, it led to the necessity of secrecy & encryption. How could communities guard their most trusted secrets, or trust messengers carrying their most trusted secrets, without risking said secrets falling into the wrong hands?

Enter Cryptography

Building Blocks of Cryptography: Ciphers

Ciphers are arguably the basic building blocks of cryptography — they’re certainly the earliest recorded implementations of encryption. In fancy crypto-lingo, a cipher is simply a series of steps that scrambles (encrypts) the original message, known as a plaintext, to a resultant message known as a ciphertext.

The earliest ciphers recorded in history were quite rudimentary, usually encrypted by hand or a simple mechanical device. In order to gain a deeper understanding of goals & principles behind ciphers, we’ll walk through three of these simple, manually-encrypted ciphers of historical significance:

While these early examples provide ample learning opportunities & highlight key points, they’re light-years behind their modern counterparts. Powerful, publicly-known ciphers make up the cornerstone of communication security — for example, the famous AES (Advanced Encryption Standard) that’s commonly used all…"
20 must-know Data Science Interview Questions,"20 must-know Data Science Interview Questions

A non-exhaustive(duh) list of some of the good data science questions I have come across. I hope this list is of use to someone wanting to brush up some basic concepts. Kudos to the authors of all the amazing posts mentioned here.

Q. Define mean, mode, median. Explain these concepts to a layman. When is either preferred over the other. Give practical examples."
Introduction to Natural Language Processing (NLP) and Bias in AI,"Introduction to Natural Language Processing (NLP) and Bias in AI

Photo by Bewakoof.com Official on Unsplash

Natural language processing (NLP) is a field that is being revolutionized by deep learning. From voice assistants to Gmail’s Smart Compose, deep learning has made it possible for machines to understand us in a more intuitive way.

Of course, working with natural data is very different than working with tabular data, because we now need to represent words and sentences in a way that machines can understand.

In this post, I will introduce key concepts of NLP such as word embeddings, and we will see how an algorithm can become biased, and how we can remove that bias.

Let’s get started!"
How a hackathon can help founders find their purpose,"How a hackathon can help founders find their purpose

The inner path to becoming SensAI Arjan Haring · Follow 5 min read · Mar 30, 2019 -- Listen Share

Recently I did a proof of concept hackathon with Yama Saraj for his startup SensAI. Such hackathon forces founders to express their vision on their business model and their technology stack. There is a big difference running a social entreprise on DIY technology or a startup with a stack in the cloud and a clear exit strategy. And in Yama’s case he might be going for both…

Yama characterizes himself as a “crazy development economist”. When he was young, his family had to leave Afghanistan and they ended up in the Netherlands where he became a succesful student. Yama first studied electrical engineering then economics, but there was something missing. After his studies, in search of his ikigai, he drove all the way from the Netherlands to Afghanistan to give kids boxing lessons.

“Technology is just a medium to get the message across.”

He then realized that through sports you can empower this generation to become more resilient and you can even inspire them to be true changemakers. With a touch of irony, martial arts could lead to a peaceful society with less violence and competition. Technology is just a medium to get the message across. But what is the message?

Yama is one of a kind, with an energy that matches mine (almost), he is someone you can not not like. He sees connections everywhere and gets almost everyone excited about his ideas. And his ideas are great, if you ask me. Normally people would say someone like Yama needs to focus to be more effective. That could be true, but that most probably doesn’t make him happy.

Team “Sustainable SensAI” working on their prototype. Hardware by MadLab Eindhoven.

Team Sellout SensAI versus team Sustainable SensAI

In the process of our hackathon Yama’s ideas were made tangible. One team wanted to work on his idea of a circular boxing bag that would help both the problem of used car tires and that of obesity in working class neighborhoods. And another team start working on the idea to gamify boxing, both for professional and private use.

Part of the slidedeck of team “Sellout SensAI”

To run a sustainable, as in long term viable, company you need a robust busines model. That much is certain. So the tendency of millenials to work on something that is “good for the world” still needs to be matched by a revenue model. There are too many nice initiatives that don’t last because there is no clear revenue model. Volunteering or bootstrapping is often not a sustainable model.

To run a sustainable, as in long term viable, company you need a robust business model.

Soul-searching; 1 proof of concept at a time

Thinking by doing is a wellknown strategy in a lot of disciplines, for example in electrical engineering. But this is not yet a very common approach to the art of living. And in some cases, like finding a life partner, you want to take a more conservative approach and think more before you do.

“a failed startup is even considered a positive addition to your resume”

But in your working life I would argue you have quite a lot of flexibility to test things out. Is it something you like doing? Is it something you see yourself doing in 10 years still? Job hopping is more and more accepted, and a failed startup is even considered a positive addition to your resume (lucky me 😜).

A working prototype of a gamified boxing bag (Rogier Brussee demonstrating his makey makey contraption and a pretty mean Jab-Jab-cross combo)

So why not test with life a little?

So why not test with life a little? Enjoy the different things it has to offer and better prepare yourself for choices that have great impact on your life.

During the hackathon in the proof of concept lab Yama could better imagine what it would be like to run a company that sold the products the teams came up with. But Yama could also better understand what people he needed on his team for all the different projects.

Find your SensAI

Yama has a special relationship with 5 time World Champion Thai boxing Yucel Fidan. Yucel is one of the persons that sees the great things that SensAI could be part of. And Yama would like to learn how to be a champion like Yucel, albeit in a different arena.

For me it was an incredible honor to be part of the follow-up day of the SensAI hackathon as well that was hosted at Fidan Gym. Yucel is an incredibly balanced champion. Having worked hard myself, I am always in awe of people that managed to metaphorically move mountains, made a small dent in the universe and stayed true to themselves no matter what.

5 time World Champion Thai boxing Yucel Fidan & CEO of SensAI Yama Saraj

I am not sure what choices Yama will take in the nearby future. But I hope, and I am pretty sure, he stays true to himself. I have a feeling that the proof of concepts have given him more grip on what the effects are of the choices he makes as an entrepreneur for his own life.

I am very grateful to have worked with Yama for my first proof of concept session at JADS. And it might not surprise you we are already planning a new proof of concept session soon. Stay tuned, stay SensAI."
Natural Language Processing in Banking: Current Uses,"Banks are using a branch of Artificial Intelligence called natural language processing (NLP) to automate certain document processing, analysis and customer service activities. Three applications include:

Intelligent document search : finding relevant information in large volumes of scanned documents.

: finding relevant information in large volumes of scanned documents. Investment analysis : automating routine analysis of earnings reports and news so that analysts can focus on alpha generation.

: automating routine analysis of earnings reports and news so that analysts can focus on alpha generation. Customer service & insights: deploying chatbots to answer customer queries and understand customer needs.

We will cover real-life examples of what banks are doing in these areas. First, let’s go over what natural language processing is capable of.

Introduction to Natural Language Processing

Natural Language Processing (NLP) is a branch of Artificial Intelligence that enables computers to understand human language and respond in kind. This involves training computers to process text and speech and interpret the meaning of words, sentences and paragraphs in context.

Human-Computer Interactions

Human-computer ‘conversations’ can be broken down as follows (we’ll get to the specific AI methods a little later):

We provide text or speech input (e.g. typing into a chatbot interface or talking to a smart speaker). The computer converts the text/speech into a format it understands (e.g. converts speech to text and words to vectors). This helps computers cluster and classify different words. The computer figures out meaning and context using its own data sets. The computer determines an appropriate response and converts it to text or speech that we understand, and responds to us.

We interact with apps that use natural language processing every day:"
An Introduction to Autonomous Vehicles,"A general understanding of what self-driving cars are really about.

Lyft’s self-driving car [Source]

Every year, there are around 1.25 million deaths caused by road accidents. That’s equivalent to 3,287 deaths on a daily basis! As a teenager just learning how to drive, this is a scary fact that lingers at the back of my mind. On top of that, there’s also a ridiculous amount of traffic that we have to suffer through, which just creates unnecessary frustration for most people.

This got me thinking … is there another way we can replicate the driving of humans, but 20x better?

This is where I discovered self-driving cars.

How do self-driving cars work?

With self-driving cars (SDC), the goal is to be able to operate a car like a human driver. The only catch is that there’s no driver behind the seat.

Caption this on a highway with no driver! [Source]

Sounds scary, right?

You’re probably wondering, how does a computer (or a car, in this case) come close to human intelligence when doing something so complex? To answer that, we need to understand the five components that make up an SDC:

Computer vision Sensor fusion Localization Path planning Control

If you don’t understand those terms, don’t worry! For now, a good way to approach this would be to think of an SDC like a human being.

Computer vision

Like a human driver, we need to be able to see the environment around us, whether that’s looking ahead for traffic or reading road signs, vision is 🔑 .

Similarly, computer vision is how a car sees its environment.

In computer vision for SDC, the goal is to be able to identify objects near the car. We do that by using an image classification network called (Convolutional Neural Networks)."
"How to Manage Your Machine Learning Workflow with DVC, Weights & Biases, and Docker","Managing a machine learning workflow is hard. Beyond the usual challenges in software engineering, machine learning engineers also need to think about experiment tracking, reproducibility, model deployment, and governance. In this article, I want to show 3 powerful tools to simplify and scale up machine learning development within an organization by making it easy to track, reproduce, manage, and deploy models.

Using Version Control with DVC

In conventional software engineering, Git is the industry standard for version control. Team members work on their own local code, but always sync up their progress and new developments with a central repository. This ensures that everyone is up to date and in sync. It’s a great way of having the work of the team compound on each other, rather than conflicting, which leads to higher output.

Machine learning is a bit trickier since a lot of it is a combination of research and software engineering. For example, the data itself is large, so shared storage can be challenging. Or displaying the changes in Jupyter Notebooks is difficult, as Git doesn’t allow the ability to show changes in graphs from commit to commit.

DVC is a handy tool built to make machine learning models shareable and reproducible. It is designed to handle large files, data sets, machine learning models, and metrics as well as code."
The Divided States of America — Historical Perspectives,"Voting share margin between Republican and Democratic candidates

The margins of voting shares of candidates of the Republican and Democratic parties have widened in many counties. The standard deviation of margins has been constantly increasing over past years: 0.26 in 2000, 0.29 in 2008, and 0.35 in 2016. During this time, distribution tails, i.e. non-competitive counties, have substantially increased.

In my previous article, I found that population and social profile data had a strong power to tell the 2016 presidential election results at county level. The question posited in this article is whether or not changes in population have contributed to the polarization in US politics. For instance, if cities had inflows of people whose values are aligned with the Democratic Party, they might become bluer. At the same time, if people with affinity for the Republican Party remain in the countryside, those counties would turn redder.

I found, however, that shifts in population have not driven the polarization of the nation at least in 2010–2017 period, while characteristics of elections (e.g. candidates, issues) were more accountable for the polarization trend.

In this analysis, I used the Data Profiles tables of the US Census American Community Survey (Five-year data) from 2010 to 2017, selecting 292 variables which are consistently available at county level. Missing values were imputed by XGBoost Regressor. During that period of time, there were two presidential elections. I constructed regression models with independent variables from US Census ACS and a dependent variable of vote share margin between Republican and Democratic candidates in the…"
Applied AI: Going From Concept to ML Components,"Opening your mind to different ways of applying machine learning to the real world. By Abraham Kang with special thanks to Kunal Patel and Jae Duk Seo for being a sounding board and providing input for this article.

Photo by Franck V. on Unsplash

Executive Summary

Candidate Problem

Many people are interested in automating redundant processes within the organization using AI. Let’s start with a concrete problem, what I noticed is that lawyers typically gather facts from clients when something bad happens. These facts form the basis of causes of action (negligence, battery, assault, intentional infliction of emotional distress) that an individual can sue on. Once the causes of action have been determined based on legal justification and the facts, a complaint is written up and submitted to the court for commencement of the legal action. The complaint is a legal document which sets out the facts giving rise to a legal basis for taking action against another party. Manually creating this document can be time consuming and similar facts result in similar causes of action. For example, if someone hits another person there is usually a “battery”. If someone accidentally hurts someone else or someone slips and falls within a store there could be an action for negligence. Based in this problem we have a customer who would like to use AI to learn how to write a complaint from a fact paragraph describing what happened.

Understanding the Problem

Trying to get AI/ML to read facts and figure out a way for AI/ML to write a whole complaint might be biting off more than the model can chew and may be an effort that would take years to solve. However, if you take the time to understand and think about the underlying problem, you can find existing techniques (with some slight modifications) that could be used to solve different pieces of the puzzle. For example, when you look at a complaint it starts with a description of the parties and their positions (plaintiff vs defendant) as well as counsel representing them. There may be a class action section, a justification of jurisdiction (does court have power over parties), description of the parties, a justification of venue (are we in the proper court location), a listing of the causes of action, and description of the facts. When you look at the sections you have to think about where the data that is going to build the individual sections is going to come from. In certain cases you will not have an answer but if you look carefully you will see patterns and correlations between different sections of the complaint. This will allow you to think about what your inputs to the neural network will be and the candidate outputs.

Getting Inputs for the Neural Network

We don’t have any data per se but there may be a way to parse the facts out of all existing complaints and use them as the input for our neural network. Every complaint that is submitted to the court becomes public information so there will be plenty of data. This solution will require attorneys to write their facts as if they were inserting them directly into the complaint, but this is a minor inconvenience to be able to have machine learning provide generated complaints. Generating a complete complaint may be difficult. So let’s break the problem down.

Breaking the Problem Down

Logically how would you break the generation of a document down into smaller pieces? Well you need to look at one so here is an example: https://www.heise.de/downloads/18/1/8/9/1/3/4/6/NP-v-Standard-Innovation-Complaint.pdf. To make it interesting I picked a maker of adult toys so it might peak your curiosity. Basically, we want to eventually generate a complaint (above pdf) from the facts provided by a lawyer. So if you look at the document and at other complaints you will find similar patterns as to structure.

So what do you think would be the best way to break things down… don’t scroll down until you have had time to think about it.

….Really think about it…..

Well if you said to break things down by section using templating, then this would be the route that would probably be best.

When you break down a complaint there are causes of action listed in the complaint. Each cause of action (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.) has supporting rules and justification based on the facts. So now there are two problems. How do you come up with the causes of action from the facts text and how do you generate the supporting text under each cause of action?

Finding the Causes of Action

When we look at the facts of the case we need to find all of the causes of action (laws that were broken) that we could sue on. There are no direct solutions for finding causes of action from text so we will have to think more fundamentally.

What existing techniques do you think we can use to look at text and infer meaning or a description of the text. If you said multi-label text classification or multi-label sentiment analysis, then you are ahead of the game (https://paperswithcode.com/task/text-classification, https://paperswithcode.com/task/sentiment-analysis). Analyzing text to determine its associated causes of action is a similar process to classifying text or finding the sentiment of related text. There are associated problems like the fact that causes of action will need to be updated as laws are introduced. There may be an alternate way to create an embedding for the facts and then tie the causes of action to the facts based on triplet (https://arxiv.org/pdf/1503.03832.pdf) or quadruplet loss (https://arxiv.org/pdf/1704.01719.pdf) to push causes of action sharing similar words together in the embedding space and unrelated causes of action further apart. Then use a clustering technique to find causes of action close to determinative word embeddings used in the supporting argument associated with the words in the individual cause of action sections of the complaint.

Generating the Text in the Supporting Arguments Section of Individual Causes of Action

Now that you have figured out how to get the high level causes of action from the text, how can you generate the supporting argument text for each of the individual cause of action sections (violation of Federal Wiretap Act, Illinois Eavesdropping Statute, Intrusion upon Seclusion, Unjust Enrichment, Fraud and Deceptive Business Practice Act, etc.)?

This one is not so straight forward. Think about a what neural network architectures which generate text (Don’t scroll down until you have some ideas)….

….Open your mind….Use the Force….

Text generation algorithms (https://paperswithcode.com/task/data-to-text-generation, https://paperswithcode.com/area/nlp/text-generation) might be an option but even the best ones create gibberish often. The better alternative might be to use an architecture like neural networks involved in translation (https://paperswithcode.com/task/machine-translation, https://paperswithcode.com/task/unsupervised-machine-translation, https://paperswithcode.com/paper/unsupervised-clinical-language-translation). In addition, it might be a good idea to have a separate “translation” neural network for each cause of action to help each neural network focus on identifying the key facts used in generating a supporting argument for each cause of action.

Clean Up

It is probably going to be a good idea to run the candidate text for the supporting argument text for each cause of action through a grammar checker/fixer (https://paperswithcode.com/task/grammatical-error-correction). This way any blatant mess ups are fixed.

Conclusion

I hope you learned how to apply the machine learning solutions more broadly. Let me know if you get stuck as I would definitely be interested in hearing about problems that people are trying to solve with machine learning."
Lightweight Visualization of Keras Models,"I love how simple and clear Keras makes it to build neural networks. It’s not hard to connect Keras to Tensorboard but that has always felt to me like a heavyweight solution is overly complicated for many of Keras’s users who often want to take a quick look at the underlying model.

With wandb, you can visualize your network’s performance and architecture with a single extra line of python code.

To show how this works, I modified a few scripts in the Keras examples directory.

To install wandb, just run “pip install wandb” and all of my Keras examples should work for you.

1. Simple CNN‍

I started with the requisite mnist_cnn.py.

I added the “from wandb import magic” line below — you can also look at my mnist_cnn.py forked from the Keras examples with the one line change.

Now when the model runs, wandb starts a process in the background saving relevant metrics and streaming them to wandb.com. You can go to https://app.wandb.ai/l2k2/keras-examples/runs/ovptynun/model and look at the output of my run.

‍

I can see exactly the data that my model is labeling and view the loss and accuracy curves automatically.

2. Resnet on Cifar‍

Next, I forked cifar10_resnet.py and made the same one line change. You can see a nice visualization of a resnet at https://app.wandb.ai/l2k2/keras-examples/runs/ieqy2e9h/model.

On the system page, I can see that this model is using a little more of my single GPU than the mnist example.

3. Siamese network‍

Next I tried the siamese network example. Here I might want to look at the TensorFlow graph, luckily with our one line of code we automatically instrument and host TensorBoard. You can find this run at https://app.wandb.ai/l2k2/keras-examples/runs/fsc63n6a?workspace=user-l2k2.

This instrumentation took me under a minute per model, adds very little compute overhead, and should work for any Keras model you are working on. As you want to track more things you may want to replace the one line with:

import wandb wandb.init(magic=True)

Then you can use our custom wandb.log() function to save anything you want. You can learn more in our documentation.

I really hope you find this useful!"
Is Online Poker Dead?,"Well, not dead, but dying. Just over one month ago, Facebook’s AI Research team published a paper outlining their results for a superhuman AI capable of consistently outplaying the world’s top poker players. On its own, this is an amazing achievement for AI, but looking past that, it’s set to deliver a devastating blow to the almost $4bn industry of online poker. In the coming months, millions of players worldwide and massive online tech companies will begin to see the effects as these AIs pop-up in online play. A major shift is required in the industry if it is to stay alive.

The paper released by Facebook is a culmination of decades of work into the first AI to beat humans at a game with more than two teams. The AI coined “Pluribus” beat world champion poker players consistently in a format identical to six-player online poker. Not only this, the AI was trained with less than $150 of resources, meaning such a system can be created by almost anyone.

Importantly, the paper doesn’t describe an exact winning formula, rather it outlines the approach used. This means that we won’t be seeing duplicates of Pluribus tomorrow, but we might next month, or next year. Now that it’s shown to be possible and the guiding light is there, various people or groups will work to recreate the AI as closely as possible. All it needs is one success and the AI can be shared instantly around the world.

With the current level of scrutiny, it will be simple for any person or syndicate with such an AI to immediately start to win money in online poker. Initially, this isn’t a problem for online poker companies, but for the humans who play it. As the poker tables become saturated with AI, the chance of any one human winning a game will drop to 0 as will their bank account. This will drive them out of the system at which point it becomes a problem for the online poker companies as their main source of revenue dries up. There may be some ongoing wars between AIs, but the quantity and volume will not be anywhere near the current levels.

To avoid such a hit to the market, stricter security measures may be required. Though it’s true it may be easy to catch such systems if they’re winning every game, a good fraudster — and let’s go with that term here, fraud — will be smart…"
How does Facebook define Terrorism in Relation to Artificial Intelligence?,"How does Facebook define Terrorism in Relation to Artificial Intelligence?

How useful is the terminology terrorism? I would argue it is not useful because it obscures the specific debates into a reactionary pattern of violence against violence. However in a political science perspective this would to some degree be a social constructivist approach. Artificial intelligence being increasingly securitised will inevitably be mixed up in the policy process of these large social media companies. So let me explore how Facebook is addressing this issue.

In this article I will look at:

(1) Facebook and its definition of terrorism;

(2) into the stated approach to artificial intelligence;

(3) Facebook’s growing security team;

(4) the practical side and possible trauma of human moderation;

(5) the question of a US-centric focus on terror on social media;

(6) government requests for user data;

(7) the coming creation of the global oversight board that may set a precedence for the use of AI for both organisations and governments;

(8) vague Snapchat terrorism, a comparative outlook – an outro.

1. Facebook and its Definition of Terrorism

In 2018 one of the largest social platforms on the planet decided to attempt defining terrorism, and it reads as the following:

“Any nongovernmental organization that engages in premeditated acts of violence against persons or property to intimidate a civilian population, government or international organization in order to achieve a political, religious or ideological aim.”

In the blog post made the 23rd of April 2018 called Hard Questions: How Effective Is Technology in Keeping Terrorists off Facebook? A central paragraph by my own approximation reads:

The democratizing power of the internet has been a tremendous boon for individuals, activists, and small businesses all over the world. But bad actors have long tried to use it for their own ends. White supremacists used electronic bulletin boards in the 1980s, and the first pro-al-Qaeda website was established in the mid-1990s. While the challenge of terrorism online isn’t new, it has grown increasingly urgent as digital platforms become central to our lives. At Facebook, we recognize the importance of keeping people safe, and we use technology and our counterterrorism team to do it. [bold added]

The claims Facebook makes through this blog post:

Our definition is agnostic to the ideology or political goals of a group. Our counterterrorism policy does not apply to governments. Facebook policy prohibits terrorists from using our service, but it isn’t enough to just have a policy. We need to enforce it.

Despite making this claim they simultaneously say their focus lies on ISIS, al-Qaeda, and their affiliates — the groups that currently pose the broadest global threat. However these are additionally of most interest and priority to the United States.

2. How does Facebook use Artificial Intelligence to Counter Terrorism?

This blog post additionally refers to a post written by Facebook called Hard Questions: How We Counter Terrorism. It is written by Monika Bickert, Director of Global Policy Management, and Brian Fishman, Counterterrorism Policy Manager. This post was made already on the 15th of June 2017.

The top point of this post is Artificial Intelligence. We want to find terrorist content immediately, before people in our community have seen it. Facebook has clearly been using AI since at least 2017 to remove posts associated with terrorism (they claim it was recent at the time). At the time they seemed to focus their efforts on ISIS and Al-Qaeda.

Image matching: When someone tries to upload a terrorist photo or video, their systems look for whether the image matches a known terrorism photo or video. This way they can avoid people uploading the same video.

Language understanding: Facebook had started to experiment with using AI to understand text that might be advocating for terrorism. They were at the time experimenting with removing text relating to what they had already seen as previously removed (historic data)

Removing terrorist clusters: Facebook claims to know from studies of terrorists that they tend to radicalize and operate in clusters. This offline trend is reflected online as well. They use signals like whether an account is friends with a high number of accounts that have been disabled for terrorism, or whether an account shares the same attributes as a disabled account.

Recidivism: Facebook said they had gotten much faster at detecting new fake accounts created by repeat offenders. Through this work, they have been able to dramatically reduce the time period that terrorist recidivist accounts are on Facebook. They argue this process is ‘adversarial’ that the other party keeps developing new methods.

Cross-platform collaboration: Because they didn’t want terrorists to have a place anywhere in the family of Facebook apps, they have began work on systems to enable us to take action against terrorist accounts across all our platforms, including WhatsApp and Instagram.

In the first quarter of 2018 they reported to have taken down 837 million pieces of spam and 2.5 million pieces of hate speech and disabled 583 million fake accounts globally. This was in relation to the statement saying it was assisted by using technology like: “…machine learning, artificial intelligence and computer vision..” to detect ‘bad actors’ and move more quickly. They mentioned this particularly in relation to the election.

In 2019 They removed what they call ‘inauthentic behaviour from Iran, Israel and Russia (focused on Ukraine) in particular.

Live-streamed attacks like Christchurch shooting require human moderation. LeCun said at a recent event that Facebook was years away from using AI to moderate live video at scale. LeCun the problem with the lack of training data. “Thankfully, we don’t have a lot of examples of real people shooting other people,” you could train for recognition of violence using footage from movies, but then content containing simulated violence would be inadvertently removed along with the real thing.

Automated systems are claimed to be used mainly as assistants to human moderators.

AI is not a silver bullet to moderation. Understanding artificial intelligence in this context is of course not enough. Facebook has a community operations team that has to distinguish from a personal profile or a news story. This ‘more nuanced approach’ requires human expertise. Understanding how Facebook uses artificial intelligence is of course not enough without understanding how their actual safety and security team manages these tools as well as frameworks.

3. Facebook’s Growing Safety and Security Team

Facebook feed, since the company’s 200-person counterterrorism team removed them. (In the wake of the Cambridge Analytica privacy scandal, Facebook is under pressure to show that it can police itself.) Reported in 2018.

Facebook was scheduled to be growing by 3,000 people over 2017— that work 24 hours a day and in dozens of languages to review these reports and determine the context. The link refers to a post made by Mark Zuckerberg stating that they already have 4,500 people hired in addition to those they had scheduled to hire.

In July the 6th 2018 (updated the 4th of December) Ellen Silver from Facebook as VP of operations claimed to be scaling globally, covering every time zone and over 50 languages. They had also rapidly grown their staff in safety and security:

“The teams working on safety and security at Facebook are now over 30,000. About half of this team are content reviewers — a mix of full-time employees, contractors and companies we partner with.”

4. Insecurity Causing Trauma for Facebook Workers

In February 2019 The Verge published an article called The Trauma Floor: The secret lives of Facebook moderators in America. This article does of course describe the challenging conditions in which these moderators work, however it also mentions a stat of 15,000 moderators working around the world. It did seem rather a few of these were subcontracted through companies such as Cognizant having to sign NDAs, with secrecy supposedly protecting employees.

“Collectively, the employees described a workplace that is perpetually teetering on the brink of chaos. It is an environment where workers cope by telling dark jokes about committing suicide, then smoke weed during breaks to numb their emotions. It’s a place where employees can be fired for making just a few errors a week — and where those who remain live in fear of the former colleagues who return seeking vengeance.”

It is perhaps ironic that in attempting to handling terror there is a degree of trauma caused to the handlers. Certain of they key findings by the report by The Verge seems interesting to stress or at least consider:

Moderators in Phoenix will make just $28,800 per year — while the average Facebook employee has a total compensation of $240,000.

Employees are micromanaged down to bathroom breaks. Two Muslim employees were ordered to stop praying during their nine minutes per day of allotted “wellness time.”

Moderators cope with seeing traumatic images and videos by telling dark jokes about committing suicide, then smoking weed during breaks to numb their emotions. Moderators are routinely high at work.

Employees are developing PTSD-like symptoms after they leave the company, but are no longer eligible for any support from Facebook or Cognizant.

Employees have begun to embrace the fringe viewpoints of the videos and memes that they are supposed to moderate. The Phoenix site is home to a flat Earther and a Holocaust denier. A former employee tells us he no longer believes 9/11 was a terrorist attack.

According to the article these centres operate through accuracy standards which means posts reviewed are being reviewed. Facebook has set a goal of 95% accuracy, but Cognizant is usually never that high (closer to 80–92%). A moderator must suggest the correct community standard violation or risk loosing accuracy. The Verge article mentions a few different set of truths that a moderator has to consider.

Community Guidelines, publicly posted ones and internal documents. Known Questions. A 15,000-word second document with commentary. Discussions amongst moderators attempting to reach a consensus. Facebook’s own internal tools for distributing information.

Further it is said that the challenge of keeping a job may be rather difficult: “The job resembles a high-stakes video game in which you start out with 100 points — a perfect accuracy score — and then scratch and claw to keep as many of those points as you can. Because once you fall below 95, your job is at risk.”

Fired employees regularly threatened to return to work and harm their old colleagues. An NDA usually seem to stop you from talking about the work you were doing or even state that you ever worked for Facebook, according to The Verge: “They do the work as long as they can — and when they leave, an NDA ensures that they retreat even further into the shadows. To Facebook, it will seem as if they never worked there at all. Technically, they never did.”

Facebook has a clear idea of how their policies should be managed:

“We want to keep personal perspectives and biases out of the equation entirely — so, in theory, two people reviewing the same posts would always make the same decision.”

In a statement that contradicts the article by The Verge they state: “A common misconception about content reviewers is that they’re driven by quotas and pressured to make hasty decisions.” They is stated to have four clinical psychologists across three regions who are tasked with designing, delivering and evaluating resiliency programs. Yet it is questionable whether this decentralised mental care without professionals on-the-ground is advisable given the work these employees have to go through.

5. US-Centric Global Moderation of Terror

We can ask a simple question: when policy and guidelines are designed in US for the world what perspectives are prevalent in the given framework? As you may have guessed for the section title I am sceptical whether a universal framework based on one location can work well across the planet.

Their enforcement have focused heavily on Islamic Terrorist groups rather than right-wing extremism or other forms of ‘terror’. They have had a partnership with Microsoft, Twitter and YouTube on hashes relating to terrorist content. These are all companies based in the United States.

Counterspeech programs. Facebook support several major counterspeech programs. For example, last year we worked with the Institute for Strategic Dialogue to launch the Online Civil Courage Initiative. The project challenge was to design, pilot, implement and measure the success of a social or digital initiative, product or tool designed to push back on hate and violent extremism. Reportedly it engaged with more than 100 anti-hate and anti-extremism organizations across Europe.

They’ve also worked with Affinis Labs to host hackathons in places like Manila, Dhaka and Jakarta, where community leaders joined forces with tech entrepreneurs to develop innovative solutions to push back against extremism and hate online.

We want Facebook to be a hostile place for terrorists.

Saying this they quoted the 1984, the Irish Republican Army (IRA) statement after a failed assassination: “Today we were unlucky, but remember that we only have to be lucky once — you will have to be lucky always.” In one way the statement resounds, yet you cannot avoid everything forever. If there is no room for failure, then any smudge on the perfect surface can stain the image — of course this is important for Facebook. We can ask whether this decision of decentralised moderation makes it easier to blame external actors for any ‘externalities’ relating to safety and security.

6. Government Requests for User Data

It is of course possible to access Facebook’s data if there is a security event that requires access. Government requests for user data increased globally by 7% from 103,815 to 110,634 in the second half of 2018. With the United States continues to submit the highest number of requests, followed by India, the United Kingdom, Germany and France. This reflected a normal growth according to Facebook.

As part of the requests 58% included a non-disclosure order prohibiting Facebook from notifying the user. In an internal review of their US national security reporting metrics Facebook found that it had undercounted requests from the Foreign Intelligence Surveillance Act (FISA). Facebook divides these requests into emergency requests and legal processes.

Facebook may voluntarily disclose information to law enforcement where we have a good faith reason to believe that the matter involves imminent risk of serious physical injury or death.

It may be useful to understand these two different data requests:

Legal Process Requests: Requests we receive from governments that are accompanied by legal process, like a search warrant. We disclose account records solely in accordance with our terms of service and applicable law.

Emergency Disclosure Requests: In emergencies, law enforcement may submit requests without legal process. Based on the circumstances, we may voluntarily disclose information to law enforcement where we have a good faith reason to believe that the matter involves imminent risk of serious physical injury or death.

“Government officials sometimes make requests for data about people who use Facebook as part of official investigations. The vast majority of these requests relate to criminal cases, such as robberies or kidnappings”

During this period Facebook and Instagram took down 2,595,410 pieces of content based on 511,706 copyright reports; 215,877 pieces of content based on 81,243 trademark reports; and 781,875 pieces of content based on 62,829 counterfeit reports.

Facebook recently started partnering with ethics institutions focused on artificial intelligence. The focus of this partnership seem to be in the direction of safety, at least in Munich the Institute they have partnered with will address issues that affect the use and impact of artificial intelligence, such as safety, privacy, fairness and transparency. I have previously described that this can be problematic: an issue of self-policing ethical conduct.

7. The Global Oversight Board Ensuring a Global Perspective

Facebook is creating a global oversight board. In a post by Nick Clegg, the new VP of Global Affairs and Communications in January 2019 a draft charter was released. The draft lists 11 questions alongside considerations and suggested approaches. More recently in late June 2019 another post was made by Facebook on this topic.

It was stated they (Facebook) had traveled around the world hosting six in-depth workshops and 22 roundtables attended by more than 650 people from 88 different countries. They had personal discussions with more than 250 people and received over 1,200 public consultation submissions.

Subsequently a 44-page report was released by Facebook called Global Feedback & Input on the Facebook Oversight Board for Content Decisions. This talks of a global constitution, board membership, content decisions and governance. Nick Clegg states in the introduction:

“Our task is to build systems that protect free expression, that help people connect with those they care about, while still staying safe online. We recognize the tremendous responsibility we have not only to fairly exercise our discretion but also to establish structures that will evolve with the times. Our challenge now, in creating this Oversight Board, is to shore up, balance, and safeguard free expression and safety for everyone on our platforms and those yet to come onto them, across the world.”

The report argues that there needs to be more democracy in Facebook. There needs to be a system to appeal decisions. The report gives different examples of moderation. It also states that Facebook undertook research to study the range of oversight models that exist globally which identified six ”families“ of oversight design. The grid they presented looks like this.

According to the report public reason giving will be a crucial feature of the Oversight Board, one which drives at the heart of the legitimacy of its decisions.

The Draft Charter suggests that Facebook will select the first cohort of members, with future selection to be taken over by the Board itself. The report stated that questioned were raised to this proposal of leaving future selection up to the Board itself, as this could result in both a “recursion problem” and possibly the “perpetuation of bias.” A few approaches were suggested for membership in the board:

Membership be left to a fully democratic vote by Facebook users. A hybrid approach, combining selection procedures so that Facebook, outside groups, and users could all participate. Soliciting public comment on a slate of applicants. Inviting civil society groups to select some of the Board members. Asking governments to weigh in on names and candidates. Opening a public nomination process. A randomised lottery system to select members from Facebook users.

There was an agreed importance of diversity, though it was mentioned that perfect representation is not possible. It was mostly agreed that Facebook employees (current and former) should be excluded from the Board. It was suggested a fixed term of three years, renewable once.

In the report it is suggested two ways to submit both for Facebook to send important or disputed content and for the users. Facebook has proposed that smaller panels, not the Board as a whole, will hear and deliberate on cases. It was clear that: “A strong consensus emerged that the Board’s decisions should influence Facebook’s policy development.”

It was noted that Facebook was to establish an independent trust to remunerate (pay) board members. It was argued this board needed its own staff and that these be wholly independent of Facebook. The scope for the board will be content governance. However it was indicated that the board could hear other policy issues, such as: “…algorithmic ranking, privacy, local law, AI, monetization, political ads, and bias.”

Thus it can be said that Facebook and the field of artificial intelligence may be rather influenced by decisions made by this board in the future should it possibly be established. Indeed considering the scale of Facebook this can both influence private companies to adopt certain practices or nations to make legislation based on the decisions made by this semi-independent council. In the conclusion of the internal report it is stated:

“Facebook finds itself in a historically unique position. It cannot deprive or grant anyone the freedom of expression, and yet it is a conduit through which global freedom of expression is realized.”

Vague Snapchat Terrorism? A comparative look — an outro

In their Community Guidelines Snapchat does not define terrorism, yet they write: “Terrorist organizations are prohibited from using our platform and we have no tolerance for content that advocates or advances terrorism.” Yet we may ask ourselves two questions: what is a terrorist organisation and what does advocating terrorism mean in practice if it remains undefined? You could take the: “I know terrorism when I see it”-approach yet that leaves a lot up to ambiguous choices without transparency of decisions involved. This seems part of the wicked problem of terrorism: definitions.

Terrorism in international politics is hard to define, and how you define it may also says a lot about how you think about politics more broadly. Although it is notoriously difficult to define it may be one of the future discussions to be undertaken should an oversight board from Facebook appear. The focus that Facebook has had on Islamic terror as opposed to right-wing extremism or gun violence in the United States is a worrying example. Yet their move to establish a board may be an appropriate response.

The policing or ways that different governments request user data should continue to be under strong scrutiny with transparency. The state is an actor that can inflict violence; state-inflicted violence can be ambiguous, particularly when there are claims to state-sponsoring of terrorism. Most certainly the state can act using terror, and it is occurring, so does terrorist have to be in a minority group; is it genocide or terror; and does this distinction matter?

Terror in some cases is about scaring people — violence is used in a restrictive case. Is it illegitimate use of violence by non-state actors aiming for the spread of ideology? If so whose ideology in a board run by Facebook, and the concerns of diversity is real. When to justify intervention and not alongside how it is justified may be important as pragmatic definitions arise as products of the prevailing interests.

When is an act of violence a weapons of the weak in asymmetrical power distributions? What is the difference between narco traffickers and large resource interests that funds political power? The goals aspect is worth considering: knowing someone’s intention, yet the environment that shapes this intention is equally as important. Moderating terror in terrible working conditions is just one example of many.

If we take seriously that we are individuals with ideas, there are some patterns, but a lot of it is quite hard to predict. If it is hard to predict human behaviour then it is hard to know people’s aims and quite difficult to see the people’s intention.

Where is the money coming from? We have data brokers and there is not currently enough regulation to ensure that the flow of data is responsible or is sold of unintentionally to groups intending to use the data for such purposes. Terrorism obscures — it is not a value-neutral term. Technology is not value-neutral at all. It ties into ideas of securitization and state powers alongside its ethical discourse of technological for good.

Slapping the terrorist label puts it into a different category. Understanding can be an important tool in how to prevent it. Putting the T-word on it is tempting in a rapid pace of content moderation, yet we need to engage with it.

As much as there is a need to be respectful of the way large companies are trying to moderate and cooperate with state institutions we need to be critical. Robert Cox said it well: “Theory is always for someone or for something.” In this respect perhaps technology is always for someone or something too. I will end with a video that was shared in my class today that proposes a critical view on the labelling of terrorism:"
Support Vector Machines,"Support Vector Machines

1. The problem

Support vector machines set out to address the problem of “classification”. If we’re given some data, it’s going to be composed of a certain number of entries/ rows/ points. Now, we want to classify each of these points into some classes we have in mind. To keep things simple, we assume we’re only interested in two classes, “positive” and “negative”. Examples of the important questions (some more than others) this might help answer:

Weather or not a photo has a cat in it (has cat meaning positive label) given the pixel data. Weather or not an email is spam given its subject, sender, text, etc. Determining if a patient has a certain disease or not.

The idea is that we come up with some rule for splitting the data into the two classes when we already know the right answers (for SVM’s, that “rule” happens to be drawing a plane and calling all points on one side positive and the other, negative). Then, when we encounter a new data point where we don’t know the answer, we use the same rule we (or our “machine”) “learnt” to classify it. This topic relies heavily on and is a great illustrative example of the theory of constrained optimization about which I wrote a blog a while back. Also, we’ll loosely follow the paper by Andrew Ng.

1.1. In pictures

I feel like I don’t truly understand things until I draw a picture. Also, everyone likes pictures. So let’s look at some.

We have some points in a feature space. For easy visualization, let’s consider a 2-d feature space so we can see it on a screen. We have some points sprinkled across this space, each with a binary label (1/-1). In the figure below, we can consider the green points to have positive labels and the red points to have the negative ones. For the yellow points, we don’t know what their labels are (positive or negative). If you had to guess a label for each of them, what would you pick? You might find that some of the points are not so clear-cut.

Fig 1: 2-D classification problem. Green points are positive and red ones are negative. Can you guess the labels for the yellow points? Created using: https://github.com/ryu577/pyray

Now, if I drew a purple line that separates the two classes, it becomes much clearer which class each of the yellow points…"
Automating Traffic Analysis with Machine Learning,"Summary

Data science and machine learning present new opportunities for improving public spaces. Leveraging these technologies for smart cities can make our communities more livable, more sustainable, and benefit local economies. They can assist with understanding key questions in city planning and urban design such as how public spaces are used, how many users there are, and who the users are.

In this post, we’ll look at a proof-of-concept system we implemented to answer these questions using machine learning for video analysis. It’s an approach that’s easy to deploy and cost-effective. We’ll focus on street intersections in particular, where design impacts traffic congestion, local businesses, and pedestrian safety. The work here is motivated by our own experiences, the Vision Zero goal (eliminating serious accidents on the roads), and the Smart Cities initiative.

To learn more about our technology, send us a message.

“The best way to plan for downtown is to see how people use it today; to look for its strengths and to exploit and reinforce them.” — Jane Jacobs

Introduction

Automated traffic detection and tracking, showing bounding boxes of tracked objects and lane counter on West Cordova at Abbot Street in Vancouver.

City planners are tasked with making cities that are functional, livable, and support economic growth. In addition, they must consider environmental impacts, future growth, and the needs of a diverse set of users. Can we leverage data science and machine learning to support them in those tasks by increasing the information they have to make decisions? We’ve developed a prototype system based on automated video analysis, which can provide key information on how to improve public spaces. In this post we’ll look at traffic intersection in particular, since British Columbia is pursuing the goal of Vision Zero — no fatalities or serious injuries from road accidents.

What are some of Vancouver’s worst intersections? According to ICBC data for 2013–2017, there were 692 road accidents at the Main Street and Terminal intersection, 256 of which resulted in injuries or fatalities. For pedestrians, East Hastings and Main Street was top of the list with 35 accidents. Across town, Burrard Street & Pacific was the most dangerous for cyclists with 34 crashes. These accidents are a tragedy for those involved and their families, and impact us all through increased costs for the city and ICBC. If accidents regularly occur somewhere, then an intervention in the street design could save lives.

Video analysis can support agile interventions at dangerous intersections. It can help identify which behaviours lead to accidents, and how contributing traffic patterns arise. This knowledge can be used by cities to implement appropriate design changes. Afterwards, video can be used to understand the impact of any change. The success of an intervention can be measured without having to wait years to see if the statistics have improved. This means a municipality can iterate quickly if the design intervention didn’t have the desired impact. It also allows for knowledge specific to locations. What cyclists need at Burrard Street and Pacific is not the same intervention that would work at Main Street and East Hastings.

Traces of one minute of traffic at the Homer Street and West Georgia intersection in Vancouver.

Taking a video of a traffic intersection is cost effective, fast to setup, and easy to process. It doesn’t require permament infrastructure, only access to a suitable viewpoint. This means that cameras can be installed rapidly and for temporarily periods of times. The quality of videos we used could be replicated by the current generation of mobile devices. Video analysis can effectively replace or supplement current methods such as pneumatic tubes, induction loops, and manual counts. Advantages of video analysis include being able to distinguish traffic in different lanes, detect intersection turn patterns, and monitor events such as near-misses. Additionally, it can uniquely provide reliable pedestrian counts and movement patterns, quantifying information such as walking speed, stickiness, pedestrian density.

Traffic Analysis

Let’s look at applying machine learning to videos of street intersections, and the data we can pull out to support municipalities. We’ve constructed a proof-of-concept system, combining deep learning (neural networks) and object tracking algorithms, that can automatically process videos.

We’ll apply our system to analyze videos of two intersections in the downtown area of Vancouver. The first is Homer Street and West Georgia, and the second is Abbot Street and West Cordova. These locations offer different views of the streets, and different volumes of traffic. From the video of Homer Street at West Georgia, we’ll see that the system scales to handle large amounts of traffic. In the analysis of the West Cordova intersection, we’ll also see how we can automatically spot unexpected pedestrian crossings. After the initial setup of our codebase and system, analyzing further intersections was a simple task.

Traffic Analysis at the Homer Street and West Georgia intersection in downtown Vancouver shown at 2x normal speed. The Traffic Information panel displays a count of the different types of traffic. The colours in the panel correspond to the bounding boxes in the video. The Plan View panel shows a top-down view of traffic at the intersection. A larger view of the Plan View panel is shown in the next video.

Our dashboard overlays the video with the results of our computer vision algorithms. Objects in the traffic flow that have been identified have a bounding box, and are assigned a unique identifier in the scene. The colour of the bounding box corresponds to the entries in the Traffic Information panel. There we display the counts of the different types of users. In this scene we count the spectrum of traffic: pedestrians, cyclists, cars, trucks, and buses. In the lower left hand corner we have a plan view (top-down) of traffic that we extrapolate from our video data. A header at the top displays the location and time of the video.

Plan View of traffic at the Homer Street and West Georgia intersection in downtown Vancouver played at 2x normal speed. The Traffic Information panel displays a count of the different types of traffic, with colours corresponding to those in the Plan View Panel.

An advantage of using algorithmic counting is that it scales well with the volume of traffic. Our system counts over 50 cars and over 100 pedestrians in the video. Manually counting this amount of traffic, even in a short video clip, would be challenging and time intensive. We not only get a count of how users there are, but the path they took through the intersection. The traces of traffic from a part of the video are shown in the second image. We can compare the amount of vehicles driving through the intersection as opposed to turning. We can also notice a number of pedestrians begin crossing the street outside of the crosswalk lines.

Our system reprojects traffic flow into a top-down view. While we currently take a simple approach to this operation, a more robust system would enable several features. These include measuring the speed of vehicles, and the distances between different users. This would allow us to quantify near-misses, such as when a moving car comes within a certain distance of cyclist or pedestrian. Currently we have data on accidents that occur, but not on close calls that may cause users to take different routes or alter their mode of transport.

Traffic going East on West Cordova at Abbot Street in downtown Vancouver.

Automated analysis allows us to identify and quantify events of interest. We defined two counters in the video of West Cordova at Abbot street to determine how many cars use each lane. This information would be useful if the city wanted to remove street parking, install a dedicated bike line, or look at the impact of introducing bus priority lanes. Further, we can integrate the timing of the traffic lights into our video. We did this to set an alert when pedestrians cross the road unexpectedly. We can investigate whether this is a common occurrence and whether pedestrians aren’t given sufficient opportunities to cross at intended times.

Since our system is a proof-of-concept, our features can be improved upon, and new ones developed. There are lapses in the identification of traffic — bicycles can be misclassified as people, and the labelling of cars, trucks, and buses is sometimes mixed up. We also see that when there are is a large group of pedestrians, the system has trouble distinguishing people and tracking them as they get obscured in the crowd. More rigorous algorithms can be implemented to handle these situations. Our proof of concept relies on a single camera. We can further extend our system by incorporating cameras at multiple viewpoints, adding an infrared camera, and using dual-lens systems for depth perception.

Where else can we apply this technology? The city of Vancouver recently spent 5 million dollars renovating Robson Square. Is the space being used to its full potential? We can use automated video analysis to study how the space is used through the day. This includes quantifying the patterns of people using the space to travel, as a meeting place, and recreationally. A modern update to the time-lapse cameras used by noted urbanist William H. Whyte for studying public spaces in the seventies. Similarly, Granville Island is in the process of significant changes. These will impact pedestrian movement, as well as pedestrian/bicycle/car interactions. Reliable traffic information allows us to maximize both the investment in and the benefits of the space.

Conclusions

Leveraging machine learning, we’ve built a proof-of-concept system that automatically provides traffic information from video data. We’ve applied it to two intersections, showing that it track volumes of traffic that would otherwise be prohibitive to count manually, and that it can capture events like unexpected pedestrian crossings. This information can be used by municipalities to make design improvements at unsafe intersections, and test the effectiveness of interventions, saving lives and money. This simple and efficient system can positively contribute to the Vision Zero goal, eliminating serious accidents on our roads.

Are you an organization interested in this technology? Contact us for more information."
Achieving a top 5% position in an ML competition with AutoML,"Achieving a top 5% position in an ML competition with AutoML

AutoML pipelines are a hot topic. The general goal is simple: enable everyone to train high-quality models specific to their business needs by relying on state-of-the-art machine learning models, hyper-tuning techniques and large volumes of compute.

In this blog post I will be applying Microsoft’s AutoML pipeline to a public ML competition, and by dissecting the results I hope to learn where AutoML pipelines add significant value and areas where they can still improve. With minor coding efforts and the AutoML pipeline, I was able to get to the 5th position with only 0.004 points from the top F1 score! You can find the notebook that I used on GitHub.

Richter’s Predictor: Modelling Earthquake Damage

The competition I picked is hosted at DrivenData.org. It is an intermediate-level practice competition, where the goal is to predict the level of damage to buildings caused by the 2015 Gorkha earthquake in Nepal.

Image from the DrivenData competition.

The data is clean, without any missing values and mainly consists of information on the buildings’ structure and their legal ownership (both categorical and numerical features). In this competition, we are trying to predict the ordinal variable damage_grade , which represents a level of damage to the building that was hit by the earthquake. There are 3 grades of the damage.

The training and test dataset consists of respectively 260602 and 86869 buildings.

AutoML

AutoML is integrated into Microsoft’s Azure Machine Learning Service. This means it is relatively simple to execute and monitor AutoML pipelines using storage and compute in Azure.

The general workflow of an AutoML pipeline is fairly straightforward. You define the training dataset (the Richter dataset), a cross-validation scheme (5-fold cross validation), primary metric (weighted AUC), the experiment type (classification) and you are good to go. Depending on where you want to execute the experiment (on your local pc, a VM in the cloud or a large cluster) and where the data resides (local or in cloud), you also have to specify a data and compute configuration.

An AutoML pipeline consists of the following steps:

1. Given the experiment type, generate a set of initial pipeline parameters.

2. Execute the experiments with the different parameters and measure the primary metric using cross-validation.

3. Pick a new set of (optimal) pipeline parameters and continue until a threshold is reached. This can be a number of experiments, the time spent on the experiments or a different metric.

4. At the end, build an ensemble of the different models to obtain optimal performance on the test set.

Note that the general problem of selecting a good set of algorithms and hyperparameters is a complex problem. The search space (i.e. the model space) is very large and evaluating different experiments often takes a lot of compute and time. For quite a long time, hyper-tuning was done by performing either a random or full search of the model space. As compute is limited, this meant that these model spaces had to be heuristically constrained.

Nowadays, Bayesian methods are often seen as a superior approaches to hyperparameter optimization, although RL-based methods have also become quite popular in e.g. Neural Architecture Search. The general idea behind these methods is that they will ‘intelligently’ and ‘efficiently’ search the model space, trading off exploration (new model types) vs. exploitation (improving previously optimal model types).

Results

Directly applying the AutoML pipeline to the dataset results in a model that performs worse (F1 of 0.5441) than the competition baseline (a Random Forest model with F1 of 0.5815). Analysing the model results quickly illustrates why such a low F1 is achieved. The dataset is unbalanced (class 1: 7.5%, class 2: 52.5%, class 3: 40%). As the AutoML pipeline does not perform undersampling or oversampling, this means the different models in the AutoML pipeline are trained on unbalanced datasets.

The confusion matrix of the AutoML pipeline also confirms the bias towards the two majority classes that is present in the model. As the competition is evaluation on F1 micro score, this model doesn’t perform very well.

Although the AutoML does incorporate some feature engineering, preprocessing, missing value imputation, it is still important to provide a clean and balanced dataset to the AutoML pipeline.

To improve the results, I took the following steps:

Verify that the different features are converted to the correct format (numerical or categorical). Perform synthetic data generation by leveraging SMOTE. This ensures that the data is balanced, without having to undersample or throw any of the data away.

The experiment was deployed on AzureML compute (managed VM’s) and 100 experiments were run with the AutoML pipeline.

As you can see in the plot below, running the pipeline for even more experiments wouldn’t have lead to a significant improvement in terms of performance, although this is most likely necessary to get to the 1st place in the competition.

Zooming in on the confusion matrix, it is clear that the model performs better, although there is still significant confusion between the second and the third class. Additional data augmentation techniques (e.g. undersampling or oversampling) or manual feature engineering might help in this scenario.

The following graph also showcases the importance of some of the features. Clearly, there are a lot of (sparse) features that have limited impact on final performance. As such, it is worthwhile to experiment with leaving out these features. Primarily the location features have the biggest impact, which is as expe

The final ‘best performing’ model achieves an F1 score of 0.7419. This is slightly lower than the best score in the competition, but considering the amount of work that was involved to get these results, I’d say this is a quite promising result for AutoML.

Conclusion

AutoML is a great addition to any data scientist’s toolbox. Hyperparameter tuning frameworks are not new (e.g. hyperopt), but a framework that easily allows data scientists to combine compute, data and experimentation in a single Python package is a great improvement. That said, in order to get the most out of these tools, it is necessary to perform the proper data preprocessing, to select the right optimization metric, etc. In addition, in order to avoid spending large amounts of compute and time on these workloads, it is possible to narrow down the search space based on data exploration and experience.

If you have any questions on AutoML, I’ll be happy to read them in the comments. Follow me on Medium or Twitter if you want to receive updates on my blog posts!"
A few things you should know before going into Tech,"A few things you should know before going into Tech

Median Salary in Tech in the US (2018), based on Operating System used

There are few industries out there which are as dynamic and booming with new trends like tech. Every year there are new frameworks, programming languages and technologies disrupting the industry in its very core. Tech is surely one of the most interesting and lucrative job scenes to work nowadays.

While looking for a job in this field, a newcomer should definitely do some research on the actual trends and skills needed in the industry. Whether one is interested in the newest technologies or simply wants a well-paid job in tech, it is important to know what’s hot and what’s not.

This article offers some interesting insights based on questionnaires conducted by Stackoverflow. Questionnaires date from 2013 and 2018 and contain around 20.000 (clean) observations. To ensure that results are comparable, the data sets focus on the US Market and on full-time job positions only. The questionnaire structure between the years has changed significantly, but after extensive data cleaning, interesting conclusions could still be made. We will go through them in the following passages."
Is ReLU ReLUvant?,"Despite such fancy activation functions available, most of the recent CNN models have adopted only ReLU, which probably means that if not best, it performs as good as any other activation function. The probable reason being: it is simple and makes it easier for backpropagation to rectify the weights and maintain fewer parameters. Hence it helps the model to train and converge faster.

This majorly justifies the preference of ReLU, but we haven’t got the answer to our second question which is: Is there a need to use an activation function after each convolution layer within the network? To answer that, let’s begin our experiment.

Experiment

Let us begin with a ResNet18¹ model with a fully pre-activated ResNet¹ block. In this experiment, we will randomly drop some ReLU activation functions starting from 100% and dropping by 10% in each succeeding experiment keeping all other parameters consistent.

For this experiment, we will be using the CIFAR10² dataset.

Hyperparameters

BATCH_SIZE = 512

MOMENTUM = 0.9

WEIGHT_DECAY = 5e-4

LEARNING_RATE = 0.4

EPOCHS = 24

ReLUPercent =100

Image Augmentation

Height and Width shift range = 0.15625,

Rotation range = 12

Random LR Flip

Optimizer

SGD with momentum of 0.9 and weight decay of 5e-4 is used. One Cycle Learning Rate Strategy with a gradual drop towards the later epochs which has a MaxLR of 0.4 at 5th epoch.

Architecture

The model has 4 ResNet¹ blocks. Unlike traditional ResNet¹, at the end of a ResNet¹ block, we concatenate the input and residual path followed by a pointwise convolution to filter the information being taken ahead.

The model has 25 activation layers (24 ReLUs and 1 softMax).

Benchmark

On the CIFAR10² dataset, this model gives a validation accuracy of≈88% in 24 epochs under 800 secs on Google Colab. This model has ≈ 27M parameters.

Training"
How to precisely align face in Python with OpenCv and Dlib,"How to precisely align face in Python with OpenCv and Dlib Volodymyr Kovenko · Follow 6 min read · Aug 11, 2019 -- 3 Listen Share

Google photo

Introduction

Hello, dear readers! Today, I’d like to share a method of a precise face alignment in python using opencv and dlib. First of all, the code I will further consider was written as the part of a bigger project (which I hope to write article about soon), where it was used as a preprocessing tool. Secondly, the one can wonder, why does he need to read all this stuff about yet another face-alignment application? That’s because this method doesn’t rely on the position of the eyes only, but also depends on the position of the nose, meaning that it’s much more accurate. With all this said let’s get started!

Algorithm

The algorithm consists of 7 stages (the gif describing the method is pinned below).

Nose/eyes detection. Get the central coordinates of each eye and nose. Make a triangle using nose and eyes coordinates, find the median of this triangle. Face detection. Get the coordinates of the centre of a top side of a face rectangle (center of a top side). Make a second triangle using the coordinates of the end of the median, nose coordinates and center of top side coordinates. Find the angle between the median and the side of a second triangle (between black and orange lines). Rotate the endpoint of the median around the nose coordinates with respect to the obtained angle. If rotated point lies in the space of the second triangle (belongs to it), than the final angle (the angle we will use to rotate our image) equals to negative angle. Otherwise final angle equals to our origin angle. Rotate the image by the final angle.

Also the sheme of the algorithm is shown below :

Implementation

The algorithm was implemented in Python using OpenCV and dlib libraries as the main ones. In this article I will describe only the dlib mode of the algorithm, as opencv mode actually does the same job (only with some small changes in a few functions) and dlib is more accurate.

Firtly we need to load the libraries we will use:

import cv2

import numpy as np

from PIL import Image

import dlib

Then, let’s load the dlib detector :

detector = dlib.get_frontal_face_detector()

predictor = dlib.shape_predictor('shape_predictor_5_face_landmarks.dat')



Given an image of a person we can determine eyes and nose coordinates using dlib :

gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

rects = detector(gray, 0)

if len(rects) > 0:

for rect in rects:

x = rect.left()

y = rect.top()

w = rect.right()

h = rect.bottom()

shape = predictor(gray, rect)

Here we firstly cast our rgb image to grayscale in order to make a prediction on it. After that, if the face was detected we retrive the coordinates of a face rectangle and the shape of our facial landmarks (eyes and nose coordinates).

In order to continue working with facial landmarks we need to cast them to the normal representation. We do it using this two functions :

def shape_to_normal(shape):

shape_normal = []

for i in range(0, 5):

shape_normal.append((i, (shape.part(i).x, shape.part(i).y)))

return shape_normal def get_eyes_nose_dlib(shape):

nose = shape[4][1]

left_eye_x = int(shape[3][1][0] + shape[2][1][0]) // 2

left_eye_y = int(shape[3][1][1] + shape[2][1][1]) // 2

right_eyes_x = int(shape[1][1][0] + shape[0][1][0]) // 2

right_eyes_y = int(shape[1][1][1] + shape[0][1][1]) // 2

return nose, (left_eye_x, left_eye_y), (right_eyes_x, right_eyes_y)

Now we can obtain the central coordinates of nose and eyes :

shape = shape_to_normal(shape)

nose, left_eye, right_eye = get_eyes_nose_dlib(shape)

Let’s find the center of the line between two eyes (endpoint of the median) using this formula:

Midpoint of a line having two coordinates

center_of_forehead = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)

Using dlib we had already obtained the coordinates of our face rectangle, now let’s find the center of its top side:

center_pred = (int((x + w) / 2), int((y + y) / 2))

The next thing we do is finding the median of the first triangle along with two other sides of the second triangle (we don’t need to build a first triangle itself as we already know start and end point of the median). Knowing two points (start and end point) we can find the distance between them using this formula :

Distance between two points

def distance(a, b):

return np.sqrt((a[0] - b[0]) ** 2 + (a[1] - b[1]) ** 2)

length_line1(our median), length_line2 — lines between which we need to find the angle.

length_line1 = distance(center_of_forehead, nose)

length_line2 = distance(center_pred, nose)

length_line3 = distance(center_pred, center_of_forehead)

In order to find the angle between two sides of a triangle , knowing three of them, we can use a formula from a cosine rule :

Cosine Rule

def cosine_formula(length_line1, length_line2, length_line3):

cos_a = -(length_line3 ** 2 - length_line2 ** 2 - length_line1 ** 2) / (2 * length_line2 * length_line1)

return cos_a

Let’s retrieve the angle in radians:

cos_a = cosine_formula(length_line1, length_line2, length_line3)

angle = np.arccos(cos_a)

Now in order to understand what the final angle we will use to rotate our image is, we need to rotate the end point of a median and check if it belongs to the space of the second triangle. In order to cope with it, we will use this functions :

def rotate_point(origin, point, angle):

ox, oy = origin

px, py = point



qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)

qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)

return qx, qy





def is_between(point1, point2, point3, extra_point):

c1 = (point2[0] - point1[0]) * (extra_point[1] - point1[1]) - (point2[1] - point1[1]) * (extra_point[0] - point1[0])

c2 = (point3[0] - point2[0]) * (extra_point[1] - point2[1]) - (point3[1] - point2[1]) * (extra_point[0] - point2[0])

c3 = (point1[0] - point3[0]) * (extra_point[1] - point3[1]) - (point1[1] - point3[1]) * (extra_point[0] - point3[0])

if (c1 < 0 and c2 < 0 and c3 < 0) or (c1 > 0 and c2 > 0 and c3 > 0):

return True

else:

return False

The function rotate_point — rotates point by an angle around the origin point, while function is_between when given three tops of the triangle and one extra_point checks if the extra point lies in the space of the triangle.

rotated_point = rotate_point(nose, center_of_forehead, angle)

rotated_point = (int(rotated_point[0]), int(rotated_point[1]))

if is_between(nose, center_of_forehead, center_pred, rotated_point):

angle = np.degrees(-angle)

else:

angle = np.degrees(angle)

Knowing the final angle in degrees we can rotate our image using Pillow library or OpenCV :

img = Image.fromarray(img)

img = np.array(img.rotate(angle))

Summary

To sum up, knowing some geometry and open source tools, the one can construct quite a precise face-alignment application.

That’s all guys, if you want to see the overall code, feel free to visit the github page of the project :

Hope this article was useful for you! Thanks for reading and good luck!"
Why your AI might be racist and what to do about it,"Individually reasonable correlations can cause an AI to gain a racial bias

Even well-designed AI systems can still end up with a bias.

This bias can cause the AI to exhibit racism, sexism, or other types of discrimination. Entirely by accident.

This is usually considered a political problem, and ignored by scientists. The result is that only non-technical people write about the topic.

These people often propose policy recommendation to increase diversity among AI researchers.

The irony is staggering: A black AI researcher is not going to build an AI any different from a white AI researcher. That makes these policy recommendations racist themselves. It still makes sense to increase diversity among AI researcher for other reasons, but it certainly won’t help to make AI system less racist.

Racism among humans is a social problem, but racism in an AI is an engineering problem.

Racism in an AI needs to be addressed just like any other kind of engineering problem. Getting political is likely to backfire and can cause more harm than good.

So how can an AI be racist, exactly?

The good news:

An AI has no political agenda of its own. It is only going to be deliberately racist if it has been trained to be racist.

The bad news:

It is very easy to train an AI to be racist by accident.

In this article, I’m going to explain how racial biases can appear in AI. I will also discuss some ways to deal with this problem on a technical level.

(By the way: The same arguments also apply to biases against women or other types of discrimination.)

1. Biased data

It is possible to make an AI biased by training it on biased data. This can easily happen by accident unless you are very careful.

Take for example this article, about an AI that was trained on public data. The AI ended up with a racial bias because its training data was based on the internet: https://www.theregister.co.uk/2019/09/05/ai_racist_sexist/

As we all know, the internet is not the nicest place. An AI trained on it will adopt its preconceptions, and turn out horrible. It’s a general principle of training an AI: garbage in, garbage out.

It is also possible to use reasonable data without any racial bias, and still end up with a biased AI anyway:

The dataset must have a representative amount of data from each racial group. That’s because the amount of effort an AI puts into learning about a race is proportional to its frequency in the dataset. Face recognition AI’s tend to work better on white people than any other race. Skewed training datasets are part of the reason for this.

If you are conscientious, it is not too difficult to fix this problem. Often, you can choose your dataset more diligently, so it contains less bias and has a more representative distribution of races.

Where that isn’t possible, you can at least annotate your data with indicator variables about their source. In this way, you can teach the AI to model biases in the training data explicitly. After that, pick the most racist of the data sources. Tell the AI to unlearn anything that differentiates that data from the rest. This is like pointing at the most racist people and ordering the AI not to emulate them. It’s not a perfect solution, but it is better than nothing.

Note that problems can also arise even if the training process is unbiased. It’s possible that a particular AI algorithm is just objectively less capable of some tasks than others.

For example: a self-driving car has a harder time detecting black people at night than white people, because their skin is harder to see in the dark. This is not racist, just unfortunate. It’s obviously still a problem, though. To fix it, you need to ensure that you put an appropriate amount of effort into teaching the AI how to solve these more difficult tasks.

2. Correlation does not equal causation

Once the training data is fair and unbiased, we still have to deal with a problem in the AI itself. It doesn’t really matter if you are using a neural network here, or something else. Virtually all popular forms of AI suffer from this:

The core problem we have is that the AI has no idea what any of its inputs mean in reality.

The AI just gets numbers as input, without understanding their real-world implications. It has to learn that causality on its own, but it can only make guesses, and will often turn out to be wrong.

For example, suppose we are training an AI to accept or reject job applications. One neighbourhood in the city is a crime-ridden ghetto. All previous applicants from the area were bad. The AI ‘learned’ that coming from this area means you are a terrible applicant. Now the AI receives a new application: A young black woman, who won a Nobel prize. That woman has really bad luck: The AI has no idea what a ‘Nobel prize’ is because it has never encountered one in an application before. It just notices that she comes from the same neighbourhood where everyone before her was terrible. So the AI rejects the application immediately.

It gets worse:

Many machine learning algorithms are not explainable. That means it is not possible to make an AI explain the reasons behind its decisions.

There are some algorithms that are explainable, like decision trees. You would think that detecting racism in explainable algorithms is easy. This is only partly true.

In an explainable algorithm, you can check directly if ‘race’ is being used to make a decision. However, this does not allow you to notice indirect correlations. Maybe the AI ‘learned’ that growing up in a certain neighbourhood makes for a bad candidate. A neighbourhood that happens to be predominantly black. It takes effort to detect such correlations and account for them.

So to prevent racial bias, we have to find a way to detect spurious correlations. But we can’t just check for the effects of race on the data directly. That would not suffice, because we can’t rule out indirect correlations that still lead to a racial bias.

Even worse: We sometimes do get genuine correlations between race and other attributes. Let’s say for example that we are building a medical diagnosis system. We do want the AI to learn that some diseases appear more frequently in different races. If we blindly eliminate all racial correlations in a medical diagnosis system, we could get a lot of people killed.

So what can we do to solve this problem?

I can’t give a definite answer, because this is an extremely complex problem with a lot of special cases. The following approach could work as a baseline, but is by no means perfect:

First, make sure that you explicitly use race as an input of your AI. This goes against what you normally hear in political correctness lectures. Unlike a human, an AI does not have an implicit bias, nor does it adopt any biases from its creator. This means that adding race as a feature makes it easier to test for accidental correlations, and has no negative effects.

Create a Generative Adversarial Network (GAN). Train the GAN to create fake training data.

This takes care of any accidental correlations in the data: A fake created by the GAN will be created in such a way that the AI can’t tell the difference. This does not guarantee that the AI won’t learn spurious correlations. That task is impossible. It will however guarantee that the AI is unable work around its own anti-racism mechanism in the later stages.

Now comes the tricky part: Pick some features of the training data that you believe should not influence the prediction. For any person in the training data, you can now ask the GAN to generate an ‘equivalent’ person of a different race. While generating these fake people, you must only vary the selected safe features. This ensures that the fake person is realistic, but does not have strange characteristics that would throw off the prediction.

You now have the ability to create fake data to reduce the bias in your dataset. Because these fakes differ only in features that are irrelevant to the task, this will not have any negative effects.

Note that this only works if you have made a good choice for the features that may be altered.

It is also possible that the GAN will not be able to create any good fakes using only the irrelevant features. If this happens, you probably have a lot of genuine racial correlations in the data.

Once you are done with training the GAN, the actual predictive part begins:

Build your main AI system, the one that should make a prediction, on top of the Discriminator network of the existing GAN. This ensures that the predictor network will only use derived features that we know can’t contain a racial bias.

Now train the new system with all your data.

During the training you can explicitly test for racism. For any given person, you can create an equivalent person of another race and test if the prediction is different for that person. By giving negative feedback on each such case, you can punish racism in the AI directly.

You now have a trained neural network that does whatever you wanted it to do, and that is very unlikely to have a racial bias.

Disclaimer

You should not just blindly implement this the way I just described it. Depending on your usecase, there could be a lot of side effects. It adds a lot of complexity and multiple interactions, which could backfire in any number of ways. This is really just a general description of how the problem of racist AI could be tackled in theory. It’s not ready for implementation.

If anyone has an idea for a better approach, I would welcome hearing about it in the comments.

Also note that this adds a lot of overhead to model training. It could reduce the performance of the AI as a side effect.

Is it worth having a less racist AI, if the AI is also dumber and makes more mistakes? What if that AI makes life-or-death decisions? How do you even quantify what counts as a worthwhile tradeoff here? This is a political question that I am not touching with a ten foot pole.

Your main takeaways from this article should be:

AI can easily be racist by accident.

2. Preventing this is very difficult.

3. This is primarily an engineering issue, not a social issue."
10 Medical Innovation in the current Year….is Google and Microsoft taking the lead?,"Introduction

2019 has almost halfway gone by and I hope everyone is having a great year and there weren’t any serious health issues with anyone. With that let’s continue this discussion let’s talk about the medical industry.

The medical industry is an interesting field as a whole. There are A lot of government regulations and private firms trying to “get in” the action to make a profit. So what are some of the most promising technologies in the medical industry happening in 2019? And what are some tech giant doing to take advantage of the industry and penetrate the field?

To find some of the answers to those questions, we need to pay our attention to Cleveland Clinic’s Medical Innovation Summit, or MIS for short. Because they just revealed the top ten medical innovations. Finally, for the technological part, we need to look at Google’s 2018 I/O summit, as well as Ellie Patient Engagement and Care Management from Microsoft.

Starting With the List

Starting from the bottom we have:"
Clothes and color extraction with Generative Adversarial Network,"Clothes and color extraction with Generative Adversarial Network

In this post I’m going to talk about clothes and color extraction from images using Generative Adversarial Network (GAN). Aytan Abdullayeva · Follow Published in Towards Data Science · 6 min read · Sep 11, 2019 -- 3 Listen Share

Let’s briefly go through the key notions used in this article.

Some key notions:

Image segmentation: In computer vision, image segmentation is the process of partitioning a digital image into multiple segments (sets of pixels). The goal of segmentation is to simplify the representation of an image into something that is more meaningful and easier to analyze.

There are two levels of granularity within the segmentation process:

Semantic segmentation — classifies an object’s features in the image and divides sets of pixels into meaningful classes that correspond with real-world categories.

— classifies an object’s features in the image and divides sets of pixels into meaningful classes that correspond with real-world categories. Instance segmentation — identifies each instance of each object featured in the image instead of categorizing each pixel like in semantic segmentation. For example, instead of classifying nine chairs as one instance, it will identify each individual chair (see image below).

Semantic Segmentation vs Instance Segmentation (source)

Neural Network is a computational learning system that uses a network of functions to understand and translate a data input of one form into a desired output, usually in another form. The concept of the artificial neural network was inspired by human biology and the way neurons of the human brain function together to understand inputs from human senses.

Generative Adversarial Network(GAN) is a specific type of neural network(Invented by Ian Goodfellow and his colleagues in 2014) which consists of two networks: the Generator part and the Discriminator part. In the training process these networks compete with each other, thus improving one another. The training process starts with random noise, the generator generates output from the noise (because input is some random data, initial outputs are also noisy). The Discriminator compares output with the data from the dataset and realizes that it is far from the pieces from the original dataset so it returns a negative result to the generator. Over time, the generator produces better outputs according to the feedback collected from the discriminator. In the end, the discriminator finds it hard to understand whether the output produced by the generator is original or fake. The key point of this training process is to find the probability density function for the training set and using this function to generate new pieces which do not exist in the dataset.

So many interesting applications of GAN exist: text-to-image, image-to-image translations, image resolution enhancement, detecting fake paintings, fake face generator (by NVIDIA) etc.

In this post I’m going to focus on Image-to-image translation.

Clothes segmentation model

For the training process I’ve used Clothing Co parsing(CCP) dataset which contains:

2, 098 high-resolution street fashion photos with 59 tags in total

A wide range of styles, accessories, garments, and poses

All images are with image-level annotations

1000+ images are with pixel-level annotations

Clothing Co parsing dataset

For training purposes only 1000 pixel-wise segmented images are used. As a network model I used pix2pix implementation of GAN in Keras from the following repo source. I’ve changed the initial code slightly. The size of the model’s input/output was 256x256(input image resolution). For better quality I changed this size to 512x768.

Why did I decide to use a GAN network? Segmentation tasks could also be done by CNNs. But in comparison to CNNs, a GAN model could learn from lack of data and generate better results. In the training process, the generator saw both original images and their paired pixel-wise segmented versions from the training set.

First, I tried to train the network with an original CCP dataset, meaning for 59 classes. The experiment failed because the distribution of different types of clothes in the dataset was different, and also, the training set contains a small number of instances per some classes. For example, the number of trousers/pants is about 400, while the number of t-shirts is less than hundred. So an imbalanced and small dataset caused a poor quality of segmentation in inference mode. Then I experimented with merging some classes together (pants/trousers/jeans, all types of shoes, all types of bags etc.). This step reduced the number of classes to 29, but still the quality of segmentation was not as good as desired.

Segmentation results for 29 classes

Finally, I succeeded with a model for “four classes”: background, skin, hair and all clothes and accessories on the person. This kind of dataset is balanced, because all images in the training set contain instances of all four classes (almost all images with hair). The model trained 3000 epochs for 1k images.

Sample image from dataset for “four classes”

While testing in inference mode with hundreds of images, I realized that the model is sensitive to background: input images for non-blurred background, clothes and background have similar colors. These are cases of failure. For the artificial neural network model, cases of failure are normal, since it doesn’t learn things as the human brain does. Because the network learns from shapes, colors, and a lot of features which are not obvious to humans, we can’t predict with certainty how it’ll work in different cases.

Left couple: background is not blurred, right couple: background is similarly colored to clothes

But our model works quite well for images with blurry backgrounds:

Positive results

So how can we solve this problem? I tried to replace the background of the original image with a solid color manually and realized that with this kind of input the model produces much better results. But how can this job be automated? Mask-RCNN to the rescue!

Mask R-CNN is an extension of the Object detection algorithm Faster R-CNN with an extra mask head. The extra mask head allows us to pixel-wise segment each object and also extract each object separately without any background.

A sample mask R-CNN output trained on COCO-dataset (source)

I’m going to use a mask (for a person) generated by the Mask-RCNN model.

I used the following implementation of mask-rcnn in keras: source

So before feeding the input image into our model, the preprocessing job is done. First the person is segmented from the input image using the Mask-RCNN model and the mask it produces. With this mask we could substitute all pixels which do not belong to the person with a solid color. In our case it’s grey. I tried 3 colors (black, white, grey) for the background, and only grey passed the tests (The reason for this is not clear to me, it’s magic! I hope to find the answer very soon).

Thus instead of feeding original image to the model, background changed version is provided as an input.

From left to right: original image, segmentation result by our model, result after preprocessing with Mask-RCNN model. (case1 image source)

Model use cases

So far so good. But how can we use these pixel-wise segmented images?

It’s possible to cut clothes from the image, and besides that to get the main colors of the clothes, as well as skin and hair color. The cut off process for different classes from the segmented image is done by color thresholding, thus we get masks for them. For this purpose I’ve used HSV (Hue, Saturation, Value) color space. The reason why we would use this space instead of RGB is because HSV space describes colors similarly to the way the human eye tends to perceive them. In RGB color space all three components are related to color light, but in HSV space, hue vary relatively less than the changes in external lightning. For example, two shades of red might have similar hue values, but totally different R, G and B values."
Review: DeepPose — Cascade of CNN (Human Pose Estimation),"Review: DeepPose — Cascade of CNN (Human Pose Estimation)

In this story, DeepPose, by Google, for Human Pose Estimation, is reviewed. It is formulated as a Deep Neural Network (DNN)-based regression problem towards body joints. With a cascade of DNN, high precision pose estimates are achieved. This is a 2014 CVPR paper with more than 900 citations. (Sik-Ho Tsang @ Medium)"
Why Motivation is the Key to Learning Data Science,"During my journey in studying data science outside of formal education, I found that motivation was the key to navigating the complexity of the subject and not getting disheartened by the wealth of information that makes up the well-publicised data scientist skillset. In the following article, I am going to talk about why motivation is so important when studying data science, explain how you can get and stay motivated, and share with you a copy of my personal learning curriculum and scoring system for learning data science.

Why is motivation so important when learning data science? If you Google something like ‘what skills are needed for a career in data science’ you will be presented with an extensive list of skills.

These will include things like:

Software Development

Computer Science

Statistics

Mathematics

Data Engineering

Domain Knowledge

Machine Learning

Deep Learning

Communication

and many more

Data science is cited as being its own discipline, but each of these areas is a discipline in itself. How then do you learn what appears to be an insurmountable set of skills in a reasonable amount of time? How do you know which skills to learn first? When starting out it can be very easy to quickly become demotivated and abandon your learning in the face of what appears to be an almost impossible task."
How does a neural network make predictions?,"Recently, neural networks have grabbed a lot of attention. They are a computing system of interconnected nodes and act similarly to how a brain functions. This system clusters large, raw data sets, and by finding patterns they can then solve very complex problems and classify inputs or even make difficult predictions. The most amazing part? They have the ability to never stop learning!

At the core of a neural network, the basic building block of one, is a perceptron. Perceptrons are necessary because they break down complex inputs into smaller, simpler pieces. If you were to take a picture of a face and break it down, you would most likely think of common facial features: eyes, nose, eyebrows, mouth, ears, etc. Each one of these can be a perceptron in a single layer. In the next layer, these features can be broken up into smaller features. For example, a left and right eye, upper and lower lip, and those features can be broken up in another layer into smaller features like a pupil and iris, or eyelashes, etc. Each one of these features can be a perceptron, and after breaking it down into the smallest features, it will have the building blocks of a face.

For this example, image classification will be used to show a more descriptive explanation of how a neural network makes predictions. A neural network can start by taking a picture of a face and breaking it down into certain features, and rebuilds it by telling the next layer if the features are there. By the end, depending on how many 1 (or true) features were passed on, the neural network can make a prediction by telling how many features it saw compared to how many features make up a face. If most features are seen, then it will classify it as a face. Otherwise, it will be classified as not a face (notice how it is not classified as something else, it is either a face or not a face, true or false).

Beware though, a perceptron and a neuron are not the same things. Although they do sound the same, a neuron is different from a perceptron. A perceptron is a unit with weighted inputs and a bias, which produces a binary output. A neuron is a generalization of a perceptron in an artificial neural network. A neuron still takes in a weighted input and a bias but this is where it differs from a perceptron: it produces an output which is a graded value between 0 and 1. Note that nodes are biased to choosing an extreme value close to 0 or 1 using the sigmoid activation (output) function, which allows it to function very similarly to a perceptron.

Overall, a neural network is a very simple idea, but large networks can produce amazing results. Each neuron is responsible for classifying a single feature and counts on the previous neuron to do its job properly in order to make an accurate decision itself. Very similar to any good team, generally speaking, they value trust and teamwork above all else. It is no wonder why they are so powerful together."
Decorating functions in Python,"Decorating functions in Python

In this post, I’ll be explaining what Python decorators are and how you can implement one. Let’s get started!

What is a decorator?

A decorator is no more than a comfortable way to call a high-order function. You’ve probably seen it many many times already, it’s about those “@dostuff” strings that you find from time to time above a function’s signature:

@dostuff

def foo():

pass

Ok, but now, what is a high-order function? A high-order function is just any function that takes one (or more) function as an argument and/or returns a function. For instance, in the above example, “@dostuff” would be the decorator, “dostuff” would be the name of the high-order function and “foo” would be the decorated function and the parameter of the high-order function.

All clear? Great, let’s start implementing our first decorator!

Building our first decorator

In order to start implementing our decorator, I’ll introduce you to functools: Python’s module for high-order functions. Specifically, we’re going to be using the wraps function.

Let’s create a high-order function that will print the execution time of the decorated function: we’ll call it “timeme”. In this way, every time we want to compute the execution time of a function, we’ll just need to add the decorator “@timeme” above the signature of the target method. Let’s start defining the signature of “timeme”:

def timeme(func):

pass

As mentioned before, a high-order function takes another function (the decorated one) as its argument, so we’ve included “func” in its signature. Now, we need to add a wrapper function that will contain the timing logic. For this purpose, we’ll be creating a “wrapper” function that will be wrapped by functools’ wraps function:

from functools import wraps def timeme(func):

@wraps(func)

def wrapper(*args, **kwargs):

pass return wrapper

Note that “timeme” is returning the function “wrapper”, which in turn, besides printing the execution time, will return the result…"
"What Is Neuralink: A Look At What It Is, What It Wants to Be, and What It Could Become","When I first took science class in elementary school, we learned about the five senses. Maybe not far into the future, that information might be as outdated as the idea of nine planets in our solar system (sorry, Pluto). This might be thanks to Neuralink, a company that’s been making headlines for its controversial brain-chip interface. On July 17, 2019, the company finally unveiled its hard work in a YouTube live stream.

When you first hear it, Neuralink’s pursuit sounds like it comes straight out of a mad genius’s diary; certainly, with Elon Musk at the head of the company, that image might not be a far stretch. But if you look beyond what seems to be a Sci-Fi horror movie, you can get a glimpse of quite a different future for humanity — one that has a new, sixth sense: Neuralink."
The Potential of an Intrusion Detection System Generative Adversarial Network (IDSGAN),"It is known that Intrusion Detection Systems (IDS) are weak against adversarial attacks and research is being done to prove the ease of breaking these systems. Many have begun to recognize the flaws in machine learning, and consequently, a framework called Intrusion Detection System Generative Adversarial Network (IDSGAN) is proposed to create adversarial attacks, which can deceive and evade any IDS.

To understand more about IDSGANs and how feasible it is to break into an IDS system, let us take a look at a Generative Adversarial Network (GAN) which is a generic framework used to fool machine learning algorithms. GAN is an adversarial AI system which is built to combat AI. It is a deep neural net architecture comprised of two neural nets, pitting one against the other (thus the “adversarial”), where a generator and a discriminator tries to outsmart the other. A generator modifies malicious version of the input it was originally given and sends it to be classified by the IDS and the discriminator. The goal of the generator is to fool an IDS, and the goal of the discriminator is to mimic the IDS on classifying inputs (correct or wrong) and provide feedback to the generator. The game ends when the IDS and discriminator cannot accurately classify the input created by the generator.

Source: tvtropes.org/pmwiki/pmwiki.php/Main/TotemPoleTrench

The concept of how the IDSGAN framework functions can be explained through an example. For reference, let the malicious input be monsters and normal input be people trying to enter a village (system/network) and the IDS be a gatekeeper who is protecting that village. The gatekeeper is very good at distinguishing monsters from normal humans and lets the humans into the village while fighting off the monsters. The monsters come up with a plan to disguise themselves to fool the gatekeeper and enter the village. One monster is appointed as the fake gatekeeper and is given the task of standing near the gatekeeper to learn how to identify humans and monsters as the gatekeeper does (discriminator). Another becomes a costume maker and learns how to create people-like disguises with the feedback from the fake gatekeeper (generator) to get these monsters into the village. The costume maker creates a costume and sends the monster over to the fake and real gatekeepers with other real humans, and the fake and real gatekeepers will classify the monsters as people or monsters. The fake gatekeeper will compare his decision with the real gatekeeper’s decision and learn which features/characteristics made the monster look like a monster, and send his feedback to the costume maker to make better disguises. When the fake gatekeeper can’t distinguish the monsters and people apart anymore, the costume maker has successfully created a disguise that can fool the real gatekeeper.

This is how the IDSGAN framework operates; a generator generates malicious traffic and mixes it in with normal traffic, and sends the traffic to the IDS and the discriminator. The discriminator and generator train in parallel. The discriminator tries to identify each piece of traffic it encounters and compares its decision with the IDS (loss vs loss), and adjusts its weights accordingly. Meanwhile, the generator learns how it is performing based on the decision the discriminator made. Eventually, the generator learns how to disguise a malicious input and creates a near-perfect model, which completely fools the discriminator.

The GAN is still a new concept and is still being developed. That being said, there are many applications of this in different fields, primarily image recognition. The IDSGAN is a new spin of this, and although this is still a very new framework, there is a lot of potential. The IDSGAN has already been proven to fool a simple IDS, and with enough time it can fool the most robust security systems. Hopefully, this development will be used to improve today's IDS to be able to defend against the most robust attacks. Because you wouldn’t want your information open to just anybody, right?"
Exploring How Neural Networks Work and Visualising Them,"Exploring How Neural Networks Work and Visualising Them

Deploying a neural network that can recognise handwritten digits to Excel so all the calculations and interactions can be visualised and inspected Mikkel Duif · Follow Published in Towards Data Science · 9 min read · Dec 30, 2019 -- 1 Listen Share

In this article we will be using an Excel file which can be found by clicking here. Even though you should be able to follow along without the file, I strongly recommend you download it to explore more in depth.

Preview of the Excel file | Mikkel Duif (2019)

Have you ever wondered how all the mathematics behind neural networks are actually working? The mathematics is often presented as vectors and matrices, and even though it makes it much simpler when studying, it can sometimes be difficult to grasp that you’re just doing the same calculations across hundreds, thousands, or millions of numbers. I’ve broken a one-layer neural network with 30 neurons down so you can see all the interactions from beginning to the end. You can also see how it adapts to changes in the input in real time, and input your own numbers to see what it predicts.

Note: This is a pre-trained neural network, so we will, for now, not go through all the mathematics of actually training it (which is much more complicated) but rather just look at how to use the outcome of a trained neural network. Moreover, the neural network has been kept simple to 30 neurons in 1 layer, however, it would not be very different for larger networks, maybe just harder to follow. I do not remember the accuracy rate of this network, but I imagine something around 90% (which is not terribly good for this dataset). I’ve also not loaded the real numbers that it’s supposed to be, but you can evaluate what number you would guess yourself (some of them are actually very difficult to evaluate even for humans). Lastly, this article does also assume some basic understanding of neural network, however, most people should be able to follow along though.

Understanding the data

Before we dig too deeply into the calculations, let’s just understand the data we will use, and thereafter the architecture of the neural network.

The dataset we will use is the MNIST Handwritten Digits dataset, which is basically just digits that have been written by humans and then scanned and converted into images of 28x28 pixels. The data that we use is based on the intensity of each pixel, so if an area is very dark it has a value closer to 1 and areas without any ink have values closer to 0.

Here you can see what one digit could look like and its corresponding values rounded to one decimal:

Visualizing the data | Mikkel Duif (2019)

As we have 28x28 pixels in total, this gives us a total of 784 input pixels, which are basically our datapoints. For our neural network, we will not work with this data in a matrix, but rather flatten the image (i.e. turn it into a vector), I have noted above the number of four different inputs (pixels) so you can follow along. The upper left corner is basically the first number in our vector, and the lower right corner is the last number in our vector.

Understanding the architecture of the model

Now that we know what our data looks like, let’s have a look at how the neural network is built for classifying these handwritten digits. A neural network basically consists of an input layer (i.e. our pixels), a hidden layer (which is just one for this model) and then an output layer (which is the digit that the model classifies from 0 to 9). Furthermore, it consists of weights and biases, which I’ll briefly explain below the image.

Visualisation of Neural Network | Mikkel Duif (2019)

Weights: As we have 784 pixels in total for each image, we will also have 784 input neurons in our model. As 784 neurons would be too much to display here, I’ve skipped most of the neurons, and just shown some input neurons in the middle that are not just zeros (try to locate them in the GIF). In the hidden layer, I’ve chosen to have 30 hidden neurons (with 20 of them skipped in the image above to keep it simple). Then finally our output layer with 10 neurons corresponding to digits from 0 to 9. All of the neurons are connected with a weight from the very first input neuron to each of the neurons in the hidden layer. I.e. input neuron 1 is connected with a weight to each of the 30 neurons in the hidden layer — and the same is input neuron 2, etc., all the way to input neuron 784. Doing some simple math, this gives us a total of 30 x 784 = 23,520 weights from the input layer to the hidden layer. Each of these weights has a value assigned to them and together they decide each of the values in the hidden layer. The weights can be found in the excel file on the Parameters tab in area A1:AB870.

The same goes from the hidden layer to the output layer. This time we just have 30 neurons in the hidden layer however, which is each connected to just 10 output neurons, giving us just 30 x 10 = 300 weights this time. Much less, but still too many to show every single weight. If you don’t understand yet how it all interconnects, don’t worry, in the next section I’ll explain how it’s all put together — but first, we just need to understand biases. The weights can be found in the excel file on the Parameters tab in area A871:A1180.

Terminology: a weight from neuron 1 in layer 1 to neuron 1 in layer 2 can be written as seen on the picture just above the very first weight, which corresponds to the value of cell A2 on the parameters tab of the Excel file. The weight from the very last input neuron to the very last neuron in the hidden layer, i.e. w(2–1,784) in the lower bottom part of the weights to the left can be found in cell AB870. Weight w(3–1,1) in cell A872 and weight w(30–10,30) in cell A1180. It is not that important where each weight is located — just for you to have an idea how to navigate around in the Excel sheet.

Biases: Besides weights, a neural network also has biases, which is something that is added to the value for each of the hidden layers and the out layers. These biases are not shown in the illustration above, but can be found in the excel file on the Parameters tab in area A1181:A1222.

N ote: you may be thinking, so where do these values for the weights and biases come from? Remember, this is a trained neural network, which means that we will not go through how to get these weights, but just how to use them. If you’re curious to know how to train a neural network, I recommend you to study Michael Nielsen’s resources who uses the same dataset — and who has inspired me a lot to study AI.

Understanding the calculations

Now that we have the weights and biases in place, let’s try to figure out how to calculate what we need. It is actually not that complicated, we just need to calculate 40 different numbers (30 for the hidden layers and 10 for the output layers). The output layer depends however on the calculations in the hidden layer (if we had more hidden layers each layer would always be dependent on the layer just before it). To keep the mathematical notation simple, let’s write the value of the hidden neuron like this:

Activation function for first neuron in hidden layer

Note: the superscript 2 does not means we are squaring the value, it simply means from the first layer (subscript 1) to the second layer (superscript 2).

For those of you who are familiar with the sigmoid function, this is basically it. However, in neural networks it’s often referred to as an activation function. Let’s just quickly clarify what the letters in the equation above mean.

e (simply Euler’s number, a constant approximately equal to 2.71828)

(simply Euler’s number, a constant approximately equal to 2.71828) I (the input neurons, i.e the value of each of the 784 pixels — same values for each hidden neuron)

(the input neurons, i.e the value of each of the 784 pixels — same values for each hidden neuron) W (the weights from each of the input pixels to the first neuron of the hidden layer — also 784 values, but new values for each hidden neuron, i.e. total of 23,520 different values)

(the weights from each of the input pixels to the first neuron of the hidden layer — also 784 values, but new values for each hidden neuron, i.e. total of 23,520 different values) B (the bias term belonging to the first neuron of the hidden layer — for the first hidden neuron this corresponds to cell A1182 which has a value of 0.43)

Notice that we are taking the dot-product between the input neurons and the weights. Remember, we have 784 input neurons and 784 corresponding weights (with the weights changing for each neuron in the hidden layer). The value of input neuron 1 and weight from input neuron 1 to neuron in the hidden layer are multiplied — we do this for each of them and take the sum of the product, to which we add the bias term. The negative value of this is taken as the power of Euler’s number, to which we add 1. We then finally divide 1 by the sum of these numbers. If we had to provide an example with all the numbers, it would be very long — but check cell AD2 which has the final value for this number — which in Excel can be written as:

=1/(1+EXP(-(SUMPRODUCT(number;weight_1_1)+INDEX(bias_1;1)))) Where the ‘number’ is the array of the value of the pixels from the number, and ‘weight_1_1’ represents the weights from the input layer to the first neuron in the hidden layer. I’ve named the arrays so the formulas make more sense — if you want to explore the arrays, go to Formulas->Define Name in Excel to see what the arrays represent.

The value of the first hidden neuron is shown in the image as 1.00, but it this is just due to rounding. It’s really 0.99676789. So what does that really mean? There is not really any easy way to interpret this — the good thing is however that it works.

Going back to the equation, we basically just have to apply this for each of the neurons in the hidden layer (30 in total), and we thus have the values of our hidden layer. It’s actually not very different doing this for the final output layer, the only difference is just that we do no longer use the 784 pixels as our inputs anymore — we use the values for each of the neurons in the hidden layers, i.e. the values we just calculated. Each of these neurons in the hidden layers has its own weight (A871:A1180) assigned to them (just as in the picture), as well as each of the neurons in the output layer has its own bias (A1212:A1222) attached to them. You find the final calculations for the output layer in cells AE1:AE11.

Draw your own numbers

Try to go to the ‘Draw’ tab of the Excel file to draw your own numbers and see how the neural network classifies your own digits. Hopefully it should do so a bit better — but can still be challenging, try to draw a 7.

Preview of the Excel file | Mikkel Duif (2019)

I hope this short article and the Excel file has been helpful for you to understand neural network better. Feel free to leave your comments or questions. Maybe I will make a new post in the future how to train a neural network and derive the weights and biases."
Exploring the full-text search index in Neo4j on a movies dataset,"The full-text search index was introduced in Neo4j version 3.5 with Apache Lucene powering the index. The agenda of this blog post is to introduce basic Lucene query language and describe how to use it in Neo4j. As you will see, we can also combine Lucene with Cypher to manipulate results.

I got most of the inspiration for this blog post from Christophe Willemsen, who is part of the Graphaware team. He has described the use of FTS very nicely for noobies like me, thank you!

We will be using the Movie dialog corpus dataset available on kaggle. It is rich with meta-data as it contains all the conversations between the characters in 617 different films. I have a couple of blog posts lined up using this dataset, but for starters, we will only use the data about the movies to explore the full-text search index capabilities.

Graph model

Made using apcjones.com/arrows/

We have two labels(Movie, MovieTag) in our graph. They are connected with the HAS_TAG relationship. Movies can have additional attributes like release year, IMDB rating, and the number of votes on IMDB.

Create constraints

To optimize import and later queries, we define unique constraints for both movies and movie tags.

CREATE CONSTRAINT ON (m:Movie) ASSERT m.id IS UNIQUE;

CREATE CONSTRAINT ON (m:MovieTag) ASSERT m.id IS UNIQUE;

Import

Copy the movie_titles_metadata.tsv file to $Neo4j/import folder before running the import query.

LOAD CSV FROM ""file:///movie_titles_metadata.tsv"" as row FIELDTERMINATOR ""\t""

MERGE (m:Movie{id:row[0]})

SET m.title = row[1],

m.release_year = toInteger(row[2]),

m.imdb_rating = toFloat(row[3]),

m.no_votes = toInteger(row[4])

WITH m, apoc.convert.fromJsonList(

replace(row[5],"" "","","")) as tags

UNWIND tags as tag

MERGE (mt:MovieTag{id:tag})

MERGE (m)-[:HAS_TAG]->(mt)

Full-text search index

In simplified terms, the lucene indexing pipeline consists of two steps. The first step is the “Analyzer” step, which takes care of the preprocessing of the text. In the second step, the “IndexWriter” stores the results of the “Analyzer” to the index.

Image from https://www.tutorialspoint.com/lucene/lucene_indexing_process.htm

In this blog post, we will be using the default “standard” analyzer in Neo4j. It tokenizes on non-letter and filters out English stop words and punctuation. Does no stemming, but takes care to keep likely product names, URLs, and email addresses as single terms.

You can check out other available analyzers.

CALL db.index.fulltext.listAvailableAnalyzers;

Create a full-text search index

First, we have to transform all of the attributes, that contain numbers, to strings as we can’t index integers or floats. The indexing process silently ignores numbers.

IMDB rating attribute is a number with a single decimal point. To convert it to a string, we will first multiply it by ten and then convert it to a string. For example, rating 6.1 will be saved as “61”.

MATCH (m:Movie)

SET m.string_rating = toString(toInteger(m.imdb_rating * 10)),

m.string_release_year = toString(m.release_year)

When using range queries, we have to be careful as the ordering is done lexicologically(alphabetically). In our case, all of the ratings are between “00” and “99”, so there should be no issue. If, for example, the range spanned to “150”, we would have an issue, as looking for intervals between “50” and “150” will not work by default. The workaround is described in Lucene range blog post. To allow range search between numbers with different count of digits, we need to preprocess the values by prepending zeros to them to ensure that all values have the same count of digits.

In this example, we preprocess all the numbers to have seven digits.

WITH 7 as total_length

MATCH (m:Movie)

WHERE exists (m.imdb_rating)

WITH m, total_length,

toString(toInteger(m.imdb_rating * 10)) as string_rating

WITH m, total_length — length(string_rating) as zeros, string_rating

WITH m, apoc.text.join([x in range(1,zeros) | “0”],””) +

string_rating as final_rating

SET m.range_rating = final_rating

Now we create the “MovieIndex” index, which contains nodes with label Movie and the four specified attributes. The first parameter is the name of the index. The second parameter defines the labels of the nodes, and the third parameter the attributes we want to index.

CALL db.index.fulltext.createNodeIndex(

""MovieIndex"",[""Movie""],[""title"",""string_rating"",""range_rating"",""string_release_year""])

Lucene queries

Let’s take a look at some basic lucene query operators.

Specific attribute

Search for movies that have the word “dream” in their title.

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""title:dream"") YIELD node, score

RETURN node.title as title, score

Results

Logical operator

There are two logical operators available, “OR” and “AND”.

Search for movies that were released in 1999 or 2000.

CALL db.index.fulltext.queryNodes(""MovieIndex"",

""string_release_year:1999 or 2000"") YIELD node, score

RETURN node.title as title, score

LIMIT 5

Results

Single-character wildcard

The single character wildcard operator ? looks for terms that match that with the single character replaced.

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""title:th?"") YIELD node, score

RETURN node.title as title, score

Results

One thing to note is that “the matrix” movie does not show up in results because the standard analyzer removes the stop words like “the”.

Multi-character wildcard

The multi-character wildcard operator looks for zero or more characters. You can also put the operator in the middle of the term like dre*am . To prevent extremely slow wildcard queries, a term should not start with the wildcard *dream .

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""title:drea*"") YIELD node, score

RETURN node.title as title, score

Results

Fuzzy search

Fuzzy search works by using mathematical formulae that calculate the similarity between two words. A commonly used method for calculating similarity is Levenshtein distance.

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""title:dream~"") YIELD node, score

RETURN node.title as title, score

LIMIT 5

Results

Range query

Lucene differentiates between range operators with inclusive or exclusive ends.

Operators:

{} ->excluding edges

[] -> including edges

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""string_rating:[50 TO 99}"") YIELD node, score

RETURN node.title as title,score

LIMIT 5

Results

To demonstrate why we need numbers with the same amount of digits when doing range search, let’s try the following query.

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""string_rating:[50 TO 100]"") YIELD node, score

RETURN node.title as title,score

Intuitively you would expect it to work just fine, but because of lexicological ordering, this is not the case. The workaround we used to solve this issue is the range_rating attribute, where values are prepended with zeros to allow alphabetical sorting between numbers with a different order of magnitude.

CALL db.index.fulltext.queryNodes(“MovieIndex”, “range_rating:[0000050 TO 0000150]”) YIELD node, score

RETURN node.title as title,score

LIMIT 5

Results

Boosting score

Lucene query language also supports boosting the score of the results with the caret (^) operator.

Boost results that have the string_rating between 50 and 99.

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""title:dream string_rating:[50 TO 99]^2"") YIELD node, score

RETURN node.title as title,score

LIMIT 5

Results

Time decay

I saw a lucene time decay query in the Nodes 2019 webinar by Christophe Willemsen and wanted to replicate it. Basically, we combine many boosting parameters to achieve a time decay effect. To show what that means we will use apoc.text.join() to generate nine boosting parameters.

RETURN apoc.text.join([

x in range(0,8) |

""string_release_date:"" + toString((date().year - x)) + ""^"" + toString(10-x)

],"" "")

Results

string_release_date:2019^10 string_release_date:2018^9 string_release_date:2017^8 string_release_date:2016^7 string_release_date:2015^6 string_release_date:2014^5 string_release_date:2013^4 string_release_date:2012^3 string_release_date:2011^2

As you can see movies with the release year 2019 will be boosted by a factor of ten, movies from 2018 will be boosted by nine and so on.

To wrap this blog post, let’s combine the rating-boosting and the time decay effect in a single query. And just so you know you can always manipulate results later with cypher, let’s add a filter that will return only thrillers.

WITH apoc.text.join([x in range(0,10) |

""string_release_date:"" + toString((date().year — x)) + ""^"" +

toString(10-x)],"" "") as time_decay

CALL db.index.fulltext.queryNodes(""MovieIndex"", ""title:dream string_rating:[50 TO 99]^2 ""+ time_decay) YIELD node, score

// filter only thrillers

MATCH (node)-[:HAS_TAG]->(:MovieTag{id:'thriller'})

RETURN node.title as title,score

LIMIT 5

Results

Conclusion

Full-text search is a very welcomed feature, that can help us develop better and faster Neo4j applications and also solve other problems. Join the Neo4j community forum if you have any questions or feedback.

All the code is available on Github."
Introduction: Fast R-CNN (Object Detection),"Introduction: Fast R-CNN (Object Detection)

A beginners guide to one of the most fundamental concepts in object detection. shafu.eth · Follow 3 min read · Jul 18, 2019 -- 2 Listen Share

This is the second part of a three-part series, covering systems that combine region proposals with Convolutional Neural Networks (CNN). In the first part, we covered the R-CNN system. You will need to read that first to fully understand this article. You can find it here.

In this part, we will talk about the fast R-CNN system. This paper was published one year after the original R-CNN paper and directly builds on top of it. The R-CNN paper was a major breakthrough in 2014, combining region proposals with a CNN. But it had some problems:

It was slow : You had to calculate a feature map (one CNN forward pass) for each region proposal.

: You had to calculate a feature map (one CNN forward pass) for each region proposal. Hard to train : Remember that in the R-CNN System we had 3 different parts (CNN, SVM, Bounding Box Regressor) that we had to train separately. This makes training very difficult.

: Remember that in the R-CNN System we had 3 different parts (CNN, SVM, Bounding Box Regressor) that we had to train separately. This makes training very difficult. Large memory requirement: You had to save every feature map of each region proposal. This needs a lot of memory.

Fast R-CNN System

So, how did the author try to solve those problems. The major thing introduced in this paper is the following:

We only have one system, that we can train end-to-end.

If you only take one thing with you from this article it’s this: We combine the three different parts that we had in the R-CNN system (CNN, SVM, Bounding Box Regressor) into one architecture.

Architecture

Fast R-CNN architecture. First image in the paper.

This architecture looks more complicated than it actually is. It works like this:

1. Process the whole image with the CNN. The result is a feature map of the image.

The CNN input and output

2. For each region proposal extract the corresponding part from the feature map. We will call this the region proposal feature map. We take the region proposal feature map from the feature map and resize it to a fixed size with the help of a pooling layer.

This pooling layer is called the Region of interest (RoI) pooling layer. If you want to find out more about it, here you can find a greate explanation.

RoI pooling layer

3. Then we flatten this fixed sized region proposal feature map. This is now a feature vector, that always has the same size.

4. This feature vector is now the input to the last part. These are fully connected layers that have 2 outputs. The first is the softmax classification layer, where we decide which object class we found. The second it the Bounding Box Regressor, where we output the bounding box coordinates for each object class.

Results

The fast R-CNN trains the VGG16 network 9 times faster than R-CNN. But the amazing thing about this system is this:

The inference is 213 times faster and achieves a higher mAP. Wow!

Conclustion

In the second part of this series, we talked about the fast R-CNN system and how it tried to improve the R-CNN system. Rember that only one year passed between the two papers and the system improved dramatically, especially in inference time. This illustrates how fast the whole deep learning space moves.

And again, the most important thing summarized in one sentence:

We now have one end-to-end system that we can train with back-propagation.

Thank you for reading and keep up the learning!

If you want more and stay up to date you can find me here:"
On the Journey to Machine Learning / AI,"What is Machine Learning?

There are millions of definitions about Machine Learning everywhere, I’ll point out here my favorite ones.

The analogy with the human brain is used as a guiding principle … The investigation mainly centers around an analogous teaching process applied to machines. Turing, A. Intelligent Machinery, 1948. A field of study that gives the computer(s) the ability to learn, without being explicitly programmed. Arthur Samuel, 1959

How do I define it?

Machine Learning is an area of science that helps you to detect patterns at a faster speed than we humans could do with the help of course of computers. Imagine it like applying all your math knowledge to data and applying techniques to make possible to take those patterns and get answers out of the data you are giving as an Input to algorithms.

How the programming paradigm is shifting with ML?

If you are a Software Engineer, the following explanation might resonate with you.

Imagine you are building a smartwatch application that will help you to detect the type of workout you are doing at the gym, and as of now and for the purpose of this example your application only detects when you are using the treadmill or the stair stepper.

Great! You have built-in some rules in the backend of the application to detect if it’s using any of those machines and calculate the calories, as observed below:

But, here is the catch:

What would happen in 1 year when 5 more machines with different brands come into the market and the owner of the smartwatch start using one of these machines you haven’t added yet?

What would happen when a Gym owner wants you to add all of his machines to your app, and he owns lots of brands and there are approx +20 machines?

On top of these new restrictions, you also should account for the person who is doing the exercise, weight, heart rate…etc.

Ok, this seems like a lot of rules to code in the application. This paradigm is still used in many applications and still applies to the common use of programming, as explained below:

Traditional Programming Paradigm

Rules and Data are the Input and We get Answers as the output :)

Example: You used today the treadmill for 15 minutes and your calories burned were 200 :)

What if I told you that you could turn this around and let the computer figure it out the rules

New Programming Paradigm

Data and Answers are fed into Algorithms and the outcome are the rules

Example: You let the algorithm know what are the features for using:

Machine A ( velocity, speed, inclination) and its values and you said: “This is what a Treadmill use looks like”

Machine B ( steps per minute, velocity, program ) and its values and you said: “This is what a Stair Stepper use looks like”

And so on and so forth, you feed the algorithm with lots of examples. So next time the smartwatch owner uses a new machine your smartwatch will be able to infer is using a leg press and will calculate everything on its own.

How great is this? To be able to input data and let the computer figure it out the rules for you so you don’t have to spend hours of your time writing infinite rules ;-)

Now, let’s dive into the last section of the post on which I’ll explain how useful this is and why do we want to actually use it.

Why do we want to use Machine Learning?

There is a lot of research being done around Intelligent Machines, and I have no doubts we might live at some point a new era of sharing our daily lives with those machines. We could even say we are doing it now with the presence of Smart Assistance, Recommendation Systems embedded in the software and applications we use daily, such as Netflix, Airbnb, Amazon between others, but still, I would say there are lots to be done and to experiment with.

If you have read about Machine Learning and Artificial Intelligence before, you might know there is a lot of controversy around this topic, specifically related to automation, machines taking our jobs and we all being fired due to the rise of Machine Intelligence. Well, let me tell you this is far from the truth.

From my point of view, Machine Learning / Artificial Intelligence should be seen as a complement to our skills, to our strengths but also to our weakness, ultimately as a tool where we help the computer by building the model and the computer help us to do calculations at the speed we cannot.

The benefits I’ve seen so far in Business, Society, and Life are massive, a couple of examples here:

The possibility to detect pneumonia from chest X-rays, a Deep Learning algorithm developed by Standford

Farmers Companion App, an application that helps to detect and identify when a crop has been infected by a caterpillar and advice you on how to treat it to stop it from happening.

The later is a good example of how we actually can make Machine Learning part of our lives and skills to boost our work and perhaps produce more with less in our business or simply a better world to live in.

If you are more curious about what types of Machine Learning are around and how you could start thinking if it’s worth it or not to apply it to your business use cases, stick around and follow me as I’m planning to write more about it in next posts."
My first small project in Python for browsing Reddit in office safely,"TL;DR: You can use this package to browse Reddit in Command Prompt and pretend you are working in the office.

What you can see in your command prompt when you browse subreddit ‘learnpython’ using braw

People always say learning in an interesting way is more effective than in a boring way. And I believe this is also true in programming. That’s why when I am now learning how to build a project on Python, I always look for some funny and interesting projects to give me a passion to work on. After a few days my first, small project is done and I call it braw (Browse Reddit At Work). At this moment, the package is only allowed for reading posts but not creating a new post or giving comments. This will be my second part of this small project.

The full code is here: https://github.com/wyfok/braw

Preparation

The Python Reddit API Wrapper (praw) is a Python package for wrapping Reddit. In my package, I will use it to gain access to Reddit and wrap content. For further information about praw, you can visit https://praw.readthedocs.io/en/latest/index.html

In order to use this braw package, first you need to have a Reddit account. And you also need to get your personal client ID and client secret from Reddit Preference ->App ( https://www.reddit.com/prefs/apps/)

name: Give a name

select script

description: blank

about url: blank

redirect url: http://www.example.com/unused/redirect/uri (it won’t use in the script, just for filling in)

Then in the next page, you will obtain your client Id and client secret"
5 Amazing Tips for Data Visualization,"5 Amazing Tips for Data Visualization

Shutterstock by majcot

Is there a such thing as data visualization block for analysts? Like a painter who stares at a blank canvas or a writer who can’t write that first sentence — you might be an analyst just staring at a wall of data in Excel.

Or maybe you just feel like an old dog that wants to learn a new trick?

This list contains my 5 favorite techniques for consistently pumping out effective data visualizations. It also helps prevent this dreaded data visualization block. Goodbye wall of data… hello insight!

Scatter plot your two most interesting metrics

While reviewing a new dashboard for a stakeholder, we got a comment that really got me thinking. They said, “This is great in describing exactly what happened, but there is not much insight for taking action.”

In Business Intelligence, it is often much easier to visualize what happened rather than visualizing actionable insights.

This is where the scatter plot comes in.

Scatter plots are famously used to assess correlation between two metrics. Often overlooked, they actually have another use case. This variation is often referred to as a quadrant plot.

First, identify two important or interesting metrics. Second, have at least one dimension that you can take action with such as state, employees, or marketing campaigns. Then scatter away!

For example, in marketing, we often care about two metrics. Marketing performance in efficiency (acquiring as many customers as possible at a low marketing spend). Marketing performance in quality (acquiring customers that are high value).

When we scatter these two metrics by campaign we can quickly and clearly see the good and the bad.

Made with Tableau

The action from this scatter plot is to take money away from campaign E and J and put it towards D and H. This works so well because it sorts your dimension into four easy to understand quadrants."
Keeping Data Science Scientific,"Below, I’ve paraphrased the following research question originally raised in Daniel Khaneman’s best-selling book Thinking, Fast and Slow:

Both the highest and lowest rates of liver cancer have been shown to exist in rural communities. Why?

I’ll return to this question at the end of the post. But for now, let’s discuss what knowledge stands to gain — and lose — from the unprecedented amount of information we are collecting about ourselves.

The standard definition of data science is: “a multidisciplinary field that uses scientific methods, processes, algorithms and system to extract knowledge and insights from structured and unstructured data.” As the information age continues to come to fruition, Internet connectivity and personal computing are almost ubiquitous. With every person under age 50 in a developed country constantly tapping at a phone screen, typing on a laptop, and enjoying all sorts of digital content, the world’s tech companies are amassing unprecedented amounts of data. The field of data science has emerged to both organize and optimize this data, often to improve the performance of digital services like the algorithms behind Amazon store product recommendations and Spotify’s “Discover Weekly” playlists. However, data science has also allowed us to study the intricacies of social behavior on a previously unimaginable level. As smart speakers enter our homes and intelligent cars hit the roads, we will only continue to gather more information about ourselves.

Data data data (Source: Ourworldindata.org)

However, having more data and information at our fingertips does not automatically translate to higher knowledge and understanding. Our ability to turn data into genuine insight still depends on properly exercising the scientific method. This is by no means a guarantee; according to some intellectuals, society has leaned heavily on unrefined science for decades.

Karl Popper, an Austrian-born British philosopher known for his views on falsifiability, the scientific method and his criticism of induction, encapsulates this cautionary line of thinking. Popper argued that true science depends on hypotheses that can be falsified. Scientifically rigorous theories make assumptions based on thorough research and logical examination, then use all available means to prove the hypothesis wrong. This type of science moves humankind forward; as Popper disciple Nassim Nicholas Taleb points out, Newton’s theory of motion did wonders for mankind’s understanding of physics despite being proved wrong by Einstein several hundred years later. Progress comes from finding assertions that cannot be proven wrong and relentlessly experimenting to disprove the theory.

Karl Popper

The dichotomy between deduction and induction perfectly encapsulates what Popper and his ilk view as the improper implementation of science. In the former, we deduce by moving from theory to hypothesis, to empirical observation, and finally, rejection of the hypothesis. Induction, on the other hand, uses empirical facts to detect patterns, generate hypotheses and formulate theories. Induction is harder to disprove and more dangerous because it allows us, as Taleb says, to be “fooled by randomness”. Induction may seem harmless on the surface but has pernicious side effects in reality; when you use historical data to build models operating under the assumption that housing prices can never go down, you run into trouble.

Popper went after social scientists who created sweeping, grandiose theories that seemed revolutionary on the surface but proved wobbly in practice. This is because these types of theories are impossible to disprove. For example, consider Sigmund Freud’s theory about the Oedipus complex. How could one possibly disprove that children are subconsciously sexually attracted to their parents? You can’t. Therefore this is an unscientific analysis no more rigorous than suggesting the existence of gods. Yet, in many circumstances, unscientific theories are widely regarded as truisms today. Mainstream economics is a perfect example of this phenomenon. This discipline’s models have become increasingly complex, mathematical, and detached from reality without showing any improvement in forecasting success. And as statistician Nate Silver put it: “Any illusion that economic forecasts were getting better ought to have been shattered by the terrible mistakes economists made in advance of the financial crisis.” [1]

When we exclusively use historical data, also known as empirical knowledge, to formulate theories, we create models that can be highly accurate on average but remain vulnerable to edge-case scenarios. Just as throwing more money at a problem doesn’t necessarily equate to solving that problem, adding more data to our models doesn’t necessarily make them any more reflective of the real world. By definition, models are a simulation, and with more data, it is tempting to overfit and mistake signal for noise. But the value of data is not in its overall mass but our ability to hone, shape and interpret it. It’s about the process.

A good data scientist should have a strong understanding of the scientific method, ask the right questions, and remain skeptical of empirical knowledge, especially when building models with severe consequences. And we should keep in mind that there is some truth to the statement “the more we learn, the less we know”; Einstein’s Theory of Relativity arguably created more questions about the universe than answers.

So if you deeply research social behavior in rural communities and use data science to analyze chemical exposures, lifestyle discrepancies, cultural norms, nutrition patterns, etc. I’m sure you could find some convincing results to explain why rural communities suffer from liver cancer at the highest AND lowest rates in America. When I first encountered this research question, I certainly racked my brain for answers like this. But Popper would be skeptical of using your empirical analysis to build a model to project future liver cancer rates in rural communities. Why?

Kahnman reminds readers shortly thereafter that rural communities have smaller populations than urban areas, so given their statistical distribution, they have a higher variance and more varied outcomes, leading to both higher and lower rates of cancer. The smaller sample size increases the effect of randomness. Humans are prone to creating stories and hypothesizing about causality when there is none, so our gut instinct is to come up with “scientific” explanations of events, even when the laws of statistics are a perfectly reasonable explanation of why cancer rates swing further in small rural communities. When we begin to investigate the treasure trove of information served to us on a platter of clicks and likes, it is important to keep the role of chance in mind and remain skeptical of empiricism."
Demystifying Model Training & Tuning,"Different machine learning algorithms are searching for different trends and patterns. Consequently, one algorithm isn’t the best across all datasets or for all use-cases. To find the best solution, we conduct a lot of experiments, evaluating different machine learning algorithms and tuning their hyper-parameters. This post introduces various important topics:

Train, Test & Validation Data

Algorithm Exploration

Hyper-parameter Optimization

Ensembles

Terminology

Bias

Bias is the expected difference between the parameters of a model that perfectly fits your data and those your algorithm has learned. Low bias algorithms (Decision Trees, K-nearest Neighbors & Support Vector Machines) tend to find more complex patterns than high bias algorithms.

Variance

This is how much the algorithm is impacted by the training data; how much the parameters change with new training data. Low variance algorithms (e.g. Linear Regression, Logistic Regression & Naive Bayes) tend to find less complex patterns than high variance algorithms.

Underfitting

The model is too simple to capture the patterns within the data; this will perform poorly on data it has been trained on as well as unseen data. High bias, low variance. High training error and high test error.

Overfitting

The model is too complicated or too specific, capturing trends that do not generalize; it will accurately predict data it has been trained on but not on unseen data. Low bias, high variance. Low training error and high test error.

Bias-Variance Trade-off

The Bias-Variance Trade-off refers to finding a model with the right complexity, minimizing both the train and test error.

Further Reading: Bias-Variance Tradeoff Infographic

Train, Validation & Test Data

Machine learning algorithms learn from examples and, if you have good data, the more examples you provide, the better it will be at finding patterns in the data. However, you must be cautious of overfitting; overfitting is where a model can accurately make predictions for data it has been trained on but unable to generalize to data is hasn’t seen before.

This is why we split our data into training data, validation data and test data. Validation data and test data are often referred to interchangeably, however, they are described below as having distinct purposes.

Training data

This is the data used to train the model, to fit the model parameters. It will account for the largest proportion of data as you wish for the model to see as many examples as possible.

Validation data

This is the data used to fit hyper-parameters and for feature selection. Although the model never sees this data during training, by selecting particular features or hyper-parameters based on this data, you are introducing bias and again risk overfitting.

Test data

This is the data used to evaluate and compare your tuned models. As this data has not been seen during training nor tuning, it can provide insight into whether your models generalize well to unseen data.

Cross-Validation

The purpose of cross-validation is to evaluate if a particular algorithm is suited for your data and use-case. It is also used for hyper-parameter tuning and feature selection.

The data is split into train and validation sets (though you should have some test data put to one side too) and a model built with each of the slices of data. The final evaluation of the algorithm is the average performance of each of the models.

Hold-out Method

The simplest version of cross-validation is the hold-out method where we randomly split our data into two sets, a training set and a validation set. This is the quickest method as it only requires building a model once. However, with only one validation data set, there is a risk that it contains particularly easy, or difficult, observations to predict. Consequently, we could find we are overfitting to this validation data and on a test set it would perform poorly.

K-fold Cross-Validation

The k-fold cross-validation method involves splitting the data into k-subsets. You then train a model k times, each time using one of the k-subsets as its validation data. The training data will be all other observations not in the validation set. Your final evaluation is the average across all k folds.

Leave-one-out cross-validation

This is the most extreme version of K-fold cross-validation, where your k is N (the number of observations in your dataset). You train a model N separate times using all data except for one observation and then validate its accuracy with its prediction for that observation. Although you are thorough evaluating how well this algorithm works with your data set, this method is expensive as it requires you to build N models.

Stratified cross-validation

Stratified cross-validation enforces that the k-fold sets have similar proportions of observations for each class in either categorical features or the label.

Algorithm Exploration

The algorithms you explore should be driven by your use-case. By first identifying what you are trying to achieve, you can then narrow the scope of searching for solutions. Although not a complete list of possible methods, below are links that introduce algorithms for Regression, Classification, Clustering, Recommendations, and Anomaly Detection. A colleague and I also created this tool to help guide algorithm selection (Click here to visit Algorithm Explorer).

Regression

Regression algorithms are machine learning techniques for predicting continuous numerical values. They are supervised learning tasks which means they require labeled training examples.

Further reading: Machine Learning: Trying to predict a numerical value

Classification

Classification algorithms are machine learning techniques for predicting which category the input data belongs to. They are supervised learning tasks which means they require labeled training examples.

Further reading: Machine Learning: Trying to predict a classify your data

Clustering

Clustering algorithms are machine learning techniques to divide data into a number of groups where points in the groups have similar traits. They are unsupervised learning tasks and therefore do not require labeled training examples.

Further reading: Machine Learning: Trying to discover structure in your data

Recommendation Engines

Recommendation Engines are created to predict a preference or rating that indicates a user’s interest in an item/product. The algorithms used to create this system find similarities between either the users, the items, or both.

Further reading: Machine Learning: Trying to make recommendations

Anomaly Detection

Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior. Those identified are often referred to as anomalies or outliers.

Further reading: Machine Learning: Trying to detect outliers or unusual behavior

Hyper-Parameter Optimization

Although the terms parameters and hyper-parameters are occasionally used interchangeably, we are going to distinguish between the two.

Parameters are properties the algorithm is learning during training.

For linear regression, these are the weights and biases; whilst for random forests, these are the variables and thresholds at each node.

Hyper-parameters, on the other hand, are properties that must be set before training.

For k-means clustering, you must define the value of k; whilst for neural networks, an example is the learning rate.

Hyper-parameter optimization is the process of finding the best possible values for these hyper-parameters to optimize your performance metric (e.g. highest accuracy, lowest RMSE, etc.). To do this, we train a model for different combinations of values and evaluate which find the best solution. Three methods used to search for the best combinations are Grid Search, Random Search, and Bayesian Optimization.

Grid Search

You specify values for each hyper-parameter, and all combinations of those values will be evaluated.

For example, if you wish to evaluate hyper-parameters for random forest, you could specify provide three options for the number of trees hyper-parameter (10, 20 and 50) and for the maximum depth of each tree you also provide three options (no limit, 10 and 20). This would result in a random forest model being built for each of the 9 possible combinations: (10, no limit), (10, 10), (10, 20), (20, no limit), (20, 10), (20, 20), (50, no limit), (50, 10), and (50, 20). The combination that provides the best performance will be those you use for your final model.

Pros: Simple to use. You will find the best combination of the values you’ve provided. You can run each of the experiments in parallel.

Cons: Computationally expensive as so many models are being built. If a particular hyper-parameter is not important, you are exploring different possibilities unnecessarily.

Random Search

You specify ranges or options for each hyper-parameter and random values of each are selected.

Continuing with the random forest example, you might provide the range for the number of trees to be between 10 and 50 and max_depth to be either no limit, 10 or 20. This time, rather than it compute all permutations, you can specify the number of iterations you wish to run. Say we only want five, then we might test something like (19, 20), (32 no limit), (40, no limit), (10, 20), (27, 10).

Pros: Simple to use. More efficient and can outperform grid search when only a few hyper-parameters affect the overall performance. You can run each of the experiments in parallel.

Con: It involves random sampling so will only find the best combination if it searches that space.

Coarse to Fine

For both grid search and random search, you can also use the coarse to fine technique. This involves exploring a broader range of variables with wide intervals or all possible options. Once you’ve got your results from this initial search you explore the results to see if there are any patterns or particular regions that look promising. If so, you can repeat the process but refine your search.

For the random forest example above, we might notice that results are promising when the maximum depth has no limit and when the number of trees hyper-parameter is either 10 or 20. The search process is repeated but keeping the maximum depth hyper-parameter constant and increasing the granularity of the number of tree options, testing values of 12, 14, 16, 18, to see if we can find a better result.

Pro: Can find more optimized hyper-parameters, improving the performance metric.

Con: Evaluation of the results to find the best regions to explore can be cumbersome.

Bayesian Optimization

Bayesian Optimization uses the prior knowledge of success with hyper-parameter combinations to choose the next best.

The technique uses a machine learning approach, building a model where the hyper-parameters are the features and the performance is the target variable. After each experiment, a new data point is added and a new model built.

It assumes similar combinations will have similar results and prioritize exploring regions where promising results have already been seen. However, it also takes into consideration uncertainty as a possibility for large gain; if there are large areas that have not yet been explored, it will also prioritize these as.

Taking just one hyper-parameter, the number of trees, the algorithm might first try 10 and get a pretty good performance. It then tries 32 and the performance is significantly better. Bayesian optimization builds a model based on the first two data points to predict performance. The model is likely to be linear with just the two data points so the next value chosen is 40, with the expectation that the performance will continue to improve as the number of trees increase. It does not. Now, it builds another model that suggests there might be improvement around 32 where it has seen the best result so far, however, there’s still a large gap between 10 and 32 that hasn’t been explored and due to large uncertainty, it chooses 21. Again the model is tweaked with this new data and another value chosen…

Pros: Can find more optimized hyper-parameters, improving the performance metric. It can reduce the time spent searching for an optimum solution when the number of parameters is high and each experiment is computationally expensive.

Cons: You cannot run each experiment in parallel as the next combination of hyper-parameter values is determined by the prior runs. It also requires tuning — choosing a scale for each hyper-parameter and an appropriate kernel.

Ensemble Learning

Ensembles combine several machine learning models, each finding different patterns within the data, to provide a more accurate solution. These techniques can both improve performance, as they capture more trends, as well as reduce overfitting, as the final prediction is a consensus from many models.

Bagging

Bagging (bootstrap aggregations) is the method of building multiple models in parallel and average their prediction as the final prediction. These models can be built with the same algorithm (i.e. the Random Forest algorithm builds many decision trees) or you can build different types of models (e.g. a Linear Regression model and an SVM model).

Boosting

Boosting builds models sequentially evaluating the success of earlier models; the next model prioritizes learning trends for predicting examples that the current models perform poorly on. There are three common techniques for this: AdaBoost, Gradient Boosting and XGBoosted.

Stacking

Stacking involves building multiple models and using their outputs as features into a final model. For example, your goal is to create a classifier and you build a KNN model as well as a Naïve Bayes model. Rather than choose between the two, you can pass their two predictions into a final Logistic Regression model. This final model may result in better results than the two intermediate models."
Reimagining Plutarch with Tensorflow 2.0,"Reimagining Plutarch with Tensorflow 2.0

Preamble

Plutarch’s Lives of the Noble Greeks and Romans, also called Parallel Lives or just Plutarch’s Lives, is a series of biographies of famous Ancient Greeks and Romans, from Theseus and Lycurgus to Marcus Antonius.

In the recently published article we looked into training our own word embeddings using gensim library. Here, we’ll primarily focus on the word embeddings layer leveraging the TensorFlow 2.0 platform; the intent is to better understand how the layer works and how it contributes to the success of the larger NLP models.

To help with easy replications, I have adapted the code to Google Colab, and highlighted what is unique to the platform — otherwise the entire code can be run on your local machine using Python 3.6+ and relevant packages. The code is presented throughout the article but I will skip some supplementary or minor code — the entire code can be found in a Github repository of mine.

The text used in this analysis has been made available by Project Gutenberg.

Setting Things Up

On Colab, let’s change the Runtime Type to GPU, then import the latest TensorFlow version — this snippet below will only work on Colab, otherwise simply use pip or conda install commands to upload the latest TensorFlow on your machine.

We will also need the OS and regular expression libraries, and then save & print the file path for future reference:

import os

import re

fpath = os.getcwd(); fpath

Let’s import the text (Plutarch.txt) into the Google Colab drive — we need to keep in mind that our files there are ephemeral and we’ll need to upload them every time after taking a longer break from using the platform:

The code above is also available under the Code Snippets tab in Colab — among many other very useful ones. When executing this code we’ll see Colab uploading the file and then we can click on the Colab Files tab on the left to make sure the file is there along with the Google’s default Sample Data directory.

Let’s read the text and do some basic regex operations:

import re corpus = open(fpath + '/Plutarch.txt', 'rb').read().lower().decode(encoding='utf-8') corpus = re.sub('

', ' ', corpus) #remove new line

corpus = re.sub('\r', ' ', corpus) #remove ""return""

Since we’ll be splitting the text into sentences, new line has no meaning for our analysis. Also, while using the text tokenizer I noticed that having “\r” (which signifies the carriage return) creates false unique words, such as “we” and “we\r” — again, not important in our case. Hence both “

” and “\r” need to go.

Building the Dictionary

As we ramp up towards actual word embeddings, let’s tokenize the text into sentences:

import nltk

from nltk.tokenize import sent_tokenize

nltk.download('punkt') #need in Colab upon resetting the runtime



# tokenize at sentence level

sentences = nltk.sent_tokenize(corpus)

print(""The number of sentences is {}"".format(len(sentences)))

We will see that the text has a total of 16,989 sentences. Next, we need to calculate the number of words in the longest sentences — the reason will become evident later in the tutorial:

from nltk.tokenize import word_tokenize word_count = lambda sentence: len(word_tokenize(sentence))

longest_sentence = max(sentences, key=word_count)

length_longest_sentence = len(word_tokenize(longest_sentence)) print(""The longest sentence has {} words"".format(length_longest_sentence))

It turns out the longest sentence is 370 words long. Next, let’s convert the entire text into positive numbers so that we can start speaking a common language with TensorFlow:

From the above we also find out that the text has 20241 unique words, as the tokenizer assigns only one number per same word. To standardize the lengths of all of the sentences (i.e. make the input data into a single, same shape tensor to make it processable / easier for the model— we are here to serve the machines’ needs), we’ll need to convert the list of numbers representing the words (sent_numeric) into an actual dictionary (word_index), and add padding. We could also combine truncating very long sentences with padding the short ones, but in this case we’ll just pad up to the longest sentence’s length.

Vocabulary size (a.k.a. the number of unique words) will go up by 1, to 20,242 as a result of adding the 0 for padding. Type “data[0]” (i.e. the first sentence) to see what the first sentence will look like with the padding.

To be able to translate back and forth between the words and their numeric representations, we’ll need to add the reverse word index for look-ups:

reverse_word_index = dict([(value, key) for (key, value) in word_index.items()]) def decode_data(text):

return ' '.join([reverse_word_index.get(i, '?') for i in text])

It makes sense to double-check the word indexing and conversion — a single mistake will likely throw the whole dataset off to make it incomprehensible. Examples of cross-checking — before and after the conversion — are available in my Github repository.

Photo by Sandra Povilaitis

The Model

Finally, let’s build and run the model. There is a nice tutorial provided by TensorFlow which we are adapting to our needs.

But first, we can simply run the embedding layer only, which will produce an embeddings’ array. I have read that such an array could be saved and used in another model — yes, it can, but outside of skipping the embedding step in the new model, I am not so sure of the utility, as the vectors generated for each word are agnostic to the problem being solved:

We will not spend much time on the above and will rather focus on the models where embedding is just the first part.

Let’s move on and construct the new, very basic model architecture after importing relevant libraries:

The embedding layer — which can typically be used as the first layer in a model — will convert sequences of numerically encoded unique words (as a reminder, 20,241 of them plus padding coded as a zero) into sequences of vectors, the latter being learned as the model trains . Each vector will have 100 dimensions (embedding_dim=100), hence we’ll have a matrix of 20242 x 100 as a result.. The input length will be fixed to the length of the longest sentence, i.e. 370 words, as every single word is perceived by the model to have the same size due to padding. Mask_zero informs the model whether the input value 0 is a special padding value that should be masked out, which is particularly useful in recurrent layers where variable input lengths can be processed by the model.

After training on enough meaningful data words with similar meanings will likely have similar vectors.

Here is the model summary (the model with an additional dense layer is in the github repository):

In the model summary we’ll see that the number of parameters for the embedding layer is 2,024,200, which is 20,242 words times the embedding dimension of 100.

The previously mentioned TensorFlow tutorial is using a reviews dataset with each of the reviews being labeled 1 or 0 depending on the positive or negative sentiment. We do not have the labeling luxury but still want to test drive this model, so will simply create an array of 0s and attach to each of the sentences; the model requires such a structure. This will not be the first or the last time that machine intelligence gets assaulted with an unsolvable task yet still obliges us with a solution. Let’s train this model:

import numpy as np adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)

model.compile(optimizer='adam',

loss='binary_crossentropy',

metrics=['accuracy']) batch_size = 16989 #number of sentences

data_labels = np.zeros([batch_size, 1]) history = model.fit(

data,

data_labels,

epochs=200,

batch_size=batch_size,

verbose = 0)

The embeddings are trained. Before we turn to visualization, let’s quickly check on gensim for word similarities. First, we need to create the vectors’ file — keep it temporarily in Colab or download to the local machine:

f = open('vectors.tsv' ,'w')

f.write('{} {}

'.format(vocab_size-1, embedding_dim))

vectors = model.get_weights()[0]

for words, i in tokenizer.word_index.items():

str_vec = ' '.join(map(str, list(vectors[i, :])))

f.write('{} {}

'.format(words, str_vec))

f.close() # download the file to the local machine by double-clicking the Colab file or using this:

try:

from google.colab import files

except ImportError:

pass

else:

files.download('vectors.tsv')

Second, let’s

import gensim w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.tsv', binary=False) w2v.most_similar('rome')

Finally, let’s check on similarity between Pompey and Caesar, which showed high in the CBOW model we had previously trained:

round(w2v.similarity('pompey', 'caesar'),4)

The relationship between the words is high. Also, as one would expect, Caesar shows highly similar to Rome.

For those interested in more complex models, additional variants, including Recurrent Neural Networks (Long Short-Term Memory) are available in my Github file, but keep in mind they will train much slower than the simple model above.

Visualization

For the embeddings’ visualization, it is hard to beat the TensorFlow Projector, so let’s create the vector and meta (i.e. words corresponding to those vectors) files for its use:

Import the files locally and then we can go to TensorFlow’s Projector, upload the files to replace the default data, and try various options available on the site. Here is the Principal Components Analysis view of the entire vector space for the text:

And here is just the vector space for 100 words that showed up as most similar to “Rome”.

Conclusion

In this article, we briefly looked at the role of the word embedding layer in a deep learning model. In the context of such a model, the layer supports solving a particular NLP task — e.g. text classification — and through iterations trains the word vectors to be the most conducive in minimizing the model loss. Once the model is trained, we can inspect the embedding layer output through similarity calculations and visualizations.

The embedding layer can also be used to load pre-trained word embeddings (e.g. GloVe, BERT, FastText, ELMo), which I believe would typically be a more productive way to utilize models requiring such embeddings — in part due to the “industrial grade” effort and data size required to generate them. However, in cases of specialized text and especially if the corpus on which the word embeddings can be trained is sizeable, training own embeddings can still be more effective."
Careful! Looking at you model results too much can cause information leakage,"It’s always to use as much data as you can when building machine learning models. I think we all are aware of the issue of overfitting, which is essentially where the model you build replicates the training data results so perfectly its fitted to the training data and does not generalise to better represent the population the data comes to, with catastrophic results when you feed in new data and get very odd results.

Photo by Jacek Dylag on Unsplash

What can you do to prevent this?

Look online and you will find reams and reams of articles about overfitting, what it is and how to prevent it. These have been done to a great degree so I won’t repeat them all here but usually these can be controlled most easily by segregating the available data you have before you start.

What data segregation's are there?

The most basic way is to take the whole set of available data and split it into three different samples (with no duplication) and given the following labels:

Training

Validation

Test

Training, Test and Validation sets of data each carry out a specific purpose and will usually not be of the same size. Indeed some of the most used values are a 3:1:1 split (or 60% training, 20% validation and 20% test)

Training Data

This is the meat of the data. This is what is used to train and create the models you use. Naturally the more data you have in training the better you can expect the model to generalise to it. This generally has around 60% of all the available data in it.

Validation Data

This is the set of data that is used for tuning and improving your model. Whenever you train a model you use it against the validation set in order to generate some predictions and score its performance. You can then tweak the model, re-train it and run it against this data again to see if an improvement has been made. This can be a very iterative step. You want a reasonable size of data here to cover the parameter space the model will be exposed to, but not too much that your model does not have enough data to train…"
Machine Learning 102: Logistic Regression With Polynomial Features,"Data Scientists are rock stars! Rock and Roll!

In my previous ML 101 article, I explained how we could apply logistic regression to classify linear questions. In this post, I want to complicate things a little bit by including nonlinear features. Just like the real world, things are intertwined and messy.

Let’s delve into the R.

# Load the dataset

library(tidyverse)

data5 = read_csv(""nonlinear.csv"")

require(ggplot2)

qplot(X1,X2,colour = Y,data=data5)

As we can see, black dots are surrounded by blue dots. Our job is to find a ML technique to neatly separate these two types of dots.

# build a regular logistic regression

glm_5b = glm(Y~X1+X2,data=data5)

summary(glm_5b) Call:

glm(formula = Y ~ X1 + X2, data = data5) Deviance Residuals:

Min 1Q Median 3Q Max

-0.6944 -0.5504 0.1937 0.3584 0.6213 Coefficients:

Estimate Std. Error t value Pr(>|t|)

(Intercept) 0.71038 0.05672 12.524 <2e-16 ***

X1 -0.05446 0.02496 -2.182 0.0325 *

X2 0.04278 0.02708 1.580 0.1187

---

Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 (Dispersion parameter for gaussian family taken to be 0.2109993) Null deviance: 16.000 on 71 degrees of freedom

Residual deviance: 14.559 on 69 degrees of freedom

AIC: 97.238 Number of Fisher Scoring iterations: 2

As can be seen, the regular logistic regression fails to consider the nonlinear feature and performs poorly.

Next, let’s project class labels over to finely sampled grid points and plot predictions at each point on the grid colored by class labels."
XLNet — a clever language modeling solution,"XLNet — a clever language modeling solution

Language modeling with a deep model — the challenge and a solution

TL;DR

Unsupervised learning of probability distribution of word sequences in a language by predicting each word within its sentence context in a large corpus, has proven to be useful to create models and word representations that can then be fine tuned for downstream NLP tasks. Two factors seem to play a key role to boost performance when fine tuning models for downstream syntactic and semantic tasks:

Is the representation of a word capturing the full context of that word in a sentence? Full context is the entire sequence of words that precede and follow a word in a sentence(bidirectional context)

Full context is the entire sequence of words that precede and follow a word in a sentence(bidirectional context) Is the model multilayered to output multiple representations for a word, one from each layer? Representations of a word from the different layers of a model have been found to be useful for downstream syntactic and semantic tasks

Satisfying both these design factors poses a challenge (described in detail below) that XLNet addresses by a clever solution. This enables XLNet in part to perform better than previous state-of-art model BERT which satisfies the above two requirements but with some deficiencies. However, XLNet requires more compute power and memory (GPU/TPU memory) in comparison to BERT. It also remains to be seen if improvements to reduce compute/memory requirements etc, will make XLNet good enough from a practical sense to replace BERT in all the NLP tasks that BERT currently excels in (Q&A, text classification, sequence tagging tasks like NER etc.).

The challenge to create deep bidirectional context representations

Language modeling, which is essentially learning the probability distribution of word sequences in a language by predicting words within their sentence contexts in a large corpus, can be done in multiple ways as illustrated in figure below"
The complete list of books for Quantitative / Algorithmic / Machine Learning trading,"The complete list of books for Quantitative / Algorithmic / Machine Learning trading

The only thing between you and your financial freedom is 108 books, so what’re you waiting for? Just begin reading! Peter Nistrup · Follow Published in Towards Data Science · 8 min read · May 2, 2019 -- 4 Share

To those who seek wisdom ahead:

Beware, for this is a long list.. (100+ books)

“You wont become an algorithmic genius savant extraordinaire philanthropist billionaire overnight.”

You wont be one in 3 months. You wont be one in a year either. You might not ever become one. But as with mostly everything in life if you apply yourself you might get closer and closer to achieving that dream of financial freedom.

Now obviously some of these have overlapping contents so you should do your own research about which books are more suited for your mindset and current skill set.

This list is comprised from multiple sources online and filtered to the best of my abilities into the following categories:

General Reading

Light Reading

Programming

Mathematics

Economics & Finance

Technical & Time-Series Analysis

Other

Derivatives

Make sure to follow my profile if you enjoy this article and want to see more!

GENERAL READING — The fundamentals

This is probably a good place to start, together these books cover a wide variety of subjects and might be a decent introduction to the various elements of quantitative trading."
The Path to Data Science: MOOC Reviews,"Photo credit: Annie Spratt, UK

I’ve become something of a MOOC connoisseur; at this point in my journey to employment, I’ve tried every major MOOC and I still dabble to keep things fresh on the brain.

The great thing is, I learned a ton without spending much money. So far Udacity was the most expensive, and I got it on an interest-free loan. Often they offer discounts and many of them run on a monthly subscription for $30–50. For brevity, I don’t want to focus on the syllabus of each program here, but explain their strengths and weaknesses.

Part of my motivation for doing so many courses was my interest in education; I think the MOOC format’s made a lot of bright people rethink learning, and Coursera and SharpestMinds offer an important format that could compete with some aspects of academia. If you remember nothing else, though: MOOCs are a great way to build an industry skill and augment your education; they supply you with the skill-building academia can not. They are not a substitute for academic education and you will still need to

build your own projects, and answer your own questions

to become a data scientist. The closest one to academia would be Coursera. Ultimately I became a data scientist by using a combination of MOOCs, self-studying books, and leveraging my formal education.

Codecademy:

I highly recommend Codecademy Pro. Price is worth it and exercises really give the right amount of brainhurt, along with good explanations. I recommend clearing your progress on certain lessons and redoing them if you had trouble (reworking missed problems is good practice for anything technical).

I did their essential Python 3 course to make me a better Python user, but they have a Computer Science career track which is more involved and covers topics like recursion, algorithms and data structures.

The strength of Codecademy lies in its combination of code-alongs, guided projects, quizzes and videos: they are really pushing for a holistic understanding of the material. You need all those formats to truly understand something technical.

Dataquest:

Imagine Codecademy, but harder. These lessons are incredibly detailed. It’s a great place to learn, but it will take a lot of time to do even a normal mission. I love their ethos of “quality education at a low price” ($30/month currently, $50 for the Premium plan, much cheaper if you buy a year) which makes it accessible to anyone willing to roll up their sleeves and work. For me, since I already covered a lot of these concepts, I didn’t find it worthwhile to complete any of the major certificates, but struggling through missions made me a better coder and practitioner of data science.

On the Premium plan you get a monthly call with a mentor and he’ll review your resume or give technical advice; take advantage of this opportunity. The most valuable thing they can give you is criticism.

Dataquest offers a nice curriculum selection that will take you on up to advanced Python topics and, as somewhat of a statistics pro, I found their explanation of stats very good. I recommend going from Codecademy to Dataquest to move beyond foundational skills and get strong.

My one complaint is they could turn down the difficulty a hair; there’s something to be said for repetition and plug-and-chug when learning an applied skill. However, they cover an enormous amount of material and seem to have the best SQL courses of any MOOC. They go into Redshift and really advanced data engineering stuff. I’d consider a few DQ missions essential for any aspiring data scientist and it almost makes other MOOCs obsolete.

DataCamp:

My opinion is: DataCamp’s an earnest shot at education. It’s a great place to get your feet wet and the video format is noobie-friendly. I enjoyed it at the time, but it‘s only step 1 of a long process. The code-along format is like Codecademy, but you don’t write code, just fill in blanks. There is some nice brainhurt here, but it’s not the best bang for your buck.

Videos (and lectures in general) aren’t a great way to teach a technical concept. If you’re totally new to data science, this is a nice place to start and get motivated, but I’d still recommend DataQuest.

Many DataCamp learners (myself included) report doing a DC course then going over to DataQuest to strengthen their skills, which is something I recommend (studying the same thing from different sources is always good practice).

I will say DataCamp has the best selection of R courses and if you are looking to familiarize yourself with a new R package, you can’t go wrong with DataCamp; their lecturers are world-class. It also exposes you to what good code looks like. At $30/month I’d say a few months is a good way to augment your education.

Udacity (Machine Learning Engineer):

This program covers the major ML algorithms and I thought the videos were helpful for things I’d already studied; they expanded my understanding of things like SVMs, but I don’t think they’d teach it to you alone.

The projects here are also good and the mini-labs (not required) are nice skill-builders. I thought the staff grading my projects were great and very thorough. They really pushed me to write good explanations of every tool I used, and justify my decisions.

$2000 is the current full list price for this program, and I think they’ve actually dismantled it and changed it fundamentally. The Machine Learning Engineer program I see on the site now seems smaller and focused on deployment and data engineering, whereas what I took discussed all the major ML and Deep Learning algorithms (with no coverage of deployment).

Ultimately I’d say $2000 is way too much for what I got out of this, and the “Career Services” from Udacity are a joke (I got a good resume review and that was it). It will, however, look good on your resume and has probably helped me land interviews.

Springboard (Intermediate Data Science w/ Python):

Springboard does not create much content of their own, instead linking you to free or low-cost education resources around the web. For machine learning fundamentals I think this is a good idea; there’s plenty of quality content out there so why reinvent the wheel at students’ expense? It’s oriented for students making data science projects in Jupyter for the first time, and does its job well (previous MOOCs I listed are all in-browser code-alongs without pulling up your own Jupyter notebook).

The really valuable asset of SpringBoard is their mentors. This was my first time seeing a lot of Machine Learning concepts and my mentor really grilled me to make sure I know my stuff. Brilliant guy.

At $500/month I’d consider this program a great way to build your chops. Get ready to be challenged, earn a nice certificate, and interact with an experienced mentor. Student support from the staff and community is quite cool and staff quickly responds to emails. I wouldn’t be anywhere in data science without them.

SharpestMinds:

SharpestMinds offered what I’d been looking for a long time, being a guy in a smallish city with a smaller tech scene: a chance to interact with real data scientists. You need to apply to get in, and to acquire a mentor you should be like 80% of the way there on education/skillset, but once you’re in you’re golden.

The other mentees are bright and about as far along in data science as I, so the community Slack has interesting projects and posts. Often in the Slack, folks will say “My company’s hiring, shoot me a message if you’re interested in X topic.”

After walking across the graduation stage with my MS in Pure Math, I felt like someone pushed me out a plane and said “Quick, get a job before you hit the ground.” SharpestMinds was like the 101st Airborne Division appearing out of the sky and handing me a parachute.

This is probably the best ‘career services’ of any MOOC or Bootcamp, ever, since they don’t get paid until you find a job. SM now averages one new hire every 3 days (an impressive feat considering how difficult landing an entry-level data science role is in 2019).

Part of its success stems from its specificity and selectivity: this NOT a program for people wondering “Is data science for me?” It’s for people with significant knowledge under their belt trying to land the best entry-level DS role. You will need to work hard to with SM and build an ambitious project with your mentor, and once you’re hired you owe them a small portion of your first year’s salary (they’re upfront about this arrangement). They also put a lot of focus on your resume and online presence, and the mentors coach you with mock interviews.

Essentially, this is not a MOOC or a bootcamp, but an education and career service for those trying to turn pro.

Coursera:

A great format for learning. It’s fundamentally different from code-along MOOCs but instead focuses on conceptual understanding. The quizzes here genuinely make you think and I never felt the videos were wasting my time.

I used Coursera for an introductory R course and introductory SQL, which helped me snag a lot of interviews because employers are always looking for someone who knows SQL. I probably don’t need to tell you about Coursera, but if you’re applying for jobs, make sure you have a few of their certificates on your resume and LinkedIn; the old guard don’t know all the new MOOCs but everyone knows (and respects) Coursera.

Be the Real McCoy:

Don’t forget that, to build coding skills, you’ll need to stop by Edabit or Hackerrank and see if you can level up. Start easy and don’t get ahead of yourself, make those fundamentals really strong (you’ll need them when you’re slicing strings backwards and building time series graphs). People become programmers by coding hours a day for months. You will inevitably need to find interesting datasets and test your knowledge yourself on them; no MOOC can recreate this. If you want to become a data scientist, don’t just knock out MOOCs to make your resume look good; actually build the skills employers are looking for."
Interpretable Convolutional Neural Network,"This paper by Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu from University of California, Los Angeles proposes a method to modify traditional convolutional neural networks (CNNs) into interpretable CNNs, in order to clarify knowledge representations in high conv-layers of CNNs.

Figure 1: Comparison of a filter’s feature maps in an interpretable CNN and those in a traditional CNN

Problem: without any additional human supervision, can we modify a CNN to obtain interpretable knowledge representations in its conv-layers?

Bau et al. [1] defined six kinds of semantics in CNNs, i.e. objects, parts, scenes, textures, materials, and colors.

In fact, we can roughly consider the first two semantics as object-part patterns with specific shapes, and summarize the last four semantics as texture patterns without clear contours. Filters in low conv-layers usually describe simple textures, whereas filters in high conv-layers are more likely to represent object parts.

Their approach is to train each filter in a high conv-layer to represent an object part. In a traditional CNN, a high-layer filter may describe a mixture of patterns, i.e. the filter may be activated by both the head part and the leg part of a cat (Figure 1). Such complex representations in high conv-layers significantly decrease the network interpretability. Therefore, their approach forces the filter in an interpretable CNN is activated by a certain part.

Learning a better representation

This paper invented a generic loss to regularize the representation of a filter to improve its interpretability.

The loss encourages a low entropy of inter-category activations and a low entropy of spatial distributions of neural activations which means forcing feature map of a layer in a CNN not to be randomly activated by different region of an object and to have consistent distribution of activations.

The filter must be activated by a single part of the object, rather than repetitively…"
GAN Loci,"Model Training

In the model training stage, we use the collected image sets to train GAN models capable of generating new images related to each selected urban context. To this end, two distinct GAN architectures are employed: StyleGAN and Pix2Pix, the particular implementations of which are discussed below. Once trained, each of these models prove valuable in their own way, as each offers a distinct interface for the production of synthetic urban images.

Pix2Pix

Pix2Pix (Isola et al., 2016) is an architecture for a particular kind of GAN: a conditional adversarial network that learns a mapping from a given input image to a desired output image. From the perspective of a user of a trained Pix2Pix model, we offer an input image that conforms to some mapping convention (such as a color-coded diagram of a facade, or an edge drawing of a cat) and receive in return an image that results from the transformation of this input into some desired output (such as a photographic representation of a facade, or of a cat).

The particulars that guide the training of a Pix2Pix model strongly depend upon the specifics of the implementation employed. This project relies upon a “high-definition” version of this architecture implemented in Pytorch (Wang, 2019). Some modifications of this implementation were required: in particular, to correct problems with unwanted artifacts forming in cases of low-contrast source images (as seen in the figure below). Following suggestions offered by the community of Pix2Pix users, zero paddings were replaced with reflection paddings, and the learning rate was temporarily adjusted to 0.0008.

Synthetic image artifacts encountered while training.

Once trained, each model operates as implied by the nature of a conditional GAN and by the structure of the training data: given a greyscale depthmap image that describes a desired three-dimensional urban scene, a synthetic RGB sceneographic image is returned. Since these models are trained on subsets of data segregated by site, each model produces synthetic images specific to just one urban place: the Rotterdam model produces images that “feel” like Rotterdam, while the San Francisco model generates ones that appear more like San Francisco. This feature allows for direct comparisons to be drawn.

Results derived from Pix2Pix model: synthetic images of San Francisco, CA (left) and Rotterdam, NL (right).

StyleGAN

In contrast with a traditional GAN architecture, StyleGAN (Karras et al., 2018) draws from “style transfer” techniques to offer an alternative design for the generator portion of the GAN that separates coarse image features (such as head pose when trained on human faces) from fine or textural features (such as hair and freckles). Here, in comparison to the Pix2Pix model, the user experience is quite different: rather than operating by mapping an input image to a desired output, users select a pair of images from within the latent space of a trained model, and hybridize them. Rather than a simple interpolation between points in latent space, however, these hybrids correspond to the coarse and fine features of the given pair.

Fake images drawn from all nine sites studied.

As above, the particulars that guide the training of a StyleGAN model strongly depend upon the specifics of the implementation. This project relies on the official TensorFlow implementation of StyleGAN , which was employed without modification to train a single model on a combination of RGB sceneographic data drawn from all nine urban places. Once trained, the model may be queried either by sampling locations in latent space, or by providing coarse-fine pairs of locations in latent space to more precisely control different aspects of the synthetic image.

Building upon the former technique of taking samples in latent space, linear sequences of samples may be combined to produce animations such as the ones discussed below.

Image Generation

In the image generation stage, we develop methods for interfacing with the trained models in useful ways. This task is non-trivial, since each GAN model, once trained, is capable of producing a vast and overwhelming volume of synthetic images, which is described in terms of a high-dimensional latent space. The StyleGAN model offers a unique form of guiding the generation of images as combinations of features drawn from other images selected from latent space. The Pix2Pix model offers quite a different interface, with new synthetic images generated as transformations of arbitrary given source images: in our case, these are depth-maps of urban spaces. We present here a brief overview of these methods, and leave a more complete unpacking and visual analysis of the resulting images to a future post.

Pix2Pix Image Generation

Here, greyscale depthmap images are produced by sampling a scene described in a 3d CAD model. These depthmaps of constructed scenes are then used as the source image by which the Pix2Pix models for each urban place produces a synthetic photographic scene. By providing precisely the same input to models trained on different urban places, direct comparisons between the salient features picked up by the transformation models may be made.

For example, while each of the synthetic images below were produced by sampling the same depthmap, we can clearly see those imagistic properties that characterize each of the urban places sampled. A large massing that appears in the depthmap is interpreted by the Rotterdam model as a large brick housing block, as is typical in the Dutch city, while the Pickwick Park model renders this massing in a manner typical of the Northern Florida flora, suggesting the mass of a mossy Live Oak. A long and receding urban wall is broken up by the Alamo Square model into a series of small scale forms, an interpretation that expresses the massing of a line of Edwardian townhouses that dominate this San Francisco neighborhood; this same urban form is understood as something resembling a red-brick industrial warehouse building by the model trained on images from the Bushwick area of Brooklyn."
How to Automate Hyperparameter Optimization,"In the machine learning and deep learning paradigm, model “parameters” and “hyperparameters” are two frequently used terms where “parameters” define configuration variables that are internal to the model and whose values can be estimated from the training data and “hyperparameters” define configuration variables that are external to the model and whose values cannot be estimated from the training data ( What is the Difference Between a Parameter and a Hyperparameter? ). Thus, the hyperparameter values need to be manually assigned by the practitioner.

Every machine learning and deep learning model that we make has a different set of hyperparameter values that need to be fine-tuned to be able to obtain a satisfactory result. Compared to machine learning models, deep learning models tend to have a larger number of hyperparameters that need optimizing in order to get the desired predictions due to its architectural complexity over typical machine learning models.

Repeatedly experimenting with different value combinations manually to derive the optimal hyperparameter values for each of these hyperparameters can be a very time consuming and tedious task that requires good intuition, a lot of experience, and a deep understanding of the model. Moreover, some hyperparameter values may require continuous values, which will have an undefined number of possibilities, and even if the hyperparameters require a discrete value, the number of possibilities is enormous, thus manually performing this task is rather difficult. Having said all that, hyperparameter optimization might seem like a daunting task but thanks to several libraries that are readily available in the cyberspace, this task has become more straightforward. These libraries aid in implementing different hyperparameter optimization algorithms with less effort. A few such libraries are Scikit-Optimize, Scikit-Learn, and Hyperopt.

There are several hyperparameter optimization algorithms that have been employed frequently throughout the years, they are Grid Search, Random Search, and automated hyperparameter optimization methods. Grid Search and Random Search both set up a grid of hyperparameters but in Grid Search every single value combination will be exhaustively explored to find the hyperparameter value combination that gives the best accuracy values making this method very inefficient. On the other hand, Random Search will repeatedly select random combinations from the grid until the specified number of iterations is met and is proven to yield better results compared to the Grid Search. However, even though it manages to give a good hyperparameter combination we cannot be certain that it is, in fact, the best combination. Automated hyperparameter optimization uses different techniques like Bayesian Optimization that carries out a guided search for the best hyperparameters ( Hyperparameter Tuning using Grid and Random Search). Research has shown that Bayesian optimization can yield better hyperparameter combinations than Random Search ( Bayesian Optimization for Hyperparameter Tuning).

In this article, we will be providing a step-by-step guide into performing a hyperparameter optimization task on a deep learning model by employing Bayesian Optimization that uses the Gaussian Process. We used the gp_minimize package provided by the Scikit-Optimize (skopt) library to perform this task. We will be performing the hyperparameter optimization on a simple stock closing price forecasting model developed using TensorFlow.

Scikit-Optimize (skopt)

Scikit-Optimize is a library that is relatively easy to use than other hyperparameter optimization libraries and also has better community support and documentation. This library implements several methods for sequential model-based optimization by reducing expensive and noisy black-box functions. For more information you can refer neptune.ai’s article where they have done a comprehensive analysis on the capabilities and usage of skopt.

Bayesian Optimization using the Gaussian Process

Bayesian optimization is one of the many functions that skopt offers. Bayesian optimization finds a posterior distribution as the function to be optimized during the parameter optimization, then uses an acquisition function (eg. Expected Improvement-EI, another function etc) to sample from that posterior to find the next set of parameters to be explored. Since Bayesian optimization decides the next point based on more systematic approach considering the available data it is expected to yield achieve better configurations faster compared to the exhaustive parameter optimization techniques such as Grid Search and Random Search. You can read more about the Bayesian Optimizer in skopt from here.

Code Alert!

So, enough with the theory, let’s get down to business!

This example code is done using python and TensorFlow. Furthermore, the goal of this hyperparameter optimization task is to obtain the set of hyperparameter values that can give the lowest possible Root Mean Square Error (RMSE) for our deep learning model. We hope this will be very straight forward for any first-timer.

First, let us install Scikit-Optimize. You can install it using pip by executing this command.

pip install scikit-optimize

Please note that you will have to make some adjustments to your existing deep learning model code in order to make it work with the optimization.

First, let’s do some necessary imports.

import skopt

from skopt import gp_minimize

from skopt.space import Real, Integer

from skopt.utils import use_named_args

import tensorflow as tf

import numpy as np

import pandas as pd

from math import sqrt

import atexit

from time import time, strftime, localtime

from datetime import timedelta

from sklearn.metrics import mean_squared_error

from skopt.plots import plot_convergence

We will now set the TensorFlow and Numpy seed as we want to get reproducible results.

randomState = 46

np.random.seed(randomState)

tf.set_random_seed(randomState)

Shown below are some essential python global variables that we have declared. Among the variables, we have also declared the hyperparameters that we are hoping to optimize (the 2nd set of variables).

input_size=1

features = 2

column_min_max = [[0, 2000],[0,500000000]]

columns = ['Close', 'Volume']



num_steps = None

lstm_size = None

batch_size = None

init_learning_rate = None

learning_rate_decay = None

init_epoch = None

max_epoch = None

dropout_rate = None

The “input_size” depicts a part of the shape of the prediction. The “features” depict the number of features in the data set and the “columns” list has the header names of the two features. The “column_min_max” variable contains the upper and lower scaling bounds of both the features (this was taken by examining validation and training splits).

After declaring all these variables it’s finally time to declare the search space for each of the hyperparameters we are hoping to optimize.

lstm_num_steps = Integer(low=2, high=14, name='lstm_num_steps')

size = Integer(low=8, high=200, name='size')

lstm_learning_rate_decay = Real(low=0.7, high=0.99, prior='uniform', name='lstm_learning_rate_decay')

lstm_max_epoch = Integer(low=20, high=200, name='lstm_max_epoch')

lstm_init_epoch = Integer(low=2, high=50, name='lstm_init_epoch')

lstm_batch_size = Integer(low=5, high=100, name='lstm_batch_size')

lstm_dropout_rate = Real(low=0.1, high=0.9, prior='uniform', name='lstm_dropout_rate')

lstm_init_learning_rate = Real(low=1e-4, high=1e-1, prior='log-uniform', name='lstm_init_learning_rate')

If you look closely you will be able to see that we have declared the ‘lstm_init_learning_rate’ prior to log-uniform without just putting uniform. What this does is that, if you had put prior as uniform, the optimizer will have to search from 1e-4 (0.0001 ) to 1e-1 (0.1) in a uniform distribution. But when declared as log-uniform, the optimizer will search between -4 and -1, thus making the process much more efficient. This has been advised when assigning the search space for learning rate by the skopt library.

There are several data types using which you can define the search space. Those are Categorical, Real and Integer. When defining a search space that involves floating point values you should go for “Real” and if it involves integers, go for “Integer”. If your search space involves categorical values like different activation functions, then you should go for the “Categorical” type.

We are now going to put down the parameters that we are going to optimize in the ‘dimensions’ list. This list will be passed to the ‘gp_minimize’ function later on. You can see that we have also declared the ‘default_parameters’. These are the default parameter values we have given to each hyperparameter. Remember to type in the default values in the same order as you listed the hyperparameters in the ‘dimensions’ list.

dimensions = [lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,

lstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate]



default_parameters = [2,128,3,30,0.99,64,0.2,0.001]

The most important thing to remember is that the hyperparameters in the “default_parameters” list will be the starting point of your optimization task. The Bayesian Optimizer will use the default parameters that you have declared in the first iteration and depending on the result, the acquisition function will determine which point it wants to explore next.

It can be said that if you have run the model several times previously and found a decent set of hyperparameter values, you can put them as the default hyperparameter values and start your exploration from there. What this may do is that it will help the algorithm find the lowest RMSE value faster (fewer iterations). However, do keep in mind that this might not always be true. Also, remember to assign a value that is within the search space that you have defined when assigning the default values.

What we have done up to now is setting up all the initial work for the hyperparameter optimization task. We will now focus on the implementation of our deep learning model. We will not be discussing the data pre-processing of the model development process as this article only focuses on the hyperparameter optimization task. We will include the GitHub link of the complete implementation at the end of this article.

However, to give you a little bit more context, we divided our data set into three splits for training, validation, and testing. The training set was used to train the model and the validation set was used to do the hyperparameter optimization task. As mentioned before, we are using the Root Mean Square Error (RMSE) to evaluate the model and perform the optimization (minimize RMSE).

The accuracy assessed using the validation split cannot be used to evaluate the model since the selected hyperparameters minimizing the RMSE with validation split can be overfitted to the validation set during the hyperparameter optimization process. Therefore, it is standard procedure to use a test split that has not used at any point in the pipeline to measure the accuracy of the final model.

Shown below is the implementation of our deep learning model:

def setupRNN(inputs, model_dropout_rate):



cell = tf.contrib.rnn.LSTMCell(lstm_size, state_is_tuple=True, activation=tf.nn.tanh,use_peepholes=True)



val1, _ = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)



val = tf.transpose(val1, [1, 0, 2])



last = tf.gather(val, int(val.get_shape()[0]) -1, name=""last_lstm_output"")



dropout = tf.layers.dropout(last, rate=model_dropout_rate, training=True,seed=46)



weight = tf.Variable(tf.truncated_normal([lstm_size, input_size]))

bias = tf.Variable(tf.constant(0.1, shape=[input_size]))



prediction = tf.matmul(dropout, weight) +bias



return prediction

The “setupRNN” function contains our deep learning model. Still, you may not want to understand those details, as Bayesian optimization considers that function as a black-box that takes certain hyperparameters as the inputs and then outputs the prediction. So if you are not interested in understanding what we have inside that function, you may skip the next paragraph.

Our deep learning model contains an LSTM layer, a dropout layer and an output layer. The necessary information required for the model to work needs to be sent to this function (in our case, it was the input and the dropout rate). You can then proceed with implementing your deep learning model inside this function. In our case, we used an LSTM layer to identify the temporal dependencies of our stock data-set.

We then fed the last output of the LSTM to the dropout layer for regularization purposes and obtained the prediction through the output layer. Finally, remember to return this prediction (in a classification task this can be your logit) to the function that will be passed to the Bayesian Optimization ( “setupRNN” will be called by this function).

If you are performing a hyperparameter optimization for a machine learning algorithm (using a library like Scikit-Learn) you will not need a separate function to implement your model as the model itself is already given by the library and you will only be writing code to train and obtain predictions. Therefore, this code can go inside the function that will be returned to the Bayesian Optimization.

We have now come to the most important section of the hyperparameter optimization task, the ‘fitness’ function.

@use_named_args(dimensions=dimensions)

def fitness(lstm_num_steps, size, lstm_init_epoch, lstm_max_epoch,

lstm_learning_rate_decay, lstm_batch_size, lstm_dropout_rate, lstm_init_learning_rate):



global iteration, num_steps, lstm_size, init_epoch, max_epoch, learning_rate_decay, dropout_rate, init_learning_rate, batch_size



num_steps = np.int32(lstm_num_steps)

lstm_size = np.int32(size)

batch_size = np.int32(lstm_batch_size)

learning_rate_decay = np.float32(lstm_learning_rate_decay)

init_epoch = np.int32(lstm_init_epoch)

max_epoch = np.int32(lstm_max_epoch)

dropout_rate = np.float32(lstm_dropout_rate)

init_learning_rate = np.float32(lstm_init_learning_rate)



tf.reset_default_graph()

tf.set_random_seed(randomState)

sess = tf.Session()



train_X, train_y, val_X, val_y, nonescaled_val_y = pre_process()



inputs = tf.placeholder(tf.float32, [None, num_steps, features], name=""inputs"")

targets = tf.placeholder(tf.float32, [None, input_size], name=""targets"")

model_learning_rate = tf.placeholder(tf.float32, None, name=""learning_rate"")

model_dropout_rate = tf.placeholder_with_default(0.0, shape=())

global_step = tf.Variable(0, trainable=False)



prediction = setupRNN(inputs,model_dropout_rate)



model_learning_rate = tf.train.exponential_decay(learning_rate=model_learning_rate, global_step=global_step, decay_rate=learning_rate_decay,

decay_steps=init_epoch, staircase=False)



with tf.name_scope('loss'):

model_loss = tf.losses.mean_squared_error(targets, prediction)



with tf.name_scope('adam_optimizer'):

train_step = tf.train.AdamOptimizer(model_learning_rate).minimize(model_loss,global_step=global_step)



sess.run(tf.global_variables_initializer())



for epoch_step in range(max_epoch):



for batch_X, batch_y in generate_batches(train_X, train_y, batch_size):

train_data_feed = {

inputs: batch_X,

targets: batch_y,

model_learning_rate: init_learning_rate,

model_dropout_rate: dropout_rate

}

sess.run(train_step, train_data_feed)



val_data_feed = {

inputs: val_X,

}

vali_pred = sess.run(prediction, val_data_feed)



vali_pred_vals = rescle(vali_pred)



vali_pred_vals = np.array(vali_pred_vals)



vali_pred_vals = vali_pred_vals.flatten()



vali_pred_vals = vali_pred_vals.tolist()



vali_nonescaled_y = nonescaled_val_y.flatten()



vali_nonescaled_y = vali_nonescaled_y.tolist()



val_error = sqrt(mean_squared_error(vali_nonescaled_y, vali_pred_vals))



return val_error

As shown above, we are passing the hyperparameter values to a function named “fitness.” The “fitness” function will be passed to the Bayesian hyperparameter optimization process ( gp_minimize). Note that in the first iteration, the values passed to this function will be the default values that you defined and from there onward Bayesian Optimization will choose the hyperparameter values on its own. We then assign the chosen values to the python global variables we declared at the beginning so that we will be able to use the latest chosen hyperparameter values outside the fitness function.

We then come to a rather critical point in our optimization task. If you have used TensorFlow prior to this article, you would know that TensorFlow operates by creating a computational graph for any kind of deep learning model that you make.

During the hyperparameter optimization process, in each iteration, we will be resetting the existing graph and constructing a new one. This process is done to minimize the memory taken for the graph and prevent the graphs from stacking on top of each other. Immediately after resetting the graph you will have to set the TensorFlow random seed in order to obtain reproducible results. After the above process, we can finally declare the TensorFlow session.

After this point, you can start adding code responsible for training and validating your deep learning model as you normally would. This section is not really related to the optimization process but the code after this point will start utilizing the hyperparameter values chosen by the Bayesian Optimization.

The main point to remember here is to return the final metric value (in this case the RMSE value) obtained for the validation split. This value will be returned to the Bayesian Optimization process and will be used when deciding the next set of hyperparameters that it wants to explore.

Note: if you are dealing with a classification problem you would want to put your accuracy as a negative value (eg. -96) because, even though the higher the accuracy the better the model, the Bayesian function will keep trying to reduce the value as it is designed to find the hyperparameter values for the lowest value that is returned to it.

Let us now put down the execution point for this whole process, the “main” function. Inside the main function, we have declared the “gp_minimize” function. We are then passing several essential parameters to this function.

if __name__ == '__main__':



start = time()



search_result = gp_minimize(func=fitness,

dimensions=dimensions,

acq_func='EI', # Expected Improvement.

n_calls=11,

x0=default_parameters,

random_state=46)



print(search_result.x)

print(search_result.fun)

plot = plot_convergence(search_result,yscale=""log"")



atexit.register(endlog)

logger(""Start Program"")

The “func” parameter is the function you would want to finally model using the Bayesian Optimizer. The “dimensions” parameter is the set of hyperparameters that you are hoping to optimize and the “acq_func” stands for the acquisition function and is the function that helps to decide the next set of hyperparameter values that should be used. There are 4 types of acquisition functions supported by gp_minimize. They are:

LCB: lower confidence bound

EI: expected improvement

PI: probability of improvement

gp_hedge: probabilistically choose one of the above three acquisition functions at every iteration

The above information was extracted from the documentation. Each of these has its own advantages but if you are a beginner to Bayesian Optimization, try using “EI” or “gp_hedge” as “EI” is the most widely used acquisition function and “gp_hedge” will choose one of the above-stated acquisition functions probabilistically thus, you wouldn’t have to worry too much about that.

Keep in mind that when using different acquisition functions there might be other parameters that you might want to change that affects your chosen acquisition function. Please refer the parameter list in the documentation for this.

Back to explaining the rest of the parameters, the “n_calls” parameter is the number of times you would want to run the fitness function. The optimization task will start by using the hyperparameter values defined by “x0”, the default hyperparameter values. Finally, we are setting the random state of the hyperparameter optimizer as we need reproducible results.

Now when you run the gp_optimize function the flow of events will be:

The fitness function will use with the parameters passed to x0. The LSTM will be trained with the specified epochs and the validation input will be run to get the RMSE value for its predictions. Then depending on that value, the Bayesian optimizer will decide what the next set of hyperparameter values it wants to explore with the help of the acquisition function.

In the 2nd iteration, the fitness function will run with the hyperparameter values that the Bayesian optimization has derived and the same process will repeat until it has iterated “n_call” times. When the complete process comes to an end, the Scikit-Optimize object will get assigned to the “search _result” variable.

We can use this object to retrieve useful information as stated in the documentation.

x [list]: location of the minimum.

fun [float]: function value at the minimum.

models: surrogate models used for each iteration.

x_iters [list of lists]: location of function evaluation for each iteration.

func_vals [array]: function value for each iteration.

space [Space]: the optimization space.

specs [dict]`: the call specifications.

rng [RandomState instance]: State of the random state at the end of minimization.

The “search_result.x” gives us optimal hyperparameter values and using “search_result.fun” we can obtain the RMSE value of the validation set corresponding to the obtained hyperparameter values (The lowest RMSE value obtained for the validation set).

Shown below are the optimal hyperparameter values that we obtained for our model and the lowest RMSE value of the validation set. If you are finding it hard to figure out the order in which the hyperparameter values are being listed when using “search_result.x”, it is in the same order as you specified your hyperparameters in the “dimensions” list.

Hyperparameter Values:

lstm_num_steps: 6

lstm_size: 171

lstm_init_epoch: 3

lstm_max_epoch: 58

lstm_learning_rate_decay: 0.7518394019565194

lstm_batch_size: 24

lstm_dropout_rate: 0.21830825193089087

lstm_init_learning_rate: 0.0006401363567813549

Lowest RMSE: 2.73755355221523

Convergence Graph

The hyperparameters that produced the lowest point of the Bayesian Optimization in this graph is what we get as the optimal set of hyperparameter values.

The graph shows a comparison of the lowest RMSE values recorded for each iteration (50 iterations) in Bayesian Optimization and Random Search. We can see that the Bayesian Optimization has been able to converge rather better than the Random Search. However, in the beginning, we can see that Random search has found a better minimum faster than the Bayesian Optimizer. This can be due to random sampling being the nature of Random Search.

We have finally come to the end of this article, so to conclude, we hope this article made your deep learning model building task easier by showing you a better way of finding the optimal set of hyperparameters. Here’s to no more stressing over hyperparameter optimization. Happy coding, fellow geeks!

Useful Materials:"
9 Data Visualization Tools That You Cannot Miss in 2021,"In the field of data science, data visualization is undoubtedly the top word today. No matter what data you want to analyze, doing data visualization seems to be a necessary step. But many people don’t have a specific concept of data visualization, and they don’t know how to implement it. So, today I am going to take you through the definition, concept, implementation process and tools for data visualization.

1. What is data visualization?

Scientific visualization, information visualization, and visual analytics are often seen as the three main branches of visualization. The new discipline “Data Visualization”, which is a combination of these three branches, is a new starting point in the field of visual research.

Generalized data visualization involves various disciplines such as information technology, natural science, statistical analysis, graphics, interaction, and geographic information.

1.1 Scientific Visualization

Scientific visualization is an interdisciplinary research and application field in science, focusing on the visualization of three-dimensional phenomena, such as architecture, meteorology, medicine or biological systems. Its purpose is to graphically illustrate scientific data, enabling scientists to understand, explain, and collect patterns from the data.

1.2 Information visualization

Information visualization is the study of interactive visual representations of abstract data to enhance human cognition. Abstract data includes both digital and non-digital data such as geographic information and text. Graphics such as histograms, trend graphs, flow charts, and tree diagrams all belong to information visualization, and the design of these graphics transforms abstract concepts into visual information.

1.3 Visual Analytics

Visual analytics is a new field that has evolved with the development of scientific visualization and information visualization, with an emphasis on analytical reasoning through an interactive visual interface.

From FineReport

2. Why do we need data visualization?

The amount of information that humans gain through vision is far beyond that of other organs. Data visualization is the use of human natural skills to enhance data processing and organization efficiency.

Visualization can help us deal with more complex information and enhance memory.

Most people don’t know much about statistical data, and basic statistical methods (mean, median, range, etc.) are not in line with human cognitive nature. One of the most famous examples is Anscombe’s quartet. It is difficult to see the law according to statistical methods, but the rules are very clear when the data is visualized.

3. How to achieve data visualization?

Technically, the simplest understanding of data visualization is the mapping from data space to graphic space.

A classic visual implementation procedure is to process and filter the data, transform it into an expressible visual form, and then render it into a user-visible view.

Visualization technology stack

In general, professional data visualization engineers need to master the following technology stack:

· Basic mathematics: trigonometric function, linear algebra, geometric algorithm

· Graphics: Canvas, SVG, WebGL, computational graphics, graph theory

· Engineering algorithms: basic algorithms, statistical algorithms, common layout algorithms

· Data analysis: data cleaning, statistics, data modeling

· Design aesthetics: design principles, aesthetic judgment, color, interaction, cognition

· Visual basis: visual coding, visual analysis, graphical interaction

· Visualization solutions: correct use of charts, visualization of common business scenarios

4. Common data visualization tools

Generally speaking, R language, ggplot2 and Python are used in academia. The most familiar tool for ordinary users is Excel. Commercial products include Tableau, FineReport, Power BI, etc.

D3.js is a JavaScript library based on data manipulation documentation. D3 combines powerful visualization components with data-driven DOM manipulation methods.

Evaluation: D3 has powerful SVG operation capability. It can easily map data to SVG attribute, and it integrates a large number of tools and methods for data processing, layout algorithms and calculating graphics. It has a strong community and rich demos. However, its API is too low-level. There isn’t much reusability while the cost of learning and use is high.

FineReport is an enterprise-level web reporting tool written in pure Java, combining data visualization and data entry. It is designed based on “no-code development” concept. With FineReport, users can make complex reports and cool dashboards and build a decision-making platform with simple drag-and-drop operations.

Evaluation: FineReport can be directly connected to all kinds of databases, and it is convenient and quick to customize various complex reports and cool dashboards. The interface is similar to that of Excel. It provides 19 categories and over 50 styles of self-developed HTML5 charts, with cool 3D and dynamic effects. The most important thing is that its personal version is completely free.

HighCharts is a chart library written in pure JavaScript that makes it easy and convenient for users to add interactive charts to web applications. This is the most widely used chart tool on the Web, and business use requires the purchase of a commercial license.

Evaluation: The use threshold is very low. HighCharts has good compatibility, and it is mature and widely used. However, the style is old, and it is difficult to expand charts. And the commercial use requires the purchase of copyright.

Echarts is an enterprise-level chart tool from the data visualization team of Baidu. It is a pure Javascript chart library that runs smoothly on PCs and mobile devices, and it is compatible with most current browsers.

Evaluation: Echarts has rich chart types, covering the regular statistical charts. But it is not as flexible as Vega and other chart libraries based on graphic grammar, and it is difficult for users to customize some complex relational charts.

Leaflet is a JavaScript library of interactive maps for mobile devices. It has all the mapping features that most developers need.

Evaluation: It can be specifically targeted for map applications, and it has good compatibility with mobile. The API supports plug-in mechanism, but the function is relatively simple. Users need to have secondary development capabilities.

Vega is a set of interactive graphical grammars that define the mapping rules from data to graphic, common interaction grammars, and common graphical elements. Users can freely combine Vega grammars to build a variety of charts.

Evaluation: Based entirely on JSON grammar, Vega provides mapping rules from data to graphics, and it supports common interaction grammars. But the grammar design is complex, and the cost of use and learning is high.

deck.gl is a visual class library based on WebGL for big data analytics. It is developed by the visualization team of Uber.

Evaluation: deck.gl focuses on 3D map visualization. There are many built-in geographic information visualization common scenes. It supports visualization of large-scale data. But the users need to have knowledge of WebGL and the layer expansion is more complicated.

Power BI is a set of business analysis tools that provide insights in the organization. It can connect hundreds of data sources, simplify data preparation and provide instant analysis. Organizations can view reports generated by Power BI on web and mobile devices.

Evaluation: Power BI is similar to Excel’s desktop BI tool, while the function is more powerful than Excel. It supports for multiple data sources. The price is not high. But it can only be used as a separate BI tool, and there is no way to integrate it with existing systems.

Tableau is a business intelligence tool for visually analyzing data. Users can create and distribute interactive and shareable dashboards, depicting trends, changes and densities of data in graphs and charts. Tableau can connect to files, relational data sources and big data sources to get and process data.

Evaluation: Tableau is the simplest business intelligence tool in the desktop system. It doesn’t force users to write custom code. The software allows data mixing and real-time collaboration. But it’s expensive and it performs less well in customization and after-sales services.

Conclusion

Data visualization is a huge field with many disciplines. It is precisely because of this interdisciplinary nature that the visualization field is full of vitality and opportunities.

You might also be interested in…

8 Best Reporting Tools& Software To Improve Your Business

5 Most Popular Business Intelligence (BI) Tools in 2021

4 Parameter Query Functions to Make your Data Visualization Interactive

A Step-by-Step Guide to Making Sales Dashboards

How Can Beginners Design Cool Data Visualizations?"
K-Nearest Neighbors (KNN) Algorithm,"K-Nearest Neighbors (KNN) Algorithm

A Brief Introduction Afroz Chakure · Follow Published in DataDrivenInvestor · 4 min read · Jul 6, 2019 -- Listen Share

Simple Analogy for K-Nearest Neighbors (K-NN)

In this blog, we’ll talk about one of the most widely used machine learning algorithms for classification, which is the K-Nearest Neighbors (KNN) algorithm. K-Nearest Neighbor (K-NN) is a simple, easy to understand, versatile and one of the topmost machine learning algorithms that find its applications in a variety of fields.

In this blog we’ll try to understand what is KNN, how it works, some common distance metrics used in KNN, its advantages & disadvantages along with some of its modern applications.

What is K-NN ?

K-NN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution i.e. the model structure is determined from the dataset.

It is called Lazy algorithm because it does not need any training data points for model generation. All training data is used in the testing phase which makes training faster and testing phase slower and costlier.

K-Nearest Neighbor (K-NN) is a simple algorithm that stores all the available cases and classifies the new data or case based on a similarity measure.

K-NN classification

In K-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.

To determine which of the K instances in the training dataset are most similar to a new input, a distance measure is used. For real-valued input variables, the most popular distance measure is the Euclidean distance.

The Red point is classified to the class most common among its k nearest neighbors..

The Euclidean distance

The Euclidean distance is the most common distance metric used in low dimensional data sets. It is also known as the L2 norm. The Euclidean distance is the usual manner in which distance is measured in the real world.

where p and q are n-dimensional vectors and denoted by p = (p1, p2,…, pn) and q = (q1, q2,…, qn) represent the n attribute values of two records.

While Euclidean distance is useful in low dimensions, it doesn’t work well in high dimensions and for categorical variables. The drawback of Euclidean distance is that it ignores the similarity between attributes. Each attribute is treated as totally different from all of the attributes.

Other popular distance measures :

Hamming Distance : Calculate the distance between binary vectors.

: Calculate the distance between binary vectors. Manhattan Distance : Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance.

: Calculate the distance between real vectors using the sum of their absolute difference. Also called City Block Distance. Minkowski Distance: Generalization of Euclidean and Manhattan distance.

Steps to be carried out during the K-NN algorithm are as follows :

Divide the data into training and test data. Select a value K. Determine which distance function is to be used. Choose a sample from the test data that needs to be classified and compute the distance to its n training samples. Sort the distances obtained and take the k-nearest data samples. Assign the test class to the class based on the majority vote of its k neighbors.

Steps to be carried in KNN algorithm

Performance of the K-NN algorithm is influenced by three main factors :

The distance function or distance metric used to determine the nearest neighbors. The decision rule used to derive a classification from the K-nearest neighbors. The number of neighbors used to classify the new example.

Advantages of K-NN :

The K-NN algorithm is very easy to implement. Nearly optimal in the large sample limit. Uses local information, which can yield highly adaptive behavior. Lends itself very easily to parallel implementation.

Disadvantages of K-NN :

Large storage requirements. Computationally intensive recall. Highly susceptible to the curse of dimensionality.

K-NN Algorithm finds its applications in :

Finance — financial institutes will predict the credit rating of customers. Healthcare — gene expression. Political Science — classifying potential voters in two classes will vote or won’t vote. Handwriting detection. Image Recognition. Video Recognition. Pattern Recognition."
Faster Training for Efficient CNNs,"Learning rate scheduler

ESPNetv2 introduces a variant of the cosine learning rate, wherein the learning rate is decayed linearly until cycle length and then restarted. At each epoch t, the learning rate ηₜ is computed as:

Figure 1: Cyclic learning rate policy with linear learning rate decay and warm restarts

F aster training

With the learning rate scheduler described above, we train ShuffleNetv2 for a total of 120 epochs with a batch size of 512 across four TitanX GPUs using SGD with momentum at two different FLOP settings: (1) 41 MFLOPs and (2) 146 MFLOPs. For the first 60 epochs, we set ηₘᵢₙ, ηₘₐₓ, and T to 0.1, 0.5, and 5, respectively. For the remaining 60 epochs, we set ηₘₐₓ=0.1, T=60, and ηₘᵢₙ=ηₘₐₓ/T. The figure below visualizes the learning policy for 120 epochs.

Figure 2: Cyclic learning rate scheduler with warm restarts for faster training proposed in the ESPNetv2 paper.

Results on the ImageNet dataset

Results are given in Figure 3 and Table 1. We can clearly see that the learning rate scheduler (discussed above) enables faster training while delivering similar performance as the linear learning rate scheduler in ShuffleNet paper."
How to understand Numpy documentation,"When we start to learn Data Science, Machine Learning, Deep Learning or any excited fields that will be using Python as programming language, most probably all of us will be using numpy as well. In this post, I will be writing numpy basics and how to read documentation properly based on my experience of using numpy. Before reading my post, it is good if you know the basics of programming language and python. I will be using python 3.6 as examples.

What is Numpy

You might wonder why all machine learning tutorials will be using numpy, why not other libraries? Please let me talk about what is Python before we go into numpy.

Python is a programming language that we can import library that is written in C++ magically. C++ is a very fast language and the code will be compiled to machine code directly, hence library in C++ can perform faster than library written purely by python.

Numpy is fast! And it is maintained by a lot of great programmers over the Internet.

Numpy is an open source library written in C++, it is the fundamental package for scientific computing with Python. It is very easy to be installed like below:

pip install numpy

It contains a lot of tools, algorithms for most of the machine learning tasks such as

powerful N-dimensional array object sophisticated (broadcasting) functions useful linear algebra, Fourier transform, and random number capabilities

Common technical terms in numpy

shape

np.random.rand(d0, d1, …, dn) — Random values in a given shape.

np.full(shape, fill_value, dtype=None, order=’C’) — Return a new array of given shape and type, filled with fill_value.

Whenever the description said “shape”, it means the array size, or dimension size. If “shape” of a numpy array is [4, 3, 2], meaning it is a 3d array, because the shape itself has 3 values 4, 3, and 2. Let see the image below.

An array with shape of [4, 3, 2]. 4 2d array with [3,2]. For each 2d array, it has 3 rows and 2 columns.

For array with shape of 4d, you can think the array contains many 3d arrays. Then you can imagine the data using 3d array like the image above."
A brief intro to the Central Limit Theorem,"A brief intro to the Central Limit Theorem

The theory they say you can’t be a data scientist without knowing… GreekDataGuy · Follow Published in Towards Data Science · 3 min read · Nov 9, 2019 -- 1 Listen Share

According to wikipedia.

In probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a “bell curve”) even if the original variables themselves are not normally distributed.

Translation: If you take enough samples from a population, the means of those samples will approximate a normal distribution.

The cool thing is that this works for a population with (almost) ANY distribution. Let’s generate some examples and prove that.

Generate random data

Generate 300 random numbers between 0 and 50.

import matplotlib.pyplot as plt

import random X = [] for i in range(0,300):

v = random.randint(0,50)

X.append(v)

Plot it so we can see the shape of the data.

plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})

plt.hist(X, bins = 50)

Definitely not a normal distribution

Take samples and calculate the mean

Now take 10,000 samples with a sample size of 5, calculate the mean of each sample, and plot the frequency of the means.

import numpy as np means = [] for i in range(0,10000):

sample = []



for ii in range(0,5):

v = random.choice(X)

sample.append(v)



mean = np.mean(sample)

means.append(mean)

Plot the distribution

Plot the frequency of the mean of each sample.

Looking a little like a normal distribution. Pretty cool, huh?

Now let’s increase the sample size from 5 to 30.

Even more like a normal distribution.

And this happened despite the original population not following a normal distribution at all.

My generated data was intended to be random. But the exact same affect can be achieved with almost any distribution, continuous or discrete, beta, gamma or uniform.

Why this is important

If the mean of enough samples approximates a normal distribution, we can “pretend” the distribution of the sample itself is also normal. And feel more comfortable making inferences about the population as a whole given a single sample.

The above is my understanding but I’m not a statistician. So I’d like to put it to the crowd. Why is the central limit theorem so important to statistics and machine learning?"
Machine Learning for Content Moderation — Challenges,"Machine Learning for Content Moderation — Challenges

Overview

For an introduction to the topic of machine learning for content moderation, read the Introduction of this series:

Now that we have gone over an overview of machine learning systems used for automatic content moderation, we can address the main challenges faced by these systems. These potential problems can lead to difficulties in evaluating the model, determining approaching classifier thresholds, and using it fairly and without unintentional bias. Since content moderation systems act on complex social phenomena, they face problems that are not necessarily encountered in other machine learning contexts.

Varying definitions

For many applications of content moderation, it is difficult to provide an explicit definition of the phenomenon of interest. These topics are often very complex social phenomena, whose definitions are topics of constant academic debate. For example, cyber-bullying has various definitions in academic texts, and so it is difficult to create an all-encompassing definition that everyone can agree on. For this reason, instructions provided to manual content labelers may not be clear enough to produce very reliable labels.

This leads to two problems.

First, it may appear to users that the content moderation system is inconsistent, if some things that users deem to be violating rules are removes and some are not. This may lead users to not trust the content moderation mechanisms or believe that it is unfairly targeting certain users. Since, from the user perspective, these systems are nebulous black boxes, it is difficult to explain how such inconsistencies may arise.

The other problem is that the labeled training data may have contradictory data points. If labeling inconsistencies cause two very similar data points to have opposing labels, then…"
Uber’s Ludwig v0.2: New features and Improvements,"Working with Audio/Speech features

Ludwig 0.2 makes it incredibly easy to work with audio data. Whilst with other libraries you need to have extensive knowledge about audio preprocessings and what models are best for working with audio data Ludwig allows you to work with audio data by only specifying the audio tag.

Using this feature we can easily build an audio classification model. I used the Audio Cats and Dogs data-set which is freely available on Kaggle.

For this data-set we first on need to create a csv file containing a column with the file paths and another column with the class of the audio file. This can be done using a simple Python script.

After executing this script we are ready to create our model_definition.yaml file. As an input, we will have the audio feature which can be specified as shown above. As an output, we will either have a cat or dog so we can use a binary or category type.

This can now be trained using Ludwig’s train command."
Categorical Embedding and Transfer Learning,"Introduction

A lot of machine learning algorithms can not deal with categorical variables directly unless they are converted to numbers. However, the problem is that their performance varies significantly by the way these categorical variables are encoded into numbers. This article explores the problem of categorical encoding and gives a glimpse of the old and new methods.

Categorical Variables

A categorical variable is a variable that can take one of the possible fixed and mostly limited set of values. They are associated with features that are qualitative and thus cannot be measured. For example, the day of the week is a categorical variable that can take 7 discrete values. The words/tokens of any language are categorical variables. Machine Learning algorithms are devoted to working with numbers so we have to convert these categorical variables to numbers to please the algorithms. This process is full of pitfalls and we run the risk of losing a lot of information.

Let’s take a look at a couple of common ways to convert categories into numbers and the problems that are associated with them.

Label Encoding

In this process, we assign a discrete number to each unique category using some defined process. For example, we can sort the variables in order of the number of occurrences and number them in increasing order. Another way is to randomly assign a number to each unique category.

Simple Label Encoder

In the above example, the days are labeled in the order of their appearance in the data. The major problems here are:-

Natural ordering is lost Common relationships between categories are not captured. (For example, Saturday and Sunday together make a weekend and hence should be closer to each other)

One Hot/Dummy Encoding

In this scheme, we split the categorical variable into separate binary variables (1 variable for each unique category) such that each new variable is set to 1 when the example belongs to a particular category and 0 otherwise. The below example will make things clear."
Steps to basic modern NN model from scratch,"Step:2&3 — Relu/init & Forward pass

After we have defined the matrix multiplication strategy, its time to defined the ReLU function and the forward pass for the Neural Network. I would request the readers to go through the Part — 1 of the series to get the background of the data used below.

The Neural Network is defined as below:

output = MSE(Linear(ReLU(Linear(X))))

Basic Architecture

n,m = x_train.shape

c = y_train.max()+1

n,m,c

Let us explain the weights for the matrix multiplication.

I will create a 2-layer neural network.

The first linear layer will do the matrix multiplication of the input with w1.

The output of the first linear layer will be the input for the second linear operation, where the input will be multiplied with the w2.

Instead of getting ten predictions for a single input, I will obtain the single output and will use MSE to calculate the loss.

Let us declare the weights and biases.

w1 = torch.randn(m,nh)

b1 = torch.zeros(nh) w2 = torch.randn(nh,1)

b2 = torch.zeros(1)

When we declare the weights and biases using the PyTorch randn , then the weights and biases obtained are normalized, i.e. they have a mean of 0 and a standard deviation of 1.

, then the weights and biases obtained are normalized, i.e. they have a of 0 and a of 1. We need to normalise the weights and biases so that they do not lead to substantial values after the linear operation with the input to the Neural Network. Because large outputs become difficult for the computers to handle. Therefore, we prefer to normalise the inputs.

For the same reason, we want out input matrix to have a mean of 0 and a standard deviation of 1, which is not at present. Lets us see.

train_mean,train_std = x_train.mean(),x_train.std()

train_mean,train_std

Let us define a function to normalise the input matrix.

def normalize(x, m, s): return (x-m)/s x_train = normalize(x_train, train_mean, train_std)

x_valid = normalize(x_valid, train_mean, train_std)

Now, we are normalising the training dataset and the validation dataset on the same mean and standard deviation so that our training and validation dataset has the same feature definitions and scale. Now, let’s recheck the mean and standard deviation.

Mean approx. to 0 & SD approx. to 1

Now, we have normalised weights, biases, and input matrix.

Let us define the linear layer for the Neural Network and perform the operation.

def lin(x, w, b): return x@w + b t = lin(x_valid, w1, b1)

t.mean(),t.std()

Now, the mean and standard deviation obtained after the linear operation is again non-normalized. Now, the problem is still pure. If it remains like this, more linear operations will lead to significant and substantial values, which will be challenging to handle. Thus, we want our activations after the linear operation to be normalized as well.

Simplified Kaiming Initialization or He Initialization

To handle the non-normalized behaviour of linear neural network operation, we define weights to be Kaiming initialized. Though Kaiming Normalization or He initialisation is defined to handle ReLu/Leaky ReLu operation, we still can use it for linear operations.

We divide our weights by math.sqrt(x) where x is the number of rows.

After the above trivia, we get the normalized mean and SD.

def lin(x, w, b): return x@w + b t = lin(x_valid, w1, b1)

t.mean(),t.std()

Let us define the ReLU layer for the Neural Network and perform the operation. Now, why we are defining the ReLU as a non-linear activation function, I hope you are aware of the Universal Approximation Theorem.

def relu(x): return x.clamp_min(0.)

t = relu(lin(x_valid, w1, b1)) t.mean(),t.std()

t = relu(lin(x_valid, w1, b1))

t = relu(lin(t, w2, b2)) t.mean(),t.std()

Did you notice something weird?

Notice above that our standard deviation gets halved of the one obtained after the linear operation, and if it gets halved after one layer, imagine after eight layers it will get to 1/2²⁸, which is very very small. And if our neural network has got 10000 layers 😵, forget about it.

From PyTorch docs:

a: the negative slope of the rectifier used after this layer (0 for ReLU by default)

This was introduced in the paper that described the Imagenet-winning approach from Kaiming He and others: Delving Deep into Rectifiers, which was also the first paper that claimed “super-human performance” on Imagenet (and, most importantly, it introduced ResNets!)

Thus, following the same strategy, we will multiply our weights with math.sqrt(2/m) .

w1 = torch.randn(m,nh)*math.sqrt(2/m) t = relu(lin(x_valid, w1, b1))

t.mean(),t.std()

It is still better than (0.1276, 0.5803).

Though we have better results, still the mean is not so good. As per the fastai docs, We could handle the mean by the below tweak.

def relu(x): return x.clamp_min(0.) - 0.5 w1 = torch.randn(m,nh)*math.sqrt(2./m )

t1 = relu(lin(x_valid, w1, b1))

t1.mean(),t1.std()

Now, it is so much better.

Let us combine the above all code and strategies and create the forward pass of our Neural Network. PyTorch has a defined method for Kaiming Normalization i.e kaiming_normal_ .

def model(xb):

l1 = lin(xb, w1, b1)

l2 = relu(l1)

l3 = lin(l2, w2, b2)

return l3 %timeit -n 10 _=model(x_valid)

The last to be defined for the forward pass is the Loss function: MSE.

As per our previous knowledge, we generally use CrossEntroyLoss as the loss function for single-label classification functions. I will address the same later. For now, I am using MSE to understand the operation.

def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()

Let us perform the above operations for the training dataset.

preds = model(x_train)

preds.shape, preds

To perform the MSE, we need the floats.

y_train,y_valid = y_train.float(),y_valid.float() mse(preds, y_train)

After all of the above operation, one question is still not answered substantially and it is

Why do we need Kaiming Initialization?

Let us understand it again.

Initialise two tensors as below.

import torch x = torch.randn(512)

a = torch.randn(512,512)

For Neural Networks, the primary step is the matrix multiplication and if we have a deep neural network with approx 100 layers, then let us see what will the standard deviation and mean of the activations obtained.

for i in range(100):

x = x @ a x.mean(), x.std()

mean=nan, sd=nan

We can easily see that mean, and a standard deviation is no longer a number. And it is justified as well. The computer is not able to store that large numbers; it cannot account for such large numbers. It has restricted the practitioners to train such deep neural networks for the same reason.

The problem you’ll get with that is activation explosion: very soon, your activations will go to nan. We can even ask the loop to break when that first happens:

for i in range(100):

x = x @ a

if x.std() != x.std(): break i

Just after 28 loops

Thus, such problems lead to the invention of Kaiming Initialization. It surely took decades to come up with the idea finally.

So, that’s how we define the ReLu and backward pass for the neural network."
Bayesian Modeling of Pro Overwatch Matches with PyMC3,"Photo by AC De Leon on Unsplash

Professional eSports are becoming increasingly popular, and the industry is growing rapidly. Many of these professional game leagues are based on games that have two teams that battle it out. Call of Duty, League of Legends, and Overwatch are all examples. Although these are comparable to traditional team sports, the novelty of eSports means that less attention has been paid to predicting the outcomes of this burgeoning industry.

How do you model adversarial team games? I was first inspired to start this project by reading Christopher Bishop’s new machine learning eBook, which has a chapter devoted to calculating player rankings in games using the Elo rating system. For those of you familiar with Chess, you may have encountered this metric before. I found a very instructive blog post on The Data Skeptic for this exact topic. All equations posted below were borrowed from the aforementioned Skeptic.

In essence, Elo is a framework where the strength of a player is measured by a single number (r_1 = 2000). The predicted strength of any given team is represented by a Logistic Function (R), and the outcome of a given match as a ratio of the strength of a single team over the sum of their rating (E).

Logistic Function for player i.

Expected Outcome for player i

If a player of strength 2000 is to face off against a player with an Elo rating of 1000, we would predict a very likely win for the former player (E=0.9968).

Rating update for player i (K~32 in Chess)

At the end of each match, the players’ scores are updated based on their relative strengths. If the favored player wins (S_i = 1.0), they gain a small positive adjustment in their score. If the underdog wins the match, they can gain a larger amount, as the result was unexpected. In the given example of the underdog winning, their score would change from 1000 to 1031. The favored player’s score would also decrease.

What does this have to do with Overwatch, the immensely popular team-based shooter? I wondered how someone could model the incredible complexity of a team-based sport that is so dynamic without resorting to incredibly convoluted architectures? I saw an example code on the PyMC3 website about encoding professional rugby teams using Hierarchical Normal Distributions. Having just done a blog post on Hierarchical models, I thought I could try something similar!

Essentially, we are treating each team as a single entity. This is in spite of the fact that teams consist of multiple players, each working autonomously. We will justify this choice in two ways: 1) It is easier to model a team as a single rating, rather than each individual player, 2) the teammates are working together (ideally), so they can be modeled as a collective unit. The Elo rating is also incredibly helpful if two teams have not played against one another before. Without any historical matchups, we can leverage team matches that the two adversaries had in common to make predictions.

Data were scraped from the 2018 season of The Overwatch League. Unfortunately, there are not that many matches from which we can train our model. As the 2019 season is currently ongoing, any interested reader could begin to add this data to the model and see how well it performs! The raw data format had to be cleaned and processed prior to training our model.

Win/Loss aggregations for the regular season

There are two basic ways we can represent our data, which will change how we construct the model. The simplest approach is to make this a classification challenge. Each Overwatch matchup involves multiple rounds, and the winner of the match secures the most winning rounds. For instance, San Francisco Shock beats Florida Mayhem 3–2 . We can represent this as a total of 5 records in the data with varying levels of success and failure (0/1).

Classification or Bernoulli trial for this simple example

We can construct our data split as follows. For each matchup in the regular season, we model each win and loss as a binary target. Each team’s Elo rating will be inferred based on the equations listed above, which transform a rating into a Bernoulli trial. Then, we will use our learned ratings to predict all of the matches in the playoffs. Our PyMC3 model will then consist of a Binomial: for a given matchup, do we predict a success or a failure?

A sample trace of our simple model.

We can also represent our matchups as fractions. For the same example of a 3–2 match, with San Francisco Shock winning, we can encode this as a 0.6 win. Now we have slightly more flexibility, as we can model this as a regression task, or still keep it as a classification. This result offers the model more nuance as well. A 0.8 win is a much more favorable outcome than a 0.6 versus the same opponent.

The ROC suggests the model is decent at predicting matchups

Regardless of your choice, the training error seems to be pretty stable. Our model is able to predict which teams will win based on their Elo distributions. What will happen when we apply this model to the Playoffs?

We are making a few assumptions during our playoffs evaluation:

Teams at the end of the regular season are fixed in skill, players, strategy, etc. That the playoffs are substantive enough that less probable events cannot dominate the results (small number of samples) That our regular season matchups were fairly distributed among all teams (round-robin)

These assumptions will definitely rear their ugly head. There are only 12 playoff matches in 2018, and eliminations come quickly. This means that if a team had a fairly poor match that is un-characteristic of their true skill, they may not have a chance to redeem themselves. Teams also can gain in strength when a true challenge appears, as is the nature of playoffs.

When we apply our model to the playoff data, we get very poor performance. Our model might as well be randomly guessing! What happened here?

Playoff Record and the predictions

In the simplest terms, we had a few teams that upset the apple cart. Philly Fusion (Team 9) beat both Boston and New York (winning 4/5 matches) , despite having a lower Elo rating. Fusion then lost to London Spitfire, who ended up winning the tournament, despite being lower ranked than LA Valiant. Go figure! This is why people love to watch sports, as you never know who might win.

If you are itching to try a few different Bayesian methods to games like this, Allen Downey’s book Think Bayes has a section on modelling teams via Poisson processes and their historical scoring record. Recently Tuan posted an article on Towards Data Science on building a betting strategy for professional soccer leagues, which I think would also apply here.

Feel free to download the data and my notebooks via my Kaggle repo. I also have a Git repo with some selected notebooks. Leave a comment or a question below and I will do my best to answer it. Thanks for reading!"
Top Competitive Data Science Platforms other than Kaggle,"The best way of to learn anything is by doing.

What do you do after you have completed hundreds of MOOCs, consumed thousands of books and notes, and listened to a million people rant about their experience in Data Science? You start applying the concepts. The only way to apply machine learning concepts is by getting your hands dirty. Either find some real-world problems in your area of interest or participate in Hackathons and Machine learning Competitions.

Competitive Data Science is not all about applying algorithms. An algorithm is essentially a tool, and anybody can use it by writing just a few lines of code. The main takeaway from participating in these competitions is that they provide a great learning opportunity. Of course, real-life problems are not necessarily the same as those provided in the competitions; still, these platforms enable you to apply your knowledge to processes and see how you fare compared to others.

Advantages of participating in Data Science Competitions

You have a lot to gain and practically nothing to lose by participating in these competitions. It has both tangible and intangible benefits like:

Great learning opportunity.

Getting exposed to state-of-the-art approaches and datasets.

Networking with like-minded people. Working in teams is even great since it helps to think over a problem from different perspectives.

Showcasing your talent to the world and a chance of getting recruited

It is also fun to participate and see how you fare on the leaderboard.

The prize is a bonus but shouldn’t be the sole criteria.

Kaggle is a well-known platform for Data Science competitions. It is an online community of more than 1,000,00 registered users consisting of both novice and expert. However, apart from Kaggle, there are other Data Mining Competition Platforms worth knowing and exploring. Here is a brief overview of some of…"
"Inspired by Optimism, Married to Technology, Crossing the AI Chasm","How can technology be a positive force in addressing the expected job losses due to Automation and Artificial Intelligence?

According to one study, millions of workers will likely need to change occupations in the next decade, 2–14% of the global workforce depending on the speed of adoption of automation/AI.

If you are in one of the job types charted below (from The Economist about the impact of AI Globally), and you are currently rolling burritos, re-siding buildings or cleaning houses, the path may be steep to obtain jobs at less risk of replacement by automation.

This begs the question as to how a level of optimism could spread into sectors of the U.S. population which are expected to be the most highly impacted economically by automation and AI, based on this McKinsey report:

“We measured job displacement as a percentage of jobs potentially lost due to automation by 2030 and found that because of their concentration in occupations at risk of automation, African Americans have one of the highest rates of potential job displacement when compared with other groups. “This translates to “a potential loss of approximately 132,000 African American jobs due to automation by 2030.”

These challenges may be higher for men than for women. The McKinsey paper notes that African American women, for example, have higher participation in K-12 education, nursing and other caregiving professions, which gives them greater access to the projected job growth in the healthcare and education sectors. These jobs “have a lower automation potential due to the need for dynamic, physical motions and deep…"
Why Companies Are Using Data Science and Analytics to Inform Benefits Packages,"Employee benefits packages can help candidates choose to take job offers or look elsewhere. They can also factor into how long a worker stays at a company and how happy they are while there. If they realize that other companies offer better benefits and they’re frustrated with their job already, they may decide it’s not worth it to stick around.

You may have noticed a trend where companies use data science and analytics to enhance their benefits packages. Here’s a look at why that’s such an appealing move for them to make.

Data Analysis Increases Personalization Possibilities

It’s not always easy for companies to figure out which benefits their workers will use. Statistics strongly suggest that one of the reasons for the disuse is because the benefits packages are too generic. A press release from Welltok showed that employers have work to do to satisfy their employees, particularly related to health benefits.

The release indicated that more than 60% of workers seek support from their employers for all aspects of health care support. Additionally, 56% of employees say they’ve received irrelevant assistance, causing them to waste time and money. That’s likely because 84% of respondents said their companies offered generic programs.

Discount retailer Walmart is one of the companies turning to data analytics to steer and streamline workers’ health care decisions. It’s working with a company called Embold Health, which will scrutinize data from public and private health care plans to find the most effective and cost-efficient providers. Besides helping workers find primary care physicians, the system can connect them to specialists that deal with cardiology, obstetrics and more.

Analytics Provide Better Visibility Into Workers’ Compensation Cases

Data science has also opened potential regarding workers’ compensation claims. The specifics surrounding a workers’ compensation claim vary by state. For example, in Pennsylvania, insurance companies have 21 days to decide whether to accept or deny requests for coverage. Some insurance providers use data science to compile a patient’s medical history and look for trends that help them draw conclusions about claims and further treatment.

Taking this approach enables making improvements to patient care decisions and spotting warning signs. For example, an analytics system could flag cases when a patient with an addiction history sees a provider that historically prescribes opioids frequently.

From an employer side of things, companies could take deep dives into incidents that cause people to file workers’ compensation paperwork. If the entity discovers that a substantial number of cases relate to a certain piece of equipment, a department or a time of day, such information might trigger them to make internal changes that reduce injury rates.

Data Science Increases Benefits Awareness

Benefits utilization is something that businesses want to maximize. If they provide employee benefits that rarely get used, those companies waste money, and their workers may see them as out of touch. Evive is a Chicago-based company that analyzes multiple sources of data to learn which ones their clients’ employees use most.

This aim can also reduce the problem of workers failing to take advantage of benefits because they don’t know they exist. According to one Evive case study associated with a food and beverage company, the company wanted to increase registrations for educational webinars that help attendees understand the financial impacts of particular benefits decisions.

Evive used its technology to enhance the employee communications platform and send targeted messages about upcoming webinars, and the efforts paid off. The registrations for a webinar about HSA basics increased by 339.1%, while the attendance rate boosted by more than 340%, for example.

Analytics Can Inform Company Choices

Data analytics are also crucial for illuminating which benefits are most relevant to workers’ changing needs. Lacoste reportedly uses a data analytics platform called Open Blend to assess employees’ well-being and receive live updates that help companies see what’ll help employees stay motivated the most.

Companies can depend on data analytics when making decisions about offering new benefits, too. Doing that could be especially effective regarding benefits that are less common but perhaps should be more widely available. A pet insurance company polled more than 4,000 policyholders and found that 70% would think a pet-friendly workplace was appealing if interviewing for a new job.

Businesses can no longer merely rely on assumptions when picking which benefits to offer or choosing the ones to discontinue. Data analytics platforms can help enterprises sift through employee feedback or other pertinent metrics, ultimately making company representatives feel more confident about their benefits package choices.

More Data Dependence Should Spur Positive Changes

It should now be evident to you why the data science push for benefits packages is favorable for employees and companies alike. When company representatives take the time to analyze the respective data, they’ll be more able to tailor their offerings to match what employees want and need. Doing that increases the likelihood of workers using the benefits available to them.

Then, when workers feel that their employers provide attractive benefits, it’ll be easier for the employees to quantify the perks of staying with an organization rather than leaving it for better opportunities.

Similarly, if workplaces regularly administer surveys to their employees to gauge what’s working and what isn’t, they’ll be able to make efficient adjustments that keep people satisfied."
Predicting vs. Explaining,"Predicting vs. Explaining

A directed acyclic graph depicting the causal pathways to foetal alcohol spectrum disorders

The Cultural War in Cognitive Science

I recently stumbled across this really juicy debate on natural language processing that took place a few years ago between the old guard of the field, Noam Chomsky, who’s considered as “the father of modern linguistics,” and the new guard, Peter Norvig, Director of Research at Google. When commenting on where the field was heading, Chomsky said the following:

“Suppose that somebody says he wants to eliminate the physics department and do it the right way. The “right” way is to take endless numbers of videotapes of what’s happening outside the video, and feed them into the biggest and fastest computer, gigabytes of data, and do complex statistical analysis — you know, Bayesian this and that — and you’ll get some kind of prediction about what’s gonna happen outside the window next. In fact, you get a much better prediction than the physics department will ever give. Well, if success is defined as getting a fair approximation to a mass of chaotic unanalyzed data, then it’s way better to do it this way than to do it the way the physicists do, you know, no thought experiments about frictionless planes and so on and so forth. But you won’t get the kind of understanding that the sciences have always been aimed at — what you’ll get at is an approximation to what’s happening.”

Chomsky reinforced that sentiment repeatedly elsewhere: that the current definition of success in natural language processing — namely predictive accuracy — is not science. Throwing “some immense corpus of text” into a “complicated machine” is merely “approximating unanalyzed data,” or “butterfly collecting,” that would not lead to “real understanding” of the language. He argues that the main goal of science is to “discover explanatory principles” of how a system actually works, and the “right approach” to achieve that goal is to “let the theory guide the data”: study the system’s basic nature by abstracting away “irrelevant intrusions” through carefully designed experiments — the same way modern science has been conducted since Galileo. In his own succinct words: “Just trying to deal with the unanalyzed chaotic data is unlikely to get you anywhere, just like as it wouldn’t have gotten Galileo…"
How Data Science will Impact Future of Businesses?,"How Data Science will Impact Future of Businesses?

With more than 6 billion (and counting) devices connected to the internet, approximately 2.5 million terabytes of data are generated every day. By the year 2020, millions of more new devices are expected to get connected, projecting around 30 million terabytes of data each day. This figure will definitely fascinate you. Sophia Martin · Follow Published in Towards Data Science · 7 min read · Sep 17, 2019 -- Listen Share

How Data Science will Impact Future of Businesses?

I am sure you are aware of the massive layoffs across different tech giants all across the world. Therefore, at this time, one thing that becomes crucial is the need to re-skill to something which is more authoritative and rewarding — Data Science.

According to forrester, by the year 2020, data-driven businesses will be “collectively worth $1.2 trillion, which is up from $333 billion in the year 2015. Data scientists are generally employed to help different companies adopt various data-centric approaches to their companies.

Since data scientists have an in-depth understanding of data, they work very well in moving organizations towards deep learning, machine learning, and AI adoption as these companies generally have the same data-driven aims. They also help in software development services for that software that includes lots of data and analytics.

Data scientists help companies of all sizes to figure out the ways to extract useful information from an ocean of data to help optimize and analyze their organizations based on these findings. Data scientists focus on asking data-centric questions, analyzing data, and applying statistics & mathematics to find relevant results.

Data scientists have their background in statistics & advanced mathematics, AI and advanced analysis & machine learning. Companies that want to run an AI based project, it is crucial to have a data scientist on the team in order to customize algorithms, make the most of their data, and weigh data-centric decisions.

In order to help the enterprise prepare for the bright future of data science, we have outlined the following 5 key factors shaping the future of the data science industry.

1. Making data actionable for data science

Making data actionable for data science

Poorly crafted data is one of the biggest obstacles to the success of data science. In order to accelerate data science projects and reduce failures, CDOs and CIOs must focus on improving the quality of data and providing data to teams that are relevant to the projects at hand and is actionable

2. Shortage of data science talent

While data science remains one of the highest growth areas for the new graduates, the need far exceeds the available supply. The solution continues to accelerate hiring, whereas, also looking at alternative means for other professionals in areas such as analytics and BI in order to accelerate the data science process and democratize data science access. This is where automation can have an impact on data science.

3. Accelerating “time to value”

Accelerating “time to value”

Data science is an iterative process. It includes creating a “hypothesis” and then testing it. This backward and forward approach involves many experts — from data scientists to subject matter experts and data analysts. Enterprises — small or big — must find ways to speed up this “effort, repeat test” process and accelerate the process of data science for greater forecasting.

4. Transparency for business users

One of the biggest barriers to the adoption of data science applications is a lack of trust on the part of business users. Although machine learning models can be very useful, many business users don’t rely on processes they don’t understand. Data science needs to find different ways to build machine learning models to convince business users and to make users easier to trust.

5. Improving operationalization

Improving operationalization

One of the other hurdles to the growth of Data Science Adoption is how difficult it can be operationalized. Different models that work well in the laboratory don’t work well in a production environment. Even when models are successfully deployed, continuous changes and increases in production data can negatively affect this model over time. This means that “fine-tuning” the ML model to be an effective post-production method- is a crucial part of this process.

6. A Staggering Amount of Data Growth

People generate data every day, but most probably do not even think about it. As per a study about current and future growth of data, 5 billion consumers interact with data on a daily basis, and this number will increase to approximately 6 billion by 2025, representing three-quarters of the world’s population.

In addition to this, the amount of data in the world totaled 33 zettabytes in the year 2018, but the estimate increases to approximately 133 zettabytes by the year 2025. Data production is increasing, and data scientists will be at the forefront of helping enterprises of all scale effectively.

Do you really need a data scientist?

Do you really need a data scientist?

Just because a firm cannot discover a team of data scientists nor does it mean that it will have to give up its goals of data science or lose opportunities for advanced AI or machine learning. Depending on whether a firm is interested in advancing its AI strategy, it may require a team of skilled data scientists.

Companies with complex use cases or large-scale approaches and large datasets, it is likely that they will certainly require more than one data scientist to complete the project in a reasonable time.

Although, if a firm plans to pursue several efforts, it can be valuable for only a few data scientists per team to work with other team members. Depending on the requirements, the data scientist can work closely with the software developers to help everyone on the team reach the goal rather than needing any specific skill set. Data scientists can work alongside the existing team members to perform as citizen data scientists.

As the relevance of AI grows and the shortage of experts around data scientists grows, many firms are wondering if they can go without one. Also, it can be difficult to find a talented data scientist, and their salary is often constant. It is also possible to move towards the AI ​​future without a data scientist on the board, but it really depends on the projects that you want to run.

As the popularity of AI continues to grow, many firms are creating tools to help reduce their reliance on data scientists. One such tool is AutoML, offered by many vendors who are creating dashboards that automate parts of the data science workflow.

Automated machine learning tools aim at eliminating elements of algorithm selection, iterative modeling, hyperparate tuning, model evaluation, and even data preparation in order to speed up the overall process and some of the complex aspects for the setup.

First skilled data scientists were required. Once data of organizations are run through the AutoML system, it produces a machine learning model that can be used directly or analyzed by a worker. Typically, these post AutoML activities can be completed by the employees with less training than data scientists, or several existing employees who have been trained in the latest skills.

In addition to this, organizations can use machine learning models that have already been trained for the problem. They can directly use these models, or extend them using transfer learning. This needs significantly fewer resources, otherwise, these need to be built from scratch. Pre-trained models are already trained on the relevant data, and offer classification, clustering, regression, or prediction required by the end-user. The mobile app development solutions based on machine learning are also in demand due to this.

The line of software developers and business analysts with limited expertise for machine learning can train optimum quality models for their business needs. With a growing list of visible pre-trained models, firms are able to use it for sentiment analysis, image classification and text without the need for large label datasets and data science resources that are needed to train a complex model.

However, vendors are offering models as a service, which can be used on private or public cloud infrastructure, allowing small firms to access complex, large, and well-trained models without their own datasets are allowed to do. All of this mitigates the need for data science roles within a firm.

As the talent gap for the data scientists continues to increase, there is no doubt that we’ll see new tools- out of necessity — that allow business employees and non-technical to test, run, and analyze crucial data. Business managers and CEOs will begin learning basic data science to help them manage and pursue AI projects. Traditional data scientists will still be required to run complex data analysis, but for the most part, basic data analysis will move into civilian data scientist roles owing to increasingly easy-to-use tools.

Let’s Wrap Up:

Therefore, in this future of Data Science, we have learned data science skills and training which are required for it. No doubt, data science has a very bright future. The requirement of the data scientists is going to increase exponentially. Machine learning or artificial intelligence is going to be a crucial component of data science. Therefore, in future artificial intelligence development company needs will be increased and mobile app development services in India will also increase in the same way."
"AI Trust, Model Bias & Explainability using IBM Watson Openscale","AI Trust, Model Bias & Explainability using IBM Watson Openscale

AI trust, bias and model explainability provide a significant piece in the business puzzle to help organizations get AI projects out of development and put them into production at scale. AI bias and model explainability helps ensure fair and unbiased outcomes while giving business process owners confidence in AI’s ability to augment decision-making. At the same time, it provides a robust framework to ensure that AI maintains compliance with corporate policies and regulatory requirements.

Watson Openscale AI Solutions

A short survey showed that business stakeholders have mixed and confusing feedback regarding their trust to AI applications for various reasons:

94% of companies believe AI is key to competitive advantage.

of companies believe to competitive advantage. 60% see regulatory constraints as a barrier to implementing AI.

see as a barrier to implementing AI. 63% cite availability of technical skills as a challenge to implementation

cite as a challenge to implementation 5% of companies have extensively incorporated AI in offerings or processes.

Furthermore, it is widely known among business owners that there are existing challenges related to bias and explainability:

Very difficult to track and measure indicators of business success in production

indicators of business success in production Impossible to teach subtle domain knowledge into AI models in production

subtle domain knowledge into AI models in production No way to validate if AI models will achieve expected business outcomes

if AI models will achieve expected business outcomes Risk violating regulatory and enterprise governance requirements

Watson OpenScale achieves these goals by focusing on 4 stages of an AI application’s lifecycle: Build, Run, Manage and In-App operations.

Watson Openscale across the data science cycle

At build time, data scientists use their preferred popular open source frameworks and tools while gaining access to toolkits for bias detection and explainability of transactions.

Development and operations teams then scale and manage AI applications using their existing DevOps tools and processes in the runtime environment.

As a result, business users are then able to measure and trace the outcome of individual AI workflows for auditing and regulatory purposes.

Watson OpenScale helps deliver and operate trusted AI apps for business users at scale:

Track and Measure business outcomes in production

a. Define business KPIs and feed into existing business applications to measure business impact

b. Define app contract to evaluate AI models at build time and get metrics to track those in run-time

c. Actionable metrics and alerts through error analysis and descriptive analytics of payload, in runtime

2. Meet regulatory constraints & govern AI in production

a. Simulate business conditions with operations data, validate and approve AI models for production

b. Trace and explain AI decisions for full audit across multiple models in an app

c. AI-powered bias detection and mitigation in run time, to drive fair outcomes

3. Adapt AI to changing business situation in production

a. Collect feedback through existing business apps to teach the AI, in runtime

b. Detect drift in business situations, alert users and trigger actions to mitigate

c. Automatically trigger model retraining pipelines with specific inputs from payload analytics to meet business goals and adapt to new data

Advanced Model Management using Watson Openscale

Indicative example: A bank was fined for unequal treatment of minority customers. Minority customers were given fewer loans and charged a higher interest rate.

How can Watson Openscale help to mitigate this bias in this case in model development?

Step 1: Monitor Predictions, Model Performance and Accuracy

Openscale provides a dashboard which displays model accuracy. Model accuracy is determined using the standard model evaluation approaches (different for various model types).

Monitor Predictions, Performance and Accuracy

Step 2: Provide an explanation for model predictions

We can use the explainability feature to understand why certain predictions are made. The two main components of explainability are:

Predictor Importance: Which factors/predictors influence the prediction

Statistical Significance: Identify the statistical importance of each predictor

Constrastive Explanation: which features do we need to change in order to control the prediction

Model Predictions Explainability

Step 3: Monitor for unfair predictions and automatically correct them

i. Define protected fields: typically demographic information about a customer, such as race, age, gender, population segment

ii. Define the context: number of records to use for determining the threshold i.e. evaluate every 100 approvals or every 1000 loan applications

iii. Define a threshold for bias, defined as a percentage of minority approvals compared to approvals of majority. Optionally, OpenScale can “de-bias” the model, i.e automatically correct the results:

De-biased results are automatically logged in the database in addition to model scoring results

The debiased results don’t have to be used in production — they can be used for review or audit

Biased Prediction Monitoring and Mitigation

Auto Bias Mitigation

Description

Models embedded in production need to make fair decisions and can not be biased in their recommendations. Those that they exhibit bias need to be corrected without interfering in current path of predictions to application.

How does it work?

Given an input record we send it to the Bias Detection Model to find out if the model is likely to act in a biased manner on this record

Once we have identified a record for which the model is likely to be biased, we fix it by changing the prediction

Auto-debias trains a shadow model executing in the background to generate trust prior to deployment in production

Most of the industries are embracing AI taking into account AI Model Bias, Model Explainability and Trust.

De-Bias Modeling Process

Value Proposition

Configuration of bias mitigation to run alongside the current deployed model, this provides de-biased outputs without affecting the current predictions that are being served Visualization of the de-biased output in the UI, take action to modify data sets based on recommendations and retrain model with new data set Usage of automatically generated de-biased model end point to score inputs

Bias Mitigation De-biased Output

Industry Use Cases in Model Bias and Explainability

Industry-wide use cases leveraging on Watson Openscale from telecom to financial services and healthcare:

1.Telecom: Predictive Maintenance

Effectively maintaining physical assets and infrastructure is essential for telco industry — asset failure can lead to service outages, a key cause of customer churn. Maintenance prioritization is an expensive and difficult process, and it’s often difficult to catch asset failure in the field before it leads to a problem on the network. Machine learning offers an opportunity to predict failure based on sensor data before a problem occurs.

Predictive maintenance of network infrastructure can prevent outages that lead to costly customer churn, but training a model on historical asset data is difficult because failures tend to be rare — training a predictive model can be time consuming, and even after it’s trained, it may not accurately perform in all failure scenarios

Watson OpenScale’s runtime monitoring features allow teams to track performance of their models in the field, on real data, to ensure continued performance — and guide retraining when necessary. OpenScale’s traceability features also help engineers and technicians capture information critical for an audit trail, so the organization can more easily connect preventative maintenance actions taken because of the model with key business outcomes — like failure prevention and increased customer satisfaction.

2. Insurance: Underwriting Risk Modeling

The insurance industry market landscape is becoming increasingly competitive. Companies are trying to streamline their processes with the help of data science and AI — and insurance underwriting is a prime target for AI insights. Traditional underwriting methods rely on complex rules-based processes and expensive manual analysis, whereas machine learning models can analyze complex interactions among diverse data to deliver risk scores.

Risk assessment models trained on historical customer and claims data can help underwriters make more consistent and accurate decisions. These models can provide price suggestions for individual customers based on different features in their profile.

Watson OpenScale’s explainability features allow underwriters and regulators to see the exact features prioritized by these risk assessment models, on a decision-by-decision basis. During a Department of Insurance audit, commissioners can review model lineage, inputs, and outputs for each decision in business-friendly language. Its bias detection and mitigation features help underwriters ensure that these models continue to make fair decisions after they’ve been deployed.

3. Financial Services: Credit Risk Modeling

Traditional lenders are under pressure to expand their digital portfolio of financial services to a larger and more diverse audience, which requires a new approach to credit risk modeling. To provide credit access to a wider and riskier population, applicant credit histories must expand beyond traditional credit — like mortgages and car loans — to alternate sources, such as utility and cell plan payment histories, plus education and job titles.

These additional features increase the likelihood of unexpected correlations that introduce bias based on an applicant’s age, gender and other personal traits. The data science techniques most suited to these diverse datasets can generate highly accurate risk models but at a cost — such models are black boxes whose inner workings are not easily understood.

Banks and credit unions need to be able to check their credit risk models for bias, not just during training but also after these models are deployed. And in order to be compliant with regulations like the Equal Credit Opportunity Act, they need to be able to explain why their models make individual credit decisions.

Watson OpenScale’s bias detection and mitigation features allow risk and governance officers to monitor bias within their models at runtime. And Watson OpenScale’s explainability support provides loan officers and credit analysts with post-facto explanations for model decisions which provide high accuracy in credit risk modeling.

4. Supply Chain: Effective Demand Forecasting

Effective demand forecasting is essential to keeping operational costs down while still meeting consumer demand — but it’s very difficult. Companies aren’t equipped to deal with the volume and diversity of data needed to account for real-time changes in demand. Forecasts that can’t adapt to constantly-changing variables in today’s market can lead to multimillion-dollar miscalculations, severely damaging a company’s bottom line

Demand forecasters must constantly monitor the performance of their deployed models to prevent miscalculations that could cost their organizations millions of dollars in lost revenue. The data these models rely on are not stationary, and the statistical properties of their distributions will keep shifting as new actuals come in.

Watson OpenScale’s runtime monitoring features allow demand planners to track performance of their deployed models in production, so they can ensure accuracy and identify skewed results and inherent bias in the data.

5. Healthcare: Disease Outcome Prediction

Specific diseases are so complex and difficult to identify early, as its symptoms overlap with those of other common illnesses. Mortality from sspecific disease diagnosis increases every hour that treatment is delayed — so it is critical for doctors and nurses to be able to catch them before patients go into disease shock. Being able to identify those patients who are at highest risk can help clinicians prioritize care. Machine learning models can be trained on hospitalization data and patient data to identify high-risk patients and predict death outcomes. The algorithms and methods used to build accurate models, such as XGBoost gradient boosted trees, do sometimes behave like black boxes.

Watson OpenScale’s explainability features provide a breakdown of the specific patient and hospitalization characteristics that contribute to the decision for each prediction that is made. These results are displayed in a language that the clinicians caring for the patient can understand, increasing their trust in the predictive models and helping them make better care decisions. OpenScale also provides traceability features and runtime monitoring, so the hospital has an audit trail for all patient care decisions that are made and can track the performance of these models over time.

Disclaimer: The views expressed here are those of the article’s author(s) and may or may not represent the views of IBM Corporation. Part of the content on the blog is copyright and all rights are reserved — but, unless otherwise noted under IBM Corporation (e.g. photos, images)."
A Sense of Purpose Enables Better Human-Robot Collaboration,"The year is 1954. Two Americans, inventor George Devol and entrepreneur Joseph F. Engelberger are discussing their favorite science fiction writers at a cocktail party. Devol has recently filed his latest idea at the Patent Office, the first Universal Automation or Unimate, an early effort to replace factory workers with robotic machinery. His creative genius had already given birth to two of the first technological marvels of the modern world: the Phantom Doorman, an automatic door with photocells and vacuum tubes, and the Speedy Weeny, a machine that uses microwave energy to cook hot dogs on demand.

Engelberger found the Unimate industrial transfer machine so compelling that seven years later, as soon as the patent was approved, he formed a company to develop the ideas of Mr. Devol. The name of that company was Unimation Inc. Their “programmed article transfer”, later rebaptized “manipulator” and finally “robot” (one can only wonder whether Devol and Engelberger had discussed Isaac Asimov’s first use of the term “robotics” over those cocktails) eventually entered full-scale production as a unit for materials handling and welding.

Unimate makes its public debut in London. “Unimate. A machine that can reach up to seven feet and perform a multitude of tasks in the factory or laboratory, as skillfully as a man but without getting tired. It’s controlled by a built-in memory system which you lead just once through the required measurements and it can then repeat it 24 hours a day, week after week”.

Unimate reshaped and sped up production lines at manufacturing plants around the world. General Motors, Chrysler, Ford and Fiat quickly recognized the benefits and placed large orders, helping turn the machine into one of the “Top 50 inventions of the past 50 years”. This was but the seed that spawned a new industry: commercial robotics. In time, Unimation Inc. and its sole rival, Cincinnati Milacron Inc. of Ohio, would see the rise of several Japanese and European competitors that incorporated new innovations such as electric micro-processors, servo gun technology and arc welding.

However, the world would have to wait a little longer to see the first truly collaborative robot (or cobot), which was installed at Linatex in 2008. Unlike its predecessors, Universal Robot’s UR5 is lightweight, inexpensive, easy to set up and can be re-programmed by untrained operators.

The cobot UR5. Image courtesy of Universal Robots.

“You don’t need to type or calculate anything to get the robot to work. You just need to show it the movements”. Universal Robots.

Cobots are designed to share a workspace with humans — which is why the UR5 doesn’t need to be placed behind a fence and can safely function alongside employees. This is in stark contrast to traditional industrial robots, usually locked in cages because their rapid movements and heavy bulk can make them unsafe for humans. Once installed, the heavy robotic arms commonly seen in factories are rarely moved; the majority are actually bolted to the floor. This newer generation is, instead, lighter, plug & play and aided by sensor and vision technology to help make workspaces easier to share.

Despite the fact that cobots were introduced eleven years ago, less than 5 percent of all industrial robots sold in 2018 were collaborative (according to the Financial Times). Cobots might not be new, but they still have the potential to revolutionize production, particularly for smaller companies — which account for an impressive 70 percent of global manufacturing. It’s estimated that the cobot market will grow from just over $100m in 2018 to $3bn by 2020, because these flexible robots are considerably cheaper than their traditional industrial counterparts (averaging a price of $24,000 each, according to Barclays Capital).

Sawyer™, a high-performance collaborative robot from Rethink Robotics

It is expected that the adoption of cobots will significantly increase within industrial environments over the coming years. But what is the case for their use in other areas, such as healthcare?

At the Copenhagen University Hospital in Gentofte, the cobot UR5 is already being used to optimize the handling and sorting of blood samples, helping the laboratory uphold its target of delivering more than 90% of results within 1 hour. Universal Robots’ mechanical arm has also been incorporated into the Modus V™, developed by Synaptive Medical in Canada to assist in neurosurgery by providing unprecedented views of patient anatomy and perform less invasive procedures with more precision. And that’s not all of it. Cobots are also beginning to be utilized in other medical areas, for example for telepresence, rehabilitation, medical transportation and sanitation.

Some of these scenarios come with a new set of challenges, as they are filled with dynamic and unpredictable interactions that are significantly different from those within an industrial environment.

For example, a robot nurse might be asked to support a patient fetch specific medicine from a cabinet. The robot would need to understand what the patient is requesting in whatever way they express it, locate the cabinet, navigate through the room avoiding obstacles, open the cabinet, grasp the right medicine and give it to the patient. In order to handle an object, a robot needs to select the correct grasp for a certain shape, understand how different materials respond to force and coordinate feedback and joint steps when handing them over to a person. During these types of interactions, humans naturally monitor the pace and workload of their partners and adapt their handovers accordingly. Robots would need to achieve similar adaptivity in performing them.

Manipulating objects involves a series of actions that are deeply entwined with perception, control, and coordination."
BLEU-BERT-y: Comparing sentence scores,"Blueberries — Photo by Joanna Kosinska on Unsplash

The goal of this story is to understand BLEU as it is a widely used measurement of MT models and to investigate its relation to BERT.

This is the first story of my project where I try to use BERT contextualised embedding vectors in the Neural Machine Translation (NMT) problem. I am relatively new to MT, therefore, any suggestion is welcomed.

Sentence similarity

When it comes to machine translation or other natural language processing (NLP) problems where the output of the process is a text, measuring the correctness of the result is not straightforward.

The question we have to ask for evaluating machine translation algorithms is “How good is this translation?” or “How close is the sentence in the target language to the sentence in the original language?”

To understand this problem, here we will look at a simpler version of it: “How similar two sentences in the same language?”

In this story, let’s use the following pocket sentences:

s0: James Cook was a very good man and a loving husband.

s1: James Cook was a very nice man and a loving husband.

s2: James Cook was a bad man and a terrible husband.

s3: James Cook was a nice person and a good husband.

s4: The sky is blue today and learning history is important.

I’d like to suggest to take a minute and think about the similarity between the sentences, how close they are to the first one!

BLEU

BLEU: Bilingual Evaluation Understudy provides a score to compare sentences [1]. Originally, it was developed for translation, to evaluate a predicted translation using reference translations, however, it can be used for sentence similarity as well. Here is a good introduction of BLEU (or read the original paper).

The idea behind BLEU is to count the matching n-grams in the sentences. A unigram is a word (token), a bigram is a pair of two words and so on. The order of the words is irrelevant in this case.

To eliminate false scores (e.g. “the the the the the” gives a relatively good score to “The cat eats the bird”), a modified version of this count is introduced. This modified unigram precision penalties the use of the same word in the reference text multiple times.

BLEU cannot handle word synonyms, however, it is a quick, inexpensive algorithm that is language independent and correlates with human evaluation.

BERT

BERT: Bidirectional Encoder Representations from Transformers is a contextualised word embedding (and more) [2]. Here is a great summary of BERT (or read the original paper).

Word embeddings are vectors mapped to words to help the computer in understanding words. While “cat” or “dog” is hard to handle for a computer, their vector representation suits better. One expectation from an embedding mapping is that similar words have to be close to each other.

Contextualised word embedding vectors have different embeddings for the same word depending on its context. One trick of BERT embedding is that it is trained with separators CLS and SEP and these separators also have context-dependent embedding vectors. It is suggested in the original paper that these embedding can be used as sentence-level embeddings. Here, we will use the embedding vector of the CLS separator of each sentence as the sentence’s embedding. [CLS] This is a sentence with separators . [SEP]

Vector distance

Here, we will calculate the similarity between sentences using the Euclidean distance and the Cosine similarity of the corresponding CLS sentence-level embeddings.

Euclidean distance

Cosine similarity

The Euclidean distance has a range of [0, ∞), therefore, to match the other scores, let’s use the function f(x)=(1/1.2)^x to get a (0,1] score. This function results in scores relatively close to the BLEU ones. The cosine similarity has the right range, however, it is not comparable with the BLEU scores. To evaluate the results we have to investigate the scores of the sentences relative to each other, not to other scores.

Similarity scores

Here is a table of the scores using BLEU. As we can see, BLEU identifies the second sentence as close to the first one (only 1 word changed) but it cannot handle the synonyms of the fourth sentence. Also, the score of a completely different sentence is relatively high.

BLEU with Smoothing Function [3] resolve this later issue.

The BERT with Euclidean distance achieves relatively similar scores as the BLEU, but it handles the synonyms as well. The Cosine similarity of the BERT vectors has similar scores as the Spacy similarity scores.

Spacy is an Industrial-Strength Natural Language Processing tool. Spacy uses a word embedding vectors and the sentence’s vector is the average of its tokens’ vectors. More about Spacy similarity here.

BLEU and BERT scores of the pocket sentences, similarity to the first sentence

BERTScore

(Updated on 06.11.2019)

This is an update as I recently found an article with the idea to use BERT for evaluating Machine Translation systems [4]. The authors show that BERTScore correlates better to the human judgement than previous scores such as BLUE.

The BERTScore is similar to the one introduced here, however, BERTScore uses the similarity of token-level BERT embedding vectors, while we used sentence-level embeddings.

Summary

This story introduces the BLEU scores for evaluating sentence similarity and compares it to BERT vector distances. The examples tried to illustrate how hard it is to define what a similar sentence means and the two methods showed possible answers to quantitatively measure some kind of similarity. The codes attached to this story provide a basic usage of the BLEU and BERT as well as Spacy. For detailed introductions, additional links are provided.

Corresponding codes are available in Google Colab.

References

[1] Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002, July). BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics (pp. 311–318). Association for Computational Linguistics.

[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.

[3] Lin, C. Y., & Och, F. J. (2004, July). Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (p. 605). Association for Computational Linguistics.

[4] Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., & Artzi, Y. (2019). BERTScore: Evaluating Text Generation with BERT. arXiv preprint arXiv:1904.09675.

Learn NMT with BERT stories"
Object Detection On Aerial Imagery Using RetinaNet,"Object Detection On Aerial Imagery Using RetinaNet

(Left) the original image. (Right) Car detections using RetinaNet, marked in green boxes

Detecting cars and swimming pools using RetinaNet

Introduction

For tax assessments purposes, usually, surveys are conducted manually on the ground. These surveys are important to calculate the true value of properties. For example, having a swimming pool can increase the property price. Similarly, the count of cars in a neighborhood or around a store can indicate the levels of economic activity at that place. Being able to achieve this through aerial imagery and AI, can significantly help in these processes by removing the inefficiencies, and the high cost and time required by humans.

To solve this problem, we’ll try to detect cars and swimming pools in RGB chips of 224x224 pixels of aerial imagery. The training dataset had 3748 images with bounding box annotations and labels in PASCAL VOC format.

This problem along with the dataset was posted by ESRI on HackerEarth as the ESRI Data Science Challenge 2019. I participated and secured the 3rd place in the public leaderboard with a mAP (mean Average Precision) of 77.99 at IoU = 0.3 using the state-of-the-art RetinaNet model. In the following post, I’ll explain how I attempted this problem.

RetinaNet

RetinaNet has been formed by making two improvements over existing single stage object detection models (like YOLO and SSD):

Feature Pyramid Network

Pyramid networks have been used conventionally to identify objects at different scales. A Feature Pyramid Network (FPN) makes use of the inherent multi-scale pyramidal hierarchy of deep CNNs to create feature pyramids.

The one-stage RetinaNet network architecture uses a Feature Pyramid Network (FPN) backbone on top of a feedforward ResNet architecture (a) to generate a rich, multi-scale convolutional feature pyramid (b). To this backbone RetinaNet attaches two subnetworks, one for classifying anchor boxes (c) and one for regressing from anchor boxes to ground-truth object boxes (d). The network design is intentionally simple, which enables this work to focus on a novel focal loss function that eliminates the accuracy gap between our one-stage detector and state-of-the-art two-stage detectors like Faster R-CNN with FPN while running at faster speeds.

Focal Loss

Focal Loss is an improvement on cross-entropy loss that helps to reduce the relative loss for well-classified examples and putting more focus on hard, misclassified examples.

The focal loss enables training highly accurate dense object detectors in the presence of vast numbers of easy background examples.

Focal Loss Function

If you are further interested in the finer details of the model, I’ll suggest reading the original papers and this very helpful and descriptive blog ‘The intuition behind RetinaNet’.

Now, let’s get started with the actual implementation and get coding. Here’s the Github repository you can follow along:

Installing Retinanet

We’ll use the awesome Keras implementation of RetinaNet by Fizyr. I am assuming you have your deep learning machine setup. If not, follow my guide here. Also, I would recommend using a virtual environment. The following script will install RetinaNet and the other required packages.

Alternatively, you can use a GPU instance (p2.xlarge) on AWS with the “deep-learning-for-computer-vision-with-python” AMI. This AMI comes pre-installed with keras-retinanet and other required packages. You can start using the model after activating the RetinaNet virtual environment by workon retinanet command.

Note: Retinanet is heavy on computation. It will require at least 7–8 GBs of GPU memory for a batch size of 4 (224x224) images.

Once the RetinaNet is installed, create the following directory structure for this project.

I’ll explain each one of these in details, but here is an overview:

build_dataset.py — Python script to create the train/test set

config/esri_retinanet_config.py — config file to be used by the build script.

dataset/annotations — directory to hold all image annotations

dataset/images — directory to hold all images

dataset/submission_test_data_images — the submission test directory for the Esri Data Science challenge. You can ignore this if you are working on your own dataset and a different project.

snapshots — the directory where all the snapshots of training will be saved after each epoch

models — the directory where snapshots, converted for evaluation and testing. will be saved

tensorboard — the directory to where training log will be saved to be used by tensorboard

predict.py — script to make predictions on the submission test files

Building the dataset

First, we need to write a config file that will hold the paths to images, annotations, output CSVs — train, test, and classes, and the test-train split value. Having such a config file makes the code versatile for use with different datasets.

In this config file, TRAIN_TEST_SPLIT = 0.75 . It is a standard practice to have a 75–25 or a 70–30 or in some cases even 80–20 split between training and testing dataset from the original dataset. But, for the purpose of this competition, I did not make a testing dataset and used the complete dataset for training. This was done because only a small dataset of 3748 images was provided. Moreover, a testing dataset of 2703 images was provided (without annotations) on which the model could be tested by submitting the predictions online.

Next, let’s write a Python script that will read all the image paths and annotations and output the three CSVs that are required during training and evaluating the model:

train.csv — This file will hold all the annotations for training in the following format: <path/to/image>,<xmin>,<ymin>,<xmax>,<ymax>,<label>

Each row will represent one bounding box, therefore, one image can be present in multiple rows depending on how many objects have been annotated in that image. test.csv — Similar to train.csv in format, this file will hold all the annotations for testing the model. classes.csv — A file with all unique class labels in the dataset with index assignments (starting from 0 and ignoring the background)

Let’s start by creating a build_dataset.py file and importing the required packages. Notice, we import the esri_retinanet_config.py file that we created earlier in the config directory and we give it an alias config .

In the code above, we create an argument parser to take in, optionally, the image and annotation paths, output CSV paths, and the train-test split. Yes, I know we defined these arguments in the config file, already. But, I also realized that there were times when I wanted to create a subsample of images for an experiment or have a different train-test split, etc. At that time, having the option to pass on these arguments when executing the script, without changing the config file, was quicker. You can see that I have provided the default values for each argument from the config file itself. So, you are not required to provide these arguments, unless you want to. After parsing the arguments assign easy variable names for each argument.

In the preceding code, we read the image paths into a list, randomize the list, split it into train and test set and store them in another list dataset in the format (<dataset_type>, <list_of_paths>, <outpuCSV>) . We’ll also initialize the CLASS set to hold all the unique class labels in the dataset.

Next, we loop over each dataset (train and test) and open the output CSV file to be written. For each dataset, we loop over each image path. For each image, extract the filename and build the corresponding annotation path. This works out because, usually, the image and annotation files have the same name but different extensions. For e.g. dataset/images/0000001.jpg has its annotations in dataset/annotations/0000001.xml . Modify this section if your dataset follows a different naming convention. Using BeautifulSoup parse the annotations (XML) file. We can then find the “width” and “height” and the “object(s)” from the parsed XML.

For every image, find all the objects and iterate over each one of them. Then, find the bounding box (xmin, ymin, xmax, ymax) and the class label (name) for each object in the annotation. Do a cleanup by truncating any bounding box coordinate that falls outside the boundaries of the image. Also, do a sanity check if, by error, any minimum value is larger than the maximum value and vice-versa. If we find such values, we will ignore these objects and continue to the next one.

Now, that we have all the information, we can proceed to write to output CSV, one row at a time. Also, keep adding the labels to CLASSES set. This will eventually end up having all the unique class labels.

The last thing left to build the dataset in the required format is to write the class labels with their respective indexes to a CSV. In the ESRI dataset, there are only two classes — cars (label: ‘1’, index: 1) and swimming pool (label: ‘2’, index: 0). This is how the classes.csv looks for Esri dataset.

2,0

1,1

Training and Evaluating the model

Now, that the dataset is ready and RetinaNet installed, let’s proceed to train the model on the dataset.

# For a list of all arguments

$ retinanet-train --help

To train the model, I used the following command:

$ retinanet-train --weights resnet50_coco_best_v2.1.0.h5 \

--batch-size 4 --steps 4001 --epochs 20 \

--snapshot-path snapshots --tensorboard-dir tensorboard \

csv dataset/train.csv dataset/classes.csv

It is advised to load a pre-trained model or weights file instead of training from scratch to speed up the training (the losses will start to converge earlier). I used the weights from the pre-trained model with ResNet50 backbone on COCO dataset. Use the following link to download the file.

The batch-size and the steps will depend on your system config (primarily GPU) and the dataset. I usually start with batch-size = 8 and then increase or decrease by a factor of 2 depending if the model training started successfully or not. If training begins successfully I’ll terminate the training (CTRL+C) and start with a higher batch size else a lower one.

Once, you have decided on the batch-size, you’ll need to calculate the steps you’ll need to cover the total dataset in each epoch. The following command will give you the count of rows in train.csv created earlier in the dataset directory.

$ wc -l datatset/train.csv

The calculation for step size is simple: steps = count of rows in train.csv / batch-size . Next, set the number of epochs . In my experience, RetinaNet converges quickly, so a smaller number of epochs usually does the work. If not you can always, pick up the training from the last epoch and train your model further. Therefore, we’ll provide a snapshot-path where the model will be saved after every epoch.

We will also provide a tensorflow-dir where all the logs will be saved and tensorboard can be run to visualize the training as it proceeds. To launch tensorboard, open a new terminal window and run the below mentioned command. Make sure you have tensorboard installed before running it.

# To launch tensorboard

$ tensorboard --logdir <path/to/logs/dir>

Finally, provide the csv files with training dataset and class labels. And execute the training command. Now, go do an Iron Man or sleep or whatever while your model trains. Each epoch with 3748 (224x224) images took a bit over 2 hours on a K80 Tesla GPU on AWS p2.xlarge instance.

Once the model has trained to your satisfaction, convert the model in a format that can be used for evaluation and predictions.

# To convert the model

$ retinanet-convert-model <path/to/desired/snapshot.h5> <path/to/output/model.h5> # To evaluate the model

$ retinanet-evaluate <path/to/output/model.h5> csv <path/to/train.csv> <path/to/classes.csv> # Sample evaluation

95 instances of class 2 with average precision: 0.8874

494 instances of class 1 with average precision: 0.7200

mAP: 0.8037

In this sample evaluation on 125 test images, the model was able to achieve 80.37% mAP (mean Average Precision) with 375 images training for 18 epochs. It’s a good result for such a small dataset.

Predictions

Build a script predict.py that will use the trained model, make predictions on the submission images and write it on the disk.

A few methods from the keras_retinanet utils are required to pre-process the image before it is fed into the model for predictions. Also, import the config file, we created earlier, for loading a few paths.

Construct the argument parser to accept arguments when executing the script, and then parse the arguments. The argument model will take in the path to the trained model file which will be used for making predictions. For class labels and predictions output directory, the default values have been taken from the config file. Therefore, these are not required arguments. The argument input will take in the path of the directory containing images to make the predictions on. Also, the confidence argument is available to filter weak predictions.

Next, load the class label mapping from the class label CSV and make it into a dictionary. Load the model to be used for prediction. Use the dir path provided in input argument to grab and make a list of all the image paths.

Iterate over each image path so that we can make predictions on each image in the provided dataset. Lines 6–9 in the code above extract the image file name from the image path and then construct and open an output text file path where the predictions for that image will be saved. In lines 11–15, we load the image, preprocess it, resize it and then expand its dimensions before passing it to the model. In Line 18, we pass the preprocessed image to the model and it returns the predicted boxes (bounding box coordinates), probability scores for each box and the associated labels. In the last line in the block above, rescale the bounding box coordinates according to the original image size.

Next, iterate over each detection that is predicted by the model. Skip the ones whose score is less than the confidence value provided. Although, if you want to calculate the mAP (mean Average Precision) keep all the predictions. For this, pass the value of confidence argument as 0.0 . Bounding box coordinates will be float values, so convert them into int . Construct the row for each prediction in the required format: <classname> <confidence> <ymin> <xmin> <ymax> <xmax> and write it to the file. Close the file once all the detections for that image have been written to the corresponding file.

$ python predict.py --model models/output.h5 --input dataset/submission_test_data_images --confidence 0.0

Run the above command to execute the predict.py script. Feel free to change the arguments according to your dataset and project.

Experiments and Results

Initially, I trained the model using only 10% of the data (375 images) for 18 epochs. This model had mAP of 71 with a confidence value of 0.5 on the test images. I resumed training the model on the complete dataset of 3748 images for another 10 epochs to result in an increased mAP of 74. I decided to engineer the model a bit and make changes to the anchor boxes. The dataset had only square bounding boxes, and I changed the aspect ratios of the boxes from [0.5, 1, 2] to [1] . It seemed like a good experiment to try, but I realized that it wasn’t as the anchor boxes ratios will change as the images are augmented. It resulted in the network training much faster than before with the total dataset as the network size decreased. The accuracy of predictions also increased a bit but then started to drop. I decided to use the 2nd epoch results with a confidence value of 0.0 to include all predictions. This resulted in the mAP of 77.99 which secured me the 3rd place in the challenge. I also, unsuccessfully, tried a few other experiments with scales of the images to be used for FPN and data augmentation parameters but stuck with earlier results for the final submission.

Summary

In this post, we talked about the state-of-the-art RetinaNet model and how I used it for the Esri Data Science Challenge 2019 to detect cars and swimming pools in 224x224 tiles of aerial imagery. We started with structuring the project directory. Next, we built the train/test dataset to be used by the model. The model was trained with the appropriate arguments, and later the trained model was converted for evaluation and prediction. We created another script to make detections on the submission test images and to write the predictions on the disk. In the end, I briefly describe the experiments I tried and the results I achieved.

References"
Relative vs Absolute: How to Do Compositional Data Analyses. Part — 2,"This is a continuation of my earlier post on compositional data analyses where I showed the pitfalls of treating compositional data as absolute data instead of relative data. In this post, I will summarize the techniques we can use to correctly analyze compositional data with specific examples demonstrated using RNA-Seq data.

Two main strategies exist for treating Compositional Data and specifically NGS data:

1. Normalization to get back the absolute counts

2. Compositional Data Analysis (CoDA) methods that transform the data using within sample references (Ex: ALR, CLR)

Normalization to Absolute Counts

This is the most widely used technique in NGS data pre-processing when comparing across samples is desired. The relative read counts are ‘normalized’ to the total read depth to ‘recover’ the absolute counts. This, however, does not recover the absolute counts when the total absolute amounts of RNA or cells or the amount of relevant biological material significantly changes across samples. This more often leads to a false sense of security for the analyst and leads to treating these ‘normalized’ samples as absolute counts. This can result in erroneous conclusions when comparing across samples. Let’s prove that to ourselves using simulated data.

Simulation Details

Here, I simulated data for 100 genes, where

a. 5 genes have the true log fold change of 1 between control and experimental conditions (approximates tolerance or no growth under selection),

b. 2 genes have the same true log fold change of > 1 in the experimental conditions (resistant and exhibit growth under selection), and

c. 2 genes have the same true log fold change of < 1 in the experimental conditions (not resistant or tolerant),

I simulated 5 different cases where different proportions of the remaining 91 genes are changed. Of the genes that change, ~90% are depleted, and ~10% are enriched in each case.

The depletion/enrichment of the other genes affects the relative count values and the read-depth normalized counts even though the total read depth is fixed at 200K reads

Read Depth Normalized (RDN) Counts

Differential Expression or Abundance: Even though all the reads have the same total depth (sum of counts), the log fold changes (LFCs) of genes calculated using the read depth normalized counts (RDN counts) are shifted compared to the true log fold changes (See Fig 1 below). Interestingly, the direction of the shift is not always predictable based on the fraction of genes changed. For example, when ~70% of the genes are changed, the LFCs calculated using the RDN counts are shifted down compared to the true LFCs. On the other hand, the LFCs calculated using the RDN counts are shifted up compared to the true LFCs when 90% of the genes are changed. This is because the absolute true counts in the former case are higher than the latter case. In general, we cannot anticipate or estimate the true total absolute counts for a sample.

Fig 1: Comparing True Log-Fold Changes to Log-Fold Changes Calculated using RDN Counts

2. Correlation Between Genes: To see how things compare between relative counts and absolute counts., I calculated the correlation for the non-constant genes across all the 5 samples (each with either 0.1, 0.2, 0.4, 0.6, 0.9 fractions of changed genes). I used both the true counts and relative counts using Polyester simulated count data at 200K read depth.

Fig 2: Comparing True Correlations Between Genes to Correlations Calculated using RDN Counts

As we can see from the figure above, some of the correlation coefficients calculated using the RDN counts, are significantly different from the true correlation coefficients, with a negative bias.

The 2 examples above show the pitfalls of using RDN counts to estimate the differential expression or correlation between genes. Instead of using RDN counts, one should always use spike-in controls when trying to recover absolute counts from relative compositional data. We will show that next

Spike-in Normalized Counts

To truly correct for the change in the absolute counts, we need spike-in controls or genes that we add into all our samples at the same abundance (amount) just before the sequencing step. Doing this will normalize all the samples to the same total abundance scale and makes the comparisons correct. This only works when the data are closed due to sequencing (because we are adding the spike-ins just before sequencing), and will not help if the constraint is biological or happens upstream of the sequencing step. In that case, we need to add in the spike-ins before this constraining step, but it is not always possible to do so due to physical and biological limitations of adding the spike-ins.

Let’s see how this works using our data. In our data, we have 92 different controls or spiked-in genes that have the true absolute abundance. Let’s use these to ‘normalize’ the data and therefore bring all samples to the same absolute count scale.

Differential Expression or Abundance: Fig 3 below is analogous to Fig 1 but with spike-in normalized counts instead of RDN counts. The plot has artificial jitter (noise) added to show all data, but the true data all lie along the diagonal. This indicates the power of spike-ins. Properly designed spike-ins can recover the absolute counts (up to a constant multiplicative factor), provided the spike-ins are added just before the step that leads to closure or constraints in the data, which is not always possible.

Fig 3: Comparing True Log-Fold Changes to Log-Fold Changes Calculated using Spike-in Normalized Counts

2. Correlation Between Genes: Looking at correlations between genes, we see that the coefficients calculated using the spike-in normalized counts can recover the true coefficients. Fig 4 below:

Fig 4: Comparing True Correlations Between Genes to Correlations Calculated using Spike-in Normalized Counts

So, it seems like we found the solution to our problem. All we have to do is add some controls and we are good! Not so fast, unfortunately. In this simulated case, the source of closure for the compositional data is sequencing and we were able to add some controls right before we simulated the sequencing data. In real-life data generation process, the sources of closure can occur anywhere in the usually complicated workflow of extracting DNA/RNA. Also, the biological system itself could be inherently compositional (For example the capacity for a cell to produce RNA is limited), in which case no spike-ins introduced outside the cell can recover the true absolute counts.

Compositional Data Analysis (CoDA) Methods

An alternative to spike-in normalization is using CoDA methods that typically transform the count data with respect to an in-sample reference. Additive Log-Transformation (ALR) and Centered Log-Transformation (CLR) are examples of some commonly used CoDA transformations. These methods are first proposed by John Aitchison originally in 1986. The core idea being that the log-ratio transformations of the components relative to another reference component can be treated as any other unconstrained data. This transforms the data from the original simplex space (as in our ternary diagram in the first part) to the Euclidean space. This allows us to use all classical analyses techniques on these data.

A cautionary note: These techniques do not claim to open the data as do the ‘normalization’ methods from the previous section. These techniques are also applicable to all data, whether they are relative or absolute. Another point to note is normalizing using spike-ins is the same as using the Additive Log-Ratio (ALR) transformation. The benefit of using the general ALR transformation is that it is applicable even when we do not have spike-ins that have constant abundance across samples. The disadvantage with the general ALR transformation is we need to choose the reference properly to make sense of the data and answer the relevant questions.

Lets now look at the CoDA methods in more detail using the same data set that we used as before.

1.Differential Expression or Abundance: There are many methods to find changes in compositional data before and after treatment. Many of these methods surprisingly come from the Microbiome literature, whereas the gene expression literature mostly relies on traditional methods like DESeq2 and EdgeR, which do not explicitly take into account the compositional nature of the data. DESeq2 and EdgeR implicitly assume that the absolute abundances do not change due to the treatment. This is equivalent to using the Centered Log-Ratio (CLR) transformation from the CoDA methods. This transformation uses the geometric mean of the genes or components as the reference, and therefore all results have to be interpreted with respect to the geometric mean. At this stage, it is tempting to translate this assumption to mean that the geometric mean of the genes does not change between control and treatment. Maybe the geometric mean changes, maybe it does not, there is no way to know for sure without orthogonal information beyond the relative counts from sequencing. Most users of DESeq2 and other Differential Expression tools fall for this trap and conclude any significant changes called by the algorithms to mean significant changes in the absolute counts. Instead, these are just significant changes with respect to the geometric mean of all components.

There are emerging methods to apply statistical rigor to DA in compositional data. The most popular methods are ALDEx2 and ANCOM. The main philosophy of these methods is to rely on log-ratio tests of transformed relative data with respect to a reference component and to carefully interpret these results. The main issue with these methods is that the results can only be interpreted with respect to the chosen reference, and no guidance is provided on how to choose the reference. Giuliano Cruz pointed me to a more recent methodology that uses Differential Ranking (DR) and lays out a more reasoned approach to choosing a reference. This is what I will use here briefly, and hopefully, in some future post go into the gory details of running some of these algorithms.

The main idea of DR is to choose some random reference component to calculate the log ratios for all components in both treatment and control. In the next step, these components are ranked in the order of their the difference Δ(log-ratio) between treatment and control conditions. This rank-order of Δ(log-ratio) values calculated using the known relative counts should be identical to the rank of the Δ(log-ratio) values calculated using the unknown true absolute counts. For example, I show below the Δ(log-ratio) values calculated using the relative counts vs. Δ(log-ratio) values calculated using absolute counts, for the case where 90% of the genes are differentially expressed:

Fig 5: Δ(log-ratio) values Calculated using Absolute vs. Relative Counts

As you can see, the magnitude of the Δ(log-ratio) values are different depending on whether we use the relative or absolute counts, but the rankings of Δ(log-ratio) values stay the same. This does not mean that the top-ranking genes have higher counts in treatment vs control, and the low-ranking genes have lower counts. It could so happen, that the top-ranking genes have depleted absolute counts in the treatment conditions compared to the control condition, but the lower-ranked genes have even worse depletion in the treatment condition. In short, we cannot say anything about the changes in absolute reads between treatment and condition.

I will now choose the top-ranking gene as my reference and again calculate the Δ(log-ratio) values using this new reference.

Fig 6: Δ(log-ratio) Values Calculated using the Top-Ranking Gene as Reference

From this plot, we can use an arbitrary cut-off of 0.5 and choose any genes beyond this as our potential DA genes to test further. Of course, if we want more genes to test, we can relax the cut-off.

Another recommendation to get around choosing reference is to have some sort of positive or negative controls in the population. Suppose, we know a gene that will increase in absolute abundance in the treatment condition, then we can use this gene as the natural reference for calculating log-ratios and rank-order the Δ(log-ratio) values. Any log-ratio greater than 1 implies that the gene is better than the positive control, and log-ratio less than 1 implies worse than the positive control. Even better, is to have 2 controls to bound the effect size, and interpret the log-ratios with reference to both of these genes.

In my simulation, I only have one sample per replicate, and therefore could not do any statistical analyses. In a future post, I will generate multiple replicates per condition and play with ALDEx2, ANCOM, and DR algorithms to test their sensitivity and specificity.

2. Correlation Between Genes: As shown in part 1 of this series, correlation is not sub-compositionally coherent and therefore does not follow one of the principles of CoDA. Briefly, the correlation coefficient between any two genes depends on the other components or genes present in the data. Aitchison first proposed using the variance of log-ratio transformed values (VLR) to estimate the dependency of 2 variables. For example, to calculate the dependency between features or genes, g, and h, across n samples, we would use:

VLR is sub-compositionally coherent and therefore doesn’t lead to spurious correlations. The main issue with using VLR is that even though it equates to 0 when genes g and h are perfectly correlated, it doesn’t have an upper limit when the genes are perfectly independent. And that makes it difficult to compare VLR for one gene pair against VLR for another gene pair because of this scaling issue. Several methods/metrics are proposed based on VLR to estimate the dependencies between compositions, the most notable being SparCC, SPIEC-EASI, and proportionality. In this blog, I only review proportionality in some detail. All these methods attempt to use VLR to derive metrics that are analogous to correlation coefficients and therefore can be compared across different pairs of components.

Three proportionality based metrics are proposed in the R package propr based on work by Lovell et. al. and Quinn et. al. These metrics are calculated on log-transformed data. For definitions see the propr package. Ai below refers to the log-transformed values for a gene or component ‘i’ in the data. Ai could be absolute or relative counts and the definitions still apply.

phi (Φ) = var(Ai -Aj)/var(Ai) rho (⍴) = var(Ai -Aj)/(var(Ai) + var(Aj)) phis (Φs) = var(Ai -Aj)/var(Ai +Aj)

The closest metric to traditional correlation coefficient is rho which ranges from -1 to 1. phi is unbounded and can vary from 0 to Inf, and phis is a symmetric variant of phi and is a monotonic function of rho. I will focus on rho in the rest of the blog.

a. Using Absolute Counts: We can recover the absolute counts from relative counts if we have a spike-in control. Since we already have spike-in data available to us, I will calculate the rho values using the spike-in transformed data, i.e. A¹ = log(TPM_counts¹/Spike_TPM_counts) for gene 1 using spike_TPM_counts as the normalization counts. This will recover the original absolute counts. Now, we can calculate the rho values using the equation above. I plot the correlation between absolute counts and the rho values below:

Fig 7: Correlations of True Absolute Counts Between Genes vs. Rho Values Calculated using Spike-Normalized Data

As can be seen from this plot, using proportionality we can capture most of the original correlations between the true absolute counts. Of course, this is a contrived example, where we have a good spike-in available to retrieve the absolute counts. Even with this contrived example, we still see some differences between the true correlations and the proportionality values calculated using the spike-in normalized counts. This is due to the way the proportionality based metrics are calculated which makes them extremely sensitive to the estimates of the variances of the log-transformed values. Here we only have 5 samples to calculate the variances and in most cases, the first 3 samples have the same values. This I suspect leads to the formulae to calculate the metrics to break down. Have to grok on this a little bit more. The evidence for this hypothesis is that, if we only look for components that have distinct values in at least 4 different samples, then the correlation values and the proportionality metrics match pretty well as can be seen below:

Fig 8: Correlations of True Absolute Counts Between Genes vs. Rho Values Calculated using Spike-Normalized Data: Only for Genes with At Least 4 Distinct Values out of 5 Samples

In general, rho and other proportionality based measures have good precision and poor recall, and having more samples gives better estimates for the variances and therefore for the rho values. Also, boot-strapping is generally used to establish a cut-off for calling relationships significant. For example, in the plot above, the cut-off for significant ‘rho’ values could be 0.75.

b. Using Relative Counts: The world is unfairly complex, and we don’t usually have a nice spike-in lying around for us to use, unfortunately 😢 . So we have to instead use relative data, or more specifically, the additive log-transformed (ALR) relative data. Or we can use centered log-transformation (CLR) if we are confident that the geometric mean of the counts does not change across samples (which we know does not hold for our simulated data here). In essence, the best we can do in such cases is to calculate the relationships between relative data. So, let’s compare the rho values for relative data (with respect to a chosen reference gene) against the correlations between true absolute counts. The plots below show this for the correlation between relative counts calculated using 2 randomly chosen reference genes:"
The Secret to Knowing the Unknowable: Predictive Analytics,"There are two branches of analytics (i.e. statistics for my educator friends) that are used widely to describe ways we can summarize large amounts of data. The first way simply describes the data. We call this descriptive analytics. Descriptive analytics has a wonderful use. It helps us summarize the data into readable chunks so we can understand what happened in the big pile of messy data. With descriptive statistics, we can begin to make decisions because we can analyze what we know.

But what happens when we want to know something we haven’t seen yet (or are not able to ask directly because gathering that data would be impossible or too costly?) Questions like…

Will my product sales be higher next year than they are this year?

What kind of product will my customers buy tomorrow?

What is the best way to respond to a brand crisis?

How much should I spend on advertising next year?

The 2nd form of analytics is much more interesting, and in my opinion useful. It’s called predictive analytics. Here’s my take on predictive analytics:

Predictive analytics is using data we have available to us to build a model that allows us to predict data that doesn’t exist yet. — Courtney Perigo

By definition, we’re inferring something we do not know (our prediction) from something we know (data we’ve collected.) The reason to use predictive analytics is when the cost to acquire some information is too much. Either the question is impossible to ask or it would take us too long to acquire that information.

With predictive analytics, I can make better decisions because I have a model that helps me understand something I didn’t know before.

To be clear, predictive analytics cannot reduce all risk. It’s impossible to know what WILL happen. The goal of predictive analytics is to understand what MIGHT happen and all of the caveats that went into the analysis.

Let’s talk about the two types of predictive analytics.

1.) Extrapolate — Time Series Forecasting

This version of predictive analytics is relatively straight forward. Most decision makers are familiar with it and how to use it. In time series forecasting, I want to know what may happen in the future given the trends of the past.

This trend line, which is typically a time series model, summaries the past trajectory of data. The magic happens when you take that model summary and use it to extrapolate future time where data doesn’t exist.

In the case of extrapolation, the reason we do not have data is that it doesn’t exist yet! We cannot measure future events (at least not yet — that I know of.) Knowing the future is the realm of science fiction and fantasy — looking at you Bran Stark (Game of Thrones.)

2.) Non-Temporal Predictive Analytics

If you’re a follower of my blog, then you already know of an example of non-temporal predictive analytics — where I used a model of news preference to predict which data science articles I want to read for the day.

Read more here: MachinaNova — News Reco Engine

In the case of MachinaNova, I was personalizing my daily news experience. In this application, I collected data on my preferences for data science articles. Based on this past preference, I built a natural language model that predicts new articles that would appeal to me based on the new article’s content (words, topics, etc.) Sounds cool, right? You can read more on how that was accomplished in my blog linked above.

The idea of non-temporal predictive analytics is the model is irrelevant. We could build a model in MANY different ways. The main idea is to make sure you have a way to understand it’s accuracy. In the case of MachinaNova, we understood the accuracy of the model by splitting our training data set and using the some of that data to predict data the model hadn’t seen yet. Since we understood actual article preference, we could compare the model’s output versus actual results to understand how good the model is.

This is ridiculously important. Anyone can model anything based on what they think they know about the world. Successful predictive models are those that can remain accurate with data the model was not trained with. A model that falls apart outside of its training data set is completely useless — or worse will give you inaccurate predictions.

In the case of non-temporal predictive analytics, the reason we do not know something is that data is really difficult or impossible to ask. Could you imagine if Amazon had to ask you what products you like every time you visited their website? Non-temporal predictive analytics to the rescue!

Concluding Thoughts:

In this post, we explored predictive analytics — using data we have to understand what MIGHT happen in the future. We also know how it’s different than descriptive analytics — which is using data to summarize what happened in the past.

The main advantage of having a predictive model is that we can extrapolate the model to areas where data doesn’t exist and make predictions. This is extremely useful because knowing everything past, present and future — is impossible. At least in the real world."
Cluster analysis: theory and implementation of unsupervised algorithms,"In simple terms, clustering is nothing but separating observations based on certain properties. In a more technical term, clustering is an unsupervised machine learning algorithm, a process, by which observations (data) are grouped in a way that similar observations are put closer to each other. It is an “unsupervised” algorithm because unlike supervised algorithms (e.g. random forest) you do not have to train it with labeled data, and instead, you put your data into a “clustering machine” along with some instructions (e.g. # of clusters you want), and the machine will figure out the rest and cluster the data based on the underlying patterns and properties.

Lee RCT (1981):

Clustering analysis is a newly developed computer-oriented data analysis technique. It is a product of many research fields: statistics, computer science, operations research, and pattern recognition.

What does it mean for data to be clustered?

The purpose of this article is to highlight some industry applications as well as discuss pros and cons of most frequently used clustering algorithms. In the second part I will demonstrate an implementation of K-means clustering as an example in Python environment. In the end I’ll leave some additional technical notes for practicing data scientists.

Industry applications

Why is clustering so popular in statistics and machine learning fields? This is because cluster analysis is a powerful data mining tool in a wide range of business application cases. Here are just a few of many applications:"
Could Julia Replace Scala?,"Let’s be honest: A lot of us would really love to remove Scala from our Data-Science workflow. Spark is a great way to manage enterprise-level Hadoop and coordinate workers for deep learning, however, the effort overhead is significantly heavier than that of traditional statistical languages like R and Python. Fortunately, there is a language developed at MIT that is making its academic rounds from MIT that holds a lot of potential to combine all the things we love about Spark, Python, and R into one convenient package.

Spark is a multi-functional data-management tool built within Scala, and is the most common segment of the scalable language that Data-Scientists tend to work in. Spark’s primary strength comes with big data, as its integration with Hadoop as well as fast speed make it extremely ideal for use with large, un-managed datasets that require a-lot of processing in order to access and manipulate.

However, Spark’s inheritence leaves it dead in the water in a-lot of ways that would traditionally make a language non-viable for machine learning operations. A debatable disadvantage of Spark is that Spark is far more difficult to use and read than the other languages typically used in Data Science. Among the languages typically used are languages like R, Python, Matlab, and Go, among many others. However, a-lot of these languages have their own fair share of issues. In Python, it can often be a tedious task, or just impossible to work with large data-sets. R has its own share of disadvantages stemming back from its origin.

Though I definitely wouldn’t say that R is all that challenging (most of the time,) languages like Python really put R into the ground from a start-up standpoint. Additionally, R has its own fair share of issues with speed, though certainly faster than Python. Last but not least, data-handling can be a little rough in R…

But when we compare all of these disadvantages to that of Scala, Scala/Spark has a significantly more difficult startup curve, sometimes being extremely hard to work with, and making it feel easier to separate your data into different files to read into a different language."
Write Clean and SOLID Scala Spark Jobs,"Creating data pipelines by writing spark jobs is nowadays easier due to the growth of new tools and data platforms that allow multiple data parties (analysts, engineers, scientists, etc.) to focus on understanding data and writing logic to get insights. Nevertheless, new tools like notebooks that allow easy scripting, sometimes are not well used and could cause a new problem: extensive data pipelines are written as simple SQL queries or scripts, neglecting important development concepts as writing clean and testable code. Thus, design principles that ensure that code is maintainable and extensible might be broken, leading to further problems in an environment where our products should be dynamic.

We will expose a process that contains a set of steps and patterns that will help you in creating better spark pipelines using a basic spark pipeline as an example.

Step 1: Define first your pipeline structure

The first and most important step (even more than code cleanliness) in every pipeline is the definition of its structure. Thus, the pipeline structure should be defined after a process of data exploration that provides the phases need to produce the expected outputs from the inputs.

Let’s work on a basic example and define a pipeline structure. Therefore, we have three datasets:

usersSourceExampleDS that contains users’ information.

genderSourceAExampleDS reads from the source ‘exampleA’ that contains genders by a specif name in a country.

genderSourceBExampleDS reads from the source ‘exampleB’ and contains another list of genders by a name. However, in this case, it does not discriminate by the country and adds a computed probability.

Then, the pipeline aim is to produce a dataset where the column gender is added to the usersSourceExampleDS as follow:

When the suffix has an explicit gender, for example, Mr or Ms add the gender right away in the column gender.

If the suffix does not have the gender then search the name on the genderSourceAExampleDS and add a new column source_a_gender . Then, search the name on the genderSourceBExampleDS and add the column source_b_gender.

and add a new column . Then, search the name on the and add the column Finally, when source_a_gender is not null set this value to gender, otherwise use the source_b_gender only if the probability is greater than 0.5.

Also, some metrics like male percentage and female percentage are produced into the metrics storage system.

Pipeline structure

The following phases are defined:

Data Reading: reads from the data sources. In this case, data is stored in S3 .

reads from the data sources. In this case, data is stored in . Data Pre-processing: As we can see in the data, there is not unique ID to join or search data, then texts within the columns name, country and suffix are used. However, these columns have invalid data to be removed (NaN), multiple letter cases, acronyms, and special characters that are pre-processed in this phase.

As we can see in the data, there is not unique to join or search data, then texts within the columns name, country and suffix are used. However, these columns have invalid data to be removed (NaN), multiple letter cases, acronyms, and special characters that are pre-processed in this phase. Data Enrichment: After data is clean and ready to be used, it is transformed and parsed within this phase. Thus, new columns are added after some business validations.

After data is clean and ready to be used, it is transformed and parsed within this phase. Thus, new columns are added after some business validations. Data Metrics: This phase contains tasks related to aggregations and calculations over the transformed data.

This phase contains tasks related to aggregations and calculations over the transformed data. Data Writing: Finally, this phase handles writes of the genderized results to S3 and the metrics to an external tool.

Is it quite simple? Yes, We know what you are thinking. This pipeline might be written easily using spark. You open your spark context, read the datasets, parse the datasets, finally join to get the final the results and write them to the output datastores. This looks like this.

Writing pipeline with SQL queries

This script works fairly well; it uses some extensive spark SQL and repeats the same logic for 3 different sources though. Besides, let’s imagine the following scenario:

This is only the first version of the pipeline and multiple new genderized sources are going to be added in the future.

Multiple new shared pre-processing and transforming tasks are going to be added.

Some cleaning functions have to be tested in isolation. As an example, the function that removes acronyms. Also, the output of each intermediate step should be tested.

New extensions and tests will be automatically configured with CI/CD.

As a result, the previous source code becomes useful for some cases where an effort for extensibility is needless; else becomes non-extensible, untestable and unmaintainable. Consequently, the following steps are recommended.

Step 2: Guarantee a proper project structure

Define a project structure that suits your pipelien phases, this will make your pipeline a data product.

A) Define a project structure for phases defined previously, this looks like this:

B) Define your project dependencies avoiding circular/unused dependencies and define clearly the dependency scope between test, provided and compile.

C) Start adding a general helper within each package where small and general functions are going to be defined. Define the rest of the functions in a companion object of the main class for now.

D) Define which version of the spark API you will use: RDDs, datasets, or dataframes depending on your requirements.

E) When you have multiple jobs using this same logic, think about creating a general spark-utils library that is shared for all your company jobs.

Step 3: Ensure Clean Code Rules Compliance

Creating jobs that behave as expected is not enough. Currently, companies cope to reduce technical debt and creating self-documented pipelines that are easy to read and maintain for all the data parties.

There are myths to be discussed here: “I write code as a data scientist” or “data engineers are too strict to read code”. We should start writing code for humans and ensure a minimum of cleanliness that guarantees our data teams’ productivity. Thus, important rules to take into account are listed below.

A) Use Meaningful Names

Using meaningful names across your pipeline is crucial. Let’s focus on Spark variables, functions, and columns.

Spark-based Variables: Use always unmutable variables and stop calling your data frames with names like “df1”. Instead, use meaningful names and suffixes that represent the API version. Examples:

val usersExampleDS = ...

val usersExampleDF = ...

val usersExampleRDD = ...

Do not use var, instead use the transform method as we are going to show later.

Functions: Along with meaningful names, for your functions create a whitelist of prefixes with verbs like clean, with, filter, transform/enrich, explode that are the only possible words used in your pipelines. Examples:

def cleanAcronymsFromText

def enrichNameWithPrefix

Columns: the names of the columns should be clear and uniform. In case you use the dataset API prefer camelcase columns names to match a case class and underscore when you prefer a database notation. As an example, the following name shows clearly that the gender probability is greater than 50.

gender_probability_gt_50

In cases when you want to use a case class to define your dataset schema, define a columns mapper on the companion object and modify the column names before to set the schema. Example:

case class UserExample(name: String,

email: String,

country: String)

object UserExample {

def columnstranslatorMap: Map[String, String] = Map(

""First Name"" -> ""name"",

""Email"" -> ""email"",

""Country Code"" -> ""country"")

}

Then you can read in this way:

val usersSourceExampleDS = spark.read

.option(""header"", ""true"")

.csv(PATH)

.select(UserExample.columnstranslatorMap.keys.toList.map(c => col(c).as(UserExample.columnstranslatorMap.getOrElse(c, c))): _*)

.as[UserExample]

In certain cases, the names of columns are used across multiple phases and functions. Hence, define a trait with the names of the columns and use it from the main spark job class.

trait GeneralColumnConstants {

val NameSourceColumn = ""firstName""

}

object NamesGenderizerJob with GeneralColumnConstants {}

Finally, only in a few cases, columns should be mutable, rather add a new column to dataframes using the withColumn function to keep track of your original data.

B) Avoid Side Effects: Always prefer side-effect-free operations. As an example prefer always if/else as a one-line ternary operator.

if (true) statement1 else statement2

C) Decide wisely between null or Option: We got told that to write clean code in scala match-pattern and Optional are mandatory. However, in spark Optional could become a bottleneck since this evaluation adds an extra cost. So, the recommendation here is to use Optional at the beginning and only for those cases when the performance might be affected change values to null.

D) Avoid overusing scala implicit: Prefer always the transform function to use implicit to transform data frames leading to monkey patching.

E) Avoid UDFs whenever it is possible: UDFs are black box most of the time, then use them only when what you are doing is not possible with custom spark functions that are optimized by Spark. However, when you write UDFs please make sure you handle all the null cases.

F) Avoid overusing Accumulators: Accumulators are not meant to store your data, then use them only when some small counts are needed, for example counting the number of errors in a pipeline.

Step 4: Guarantee extensible and maintainable pipelines

So far we can write a spark job with a proper structure and following some clean code principles. This is awesome, but it does not mean our code is already extensible and testable. In order, to reach this extensibility, the SOLID principles could play a key role. Let’s start going through these principles using our example.

Single Responsibility Principle

A class should have one, and only one, reason to change.

All the pipeline modules, classes, or functions should have responsibility for a single part of the functionality, separating the responsibilities in simple code units. As we can see the phases defined in Step1, each phase is a responsibility unit. Besides, all the internal functions in these steps should be also responsible for a single thing, being testable and chainable in bigger functionalities. Let’s focus on a general data cleaner for the example dataset.

Simple Names Normalizer.

Above, we have two functions responsible for a single thing removeUselessChars and removeAbbreviations. Then, a chaining function called clean normalizes texts. Hence, the NamesNormalizer is only responsible for normalizing names texts. Let’s write tests in isolation for each of these functions.

Names Normalizer Test.

Open/Closed Principle

You should be able to extend a class’s behavior, without modifying it.

This principle is the foundation for building code that is maintainable and reusable.

Robert C. Martin

Let’s see how to write a cleaner module for our example.

Cleaner Service Code.

The key points to comply with this principle here are:

The abstraction CleanerService define the general cleaning functions template. This abstraction is closed for modification but opened for extensions.

Multiple implementations of cleaners, for instance: SourceACleaner, SourceBCleaner, SourceUsersCleaner extend the abstraction behavior and add concrete implementations.

Then, wherever we have to use a cleaner, Dependency Injection is preferred, we would inject a CleanerService instance instead of an instance of a lower-level class.

The cleanerServiceHelper chains multiple functions using the transform function.

Finally, see again the normalizeTexts functions, it separates the loop concern from the function that normalizes text, simply calling the previously defined TextNormalizer. Besides, using foldLeft makes functions easily reusable and maintains a DRY codebase.

Let’s see how testable it is :

Cleaner Service Test Example.

Liskov Substitution Principle

Derived classes must be substitutable for their base classes.

To explain this principle easier using our example, some cleaning functions are shared by all the implementations. In this case, we can define the implementation in a general structure. For example, we can define a general behavior for deduplication in the cleanerServiceHelper and only use the concrete implementations when differs from others.

Interface Segregation Principle

Clients should not be forced to implement interfaces they do not use. Robert C. Martin

Do not write big monolithic traits or helpers where you had all the pipeline functions. Do you remember our first step where the phases were defined?. Those are useful here, we should be able to clean only, or for example clean and enrich without calling the rest of the phases. Thus, the recommendation is to create an uncoupled abstraction by phase. Let’s see an example of the enriching functions.

Enrich Service Example.

After this, the main class for a job should look like this.

Main Job Example.

Dependency Inversion Principle

Depend on abstractions, not on concretions.

In the previous example, we see how we inject the CleanerService abstraction. This is a good example of this principle in Spark. However, we could go a bit further and see how to apply dependency injection using our data frames. Let’s imagine we want to create a function that joins the usersSourceExampleDS with the genderized sources and test this in isolation. Let’s write a function for this.

Dataframe Injection Example.

then see how easy we could test.

Testing Dataframe Injection.

Conclusions"
Selenium Tutorial: Scraping Glassdoor.com in 10 Minutes,"Selenium Tutorial: Scraping Glassdoor.com in 10 Minutes Omer Sakarya · Follow 7 min read · Oct 14, 2019 -- 11 Listen Share

I scraped jobs data from Glassdoor.com for a project. Let me tell you how I did it…

What is Scraping?

It’s a method for collecting information from web pages.

Why Scraping?

Other than the fact that it is fun, Glassdoor’s library provides a limited number of data points. It doesn’t allow you to scrape jobs or reviews. You only get to scrape companies, which was useless in my case. In this guide, I will share my way of doing it along with the Jupyter Notebook.

Why Selenium?

Good question! Glassdoor renders its content with Javascript. Which means that a simple get request to the webpage below would return only the visible content. We are interested in more than that.

There are data points such as company valuation and job location under the “Company” tab and we want to access that information as well. The webpage does not show that content unless the user clicks on the “Company” tab. This makes clicking on the “Company” tab necessary. Using requests library and doing simple get requests would not work for this type of website. Therefore, the only way to scrape that data is to write a program that mimics a human user. Selenium is a library that lets you code a python script that would act just like a human user.

What we will build?

Essentially we will be building a python script that would give us a DataFrame like this:"
The Art and Science of Data Visualization,"The Mechanics of Data Visualizations

Let’s move from theoretical considerations of graphing to the actual building blocks you have at your disposal. As we do so, we’re also going to move on to mantra #2:

Everything should be made as simple as possible — but no simpler.

Graphs are inherently a 2D image of our data:

They have an x and a y scale, and — as in our scatter plot here — the position a point falls along each scale tells you how large its values are. But this setup only allows us to look at two variables in our data — and we’re frequently interested in seeing relationships between more than two variables.

So the question becomes: how can we visualize those extra variables? We can try adding another position scale:

But 3D images are hard to wrap your head around, complicated to produce, and not as effective in delivering your message. They do have their uses — particularly when you’re able to build real, physical 3D models, and not just make 3D shapes on 2D planes — but frequently aren’t worth the trouble.

So what tools do we have in our toolbox? The ones that are generally agreed upon (no, really — this is an area of active debate) fall into four categories:

Position (like we already have with X and Y)

Color

Shape

Size

These are the tools we can use to encode more information into our graphics. We’re going to call these aesthetics, but any number of other words could work — some people refer to them as scales, some as values. I call them aesthetics because that’s what my language of choice (R, using ggplot2) calls them — but the word itself comes from the fact that these are the things that change how your graph looks.

For what it’s worth, we’re using an EPA data set for this unit, representing fuel economy data from 1999 and 2008 for 38 popular models of car. “Hwy” is highway mileage, “displ” is engine displacement (so volume), and “cty” is city mileage. But frankly, our data set doesn’t matter right now — most of our discussion here is applicable to any data set you’ll pick up.

We’re going to go through each of these aesthetics, to talk about how you can encode more information in each of your graphics. Along the way, remember our mantras:

A good graphic tells a story Everything should be made as simple as possible — but no simpler Use the right tool for the job Ink is cheap. Electrons are even cheaper

We’ll talk about how these are applicable throughout this section.

Position

Let’s start off discussing these aesthetics by finishing up talking about position. The distance of values along the x, y, or — in the case of our 3D graphic — z axes represents how large a particular variable is. People inherently understand that values further out on each axis are more extreme — for instance, imagine you came across the following graphic (made with simulated data):

Which values do you think are higher?

Most people innately assume that the bottom-left hand corner represents a 0 on both axes, and that the further you get from that corner the higher the values are. This — relatively obvious — revelation hints at a much more important concept in data visualizations: perceptual topology should match data topology. Put another way, that means that values which feel larger in a graph should represent values that are larger in your data. As such, when working with position, higher values should be the ones further away from that lower left-hand corner — you should let your viewer’s subconscious assumptions do the heavy lifting for you.

Applying this advice to categorical data can get a little tricky. Imagine that we’re looking at the average highway mileages for manufacturers of the cars in our data set:

In this case, the position along the x axis just represents a different car maker, in alphabetical order. But remember, position in a graph is an aesthetic that we can use to encode more information in our graphics. And we aren’t doing that here — for instance, we could show the same information without using x position at all:

Try to compare Pontiac and Hyundai on the first graph, versus on this second one. If anything, removing our extraneous x aesthetic has made it easier to compare manufacturers. This is a big driver behind our second mantra — that everything should be made as simple as possible, but no simpler. Having extra aesthetics confuses a graph, making it harder to understand the story it’s trying to tell.

However, when making a graphic, we should always be aiming to make important comparisons easy. As such, we should take advantage of our x aesthetic by arranging our manufacturers not alphabetically, but rather by their average highway mileage:

By reordering our graphic, we’re now able to better compare more similar manufacturers. It’s now dramatically faster to understand our visualization — closer comparisons are easier to make, so placing more similar values closer together makes them dramatically easier to grasp. Look at Pontiac vs Hyundai now, for instance. Generally speaking, don’t put things in alphabetical order — use the order you place things to encode additional information.

As a quick side note, I personally believe that, when working with categorical values along the X axis, you should reorder your values so the highest value comes first. For some reason, I just find having the tallest bar/highest point (or whatever is being used to show value) next to the Y axis line is much cleaner looking than the alternative:

For what it’s worth, I’m somewhat less dogmatic about this when the values are on the Y axis. I personally believe the highest value should always be at the top, as humans expect higher values to be further from that bottom left corner:

However, I’m not as instantly repulsed by the opposite ordering as I am with the X axis, likely because the bottom bar/point being the furthest looks like a more natural shape, and is still along the X axis line:

For this, at least, your mileage may vary. Also, it’s worth pointing out how much cleaner the labels on this graph are when they’re on the Y axis — flipping your coordinate system, like we’ve done here, is a good way to display data when you’ve got an unwieldy number of categories.

Color

While we’ve done a good job covering the role position plays in communicating information, we’re still stuck on the same question we started off with: How can we show a third variable on the graph?

One of the most popular ways is to use colors to represent your third variable. It might be worth talking through how color can be used with a simulated data set. Take for example the following graph:

And now let’s add color for our third variable:

Remember: perceptual topology should match data topology. Which values are larger?

Most people would say the darker ones. But is it always that simple? Let’s change our color scale to compare:

Sure, some of these colors are darker than others — but I wouldn’t say any of them tell me a value is particularly high or low.

That’s because humans don’t perceive hue — the actual shade of a color — as an ordered value. The color a point is doesn’t communicate that the point has a higher or lower value than any other point on the graph. Instead, hue works as an unordered value, which only tells us which points belong to which groupings. In order to tell how high or low a point’s value is, we instead have to use luminescence — or how bright or dark the individual point is.

There’s one other axis you can move colors along in order to encode value — how vibrant a color is, known as chroma:

Just keep in mind that luminescence and chroma — how light a color is and how vibrant it is — are ordered values, while hue (or shade of color) is unordered This becomes relevant when dealing with categorical data. For instance, moving back to the scatter plot we started with:

If we wanted to encode a categorical variable in this — for instance, the class of vehicle — we could use hue to distinguish the different types of cars from one another:

In this case, using hue to distinguish our variables clearly makes more sense than using either chroma or luminesence:

This is a case of knowing what tool to use for the job — chroma and luminescence will clearly imply certain variables are closer together than is appropriate for categorical data, while hue won’t give your audience any helpful information about an ordered variable. Note, though, that I’d still discourage using the rainbow to distinguish categories in your graphics — the colors of the rainbow aren’t exactly unordered values (for instance, red and orange are much more similar colors than yellow and blue), and you’ll wind up implying connections between your categories that you might not want to suggest. Also, the rainbow is just really ugly:

Speaking of using the right tool for the job, one of the worst things people like to do in data visualizations is overuse color. Take for instance the following example:

In this graph, the variable “class” is being represented by both position along the x axis, and by color. By duplicating this effort, we’re making our graph harder to understand — encoding the information once is enough, and doing it any more times than that is a distraction. Remember the second mantra: Everything should be made as simple as possible — but no simpler. The best data visualization is one that includes all the elements needed to deliver the message, and no more.

You can feel free to use color in your graphics, so long as it adds more information to the plot — for instance, if it’s encoding a third variable:

But replicating as we did above is just adding more junk to your chart.

There’s one last way you can use color effectively in your plot, and that’s to highlight points with certain characteristics:

Doing so allows the viewer to quickly pick out the most important sections of our graph, increasing its effectiveness. Note that I used shape instead of color to separate the class of vehicles, by the way — combining point highlighting and using color to distinguish categorical variables can work, but can also get somewhat chaotic:

There’s one other reason color is a tricky aesthetic to get right in your graphics: about 5% of the population (10% of men, 1% of women) can’t see colors at all. That means you should be careful when using it in your visualizations — use colorblind-safe color palettes (check out “ColorBrewer” or “viridis” for more on these), and pair it with another aesthetic whenever possible.

Shape

The easiest aesthetic to pair color with is the next most frequently used — shape. This one is much more intuitive than color — to demonstrate, let’s go back to our scatter plot:

We can now change the shape of each point based on what class of vehicle it represents:

Imagine we were doing the same exercise as we did with color earlier — which values are larger?

I’ve spoiled the answer already by telling you what the shapes represent — none of them are inherently larger than the others. Shape, like hue, is an unordered value.

The same basic concepts apply when we change the shape of lines, not just points. For instance, if we plot separate trend lines for front-wheel, rear-wheel, and four-wheel drive cars, we can use line type to represent each type of vehicle:

But even here, no one line type implies a higher or lower value than the others.

There are two caveats to be made to this rule, however. For instance, if we go back to our original scatter plot and change which shapes we’re using:

This graph seems to imply more connection between the first three classes of car (which are all different types of diamonds) and the next three classes (which are all types of triangle), while singling out SUVs. In this way, we’re able to use shape to imply connection between our groupings — more similar shapes, which differ only in angle or texture, imply a closer relationship to one another than to other types of shape. This can be a blessing as well as a curse — if you pick, for example, a square and a diamond to represent two unrelated groupings, your audience might accidentally read more into the relationship than you had meant to imply.

It’s also worth noting that different shapes can pretty quickly clutter up a graph. As a general rule of thumb, using more than 3–4 shapes on a graph is a bad idea, and more than 6 means you need to do some thinking about what you actually want people to take away.

Size

Our last aesthetic is that of size. Going back to our original scatter plot, we could imagine using size like this:

Size is an inherently ordered value — large size points imply larger values. Specifically, humans perceive larger areas as corresponding to larger values — the points which are three times larger in the above graph are about three times larger in value, as well.

This becomes tricky when size is used incorrectly, either by mistake or to distort the data. Sometimes an analyst maps radius to the variable, rather than area of the point, resulting in graphs as the below:

In this example, the points representing a cty value of 10 don’t look anything close to 1/3 as large as the points representing 30. This makes the increase seem much steeper upon looking at this chart — so be careful when working with size as an aesthetic that your software is using the area of points, not radius!

It’s also worth noting that unlike color — which can be used to distinguish groupings, as well as represent an ordered value — it’s generally a bad idea to use size for a categorical variable. For instance, if we mapped point size to class of vehicle:

We seem to be implying relationships here that don’t actually exist, like a minivan and midsize vehicle being basically the same. As a result, it’s best to only use size for continuous (or numeric) data.

A Tangent

Now that we’ve gone over these four aesthetics, I want to go on a quick tangent. When it comes to how quickly and easily humans perceive each of these aesthetics, research has settled on the following order:

Position Size Color (especially chroma and luminescence) Shape

And as we’ve discussed repeatedly, the best data visualization is one that includes exactly as many elements as it takes to deliver a message, and no more. Everything should be made as simple as possible, but no simpler.

However, we live in a world of humans, where the scientifically most effective method is not always the most popular one. And since color is inherently more exciting than size as an aesthetic, the practitioner often finds themselves using colors to denote values where size would have sufficed. And since we know that color should usually be used alongside shape in order to be more inclusive in our visualizations, size often winds up being the last aesthetic used in a chart. This is fine — sometimes we have to optimize for other things than “how quickly can someone understand my chart”, such as “how attractive does my chart look” or “what does my boss want from me”. But it’s worth noting, in case you see contradictory advice in the future — the disagreement comes from if your source is teaching the most scientifically sound theory, or the most applicable practice.

Let’s transition away from aesthetics, and towards our third mantra:

Use the right tool for the job.

Think back to our first chart:

As you already know, this is a scatter plot — also known as a point graph. Now say we added a line of best fit to it:

This didn’t stop being a scatter plot once we drew a line on it — but the term scatter plot no longer really encompasses everything that’s going on here. It’s also obviously not a line chart, as even though there’s a line on it, it also has points.

Rather than quibble about what type of chart this is, it’s more helpful to describe what tools we’ve used to depict our data. We refer to these as geoms, short for geometries — because when you get really deep into things, these are geometric representations of how your data set is distributed along the x and y axes of your graph. I don’t want to get too far down that road — I just want to explain the vocabulary so that we aren’t talking about what type of chart that is, but rather what geoms it uses. Framing things that way makes it easier to understand how things can be combined and reformatted, rather than assuming each type of chart can only do one thing.

Two continuous variables

This chart uses two geoms that are really good for graphs that have a continuous y and a continuous x — points and lines. This is what people refer to most of the time when they say a line graph — a single smooth trend line that shows a pattern in the data. However, a line graph can also mean a chart where each point is connected in turn:

It’s important to be clear about which type of chart you’re expected to produce! I always refer to the prior as a trend line, for clarity.

These types of charts have enormous value for quick exploratory graphics, showing how various combinations of variables interact with one another. For instance, many analysts start familiarizing themselves with new data sets using correlation matrices (also known as scatter plot matrices), which create a grid of scatter plots representing each variable:

In this format, understanding interactions between your data is quick and easy, with certain variable interactions obviously jumping out as promising avenues for further exploration.

To back up just a little, there’s one major failing of scatter plots that I want to highlight before moving on. If you happen to have more than one point with the same x and y values, a scatter plot will just draw each point over the previous, making it seem like you have less data than you actually do. Adding a little bit of random noise — for instance, using RAND() in Excel — to your values can help show the actual densities of your data, especially when you’re dealing with numbers that haven’t been measured as precisely as they could a have been.

One last chart that does well with two continuous variables is the area chart, which resembles a line chart but fills in the area beneath the line:

Area plots make sense when 0 is a relevant number to your data set — that is, a 0 value wouldn’t be particularly unexpected. They’re also frequently used when you have multiple groupings and care about their total sum:

(This new data set is the “diamonds” data set, representing 54,000 diamonds sizes, qualities, cut, and sale prices. We’ll be going back and forth using it and the EPA data set from now on.)

Now one drawback of stacked area charts is that it can be very hard to estimate how any individual grouping shifts along the x axis, due to the cumulative effects of all the groups underneath them. For instance, there are actually fewer “fair” diamonds at 0.25 carats than at 1.0 — but because “ideal” and “premium” spike so much, your audience might draw the wrong conclusions. In situations where the total matters more than the groupings, this is alright — but otherwise, it’s worth looking at other types of charts as a result.

One continuous variable

If instead you’re looking to see how a single continuous variable is distributed throughout your data set, one of the best tools at your disposal is the histogram. A histogram shows you how many observations in your data set fall into a certain range of a continuous variable, and plot that count as a bar plot:

One important flag to raise with histograms is that you need to pay attention to how your data is being binned. If you haven’t picked the right width for your bins, you might risk missing peaks and valleys in your data set, and might misunderstand how your data is distributed — for instance, look what shifts if we graph 500 bins, instead of the 30 we used above:

An alternative to the histogram is the frequency plot, which uses a line chart in the place of bars to represent the frequency of a value in your dataset:

Again, however, you have to pay attention to how wide your data bins are with these charts — you might accidentally smooth over major patterns in your data if you aren’t careful!

One large advantage of the frequency chart over the histogram is how it deals with multiple groupings — if your groupings trade dominance at different levels of your variable, the frequency graph will make it much more obvious how they shift than a histogram will.

(Note that I’ve done something weird to the data in order to show how the distributions change below.)

One categorical variable, one continuous

If you want to compare a categorical and continuous variable, you’re usually stuck with some form of bar chart:

The bar chart is possibly the least exciting type of graph in existence, mostly because of how prevalent it is — but that’s because it’s really good at what it does. Bar charts are one of the most easily interpreted and effective types of visualizations, no matter how exciting they are.

However, some people are really intent on ruining that. Take, for instance, the stacked bar chart, often used to add a third variable to the mix:

Compare Fair/G to Premium/G. It’s next to impossible to accurately compare the boxes — they don’t share a top or a bottom line, so you can’t really make a comparison. In these situations, it’s a better idea to use a dodged bar chart instead:

Dodged bar charts are usually a better choice for comparing the actual numbers of different groupings. However, this chart does a good job showing one of the limitations dodged bar charts come up against — once you get past 4 or 5 groupings, making comparisons is tricky. In these cases, you’re probably trying to apply the wrong chart for the job, and should consider either breaking your chart up into smaller ones — remember, ink is cheap, and electrons or cheaper — or replacing your bars with a few lines.

The one place where stacked bar charts are appropriate, however, is when you’re comparing the relative proportions of two different groups in each bar. For instance, take the following graph:

In this case, making comparisons across groups is trivial, made simple by the fact that the groupings all share a common line — at 100% for group 1, and at 0% for group 2. This point of reference solves the issue we had with more than two groupings — though note we’d still prefer a dodged bar chart if the bars didn’t always sum to the same amount.

A Quick Tangent

This is usually where most people will go on a super long rant about pie charts and how bad they are. They’re wrong, but in an understandable way.

People love to hate on pie charts, because they’re almost universally a bad chart. However, if it’s important for your viewer to be able to quickly figure out what proportion two or more groupings make up of the whole, a pie chart is actually the fastest and most effective way to get the point across. For instance, compare the following pie and bar charts, made with the same data set:

It’s a lot easier to tell that, say, A is smaller than C through F in the pie chart than the bar plot, since humans are better at summing angles than areas. In these instances, feel free to use a pie chart — and to tell anyone giving you flack that I said it was OK.

Two categorical variables

Our last combination is when you’re looking to have a categorical variable on both the x and y axis. These are trickier plots to think about, as we no longer encode value in position based on how far away a point is from the lower left hand corner, but rather have to get creative in effectively using position to encode a value. Remember that a geom is a geometric representation of how your data set is distributed along the x and y axes of your graph. When both of your axes are categorical, you have to get creative to show that distribution.

One method is to use density, as we would in a scatter plot, to show how many data points you have falling into each combination of categories graphed. You can do this by making a “point cloud” chart, where more dense clouds represent more common combinations:

Even without a single number on this chart, its message is clear — we can tell how our diamonds are distributed with a single glance. A similar way to do this is to use a heat map, where differently colored cells represent a range of values:

I personally think heat maps are less effective — partially because by using the color aesthetic to encode this value, you can’t use it for anything else — but they’re often easier to make with the resources at hand."
"Julia Speed Battle: Syntactual, Recursive, And Iterative Loops","With all of the writing i have been doing for the machine learning module, Lathe, and the new algorithms i have been implementing into 0.0.4, my mind has been set on optimization to really set the bar for speedy machine learning. A couple weeks ago, while working on a time-sensitive project, I was trying to build an algorithm from an enormous data-set with Python, and the Jupyter kernel would crash just from trying to read it in. On the other hand, I thought to my Julia experience, and without even having to go to Scala I was well able to read in data-sets with over thirty-five million observations, and time the predictions out to only a few seconds at the most with Lathe. It was a difficult concept to grasp that rather than hitting the limitations of the hardware that I was running on, I was hitting the limitations of the language.

This got me thinking about low-level algorithms and their place in the speed of calculation in the form of looping. After a-lot of thought, I set my mind to testing out the validity and severity of the well-known computer science ideologies about processing and language specifically in Julia. We all know that Julia doesn’t follow anybody’s rules, and really is a unique language in that regard. I have an article here on what you should know about Julia before using it for ML you can checkout for more information.

Assumptions

In the early day computing-sphere, typically iteration was always considered to be the fastest type of loop for manipulating and individualizing indices in an array. With this in mind, a lot of older systems could transfer less stacks with recursion than modern computers, for obvious reasons (mb of memory vs gb of memory), however a lot of the legacy languages from the 80’s are still moreso optimized for iteration over syntactual and recursive loops. It should also be noted that recursion is frequently frowned upon, and regarded as the slowest way to do a quick task. As for syntactual expressions, typically the result is completely dependent on your language and its compiler."
Geometric deep learning:. Geometric deep learning is a new field…,"Geometric deep learning:

Convolutional Neural Networks on Graphs and Manifolds Miguel Taylor · Follow 5 min read · Apr 22, 2019 -- Listen Share

Geometric deep learning is a new field of machine learning that can learn from complex data like graphs and multi-dimensional points. It seeks to apply traditional Convolutional Neural Networks to 3D objects, graphs and manifolds. In this story I will show you some of geometric deep learning applications, such as:

Classify nodes with similar characteristics on graphs

3D object classification using point clouds

3D shape correspondence on 2D images

On recent years deep learning algorithms (CNN, GAN, LSTM, …) helped us achieve remarkable accuracy on a variety of problems, even surpassing human performance on tasks like:

Image classification

Speech recognition

Language translation

Image generation

It’s been incredible to experience first hand the deep learning revolution of this decade. However, a lot of the algorithms used on modern machine learning applications are actually really old. What stimulated this growth in deep learning has been the global availability of different types of datasets and computational power.

This is important to mention because lately we have been seeing more and more of an special type of data set: 3D Objects. On recent years, specialized hardware has been used to capture 3D point clouds instead of 2D images. Thanks to this we now have a wide variety of datasets containing 3D objects.

These datasets can be focused on object classification or shape detection. There is also a great diversity of 3D shape representations, one of which are manifolds. Manifolds can be explained as multi-dimensional spaces where a shape composed by many points, is represented by a single point on this new space, and similar shapes are close to each other.

Graphs are another type of emerging data. Many real world applications can be modeled as graphs, and there is a lot of data over the internet that already works as a graph. For example, we can consider social networks as graphs, where each user is a node and their interactions with other users are edges. Sensor and computer networks can also be modeled as graphs where each signals and communication represent vertices in the graph.

The problem with this type of data is that traditional deep neural network are not able to parse it correctly. The reason is that most of these networks are based on convolutions, and convolution work well on euclidean data. Graphs and 3D objects are considered non-euclidean data sets.

Images are euclidean data because we can consider them as a function on a two dimensional plane, where the intensity of a pixel is a function of its coordinates x and y. This representation is very useful to define a convolution, but graph representation not so much. Different vertices in a graph can contain very different neighbourhoods, that can vary on the number of neighbours or the connectivity. This makes it impossible to apply convolution as we would do on euclidean domains.

In order to apply deep neural networks on this type of data, we need to resort to some kind of abstraction for the convolution. This abstraction has to consider the limits of non-euclidean data:

No common system of coordinates

No vector space structure

No shift invariant

Spectral approaches

The main idea of spectral approaches such as Graph neural networks is to generalize the Fourier transform theorem for graph and manifold data and doing the convolution on the spectral domain. The generalization of the Fourier transform consists on using the already defined eigenfunctions of graph laplacian as bases for the Fourier transform.

The process to apply a convolution using this generalization is as follows.

Transform the graph the the spectral domain using the graph laplacian eigenfunctions

Perform the same transformation on the filter

Multiply on the spectral domain

Return to the original domain

This approach has presented very good results on data presented as a graph, but has an important weakness: Laplacian eigenfunctions are inconsistent across different domains. This means that this approach is especially susceptible to domain changes:

Spatial approaches

The main idea is to apply a template on a neighborhood representation witch is obtained by mapping the neighbors on a fixed structure. This is the same idea of applying a convolution over images, the difference is that on images the neighbour structure is constant for all vertices.

Regular convolution kernels vs spatial approaches

To define this new neighborhood structure the author considers distance and angle from a point on 3D images or the degree of neighboring nodes on graphs.

The position and values of kernels on the neighborhood structure are not fixed, but rather learned from the training process.

Conclusions

Deep learning does well at extracting features from euclidean data

Euclidean data can be represented on an euclidean space and follow the rules of euclidean spaces

Graphs and manifolds are considered non-euclidean data

Geometric deep learning models can learn from non-euclidean data by generalizing on some way the convolution operation.

References

[1] Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J. and Bronstein, M.M., 2017. Geometric deep learning on graphs and manifolds using mixture model cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 5115–5124)."
Dataset deduplication using spark’s MLlib,"A deduplication process is always important for companies with a huge amount of data. From one thing, deduplication minimizes the amount of space required to store business data and will bring a lower infrastructure cost and better performance for our pipelines. From another thing, reducing the number of duplicates will reduce the pipelines complexity and will increase the time to business via continuing integration and continuous delivery (CI/CD).

Sometimes a deduplication process consists of a simple text to text matching and you can simply choose either a CRC32-Checksum or an MD5 matching. However, there are some situations where the dataset rows differ only for some small text discrepancies on some of the columns, even though they represent the same entity. Thus, this article shows an entities recognition and linking process using two different spark approaches over a specific dataset of products collected by scrapping e-commerce websites will be used.

The entire code and process describe following could be found here:

The general process could be found on this trait (… Yes, I use scala for data science !!!):

package com.sample.utils



import org.apache.spark.sql.DataFrame



trait OperationsHelper {

def ds: DataFrame



def preparedDataSet()(df: DataFrame): DataFrame



def deduplicateDataSet()(df: DataFrame): DataFrame



def resultsDataFrame()(df: DataFrame): DataFrame

}

As you will see the idea behind this helper will be having a functional pipe from where easily chain dataframe transforms could be called.

Preprocessing Products Data

Techniques to reduce dimensionality are widely used by the data science community to get a smaller set of features to analyze and obtain better performance while training and evaluating models. The PCA method allows dimensionality reduction while keeping those features that describe a large amount of the information. Consequently, this pre-processing stage follows these steps:

Data Cleaning: Cleaning the data to have a common scale. For the case of the products, dataset consists of a simple text cleaning for cases, white spaces, encoding, and symbols.

Cleaning the data to have a common scale. For the case of the products, dataset consists of a simple text cleaning for cases, white spaces, encoding, and symbols. Features Selection: Using the PCA technic a set of features are selected.(“titleChunk”, “contentChunk”, “color”, “ productType”)

The content found on the features above contains most of the discrepancies for candidate duplicate products.

1 — Approach A: Locality-sensitive hashing (LSH)

Locality-sensitive hashing is a technic used for entity resolution, then records that represent the same entity will be found. The spark MLlib has a custom LSH implementation used here to find duplicates as follow:

First, hashes are generated using a concatenation of selected features (PC above). For a real-world example hashes for each feature could be generated. However, for this example and in order to get results faster a simple concatenated column is used.

Then, this column is used for generating LSH vectors as follow:

— A tokenizer generates the list of words for a record using a words stopper.

— A CountVectorizerModel creates the vectors with hashes and buckets (similar hashes) for the LSH algorithm.

val pccTokenizer = new Tokenizer()

.setInputCol(OperationsHelperLSH.ConcatComments)

.setOutputCol(OperationsHelperLSH.ColumnWordsArray)

val wordsArrayDF = pccTokenizer.transform(df)



val remover = new StopWordsRemover()

.setCaseSensitive(false)

.setStopWords(OperationsHelperLSH.stopWords)

.setInputCol(OperationsHelperLSH.ColumnWordsArray)

.setOutputCol(OperationsHelperLSH.ColumnFilteredWordsArray)



val wordsFiltered = remover.transform(wordsArrayDF)



val validateEmptyVector = udf({ v: Vector => v.numNonzeros > 0 }, DataTypes.BooleanType) val vectorModeler: CountVectorizerModel = new CountVectorizer()

.setInputCol(OperationsHelperLSH.ColumnFilteredWordsArray)

.setOutputCol(OperationsHelperLSH.ColumnFeaturesArray)

.setVocabSize(VocabularySHLSize)

.setMinDF(10)

.fit(wordsFiltered)



val vectorizedProductsDF = vectorModeler.transform(wordsFiltered)

.filter(validateEmptyVector(col(OperationsHelperLSH.ColumnFeaturesArray)))

.select(col(OperationsHelperWindowStrategy.ConcatComments),

col(OperationsHelperLSH.ColumnUniqueId),

col(OperationsHelperLSH.ColumnFilteredWordsArray),

col(OperationsHelperLSH.ColumnFeaturesArray))



(vectorizedProductsDF, vectorModeler)

Class: com.sample.products.OperationsHelperLSH.scala

In order to finish the training step, a MinHashLSHModel is used to train the products data, generating the final buckets of similar products.

Finally, using KNN similar hashes for a category could be found.

/**

* Uses the dataset to train the model.

*

*/

def deduplicateDataSet(df: DataFrame): (DataFrame, MinHashLSHModel) = {



val minLshConfig = new MinHashLSH().setNumHashTables(hashesNumber)

.setInputCol(OperationsHelperLSH.ColumnFeaturesArray)

.setOutputCol(OperationsHelperLSH.hashValuesColumn)



val lshModel = minLshConfig.fit(df)



(lshModel.transform(df), lshModel)

}





/**

* Applies KNN to find similar records.

*

*/

def filterResults(df: DataFrame,

vectorModeler: CountVectorizerModel,

lshModel: MinHashLSHModel,

categoryQuery: (String, String)

): DataFrame = {

val key = Vectors.sparse(VocabularySHLSize,

Seq((vectorModeler.vocabulary.indexOf(categoryQuery._1), 1.0),

(vectorModeler.vocabulary.indexOf(categoryQuery._2), 1.0)))



lshModel.approxNearestNeighbors(df, key, nearNeighboursNumber).toDF() }



To run an example: Go to the test com.sample.processor.products.ProcessorProductsLshTest and you will see a complete flow running.

Input Params:

category → color = ‘negro’ and productType = ‘tdi’.

nearNeighboursNumber → 4

hashesNumber → 3 (More hashes more precision but more computing cost).

3 products with almost the same text for selected features.

Results Analysis:

Pros:

Accurate: If a complete set of fields (representing the string) is used, the correct value for hashes and neighbors could detect almost all the repeated values.

Faster: compared with other ML strategies as Term-frequency-inverse, etc.

Cons :

A cluster with good resources is needed.

Needs a process for data cleaning.

2 — Approach B: Fuzzy Matching with Levenshtein + Spark Windows:

Levenshtein is an algorithm used for strings fuzzy matching. Basically, this method measures the difference between two strings. Furthermore, the spark windows functions allow dataset analytics function in a concise way, avoiding multiple groupBy and Join operations. Thus, this method defines a 2-level window to group similar data and then applies Levenshtein to values in the same windows to discover duplicates. The process is described here:

First of all, a set of records described as non-fuzzy is selected. The list contains columns that represent categories and are were free of errors most of the times in the PCA process: (“productType”, “city”, “country”, “region”, “year”).

This window represents the general windows hash for the analysis.

Secondly, a second window to discover quite similar records is applied. This list represents records that are neither part from the fuzzy list (PCA) nor from the non-fuzzy list: (“doors”, “fuel”, “make”, “mileage”, “model”, “color”, “price”)

Note: the “date” field helps to order and get only the most recent.

Then, For each group applies levenshtein (string difference only in the second window) over the concatenated most fuzzy fields from PCA results: (“titleChunk”, “contentChunk”).

As you can see an MD5 representation of the columns is used instead of each String to have better performance:

keyhash: MD5 for the category column set. The picture below shows many products within the same category.

hashDiff: MD5 hash that represents the non-fuzzy set. The picture below shows products within the same category but with a different description (> levenshteinThreshold) and also those with a small levenshtein (< levenshteinThreshold) difference having the same hashDiff.

Finally, the values with the same hashes (both) and rank only change the row_num. Filtering row_num == 1 is possible to get the deduplicate Data set.

/**

* Applies windows functions and Levenshtein to group similar categories.

*

*/

override def deduplicateDataSet()(df: DataFrame): DataFrame = {

df

.withColumn(OperationsHelperWindowStrategy.ColumnRank, dense_rank().over(windowProductKeyHash))

.withColumn(OperationsHelperWindowStrategy.ColumnHashWithDiff,

concat(col(OperationsHelperWindowStrategy.ColumnCategoryFieldsHash),

when(levenshtein(

first(OperationsHelperWindowStrategy.ConcatComments).over(windowProductsCategoryRank),

col(OperationsHelperWindowStrategy.ConcatComments)) >= levenshteinThreshold, lit(""1""))

.otherwise(lit(""""))))

.withColumn(OperationsHelperWindowStrategy.ColumnRowNum, row_number().over(windowProductsCategoryRank))

}

Class: com.sample.products.OperationsHelperWindowStrategy.scala

To run an example: Go to the test com.sample.processor.products.ProcessorProductsWindowsTest and you will see a complete flow running.

Input Params: levenshteinThreshold → 6

Results:

2 groups example with almost exact values.

The results are deduplicated after filtering rn == 1. This removes > 1/3 of the data in the sample dataset.

Results Analysis:

Pros:

More control in the spark partitioner and functions.

Cons :

Could have much more false positives.

Final Conclusion

A deduplication process depends always on the company needs and the amount of data to analyze. This article describes two different strategies. As a result, Levenshtein with windows functions is good enough for small dimensionality problems; otherwise, LSH is always the best option"
Access Azure Database for MySQL from Azure functions with SSL Certificate Verification,"Access Azure Database for MySQL from Azure functions with SSL Certificate Verification

Photo by Dayne Topkin on Unsplash

Recently I got a customer who has relatively small volume data to be managed. So, I suggested Azure Database for MySQL. Basically, the smallest instance cost about $560 AUD per month will be enough. Also, because this is DBaaS, the customer does not need to hire more people to maintain it.

The customer also wants to automate the ETL processes to this MySQL DB. Although I think Data Factory would be the ideal solution, the cost can be a big issue because the ETL has to be done every 5 minutes which will trigger the orchestration too frequently and cause a “shock” bill. Therefore, I suggest using Azure Functions which is a serverless computing solution to do this ETL. Another reason is that the data transformation is not too complicated and will rarely be changed once done.

However, I met an issue when I use python to implement this Azure function to connect to the Azure MySQL server. That is, the MySQL has enforced the SSL encryption, but the Azure Function side doesn’t provide a certificate. In our case, disabling the SSL enforcement is not an option because we don’t want to put customer’s data in a risky situation such as man-in-the-middle attacks. Therefore, I’ll need to solve this problem and finally, I did it. Here are the steps to reproduce the issue and how to solve it.

Step 1: Create Azure Database for MySQL

Go to your Azure subscription and click Create a resource -> search for Azure Database for MySQL -> Create

In the Create MySQL server page, fill in the resource group, server name and other required fields. In this example, I choose MySQL v8.0, then click Review + create -> Create

To be able to test your Azure Functions on your local machine. Don’t forget to add your local IP address to the connection security settings. Specifically, Go to the Azure MySQL we just created. On the left navigation select…"
Time Series Clustering and Dimensionality Reduction,"Time Series must be handled with care by data scientists. This kind of data contains intrinsic information about temporal dependency. it’s our work to extract these golden resources, where it is possible and useful, in order to help our model to perform the best.

With Time Series I see confusion when we face a problem of dimensionality reduction or clustering. We are used to think about these tasks in more classical domains, while they remain a tabù when we deal with Time Series.

In this post, I try to clarify these topics developing an interesting solution where I work with multidimensional Series coming from different individuals. Our purpose is to cluster them in an unsupervised way making use of deep learning, being wary of correlations, and pointing a useful technique that every data scientist must know!

THE DATASET

I got the data from UCI Machine Learning repository; I selected the Public Dataset of Accelerometer Data for Human Motion Primitives Detection. These data are a public collection of labeled accelerometer data recordings to be used for the creation and validation of acceleration models of human motion primitives.

Different types of activities are tracked, i.e. drinking, eating, climbing and so on. For a particular activity of a specific individual measured, we have 3 different sensor series at disposal: X-axis (pointing toward the hand), Y-axis (pointing toward the left), Z-axis (perpendicular to the plane of the hand).

I figure myself in this situation because it allows to carry out our initial problems of clustering (multiple individuals) and dimensionality reduction (multiple series for every individual) all in one single case.

Below I plot 2 examples of data at our disposal coming from a male and female individuals. In total, we have 20 individuals with the same measurement length.

DIMENSIONALITY REDUCTION"
D-ID Is Altering Facial Recognition’s Path Towards Privacy,"D-ID Is Altering Facial Recognition’s Path Towards Privacy

While the rest of the world was caught up in the rage of selfies, face-timing and photo uploading, three young men saw a future of increasing surveillance and are now seeking to change this. Hessie Jones · Follow Published in Towards Data Science · 8 min read · Jul 18, 2019 -- Share

DEPOSIT PHOTOS

Over the last decade we’ve witnessed the deluge of online photo-sharing. From Pinterest to Instagram to Snapchat the rise of photo-sharing has enabled the soaring activity of selfies, citizen journalism, and events in-the-moment. Brandwatch’s recent stats revealed among 18–24 year-old’s, over 75% use Instagram and Snapchat, and 94% use Youtube:

On Instagram More than 40 billion photos have been shared so far

Over 95 million photos are uploaded each day on Instagram

In Q3 2017, 3.5bn snaps were sent.

The future of image and video is clear based on these previous estimations:

An estimated 84 percent of communications in 2018 were visual.

An estimated 79 percent of internet traffic content in 2018 was video.

Posts that include images produce 650 percent higher engagement

D-ID Co-founders: Eliran Kuta, Gil Perry, and Sella Blondheim

Three young men from Israel were watching the advent of image sharing unfold. Gil Perry, Sella Blondheim and Eliran Kuta were serving in the Israel Defense forces at the time. While the rest of the world was caught up in the rage of selfies, face-timing and photo uploading, Gil, Sella and Eliran saw things very differently,"
Accuracy Performance Measures in Data Science: Confusion Matrix,"Accuracy Performance Measures in Data Science: Confusion Matrix

A brief look into various ways by which you can assess your performance measures using Confusion Matrix for Data Science models Dhruv Sharma · Follow Published in Towards Data Science · 6 min read · Sep 17, 2019 -- Listen Share

Photo by Eleonora Patricola on Unsplash

In a previous article, I had briefly explained the intricate workings and trappings of a k-NN Model, let us now try to look briefly into first implementing the model and assess the adult income dataset, and then analyzing accuracy by various performance measures.

A brief look into the k-NN algorithm shows us that the algorithm is a classification algorithm, basically performed by identifying the nearest neighbors and then their classification into various classes. It can be used for binary or multi-class problems.

Photo by Alexander Mils on Unsplash

Adult Income Dataset

The model, that we will try to implement here is the Adult Income dataset. The dataset can be found at Kaggle, or on my GitHub repository. This dataset contains the income for adults across the United States and is one of the most common problems to solve using the k-NN algorithm. Here the income was originally continuous but has been made into binary, by using a filter of $50,000. Incomes greater than $50,000 are given a 1, while incomes lesser than that are given a 0. The dataset also has 14 other predictors. A brief, first few observations of the dataset shows the following:

Result from head() on the dataset

We can see from above, that the dataset has important descriptors that can be used to explain why a person’s income would be more or less than $50,000. We can also observe that the distribution of income levels is quite skewed, that is, there are more people with income less than $50,000 than people with income more than it.

More incomes lesser than $50,000 than above it

After performing the necessary data manipulation operations of dropping nulls and scaling the features, we get our final partial dataset as:

Partial dataset after data cleaning has been performed

Model Implementation

Now that the data has been cleaned, we can implement the model on it, and then move on to performing accuracy by various measures. As previously discussed, the k-NN model is implemented in various steps, which are:

Steps for conducting a k-NN analysis, source: Recent Trends in Big Data Using Hadoop

So, let us take the default number of neighbors (k=5), given by scikit-learn, and let us also take the baseline conditions, we can execute the model as:

And then we can assess how our model is performing using various type of assessment measures available to us.

Confusion Matrix

The most common type of metric available to us is the confusion matrix, which is also called the confidence matrix. The confusion matrix is a matrix that looks like:

Sample Confusion Matrix

What we can see from above is that the confusion matrix is a matrix between actual values vs predicted values. It is generally used for classification purposes, where it is necessary to predict the target as a 1 or 0. When we observe the actual value as absent, we give it a 0, and 1 otherwise. The same is done for predicted values as well. So, how is this important?

Well, we can tell a lot of things from this matrix, such as:

Our confusion matrix looks like:

Confusion Matrix for the Adult Income dataset

Accuracy: This is the rate of the classifier being correct, so basically take a sum of True Positive and True Negative values and then divide by total. So it means that there are a total of 14,653 values, out of which we have 10,109 True Positives and 2045 True Negative values. Therefore the accuracy of our model will be (10109 + 2045)/14653 = 82.94%. We can say that our model has good accuracy. It is also called Hit Ratio since it is the measure of total hits vs. all values.

This is the rate of the classifier being correct, so basically take a sum of True Positive and True Negative values and then divide by total. So it means that there are a total of 14,653 values, out of which we have 10,109 True Positives and 2045 True Negative values. Therefore the accuracy of our model will be (10109 + 2045)/14653 = 82.94%. We can say that our model has good accuracy. It is also called Hit Ratio since it is the measure of total hits vs. all values. Misclassification Rate (MISC) : This is the rate of values that, as the name suggests, were misclassified. It is also called as the Miss Ratio since it is the count of values that were missed. Therefore, if we subtract Accuracy from 100%, we will get the misclassification rate. Our MISC value here is 0.17 or 17%. It means that in our case, we can say that when the misclassification rate is 17% that out of 100 people in the dataset, 17 people were incorrectly classified.

: This is the rate of values that, as the name suggests, were misclassified. It is also called as the Miss Ratio since it is the count of values that were missed. Therefore, if we subtract Accuracy from 100%, we will get the misclassification rate. Our MISC value here is 0.17 or 17%. It means that in our case, we can say that when the misclassification rate is 17% that out of 100 people in the dataset, 17 people were incorrectly classified. Precision : This is the rate of values that measures the accuracy of positive predictions. So when we divide True Positives, by total positives, we get the precision value. Therefore our precision here is (2045)/(2045 + 1013) = 66.87%. It means that in our case, we can say that when precision is 66.81% that out of 100 people who were predicted to have income more than $50,000, 67 people were correctly classified.

: This is the rate of values that measures the accuracy of positive predictions. So when we divide True Positives, by total positives, we get the precision value. Therefore our precision here is (2045)/(2045 + 1013) = 66.87%. It means that in our case, we can say that when precision is 66.81% that out of 100 people who were predicted to have income more than $50,000, 67 people were correctly classified. Recall : This is the rate of values that measures positive instances that were correctly identified by the classifier. It is also called sensitivity, or the true positive rate. Thus recall is (True Positive)/(True Positive+False Negative) or in our case 2045/(2045 + 1486) = 57.91% . It means that in our case, we can say that when the recall is 57.91% that out of 100 people who have income more than $50,000, 57.91 or 58 people were correctly classified.

: This is the rate of values that measures positive instances that were correctly identified by the classifier. It is also called sensitivity, or the true positive rate. Thus recall is (True Positive)/(True Positive+False Negative) or in our case 2045/(2045 + 1486) = 57.91% . It means that in our case, we can say that when the recall is 57.91% that out of 100 people who have income more than $50,000, 57.91 or 58 people were correctly classified. Specificity : This is the rate of values that measures negative instances that were correctly identified by the classifier. It is also called the True Negative rate. Specificity is then, (True Negatives) / (True Negatives + False Positives) or in our case, 10109/(10109 + 1013) or 90.89%. It means that in our case, we can say with 90.89% specificity that out of 100 people who did not have income more than $50,000, 90.89 or 91 people were correctly classified.

: This is the rate of values that measures negative instances that were correctly identified by the classifier. It is also called the True Negative rate. Specificity is then, (True Negatives) / (True Negatives + False Positives) or in our case, 10109/(10109 + 1013) or 90.89%. It means that in our case, we can say with 90.89% specificity that out of 100 people who did not have income more than $50,000, 90.89 or 91 people were correctly classified. F-1 Score: It is the harmonic mean of precision and recall. The normal mean gives equal preference to all values, while F-1 score gives much more weight to low values. F-1 Score is basically the below:

F-1 Score

In our case, F-1 Score is given as:

Precision, Recall and F1- Score values for 1 and 0 respectively

Receiver Operating Characteristics and Area Under the Curve: Receiver Operating Characteristics curve, also known as the ROC Curve, is basically a plot of True Positive Rate vs False Positive Rate. False Positive rate is the ratio of negative instances that were incorrectly classified. It can also be defined as 1- True Negative Rate, which is also the Specificity. Therefore this curve can also be thought of as a curve between Sensitivity and 1- Specificity. In Python, it can be implemented using the scikit-learn or the matplotlib library. The area under the curve is also another important measure here. The more it is closer to 1, the better the classification has been performed. The curve for our case looks like:

Code to draw the ROC Curve, Source: Hands-on ML by Aurelien Geron

ROC Curve and the AUC at bottom

Cumulative Accuracy Profile: This curve is similar to the ROC Curve, however, it is a chart of accuracy. It basically plots accuracy and helps to understand and conclude about the robustness of the model.

Code for making the curve:

Code for drawing the CAP Curve

CAP Curve

Having said the above, it is important to these note that there are multitudes of other information that you can use to make it sure that your model has the required accuracy and performance. These include Coefficient of Variation, Root Mean Square Error, and others but this article was for Confusion Matrix only."
Mastering the art of web scraping with Selenium and Python [Part 1/2],"Mastering the art of web scraping with Selenium and Python [Part 1/2]

Selenium is a powerful tool for advanced interactions with websites: login, clicks… Let’s use it for web scraping Félix Revert · Follow Published in Towards Data Science · 3 min read · Sep 11, 2019 -- Listen Share

Using Selenium to do web scraping requires a specific strategy to access the data you seek

For 90% of the websites, you don’t need Selenium. Indeed the simple request and BeautifulSoup packages combined would do the job. I wrote an article on how to do that.

But what if the website asks you to login before accessing its content? What if there’s no specific url for the pages you want to scrape, and you need to click on buttons and trigger javascript actions before landing on the correct page?

This is the goal of this article: achieve web scraping on websites that are not simple to scrape."
From Scratch: The Game of Life,"Hello everyone and welcome to the second article in the “From Scratch” series. (Previous one: From Scratch: Bayesian Inference, Markov Chain Monte Carlo and Metropolis Hastings, in python)

In this article we explain and provide an implementation for “The Game of Life”. I say ‘we’ because this time I am joined by my friend and colleague Michel Haber. The code is provided on both of our GitHub profiles: Joseph94m, Michel-Haber.

The first section will be focused on giving an explanation of the rules of the game as well as examples of how it is played/defined. The second section will provide the implementation details in Python and Haskell for the game of life. In the third section, we compare the performance and elegance of our implementations. We chose these two languages because they represent two of the most used programming paradigms, imperative (Python) and functional (Haskell), and because of the relative ease they provide in writing expressive and understandable code.

1-Introduction to the Game of Life

The game of life is a cellular automaton imagined by John H. Conway in the 1970s and is probably, the best known of all cellular automata. Despite very simple rules, the game of life is Turing-complete and deterministic.

The game of life is a game in the mathematical sense rather than a playable game. It is “zero player game”.

The game takes place on a two-dimensional finite or infinite grid whose cells can take two distinct states: “alive” or “dead”.

At each stage, the evolution of a cell is entirely determined by its current state and the state of its eight neighbours as follows:

1) A dead cell with exactly three living neighbours becomes alive.

2) A living cell with two or three living neighbours remains alive.

3) In all other cases, the cell becomes (or remains) dead.

Let’s assume a simple initial state where only 3 cells are alive: left cell, middle cell, and right cell.

Starting from the simple configuration in Figure 1, and letting the simulation run for one iteration results in Figure 2.

So how does this happen?

Figure 2

The cells on the left and on the right have only one neighbour, the middle cell, so they die. The middle cell has two neighbours, left and right, so it stays alive. The top and bottom cells have 3 neighbours (middle, right and left) so they become alive. It is really important to note that a cell does not die or live until the end of the iteration. I.e. the algorithm decides which cells are going to die or come to life and then gives them the news all together. This ensures that the order in which cells are evaluated does not matter.

Running one more iteration gives us the state represented in Figure 1(initial state).

As such, starting with this configuration, the game enters the infinite loop represented in Figure 3. This configuration is called a blinker.

Figure 3: Blinker

Figure 4: An early state from figure 5 that shows some interesting structures.

Now for a more interesting setup to show what impressed Conway the most. Starting from the simple configuration in the left image of Figure 5 and letting the code run, the board evolves into something completely unexpected. We can observe stable structures: blocks and beehives. We can also observe looping structures such as the blinkers from Figure 3 and the larger blinkers that are made up of 3 or 4 smaller ones. Another interesting structure observed in this simulation is the glider. A glider looks like an old video game spaceship. It is unbounded by space, and keeps on going — Ad vitam æternam."
Linear Regression,"Linear Regression

Linear Regression is a famous supervised learning algorithm used to predict a real-valued output. The linear regression model is a linear combination of the features of the input examples.

A note on the notation. x_{i} means x subscript i and x_{^th} means x superscript th.

Representation of the Data

As discussed in the definition, linear regression is a supervised learning algorithm, therefore, has a set of N labelled examples, represented as :

Data used to construct a linear regression model.

Here, x_{i} represents a set of properties corresponding to the i_{^th} example. These set of properties are collectively called a feature vector. All the examples from i=1,2,3,…,n have a corresponding real-valued y, which denotes a physical quantity such as cost, temperature, or any other continuous value.

Here each feature vector is 3-dimensional consisting of the area of house in square metres, the number of rooms and the age of house in years. The target variable is the price of house in USD.

Model

Now, as we have our examples ready we want to make our model f(x) that will help us to predict the output y for an unseen x.

Model of linear regression learning algorithm. Here, w is an R-dimensional parameter vector (x is an R-dimensional feature vector) and b is the bias.

The job of the model is to predict a real-value y for an unseen value of the feature vector x. But, we want to find a model such that it does the best job in predicting the values of y, therefore, we want to find values of w and b such that the predictions are as close as possible to the actual answers. It is obvious that different values of w and b result in producing different models, of varying capabilities. Therefore, our job is to find the optimal set of values w* and b* which will minimize the error between the predictions made by the model f(x) and the actual results y for the training set.

The Best Model

As discussed earlier we have N examples and a model f(x) for which we need to find the optimal values of w and b. Let us use all these N examples for finding the optimal values of w and b, popularly called as training our model. We need to find values of w and b such that the following expression is minimum.

Cost function for linear regression.

This is our objective function as we are going to minimize it. Learning algorithms have functions which we try to minimize or maximise. These functions are called as loss function or the cost function. This particular form is called the mean squared error loss function.

If you observe the loss function:

It is simply subtracting the model’s output, f(x_{i}) and the actual output y_{i},

Squaring it,

And finally taking its average.

To understand this better let us assume John, recently appeared for an examination having 10 mathematical questions and the answer key has been published. Now John decides to find out how well has he performed? so, he compares his answer, f(x)_{i} with the corresponding answer y_{i} on the answer key. If the difference between John’s answer and the actual answer f(x)_{i}-y_{i} is 0 he answered that question correctly. If he answered all the questions correctly then the average will also be 0 which corresponds to the best performance, implying the best model. Squaring the error helps to accentuate the error of the model. We could have also taken a cube or higher power but then the derivatives would have been more difficult to work out. We worry about the derivatives of the cost function as setting them to zero gives the optimal value w* and b* for the model.

General Questions and Examples

Let us discuss a few questions that perplexed me while studying about linear regression. But, before we start let’s take a look at a very primitive example of linear regression.

So, John and his friends decided to start studying linear regression from scratch so they began by collecting examples themselves. The examples they collected are shown below.

The tabular form of data. Here x is a single dimensional feature vector any y is a real-valued output corresponding to each feature vector.

After having collected the data, John decides to fit a linear regression model to it.

The linear regression model of form f(x)=wx+b.

This is a model of form f(x)=wx+b where w is a scalar, as x, the feature vector is one dimensional. A better comprehension of this model is to compare this to the equation of a straight line y=mx+c where m is analogous to w and c to b. This is a linear model.

But, can we do better? Can we come up with a model that performs better than the current one? Yes, we can. It is a common confusion that linear regression only comprises of models that are straight lines. However, we can also fit curves to our data by transforming the data. Let’s transform our feature vector by squaring each x_{i} value.

After having transformed our feature vector let us try to fit a model on the new feature vector x² and the output y (original feature vector x is not considered for training the model instead, it’s transformation x_{^ 2} has been used to train the model).

The model is a better fit than the previous linear model. Here the original feature vector x_{i} is transformed to it’s square and then the model is computed.

So, now we have predicted a polynomial model that is better than the linear one by transforming the original feature vector x_{i} to its square. The new model corresponds to f(x)=wx²+b.

This is a plot of the polynomial regression model. Note that this plot is between X and Y with the polynomial model of degree two. The previous plot was between X² and Y, therefore, it was linear.

The capability of the model to predict better results has increased by transforming the feature vectors but we need to be aware of over fitting. Over fitting happens when the model predicts too well during the training phase but makes an error while predicting unseen examples. Over fitting does not reflect the real-world scenario of being dynamic. It does not produce generalised models.

This is an example of overfitting where the model is too accurate on the training examples.

Let’s say that the feature vector is R-dimensional. We have seen the case where R=1 and also predicted a linear and a polynomial model. If R=2 a plane is predicted as the model. Generally linear regression models a hyper plane for a data set with R-dimensional feature vector, x and 1-dimensional output, y.

Hyper plane is a subspace with one less dimension than that of its surrounding space. In case of a 1 dimensional line the point is a hyper plane, in case of a 2 dimensional region a line is a hyper plane, in case of a 3 dimensional space the plane is a hyper plane and so on.

The Bias Term

Let’s discuss the utility of the bias term. Consider the equation of a straight line y=mx. In this case m controls the slope of the line and can rotate the line anywhere but only about the origin.

Suppose you decide to use this model for a trivial linear regression problem. However, any hypothesis that you generate will always pass through the origin and might fail to generalise. Adding the bias term will result in the hypothesis y=mx+c thereby, allowing you to move your line anywhere in the plane. The bias term helps in generalising the hypothesis."
Auto-Encoder: What Is It? And What Is It Used For? (Part 1),"Member-only story Auto-Encoder: What Is It? And What Is It Used For? (Part 1)

A Gentle Introduction to Auto-Encoder and Some Of Its Common Use Cases With Python Code Will Badr · Follow Published in Towards Data Science · 6 min read · Apr 22, 2019 -- 13 Share

Background:

Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.

Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.

Here is an example of the input/output image from the MNIST dataset to an autoencoder.

Autoencoder for MNIST

Autoencoder Components:

Autoencoders consists of 4 main parts:

1- Encoder: In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.

2- Bottleneck: which is the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data.

3- Decoder: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.

4- Reconstruction Loss: This is the method that measures measure how well the decoder is performing and how close the output is to the original input.

The training then involves using back propagation in order to minimize the network’s reconstruction loss.

You must be wondering why would I train a neural network just to output an image or data that is exactly the same as the input! This article will cover the most common use cases for Autoencoder. Let’s get started:

Autoencoder Architecture:

The network architecture for autoencoders can vary between a simple FeedForward network, LSTM network or Convolutional Neural Network depending on the use case. We will explore some of those architectures in the…"
"Learning SQL 201: Optimizing Queries, Regardless of Platform","Caveat: As of this writing, I’ve used the following database-like systems in a production environment: MySQL, PostgreSQL, Hive, MapReduce on Hadoop, AWS Redshift, GCP BigQuery, in various mixes of on-prem/hybrid/cloud setups. My optimization knowledge largely stems from those. I’ll stick to strategies/thinking process here, but there are definitely features and quirks in other popular databases that I’m not familiar with, especially from SQL Server and Oracle.

This article is about speed, common strategies for making things go FASTER while avoiding specific implementation details. I’m trying to express the thought process of optimization, not the specific mechanics. Before I knew it, it’s turned into a monster of an article. There’s a lot to cover!

Intro and Background

Optimizing queries is a hard topic to write about because it involves specifics. Specifics about database engines, software, and sometimes even hardware and network architecture. I’ve been asked to write about this topic multiple times, and I’ve always resisted because I couldn’t see a way to write a generally useful article for something that very quickly gets into the weeds.

There are entire books written about how to optimize different database systems, which includes queries, but also details in the tuning of the systems themselves. They’re always written about a specific platform, not in general. It’s for good reason — every platform is different and the tuning parameters you need depend on your workload and setup (write heavy vs read heavy, SSDs vs Spinning disk, etc).

But on the way home during a nasty heatwave, I had an sudden flash of insight as to what threads tie optimization together. So I’m giving this crazy article a try. I’m going to avoid too many specifics and focus on the core thinking process that goes into identifying the things that will make your queries go faster. There will be forays into specifics only for illustrative purposes, and no real code examples. Also for brevity, I can’t be super thorough, but I’ll link to examples and further reading as I go."
Wine Embeddings and a Wine Recommender,"RoboSomm

Wine Embeddings and a Wine Recommender

One of the cornerstones of previous chapters of the RoboSomm series has been to extract descriptors from professional wine reviews, and to convert these into quantitative features. In this article, we will explore a way of extracting features from wine reviews that combines the best of the existing RoboSomm series and academic literature on this topic. We will then use these features to produce a simple wine recommendation engine.

The Jupyter Notebook with all relevant code can be found in this Github repository. Our dataset consists of roughly 180,000 professional wine reviews, scraped from www.winemag.com. These reviews span roughly 20 years, dozens of countries and hundreds of grape varieties.

Wine Embeddings

In the following section, we walk through the five steps required to create our ‘wine embeddings’: a 300-dimensional vector for each wine, summarizing its sensory profile. On the way, we will explain successful approaches others have taken in similar projects. Before we proceed, let’s pick a wine to join us on this journey:

Point & Line 2016 John Sebastiano Vineyard Reserve Pinot Noir

Review: Dried red flowers and sagebrush combine for an elegant aromatic entry to this bottling by two business partners who have worked in Santa Barbara’s restaurant scene for many years. Tarragon and intriguing peppercorn flavors decorate the tangy cranberry palate, which is lightly bodied but very well structured.

Excellent! Time to get stuck in.

Step 1: Normalize words in wine review (remove stopwords, punctuation, stemming)

The first step is to normalize our text. We want to remove stopwords and any punctuation from our raw text. In addition, we will use a stemmer (Snowball Stemmer in Sci-Kit Learn) to reduce inflected words to their stem. The Pinot review becomes the following:

dri red flower sagebrush combin eleg aromat entri bottl two bus partner work santa barbara restaur scene mani year tarragon intrigu peppercorn flavor decor tangi cranberri palat light_bodi veri well structur

Step 2: Enhance the set of normalized words with phrases (bi-grams and tri-grams)

Next, we want to account for the possibility that some of the terms we want to extract from the wine descriptions are actually combinations of words or phrases. Here, we can use the gensim package Phrases to produce a set of bi- and tri-grams for the full corpus. Running our normalized wine review through the phraser consolidates terms such as ‘light’ and ‘bodi’ which are frequently found next to each other to ‘light_bodi’:

dri red flower sagebrush combin eleg aromat entri bottl two bus partner work santa_barbara restaur scene mani_year tarragon intrigu peppercorn flavor decor tangi cranberri palat light_bodi veri well structur

Step 3: Use the RoboSomm wine wheels to standardize the wine descriptors in each review

Wine reviewers are often creative in their use of language, and sometimes use different words to describe things that are seemingly the same. After all, are ‘wet slate’, ‘wet stone’ and ‘wet cement’ aromas not really manifestations of the same sensory experience? In addition, wine tasting has specific jargon. Terms such as ‘baked’, ‘hot’ or ‘polished’ have a specific meaning in the world of wine tasting.

To standardize wine jargon and creative descriptors, researchers such as Bernard Chen have developed the Computational Wine Wheel. The Computational Wine Wheel categorizes and maps various wine terms that appear in wine reviews to create a consolidated set of descriptors. This great work, together with the contributions of others (e.g. Wine Folly and UC Davis) has been used to generate the RoboSomm wine wheels. These wine wheels were created by looking at a list of the most frequently occurring descriptors in the corpus after going through steps 1 and 2 outlined above. This list was then reviewed manually, and mapped onto a set of standardized descriptors. In total, this resulted in a mapping for over 1,000 ‘raw’ descriptors.

The first of the RoboSomm wine wheels is an aroma wheel, that categorizes a variety of aromatic descriptors:

Wine Aroma Wheel

The second wine wheel is a non-aroma wheel, that accounts for other characteristics, such as body, sweetness and acid levels. These descriptors are not typically included in tasting wheels, but are prominent parts of a tasting experience:

Wine Non-Aroma Wheel

We can choose to standardize wine terms at any of the three levels of the wheel, or use the raw descriptor itself (no standardization). For now, we will map the descriptors to the outside layer of the wheel. For the Pinot Noir review we started processing, we obtain the following:

dry red flower sagebrush combin elegant aromat entri bottl two bus partner work santa_barbara restaur scene mani_year tarragon intrigu pepper flavor decor tangy cranberry palat light_bodied veri well structur

Note that all the descriptors that have been mapped are highlighted in bold. The other terms are either non-informative or ambiguous in the context of this analysis.

Step 4: Retrieve the Word2Vec word embedding for each mapped term in the review

Next, we need to consider how we will quantify our set of mapped descriptors. A common approach to doing this (and one that was used in previous chapters of the RoboSomm series!) is to represent the absence/presence of each descriptor in the corpus with a 0 or a 1. However, this approach does not take into account semantic (dis)similarities between terms. Tarragon, for instance, is more similar to sagebrush than it is to cranberry. To account for this, we can create word embeddings: vector representations of words and phrases. Researchers such as Els Lefever and her co-authors have taken a similar approach to quantifying wine reviews in their work.

For the purpose of this project, we will use a technique called Word2Vec to generate a 300-dimensional embedding for every mapped term. Since wine jargon is so specific, we have to train our Word2Vec model on a representative corpus. Fortunately, our set of 180,000 wine reviews is exactly that! Having previously mapped our descriptors using our wine wheels, we have already somewhat standardized the wine terms in our corpus. This was done to eliminate unnecessary semantic nuance (e.g. consolidate ‘wet stone’, ‘wet slate’ and ‘wet cement’ to ‘wet rock’), hopefully enhancing the quality of our Word2Vec model.

Our trained Word2Vec model consists of a 300-dimensional embedding for every term in our corpus. However, we can recall from the previous step in this analysis that we only really care about the terms that are relevant descriptors of a wine’s sensory experience.

For our Pinot Noir, these were:

dry, flower, sagebrush, elegant, tarragon, pepper, tangy, cranberry, light_bodied

In the adjacent image, we can see the word embedding for each of these mapped descriptors.

Step 5: Weight each word embedding in the wine review with a TF-IDF weighting, and sum the word embeddings together

Now that we have a word embedding for each mapped descriptor, we need to think about how we can combine these into a single vector. Looking at our Pinot Noir example, ‘dry’ is a fairly common descriptor across all wine reviews. We want to weight that less than a rarer, more distinctive descriptor such as ‘sagebrush’. In addition, we want to take into consideration the total number of descriptors per review. If there are 20 descriptors in one review and five in another, each individual descriptor in the former review probably contributes less to the overall profile of the wine than in the latter. Term Frequency-Inverse Document Frequency (TF-IDF) takes both of these factors into consideration. TF-IDF looks at how many mapped descriptors are contained within a single review (TF), as well as at how often each mapped descriptor appears in the 180,000 wine reviews (IDF).

Multiplying each mapped descriptor vector by its TF-IDF weighting gives us our set of weighted mapped descriptor vectors. We can then sum these to obtain a single wine embedding for each wine review. For our Pinot Noir, this looks something like:"
An Overview of Computer Vision,"“If we want machines to think, we need to teach them to see.”

-Fei Fei Li, Director of Stanford AI Lab and Stanford Vision Lab

Computer vision is a field of artificial intelligence that trains computers to interpret and understand the visual world. Machines can accurately identify and locate objects then react to what they “see” using digital images from cameras, videos, and deep learning models.

Starting in the late 1950s and early 1960s, the goal of image analysis was to mimic human vision systems and to ask computers what they see. Prior to this, image analysis had been completed manually using x-rays, MPIs or hi-res space photography. Nasa’s map of the moon took the lead with digital image processing, but wasn’t fully accepted until 1969.

As computer vision evolved, programming algorithms were created to solve individual challenges. Machines became better at doing the job of vision recognition with repetition. Over the years, there has been a huge improvement of deep learning techniques and technology. We now have the ability to program supercomputers to train themselves, self-improve over time and provide capabilities to businesses as online applications.

I like to think of computer vision as working with millions of calculations in order to recognize patterns and to have the same accuracy as the human eye. Patterns can be seen physically or can be observed mathematically by applying algorithms.

The Breakdown of Computer Vision

Images are broken down into pixels, which are considered to be the elements of the picture or the smallest unit of information that make up the picture.

Computer vision is not just about converting a picture into pixels and then trying to make sense of what’s in the picture through those pixels. You have to understand the bigger picture of how to extract information from those pixels and interpret what they represent.

Neural networks and Deep Learning are Making Computer Vision More Capable of Replicating Human Vision"
Predictive Maintenance: detect Faults from Sensors with CNN,"Predictive Maintenance: detect Faults from Sensors with CNN

Photo by Bruce Warrington on Unsplash

In Machine Learning the topic of Predictive Maintenance is becoming more popular with the passage of time. The challenges are not easy and very heterogenous: it’s useful to have a good knowledge of the domain or to be in touch with people who know how the underlying system works. For these reasons when a data scientist engages himself in this new field of battle has to follow a linear and rational approach, keeping in mind that the easiest solutions are always the better ones.

In this article, we will take a look at a classification problem. We will apply a simple but very powerful model made with CNN in Keras and we will try to give a visual explanation of our results.

THE DATASET

I decided to take a dataset from the evergreen UCI repository (Condition monitoring of hydraulic systems).

The data set was experimentally obtained with a hydraulic test rig. This test rig consists of a primary working and a secondary cooling-filtration circuit which are connected via the oil tank. The system cyclically repeats constant load cycles (duration 60 seconds) and measures process values such as pressures, volume flows and temperatures while the condition of four hydraulic components (cooler, valve, pump and accumulator) is quantitatively varied.

We can image to have a hydraulic pipe system which cyclically receives impulse due to e.g. the transition of particular type of liquid in the pipeline. This phenomenon lasts 60 seconds and was measured by different sensors (Sensor Physical quantity Unit Sampling rate, PS1 Pressure bar, PS2 Pressure bar, PS3 Pressure bar, PS4 Pressure bar, PS5 Pressure bar, PS6 Pressure bar, EPS1 Motor power, FS1 Volume flow, FS2 Volume flow, TS1 Temperature, TS2 Temperature, TS3 Temperature, TS4 Temperature, VS1 Vibration, CE Cooling efficiency, CP Cooling power, SE Efficiency factor) with different Hz frequencies.

Our purpose is to predict the condition of four hydraulic components which compose the pipeline. These target condition values are annotated in the form of integer values (easy to encode) and say us if a particular component is…"
Tips for Effective Data Visualization,"Photo by Carlos Muza on Unsplash

Data visualization has a strong design element to it. Given the differences in domains, applications and audience it’s hard to put a structure around the best way to visualize your data. However, there definitely are wrong ways of doing it! I have come across multitude of such instances which were a driving force behind this post.

In this article I’d like to share some useful tips to help prevent blunders in your visuals.

1. Choose the right visual

This one may seem too obvious! But I have seen several people trying to demonstrate their artistic side for no reason..

Always remember that “form follows function” — purpose of a visual should be the starting point of its design

Ask yourself — are you trying to compare values, show trends, explore distribution or relationship between variables? Then choose appropriate visual(s) depending on the message you are trying to convey.

Consider the following charts. The underlying dataset contains product complaints/defects. We are trying to show products that need attention with a defective rate ppm (parts per million) A bar chart is a simple yet effective way to display this data. One of the downside of tree maps and packed bubble charts is that is asks the reader to compare area instead of height which is visually taxing.

Choosing the right visual based on it’s function

2. Trivial are many but vital are few (data points)

Do not merely slap a visual on the dataset. Analyze and convert your data into an information ‘nugget’ that the audience can grasp.

The figure alongside shows a time series chart of a process variable. Line A-A’ shows a point in time when both period and amplitude changed which was the onset of an issue that utlimately led to the sudden dip in its value after a few cycles. Which chart does a better job at uncovering this insight?

The chart on the top simply converts the data into a visual whereas the one at the bottom “conditions” it to provide an insight.

Let’s say that we want to show total annual spend by project ID. There are 41 unique categories. Chart on the left looks crowded with all categories cramped in it. A better way to display this is to show the top 5 categories and combine the remaining into ‘others’ bucket.

Suppressing the “noise” in the data

Also, beware that filtering out the ‘others’ category may exaggerate the area of the pie chart or change the percent of total calculations. This can be misleading! While displaying percentages, always make sure that they add up to a 100% or explain why and what’s being excluded.

3. Figures don’t lie, but lairs figure!

Visuals should reflect reality and not distort it. Formatting of the chart plays an important role as it sets up a frame of reference for the audience.

In the example below, the yield of a process increased from 56% to 67% over a period of 6 months. Chart to the left is trying to overstate the improvement by formatting the y-axis to start at 50%. The latter visual paints an accurate picture where y-axis starts off at 0 and also includes a goal line.

Figures don’t lie..liars figure!

4. Use color wisely

Use of color should be made to add more information or to highlight key data points in a visual. In all other cases, it’s redundant and distracting.

This article by Lisa Charlotte Rost has several pointers to consider while choosing a color scheme for your visuals. I would also recommend trying the Viz Palette tool developed by Susie Lu and Elijah Meeks.

5. How important are aesthetics as compared to functionality?

Visualization tools available today allow us to create most stunning and rich visuals in a few mouse clicks. However, over-doing the aesthetic elements may distract people from the key message of the visuals. One of the seven wastes in the Lean philosophy is ‘Over-processing’.

Any other tips that might prevent visualization blunders? Feel free to comment below!"
A Graph DB Case Study: Chinese Idiom Chains,"The Chinese Idiom Chains Game

This is a classical education game that helps pupils learn idioms. Seated in a round table, a pupil comes up with a random Chinese idiom (a Chinese idiom consists of 4 Chinese characters). The next pupil must quickly say another idiom whose first character matches the last character of the given idiom. When someone gets stuck, he must sing a song or perform a dance.

Now with the knowledge of graph DB, a pupil can stop worrying about losing the game and having to perform in front of the class!

Graph Design

The graph structure is as simple as you can imagine. We have one types of node and one types of directed edge. It’s tempting to create a node for each idiom, because an idiom is an entity, just like a user, a movie, a product. If an edge exists between two nodes, we just create an edge. For example, there is an edge pointing from 坚定不移 to 移花接木, because the character 移 connects the two idioms.

Naive design

Someone from a relational DB background might naturally come up with this design, because an edge is simply a match in a JOIN operation. Right?Unfortunately, this seemingly innocuous design turned out to be impractical. We have to exhaustively search for all edges and add them one at a time, resulting in quadratic time complexity.

We can do much better with clever modeling. What if we embed the idiom in edges, instead of…"
Understanding the types of data in a business/organization,"Understanding the types of data in a business/organization

Try to google it, and you will guarantee to find various sources each with their own versions (some said 3 types of data, some said 5 types, some even said 13 types). We make it easy for you by summarizing them and take your comprehending into the next level Rendy Dalimunthe · Follow Published in Towards Data Science · 4 min read · Jul 18, 2019 -- Listen Share

Before you start to rolling out your data management initiatives, be it Master Data Management, Enterprise Data Warehouse, Big Data Analytics or whatever it is, you need to start by understanding the very basic ingredient: the data. Only by thoroughly recognizing their characteristics, you will know the right way on how to treat each of them.

“Data is a precious thing and will last longer than the systems themselves” Tim Berners-Lee

So let’s get started!

Transactional Data

This type of data describes your core business activities. If you are a trading company, this may includes the data of your purchasing and selling activities. If you are a manufacturing company, this will be your production activities data. If you are a ride-hailing or cab company, this will the trip data. In a very basic organizational operations, the data related to the activities of hiring and firing employees can also be classified as transactional data. As a result, this kind of data has a very huge volume in comparison with the other types and usually created, stored, and maintained within the operational application such as ERP system.

Master Data

It consists of key information that make up the transactional data. For example, the trip data in a cab company may contain driver, passenger, route, and fare data. The driver, passenger, locations, and basic fare data are the master data. The driver data may consists the name of the driver and all of the associated information. So does the passenger data. Together, they make up the transactional data.

Master data usually contains places (addresses, postal-coded, cities, countries), parties (customers, suppliers, employees) and things (products, assets, items, etc.). It is application-specific, meaning that its uses are specific for the application with business process related to it, e.g: the employees master data is created, stored, and maintained within the HR application.

By now, you should get some grasp of understandings that master data is relatively constant. While the transaction data is created at a lightning speed, the master data is somehow constant. The trip data is created in any second but the list of the driver will remain the same unless there’s a new driver on-board or get kicked out.

Nowadays, processes within the organization are usually so inter-dependable, which means that one process conducted in one system is related to the process conducted in other system. They may use the same master data. If each system manage their own master data, potential duplication and inconsistencies may arise. For instance, a customer may be stored as Rendy in system A, but listed as Randy in system B, although Rendy and Randy is actually the same entity. But no need to worries, there’s a discipline to manage this kind of situation. It’s called Master Data Management.

Reference Data

Reference data is a subset of master data. It is usually a standardized data that governed by certain codification (e.g. list of Countries is governed by ISO 3166–1. There’s an easy way to differentiate reference data from master data. Always remember that reference data is way less volatile than master data. Let’s back again to our cab company. Tomorrow, the day after tomorrow, or next week, the list of driver may change whenever there’s a new person onboard or kicked out. But I can guarantee you that the list of countries will remain the same even 2 decades from now, unless there’s a little land that declare its independence.

Reporting Data

It’s an aggregated data compile for the purpose of analytic and reporting. This data consist of transactional, master, and reference data. For example: Trip data (transaction + master) on the 13th day of July in Greater London region (reference). Reporting data is very strategic and usually being produced as ingredient of decision making process.

Metadata

It’s a data about data. Sounds confusing? Indeed. It’s the type of data that got me dizzy in the first time I enter the data management field. Thankfully, this beautiful picture make it easy for me to comprehend what metadata actually is.

Data & its metadata

If I ask you a question: what is the color of the cat? Immediately by just looking at the data you can confidently answer my question. It’s grey. But what if I come up with another questions: when and where this picture be taken? Chances are high that you will not be able to give me the right answer by only looking at the data. And here is where the metadata come to rescue. It gives you the complete information about the data including when and where it was taken.

So metadata is giving you the answer to any question that you cannot answer by just looking at the data. That’s why it said: data about data."
A Minimalist End-to-End Scrapy Tutorial (Part II),"A Minimalist End-to-End Scrapy Tutorial (Part II)

Photo by Igor Son on Unsplash

Part I, Part II, Part III, Part IV, Part V

In Part I, you learned how to setup Scrapy project and write a basic spider to extract web pages by following page navigation links. However, the extracted data are merely displayed to the console. In Part II, I will introduce the concepts of Item and ItemLoader and explain why you should use them to store the extracted data.

Let’s first look at Scrapy Architecture:

As you can see in step 7, and 8, Scrapy is designed around the concept of Item, i.e., the spider will parse the extracted data into Items and then the Items will go through Item Pipelines for further processing. I summarize some key reasons to use Item:

Scrapy is designed around Item and expect Items as outputs from the spider — you will see in Part IV that when you deploy the project to ScrapingHub or similar services, there are default UIs for you to browse Items and related statistics. Items clearly define the common output data format in a separate file, which enables you to quickly check what structured data you are collecting and prompts exceptions when you mistakenly create inconsistent data, such as by mis-spelling field names in your code — this happens more often than you think :). You can add pre/post processing to each Item field (via ItemLoader), such as trimming spaces, removing special characters, etc., and separate this processing code from the main spider logic to keep your code structured and clean. In Part III, you will learn how to add different item pipelines to do things like detecting duplicate items and saving items to the database.

At the end of Part I, our spider yields the following data:

yield {

'text': quote.css('.text::text').get(),

'author': quote.css('.author::text').get(),

'tags': quote.css('.tag::text').getall(),

}

and

yield {

'author_name': response.css('.author-title::text').get(),

'author_birthday': response.css('.author-born-date::text').get(),

'author_bornlocation': response.css('.author-born-location::text').get(),

'author_bio': response.css('.author-description::text').get(),

}

You may notice that author and author_name are the same thing (one is from the quote page, one is from the corresponding individual author page). So, we actually extract 6 pieces of data, i.e., quote text, tags, author name, birthday, born location, and bio. Now, let’s define the Item to hold those data.

Open the auto-generated items.py file and update its content as follows:

We just define one Scrapy item named “QuoteItem” with 6 fields to store the extracted data. Here, if you designed a relational database before, you may ask: should I have two items QuoteItem and AuthorItem to better represent the data logically? The answer is yes you could but not recommended in this case because items are returned by Scrapy in an asynchronous way and you will have additional logic added to match the quote item with its corresponding item — it’s much easier to put the related quote and author in one item in this case.

Now, you can put the extracted data into Item in the spider file like:

from tutorial.items import QuoteItem

...

quote_item = QuoteItem()

... for quote in quotes:

quote_item['quote_content'] = quote.css('.text::text').get()

quote_item['tags'] = quote.css('.tag::text').getall()

Or a preferred way is to use ItemLoader as follows:

from scrapy.loader import ItemLoader

from tutorial.items import QuoteItem

...



for quote in quotes:

loader = ItemLoader(item=QuoteItem(), selector=quote)

loader.add_css('quote_content', '.text::text')

loader.add_css('tags', '.tag::text')

quote_item = loader.load_item()

Hmm, the code with ItemLoader seems more complicated — why do you want to do that? The quick answer is: the raw data you get from the css selector may need to be further parsed. For example, the extracted quote_content has quotation marks in Unicode that needs to be removed.

The birthday is a string and needs to be parsed into Python date format:

The born location has “in” in the extracted string that needs to be removed:

ItemLoader enables pre/post processing functions to be nicely specified away from the spider code and each field of the Item can have different sets of pre/post processing functions for better code reuse.

For example, we can create a function to remove the Unicode quotation marks aforementioned as follows:

MapCompose enables us to apply multiple processing functions to a field (we only have one in this example). ItemLoader returns a list, such as [‘death’, ‘life’] for tags. For author name, although there is one value a list is returned such as [‘Jimi Hendrix’] , TakeFirst processor takes the first value of the list. After adding additional processors, items.py looks like:

The key issue right now is that we load two fields quote_content and tags from the quote page and then issue another request to get the corresponding author page to load author_name , author_birthday , author_bornlocation , and bio . To do this, we need to pass the item quote_item from one page to another as metadata as follows:

yield response.follow(author_url, self.parse_author, meta={'quote_item': quote_item})

And in the author parsing function, you can get the item:

def parse_author(self, response):

quote_item = response.meta['quote_item']

Now, after adding Item and ItemLoader, our spider file looks like:

Run the spider scrapy crawl quotes in the console Scrapy stats, you can find the total number of items extracted:

2019-09-11 09:49:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:

... 'downloader/request_count': 111,

... 'item_scraped_count': 50,

Congrats! you have finished Part II of this tutorial.

Part I, Part II, Part III, Part IV, Part V"
Math for Data Science: Collaborative Filtering on Utility Matrices,"Member-only story Math for Data Science: Collaborative Filtering on Utility Matrices

I highly recommend checking out one of my other posts as an introduction to recommendation engines:

Recap: What is Collaborative Filtering?

Collaborative filtering is a type of recommendation engine that uses both user and item data. More specifically, ratings from individual users on individual items. This way, items are recommended based on the ratings from other users, thus, collaborative. This data can be represented in a utility matrix, with one axis being users and one axis being items. The goal of collaborative filtering recommendation engines is to fill in the gaps in a utility matrix since not every user has rated every item, and then output the top-rated, previously-unrated items as recommendations.

A simple utility matrix, with 4 users (columns) and 4 items (rows).

There are three main techniques to fill in the utility matrix: user-user, item-item and SVD. We’ll run through each of these using our simple utility matrix above to try and predict what User 1 would rate Item 3.

User to User

There are two main steps in computing the missing value of our utility matrix:

Compute the similarity between U1 and all other users Compute U1’s rating for I3 by taking an average of other users’ I3 ratings, weighting each user’s rating by the user’s similarity to U1"
Use Cython to get more than 30X speedup on your Python code,"Cython will give your Python code super-car speed

Want to be inspired? Come join my Super Quotes newsletter. 😎

Python is a community favourite programming language! It’s by far one of the easiest to use as code is written in an intuitive, human-readable way.

Yet you’ll often hear the same complaint about Python over and over again, especially from the C code gurus out there: Python is slow.

And they’re not wrong.

Relative to many other programming languages, Python is slow. Benchmark game has some solid benchmarks for comparing the speed of various programming languages on different tasks.

There’s a couple of different ways to speed things up that I’ve written about before:

(1) Use multi-processing libraries to use all the CPU cores

(2) If you’re using Numpy, Pandas, or Scikit-Learn, use Rapids to accelerate the processing on GPU.

Those are great if what you’re doing can, in fact, be parallelized, such as data pre-processing or matrix operations.

But what if your code is pure Python? What if you have a big for-loop that you just have to use and can’t put into a matrix because the data has to be processed in sequence? Is there a way to speedup Python itself?

That’s where Cython comes in to speed up our raw Python code.

What is Cython?

At its core, Cython is an intermediate step between Python and C/C++. It allows you to write pure Python code with some minor modifications, which is then translated directly into C code.

The only adjustment you make to your Python code is adding type information to every variable. Normally, we might declare a variable in Python like this:

x = 0.5

With Cython, we’re going to add a type to that variable:

cdef float x = 0.5

This tells Cython that our variable is floating point, just like we would do in C. With pure Python, the variable’s type is determined on the fly. The explicit declaration of the type in Cython is what makes the conversion to C possible, since explicit type declarations are required+."
Innocent Interpretations for Some Suspicious Statistics; General Election Data Exploration. (part 1),"Innocent Interpretations for Some Suspicious Statistics; General Election Data Exploration. (part 1)

Looking at the 2019 elections in Israel. Some results appear weird, sure, but is there evidence of actual malfeasance, or is there a simpler explanation? Avishalom Shalit · Follow Published in Towards Data Science · 5 min read · Apr 22, 2019 -- Listen Share

My favourite analogy in statistics(made by Cassie Kozyrkov) is the analogy to the English/American legal system.

The “Null Hypothesis” being the presumption of innocence (leading to acquittal) and the rejection of that can only be due to the presentation of evidence of guilt “beyond a reasonable doubt.” the P value we select is that level of beyond a reasonable doubt. (which is different depending on the issue at stake)

Our story begins with a post on social media. The following “anomaly” in a specific polling station was making the rounds.

All the parties got votes that were “Very Round Numbers”™

These are indeed suspicious looking. OK, let’s dive in.

TL;DR and disclaimer

In this multipart series, in some cases I will be able to present simple, innocent mathematical explanations for ostensibly suspicious results. In other cases I won’t. Note that there are more than 10,000 ballot boxes, so even a rare numerical anomaly is expected to crop up several times.

Getting Started

I started looking for other anomalies. Maybe round numbers are too obvious, but what are the odds that a party will get exactly a third of the votes? Half?

Histogram Time

True Data: Here is a histogram of the vote fraction of a specific party in all polling stations; Note the most common values are reduced small rationals: 1/2, 1/3, 3/7, 1/4, 2/5, 1/5, 4/9. weird, right?

Foul play?!? Did we “get them”?

Before I answer, can you think of an explanation of why exactly 1/2 is so popular? [Take a minute]

[beat]

Well, small rational fractions have a lot more going for them. You can get EXACTLY 1/3 by having 100 out of 300, 101 of 303, etc. but to get 200/601, well, you must have a very specific vote tally, and a very specific vote count for that party. So those values aren’t as common.

Is this enough to explain the oddity?

Here are the results of a simple model (we pick random numbers for the total number of votes in the polling station, and number of votes for a specific party, given that party’s national total)

The small rationals making an appearance again, even in a random model. (described in appendix B)

Non damning histogram

So, now that we have a simple explanation for these rational numbers in the data, what would be a good way to look at our data?

How about if we let the histogram do what it wanted to from the start, which was to bin the data?If we take 100 bins, we can see if bin 50 is a lot more likely than bin 51 or 49 and thus judge.

Binning makes the rational anomalies disappear.

Well, it looks like we don’t have enough evidence to convict. In the world where the Null hypothesis is true (no fudging) the evidence presented is actually quite reasonable and not at all surprising.

That is not to say that there weren’t instances of a single tally that was fudged and set to exactly 1/2 of the votes for a specific party, it is just not the conspiracy that it appeared to have been (with dozens of polling stations agreeing on the same ratio.)

Next up

This post will be first in a series.

In the next weeks I will explore other anomalies.

I’ll revisit the anomaly that started it all, and I’ll explore some weird rational relationships between parties. (i.e. one party getting 1/2 or twice that of another party)

If you’ve noticed some other weird lines you’d like me to look at, comment and I’ll have a look.

Get Ready

Whet your appetite on this row of vote tallies (from one polling station).

Note the recurring rational relationships. e.g. 25–25–75–150 ; 27–135 it also has the numbers 21,23,24,25,25,26,27. Suspicious, sure. Guilty?

Is that weird enough to be suspect or just a coincidence? find out next week.

Appendix A — code to load data.

The results of the tallies per polling station are in here https://media21.bechirot.gov.il/files/expb.csv

Here is some boilerplate python for loading that file (11K rows), and renaming the columns to English, So you can see for yourself, and find some more suspicious results.

Also includes the code that generates the histograms above.

Appendix B simplistic model

Fitting a normal distribution to the number of valid votes per polling station (centred at 400 with sigma=100, ignoring the peak at 800)

and a poisson distribution for the number of votes per party given the national party fractions, picking lambda from a distribution that matches the national vote fraction per ballot for that party.

This is a bit simplistic I know, e.g. a PCA accounting for vote trends for different parties in different municipalities would be better. But this is good enough for now, it produces a viable reason for all those small rational numbers."
Take your Python Skills to the Next Level With Fluent Python,"Photo by Bonnie Kittle on Unsplash

Member-only story Take your Python Skills to the Next Level With Fluent Python

You’ve been programming in Python for a while, and although you know your way around dicts, lists, tuples, sets, functions, and classes, you have a feeling your Python knowledge is not where it should be. You have heard about “pythonic” code and yours falls short. You’re an intermediate Python programmer. You want to move to advanced.

This intermediate zone is a common place for data scientists to sit. So many of us are self-taught programmers using our programming languages as a means to a data analysis end. As a result, we are less likely to take a deep dive into our programming language and we’re less exposed to the best practices of software engineering.

If this is where you are in your Python journey, then I highly recommend you take a look at the book “Fluent Python” by Luciano Ramalho. As an intermediate Python programmer, I’m the target audience for this book and it exceeded my expectations.

The book consists of 21 chapters divided into five broad sections:"
Is Random Forest better than Logistic Regression? (a comparison),"Introduction:

Random Forests are another way to extract information from a set of data. The appeals of this type of model are:

It emphasizes feature selection — weighs certain features as more important than others.

It does not assume that the model has a linear relationship — like regression models do.

It utilizes ensemble learning. If we were to use just 1 decision tree, we wouldn’t be using ensemble learning. A random forest takes random samples, forms many decision trees, and then averages out the leaf nodes to get a clearer model.

In this analysis we will classify the data with random forest, compare the results with logistic regression, and discuss the differences. Take a look at the previous logistic regression analysis to see what we‘ll be comparing it to.

Table of Contents:

1. Data Understanding (Summary)

2. Data Exploration/Visualization(Summary)

4. Building the Model

5. Testing the Model

6. Conclusions

Data Background:

We have a sample of 255 patients and would like to measure the relationship between 4 proteins levels and cancer growth.

We know:

The concentration of each protein measured per patient.

Whether or not each patient has been diagnosed with cancer (0 = no cancer; 1= cancer).

Our goal is: To predict whether future patients have cancer by extracting information from the relationship between protein levels and cancer in our sample.

The 4 proteins we’ll be looking at:

Alpha-fetoprotein (AFP)

Carcinoembryonic antigen (CEA)

Cancer Antigen 125 (CA125)"
An Easy Introduction to SQL for Data Scientists,"Want to be inspired? Come join my Super Quotes newsletter. 😎

SQL (Structured Query Language) is a standardised programming language designed for data storage and management. It allows one to create, parse, and manipulate data fast and easy.

With the AI-hype of recent years, technology companies serving all kinds of industries have been forced to become more data driven. When a company that serves thousands of customers is data driven, they’ll need a way to store and frequently access data on the order of millions or even billions of data points.

That’s where SQL comes in.

SQL is popular because it’s both fast and easy to understand. It’s designed to be read and written in a similar way to the English language. When an SQL query is used to retrieve data, that data is not copied anywhere, but instead accessed directly where it’s stored making the process much faster than other approaches.

This tutorial will teach you the basics of SQL including:

Creating database tables

Populating the database tables with real data

Retrieving your data for usage in a Data Science or Machine Learning task

Let’s jump right into it!

Installing MySQL

The first thing we’ll do is actually install our SQL server! That’ll give us a workbench to start playing around with databases and SQL queries.

To install a MySQL server, you can run the following command from your terminal:

sudo apt-get install mysql-server

Now we’ll start our MySQL server. This is similar to how we start Python in the terminal by just typing out “python”. The only difference here is that it’s convenient to give our server root privileges so we’ll have flexible access to everything.

sudo mysql -u root -p

Great! Now our mysql server is running and we can start issuing MySQL commands.

A couple of things to keep in mind before we move forward:"
Attention in Deep Networks with Keras,"Attention in Deep Networks with Keras

Courtesy of Pixabay

This story introduces you to a Github repository which contains an atomic up-to-date Attention layer implemented using Keras backend operations. Available at attention_keras .

To visit my previous articles in this series use the following letters.

A B C D* E F G H I J K L* M N O P Q R S T U V W X Y Z

[🔈🔥 Latest article 🔥🔈]: M — Matrix factorization

Why Keras?

With the unveiling of TensorFlow 2.0 it is hard to ignore the conspicuous attention (no pun intended!) given to Keras. There was greater focus on advocating Keras for implementing deep networks. Keras in TensorFlow 2.0 will come with three powerful APIs for implementing deep networks.

Sequential API — This is the simplest API where you first call model = Sequential() and keep adding layers, e.g. model.add(Dense(...)) .

and keep adding layers, e.g. . Functional API — Advance API where you can create custom models with arbitrary input/outputs. Defining a model needs to be done bit carefully as there’s lot to be done on user’s end. Model can be defined using model = Model(inputs=[...], outputs=[...]) .

. Subclassing API — Another advance API where you define a Model as a Python class. Here you define the forward pass of the model in the class and Keras automatically compute the backward pass. Then this model can be used normally as you would use any Keras model.

For more information, get first hand information from TensorFlow team. However remember that while choosing advance APIs give more “wiggle room” for implementing complex models, they also increase the chances of blunders and various rabbit holes.

Why this post?

Recently I was looking for a Keras based attention layer implementation or library for a project I was doing. I grappled with several repos out there that already has implemented attention. However my efforts were in vain, trying to get them to work with later TF versions. Due to several reasons:"
Millennials’ Favorite Fruit: Forecasting Avocado Prices with ARIMA Models,"So, what can we learn from our models? Well, first of all, our average price models seem to show us that avocado prices reach their peak each year in the Autumn. In October 2017, conventional avocados peaked at $1.65 and our model forecasts that Autumn spikes will continue to occur through 2018 and 2019. However, the model predicts that conventional avocado prices will trend downwards over the following two years. Between 2015 and 2018, the average conventional avocado price was $1.09. Between 2018 and 2020, the forecasted average price is $1.01, a decrease of about 7 percent.

In stark contrast, the model forecasts that organic avocado prices will experience moderate growth, bolstered by less volatility. The forecasted average price between 2018 and 2020 is $1.84, compared to $1.55 between 2015 and 2018. The model certainly demonstrated a moderate upward trend for organic avocado prices.

However, the growth in average prices is also due in part to decreased volatility, with the standard deviation of prices down to $0.16 for the forecasted period from the $0.36 standard deviation experienced between 2015 and 2018.

Both conventional and organic avocados are forecasted to experience strong growth in terms of units sold over the 2018–2020 period. Conventional avocado production is forecasted to grow over 40 percent from 33.7 million to 47.6 million avocados sold each week on average.

Organic avocados are forecasted to experience even more meteoric growth. A consistent growth trend between 2015 and 2018 is predicted to continue through 2019 with a whopping 74 percent growth between 2018 and 2020. Between 2015 and 2018, suppliers sold about 970,000 organic avocados weekly. Between 2018 and 2020, the number is forecasted to jump to 1.68 million organic avocados.

Discussion

I think these models are really exciting. First, in terms of pattern recognition, it is interesting to see the models picking up on seasonal trends. The price models captured the Autumn price hikes and the volume models captured the spikes that occur in avocado volume each February.

Additionally, the diagnostics look pretty good. Mean Absolute Percentage Error, or MAPE, measures the error between the forecast and observed values, so a lower value indicates a better fitting model. The highest MAPE amongst the four models is for the organic volume model, at around 6 percent, indicating the model is about 94 percent accurate. I am very happy with this level of fit and I am confident in all of the models. At least, I am as confident as an forecasting newbie could be.

If I had to editorialize (which I will gladly do) I would say that the growth forecast for organic avocado volume is somewhat bullish. I am not sure how realistic 74 percent growth is over only two years, but testing several models yielded similar results.

Alternatively, I think that the conventional avocado price model is somewhat bearish. With the Autumn price hike increasing each year since 2015, I was surprised that the model forecasted overall price decreases. However, if it ends up being the case through 2020, you will not see me complaining at brunch."
Ideas: Design Methodologies for Data Sprints,"Photo by William Iven on Unsplash

I recently spent four days at a research lab with a group of data scientists and a few behavior scientists diving into a large, messy data set to see if we might find insights related to group dynamics. It was framed as an initial dive to look around, get an initial assessment of potential for useful research insights, and then build a proposal for two to three months of work based on what was found that week.

I attended theoretically in the role of a behavioral scientist, but since I’m also obsessed with creative problem solving processes, or design thinking (and I’m opinionated as hell), the following are some reflections on the process we used and ideas for what’s next.

A lot of process methodologies occupy the space in the overlap between software development, data analytics, and creative problem solving disciplines: UX, design thinking, Think Wrong, Google Design Sprint, Agile, Kanban, emerging data analysis methods for group sprints, etc. It’s great, because if you’re curious you can acquire a pretty big toolset to bring together groups of people to crash and get creative on all different kinds of problems for widely varying lengths in time — a two hour meeting to an 18 month project. The challenge, or mastery of practice as a facilitator, is to learn not just the buzz words of the methodologies or the exercises themselves, but the underlying science behind group dynamics, creativity, psychology, and neuroscience.

During this workshop, we had a two distinct types of professionals in the room — social science and data science, each with complementary skillsets within those groupings. They were all brand new to the data set, and it was pretty messy when we first looked at it. The direction from the sprint’s sponsor was extremely broad — what general insights about the group dynamics of these co-workers can be identified in a three day sprint.

Unfortunately, we didn’t clearly identify the direction for what kind of insights would be most useful until the afternoon of the second day. At that point, the social science group separated to brainstorm a range of different questions that could be asked about group dynamics, and which theories and research questions were emerging as especially intriguing for the field. Besides ensuring the whole team had a clear idea of the direction for the workshop and what question themes regarding group dynamics would be most useful (ie. performance of small work teams, changes in group composition over time, looking for patterns in the kinds of people who grouped together, etc) we also needed design constraints, but didn’t identify them. Intro design thinking uses a generic Venn diagram to illustrate the criteria a design must successfully meet in order to be green lit for funding/ effort to develop a high-fidelity solution. Companies develop their own specific criteria that usually fall somewhere into these buckets. Without identifying our own, the team relied on the expertise and gut instinct of the people gathered in the room. Not bad — everyone was incredibly smart and knew their field well; but not great if you want to maximize resources toward understanding the most important/ impactful questions.

via Legacy Innova

Our facilitator took the route of using UX and design thinking exercises — personas and needs statements — over the first two days. They were meant to get the group to identify specific questions that could then be voted on by the whole team. On the third day, small groups broke off, each taking one or two questions that might be asked of the data. Normally personas and needs statements are an incredibly reliable method; they work in all kinds of situations. But during this sprint many people found them to be frustrating when they had been told to look for insights about group dynamics, not individual personas.

The first challenge with persona methods is that they are designed to be used with deliberative modes of thinking, or System 2 (from Thinking Fast and Slow) that take a diverse, seemingly disconnected array of information — most often qualitative ethnography — and synthesize it to come to some detailed statement of need. With no prior set of information about the people in the data set, we were left looking at spreadsheets of raw data and making up cartoon characters as personas. The whole team went along gamely and tried to make use of it, but the time could have been better spent.

The second challenge regarding the selection of persona methods is that they are meant to be used during design processes where the outcome is some product or service serving archetypal individuals. There is a relationship between problem for the persona, and solution — solving a need of the persona. For this workshop, our team was not meant to produce a product or service, the insights were the deliverables. Rather than personas, we should have been exploring and mapping the space of the complex concept of group dynamics.

The somewhat awkward use of some design thinking/ UX methods and rejection of others for this workshop was totally understandable. As design sprints have become a popular, justifiably so, tool for businesses to bring multi-disciplinary teams together and develop novel concepts and products, the broad space of design and software development methodologies have edged into unfamiliar territory — data analytics. While I feel experienced to comment on design methodologies, data science is new territory for me. From the literature review I’ve done, and the limited commercial experience with analytics teams, it seems that they are themselves wrestling with what methodologies serve group data sprints best. Analysis has traditionally been done solo, or sitting side by side, not requiring a more formal group process. Three forces are pushing data scientists to develop group analytics methodologies: the data science field has bifurcated with an ever increasing number of specializations and tools, the sheer amount of data has exponentially increased, and data science is spreading to disciplines that had previously done their analysis with almost ubiquitous qualitative methods and tools — so multi-disciplinary teams were becoming the norm. In the last five to seven years, there have been several academic articles and blogs describing group data analysis processes.

My PhD research involves developing a co-design process to analyze a complex concept — wellbeing, using 3D data visualization software. I’ve been thinking about how we bring groups together to investigate and communicate about complex concepts, the very early work of group and individual need finding in order to eventually do policy, service, and product design. Trying to grasp complex systems and complex concepts is a totally different beast than even synthesizing nuanced psychological and social needs from personas. Oral cultures have very different approaches to thinking about and discussing complexity, and I believe our western dominated, literate, tech cultures have a lot to learn from them about how to analyze complexity.

I would like to offer these ideas for multi-disciplinary groups working to identify insights in large, messy data sets. I’m curious what others are doing, and would love some feedback.

First, here is the framework I use to organize design sprints organized around addressing particular needs of particular archetypal personas. Four quadrants correspond to the four phases usually associated with design thinking: interviewing and observing users, synthesis of needs, ideation, and prototyping and testing. I find that it helps me as a facilitator to understand the modes of thinking I’m trying to induce my participants to exercise for different methods. It’s also helpful to pair the mode of thinking with a clear delineation between problem and solution. Trying to brainstorm about both the problem and solution at the same time is a recipe for a chaotic descent of the group way, way off track. This 2x2 framework is most useful when designing exercises for workshops that revolve around understanding a user’s need and developing some thing to solve it — a software product interface, a household object, a social service.

Complexity, ie group dynamics, requires a different framework for thinking about which methods to select and which modes of thinking to use during a sprint. In our sprint, the work revolved around getting to an insight, refining a raw data set and teasing apart a complex topic. Using a 2x2 matrix again, complex concept and raw data set replace problem and solution on one of the axes. For the second axis, we might use different flare and focus modes than ones that serve thinking about defined needs of individual archetypes, two examples would be inductive/ deductive and declarative/ modal. Dr. Mihnea Moldoveanu of Rottman Business School, led a decade long research effort to understand the modes of thinking, or adaptive intelligence, that transcends discipline boundaries. His admonishment is essentially to consciously select the mode of thinking, or pattern of thinking modes, best fit for the problem and group of people at hand.

A hypothetical set of methods for another data sprint might go: 1) map the scales, layers, and theories related to the complex concept in question, 2) down select to the most important set of themes or meta-questions to ask of the data set, 3) exploratory analysis of the raw data set, 4) develop a schema and begin cleaning the data and organizing it into an easy to access data frame, 5) break up into small groups to each tackle a theme or meta-question and use alternating flare and focus modes of thinking like inductive/ deductive and declarative/ modal to iteratively analyze the data, develop hypothetical insights, and refine them with more data analysis.

This space of methodologies — especially sprint methodologies — to ask complex questions of big data sets using multidisciplinary teams is really exciting. So many of the most interesting questions facing our policy-makers, scientists, and researchers sit on huge piles of data at the intersection of fields that traditionally haven’t had to find a common language to work together. Facilitators, data scientists, and design practitioners might develop new theories on how best best to organize teams to more quickly and efficiently divine insights from large, chaotic data sets. Inevitably, someone will try to come along and commoditize it with pretty graphic icons and simplified descriptions. But right now it’s a fun, collaborative space of community exploration as we figure out how to bring more disciplines and professions together with data scientists and raw data."
Machine Learning Project 17 — Compare Classification Algorithms,"In the past 7 projects, we implemented the same project using different classification algorithms namely — “Logistic Regression”, “KNN”, “SVM”, “Kernel SVM”, “Naive Bayes”, “Decision Tree” and “Random Forest”.

The reason I wrote a separate article for each is to understand the intuition behind each algorithm.

#100DaysOfMLCode #100ProjectsInML

In a real scenario, when we are given a problem, we cannot predict which algorithm will perform best. Obviously from the problem, we can tell whether we need to apply Regression or Classification algorithm. But it is difficult to know which Regression or Classification algorithm to apply beforehand. It is only through trial and error and checking the performance metrics, we can narrow down and pick certain algorithms.

Today, I will show you how to compare different classification algorithms and pick the best ones. Rather than implementing the entire project using an algorithm and then finding out that the performance is not good, we will first check the performance of a bunch of algorithms and then decide which one to use to implement the project.

Let’s get started.

Project Objective

We are going to use the same dataset we used in Project 10. Our objective is to evaluate several classification algorithms and pick the best ones based on accuracy.

The sample rows are shown below. The full dataset can be accessed here."
Understanding Genetic Algorithms,"DATA STRUCTURES & ALGORITHMS

Understanding Genetic Algorithms

The USS Intrepid in New York City. Photo: Author

A genetic algorithm is a prime example of technology imitating nature to solve complex problems, in this case, by adopting the concept of natural selection in an evolutionary algorithm. Genetic algorithms, introduced in 1960 by John Holland,¹ extend Alan Turing’s concept of a “learning machine”² and are best-suited for solving optimization problems such as the traveling salesman.³

To intuitively understand the practical implementation and fundamental requirements for employing genetic algorithms, we can set up a toy problem and solve the board of the classic guessing game, Battleship, first released by Milton Bradley in 1967. But rather than calling a sequence of individual shots, let’s ask our genetic algorithm to make a series of guesses of the entire board.

Setting Up the Board

Genetic algorithms can be applied to problems whose solutions can be expressed as genetic representations, which are simply arrays of ones and zeros. Each binary element is called a gene, while an array of multiple genes is referred to as a chromosome. The optimal solution of a given problem is the chromosome that results in the best fitness score of a performance metric.

A Battleship board is composed of a 10 x 10 grid, upon which we can randomly place five ships of varying length. Our fleet includes a carrier (5), a battleship (4), a cruiser (3), a submarine (3) and a destroyer (2). We can express the board as a binary representation simply by denoting the squares occupied by our ships as ones and the unoccupied squares as zeros.

We can randomly position each of our ships by following a few simple steps:"
Databricks: How to Save Data Frames as CSV Files on Your Local Computer,"Photo credit to Mika Baumeister from Unsplash

When I work on Python projects dealing with large datasets, I usually use Spyder. The environment of Spyder is very simple; I can browse through working directories, maintain large code bases and review data frames I create. However, if I don’t subset the large data, I constantly face memory issues and struggle with very long computational time. For this reason, I occasionally use Databricks. Databricks is a Microsoft Azure platform where you can easily parse large amounts of data into “notebooks” and perform Apache Spark-based analytics.

If you want to work with data frames and run models using pyspark, you can easily refer to Databricks’ website for more information. However, while working on Databricks, I noticed that saving files in CSV, which is supposed to be quite easy, is not very straightforward. In the following section, I would like to share how you can save data frames from Databricks into CSV format on your local computer with no hassles.

1. Explore the Databricks File System (DBFS)

From Azure Databricks home, you can go to “Upload Data” (under Common Tasks)→ “DBFS” → “FileStore”.

DBFS FileStore is where you create folders and save your data frames into CSV format. By default, FileStore has three folders: import-stage, plots, and tables.

2. Save a data frame into CSV in FileStore

Sample.coalesce(1).write.format(“com.databricks.spark.csv”).option(“header”, “true”).save(“dbfs:/FileStore/df/Sample.csv”)

Using the above code on the notebook, I created a folder “df” and saved a data frame “Sample” into CSV. It is important to use coalesce(1) since it saves the data frame as a whole. At the end of this article, I will also demonstrate what happens when you don’t include coalesce(1) in the code."
Learn R Shiny through studying the rise of ocean plastics— Part one,"Learn R Shiny through studying the rise of ocean plastics— Part one

In this series of articles, learn how to build a web app in R Shiny using just-released data on ocean plastics Keith McNulty · Follow Published in Towards Data Science · 8 min read · Apr 22, 2019 -- Share

In a very recent article in the journal Nature, a team of marine scientists analyzed data that had been captured in devices called Continuous Plankton Recorders (CPRs) since the 1930s. CPRs are towed by vessels during their routine journeys and are designed to capture plankton for research purposes.

But beginning in 1957, CPRs started to accidentally capture another piece of data — one which we didn’t know we would find useful until recently — because in that year, a CPR was found to be contaminated with a man-made plastic object for the first time.

For their journal article, the team of scientists painstakingly put together a dataset of all such incidents of man-made plastics being found in CPRs in the North Atlantic, Arctic and Northern European seas from the first recorded incident in 1957 through to 2016. They also gave the public access to that dataset — a total of 208 incidents of various types of plastic ranging from fishing nets to plastic bags.

As I looked at the dataset here, I realized that it is a great dataset to help teach someone how to create a data exploration app using R Shiny, and this is what I will do in this series of articles. The dataset is limited in size so easy to work with, but contains a wide range of information including dates, text, and geographic co-ordinate data.

To follow this series, you need some basic experience in data manipulation in R, including using RStudio as a coding environment. Some previous experience using RMarkdown, Shiny, and simple plotting in ggplot2 is helpful but not essential. You will need to install the following packages before you start: shiny , RMarkdown , ggplot2 , tidyverse and leaflet .

In this series you will learn how to:"
Histograms in Image Processing with skimage-Python,"Visualizations are always been an efficient way to represent and explain many statistical details. In image processing histograms are used to depict many aspects regarding the image we are working with. Such as,

Exposure

Contrast

Dynamic Range

Saturation

and many more. By visualizing the histogram we can improve the visual presence of an image and also we can find out what type of image processing could have been applied by comparing the histograms of an image.

What is a Histogram?

Images are stored as pixels values, each pixel value represents a color intensity value. Histograms are frequency distribution of these intensity values that occur in an image.

h(i) = the number of pixels in I(image) with the intensity value i

For example, if i = 0, the h(0) is the number of pixels with a value of 0.

Creating Histogram of an Image with skimage

Grayscale Image

from skimage import io

import matplotlib.pyplot as plt image = io.imread('~/Desktop/Lenna_gray.png') ax = plt.hist(image.ravel(), bins = 256)

plt.show() Output: Figure-1

Figure-1

In the above code, we have loaded the grayscale image of Lenna and generated its histogram using matplotlib. Since the image is stored in the form of a 2D ordered matrix we converted it to a 1D array using the ravel() method.

Color Image

In color images, we have 3 color channels representing RGB. In Combined Color Histogram the intensity count is the…"
Where to Coffee Like an Istanbul Local,"In this post, I will outline the steps I took to discover similar and dissimilar coffee-neighborhoods in Istanbul locals’ favorite districts.

As part of my IBM data science course project, I was made to come up with a problem and find a solution to it by gathering, exploring and analyzing location data. Being a lover of Istanbul and coffee, I decided to come up with something that strides the two subjects.

Istanbul is one of the biggest and most populous cities in the world, the only city that exists on two continents. Both parts of the city are divided by the Bosphorous strait. Two Districts loved by Istanbul residents are Beşiktaş (be-shik-tash) and Kadıköy (ka-di-koy) on the European and Asian side respectively. While these districts have a lot in common, they have their fair share of differences as well, the surge of coffee shops for one. In fact, according to Foursquare, 8 of 15 best coffee shops in Istanbul are located in Beşiktaş and Kadıköy.

There is a fierce debate among residents about the neighborhood to best enjoy a cup of coffee. This report will address the issue by providing insights drawn from data. This study will be of interest to both visitors of Istanbul and locals who yet to discover the hidden similarities between the two most sought after neighborhoods. The report will help readers to:

Be more familiar with the discussed neighborhoods Understand the relationship between coffee shops and other neighborhood attributes Discover the similarities between neighborhoods in terms of coffee shops and other attributes Be able to make better-informed decisions about where to coffee in Istanbul like a resident

The neighborhoods that will be examined are shown on the map with the red markers."
Saving and loading Pytorch models in Kaggle,"Saving and loading Pytorch models in Kaggle

You have heard about Kaggle being awesome. You also are striving to improve your data science skills and are getting started with deep learning or just getting started on Kaggle.

Just as you think you are getting the grasp of training your deep neural network on Kaggle, you get stuck.

So what’s the problem?

So you’ve learnt you can save Pytorch models (strictly speaking, the state dictionary) and load them later at your convenience. You’ve trained your model on Kaggle and saved it. When you need to access the saved model, you just can’t find it and this might force you to start all over again. 😓😓😓

We all know that training a model can be quite a painful process so having to do that all over again is not an option. Worry not, I(and Kaggle) got you!

In this article, I will show you how to load your trained Pytorch model in 5 simple steps.

Assumptions

I will assume that:

You already know how to train a model. You understand the concept of a Pytorch’s model state dictionary. Look at this resource for more information. You have at least a basic understanding of how to save Pytorch models and load them . If not, this post is a good place to start from.

The project I was working on and the subsequent need to author this article was born out of my participation in the Udacity Pytorch Challenge."
Readers’ Choice 10 Best Data Articles,"Readers’ Choice 10 Best Data Articles

I asked the community which of this blog’s articles particularly resonated with them… here’s the list!

Image from the 1927 film Metropolis.

#10 AI: Do stupid things faster with more energy!

The pointy-haired boss is one of the four horsemen of the apocalypse…

#9 Pay attention to that man behind the curtain

Math can obscure the human element and give an illusion of objectivity…

#8 Data-Driven? Think again

Without decision-making fundamentals, your decision will be at best inspired by data, but not driven by it…

#7 Why businesses fail at machine learning

When it comes to machine learning, many organizations are in the wrong business…

#6 The simplest explanation of machine learning you’ll ever read

Machine learning is a new programming paradigm, a new way of communicating your wishes to a computer…

#5 What’s the difference between analytics and statistics?

Analytics helps you form hypotheses. It improves the quality of your questions. Statistics helps you test hypotheses. It improves the quality of your answers…

#4 What on earth is data science?

Behold my pithiest attempt: “Data science is the discipline of making data useful.” Feel free to flee now or stick around of a tour of its three subfields…

#3 Statistics for people in a hurry

Testing in a nutshell: “Does our evidence make the null hypothesis look ridiculous?” …

#2 Data Science’s Most Misunderstood Hero

Good analysts are a prerequisite for effectiveness in your data endeavors. It’s dangerous to have them quit on you, but that’s exactly what they’ll do if you under-appreciate them…

#1 What is Decision Intelligence?

Strategies based on pure mathematical rationality are relatively naïve and tend to underperform...

Surprised by the results? Me too! Five of my personal favorites aren’t on the list. [1][2][3][4][5] Outraged? Share the articles you think should be highlighted more with your friends.

Here are this list’s links for sharing on social media:

[Free Course] Making Friends with Machine Learning

If you’re looking for an applied AI course designed to be fun for beginners and experts alike, here’s one I made for your amusement:"
Why default CNN are broken in Keras and how to fix them,"Initialization method

Initialization has always been a important field of research in deep learning, especially with architectures and non-linearities constantly evolving. A good initialization is actually the reason we can train deep neural networks.

Here are the main takeaways of the Kaiming paper, where they show the conditions that the initialization should have in order to have a properly initialized CNN with ReLU activation functions. A little bit of math is required but don’t worry, you should be able to grasp the outlines.

Let’s consider the output of a convolutional layer l being:

Then, if the biases are initialized to 0, and under the assumption that the weights w and the elements x both are mutually independent and

share the same distribution, we have:

With n, the number of weights in a layer (i.e. n=k²c). By the following variance of the product of independent property:

It becomes:

Then, if we let the weights w such that they have a mean of 0, it gives:

By the König-Huygens property:

It finally gives:

However, since we are using a ReLU activation function, we have:

Thus:

This was the variance of the output of a single convolutional layer but if we want to take all of the layers into account, we have to take the product of all of them, which gives:

As we have a product, it is now easy to see that if the variance of each layer is not close to 1, then the network can rapidly degenerate. Indeed, if it is smaller than 1, it will rapidly vanish towards 0 and if it is bigger than 1, then the value of the activations will grow indefinitely, and can even become a so high number that your computer cannot represent it (NaN). So, in order to have a well-behaved ReLU CNN, the following condition must be carefully respected:

Authors have compared what happens when you train a deep CNN initialized to what was the standard initialization at that time (Xavier/Glorot) [2] and when initialized with their solution.

Comparison of the training of a 22-layer ReLU CNN initialized with Glorot (blue) or Kaiming (red). The one initialized with Glorot doesn’t learn anything

Does this graph seems familiar ? Exactly what I witnessed and shown you at the beginning ! The network trained with Xavier/Glorot initialization doesn’t learn anything.

Now guess which one is the default initialization in Keras ?

That’s right ! By default in Keras, convolutional layers are initialized following a a Glorot Uniform distribution:

So what’s happening if now we change the initialization to the Kaiming Uniform one ?"
An Introduction to Recurrent Neural Networks for Beginners,"An Introduction to Recurrent Neural Networks for Beginners

A simple walkthrough of what RNNs are, how they work, and how to build one from scratch in Python. Victor Zhou · Follow Published in Towards Data Science · 10 min read · Jul 25, 2019 -- 2 Share

Recurrent Neural Networks (RNNs) are a kind of neural network that specialize in processing sequences. They’re often used in Natural Language Processing (NLP) tasks because of their effectiveness in handling text. In this post, we’ll explore what RNNs are, understand how they work, and build a real one from scratch (using only numpy) in Python.

This post assumes a basic knowledge of neural networks. My introduction to Neural Networks covers everything you’ll need to know, so I’d recommend reading that first.

Let’s get into it!

1. The Why

One issue with vanilla neural nets (and also CNNs) is that they only work with pre-determined sizes: they take fixed-size inputs and produce fixed-size outputs. RNNs are useful because they let us have variable-length sequences as both inputs and outputs. Here are a few examples of what RNNs can look like:

Inputs are red, the RNN itself is green, and outputs are blue. Source: Andrej Karpathy

This ability to process sequences makes RNNs very useful. For example:

Machine Translation (e.g. Google Translate) is done with “many to many” RNNs. The original text sequence is fed into an RNN, which then produces translated text as output.

(e.g. Google Translate) is done with “many to many” RNNs. The original text sequence is fed into an RNN, which then produces translated text as output. Sentiment Analysis (e.g. Is this a positive or negative review?) is often done with “many to one” RNNs. The text to be analyzed is fed into an RNN, which then produces a single output classification (e.g. This is a positive review).

Later in this post, we’ll build a “many to one” RNN from scratch to perform basic Sentiment Analysis.

Note: I recommend reading the rest of this post on victorzhou.com — much of the math formatting looks better there.

2. The How

Let’s consider a “many to many” RNN with inputs x_0​, x_1​, … x_n​ that wants to produce outputs y_0​, y_1​, … y_n​. These x_i​ and y_i​ are vectors and can have arbitrary dimensions.

RNNs work by iteratively updating a hidden state h, which is a vector that can also have arbitrary dimension. At any given step t,"
Hosting a Scikit-Learn Nearest Neighbors model in AWS SageMaker,"In addition to a range of built-in algorithms, AWS SageMaker also offers the ability to train and host Scikit-Learn models. In this post, we will show how to train and host a Scikit-Learn Nearest Neighbors model in SageMaker.

Our use case will be the wine recommender model described in this article. For this exercise, we will assume that the input data for the wine recommender is fully prepared and ready to go: a set of 180,000 300-dimensional vectors, each describing the flavor profile of a specific wine. As per the aforementioned article, we will call these vectors wine embeddings. Our goal is to have a model that can return the wine embeddings that are most similar to a given input vector.

There are two main steps we will address throughout this process, as laid out in the official documentation:

Prepare a Scikit-Learn script to run on SageMaker Run this script on SageMaker via a Scikit-Learn Estimator

Preparing a Scikit-Learn script to run on SageMaker

In our Scikit-Learn script, we will load the data from our input channel (S3 bucket with our fully-prepared set of wine embeddings) and configure how we will train our model. The output location of the model is also specified here. This functionality has been put under the main guard.

We will also install s3fs, a package that allows us to interact with S3. This package enables us to identify specific S3 directories (input data, output data, model) for the script to interact with. The alternative to this is using SageMaker-specific environment variables, which specify standard S3 directories to interact with. To illustrate both options here, we will use the environmental variable SM_MODEL_DIR to store the model, and specific directory addresses for the input & output data.

So far so good! Normally, we would be able to run this script on SageMaker, first to train the model and then to return predictions by calling the ‘predict’ method. However, our Scikit-Learn Nearest Neighbors model does not have a ‘predict’ method. In effect, our model is computing cosine distances between the various wine embeddings. For any given input vector, it will return the wine embeddings that are closest to that point. This is less of a prediction than it is a way of calculating which points lie closest to one another.

Thankfully, the ‘model serving’ capability allows us to configure the Scikit-Learn script to allow for this type of customization. Model serving consists of three functions:

i) input_fn: this function deserializes the input data into an object that is passed into the prediction_fn function

ii) predict_fn: this function takes the output of the input_fn function and passes it into the loaded model

iii) output_fn: this function takes the result of predict_fn and serializes it

Each of these functions has a default implementation that is run, unless otherwise specified in the Scikit-Learn script. In our case, we can rely on the default implementation of input_fn. The wine embedding we are passing into the Nearest Neighbors model for prediction is a Numpy array, which is one of the accepted content types for the default input_fn.

For predict_fn, we will do some customization. Instead of running a ‘predict’ method on the model object, we will instead return a list of indices for the top 10 nearest neighbors, along with the cosine distances between the input data and each respective recommendation. We will have the function return a Numpy array consisting of a list with this information.

The function output_fn requires some minor customization also. We want this function to return a serialized Numpy array.

There is one more component to the Scikit-Learn script: a function to load the model. The function model_fn must be specified, as there is no default provided here. This function loads the model from the directory in which it is saved so that it can be accessed by predict_fn.

The script with all the functions outlined above should be saved in a source file that is separate from the notebook you are using to submit the script to SageMaker. In our case, we have saved this script as sklearn_nearest_neighbors.py.

Run this script on SageMaker via a Scikit-Learn Estimator

It’s smooth sailing from here: all we need to do is to run the Scikit-Learn script to fit our model, deploy it to an endpoint and we can start using it to return nearest neighbor wine embeddings.

From within a SageMaker Notebook, we run the following code:

Now, our Nearest Neighbors model is ready for action! Now, we can use the .predict method on the predictor specified above to return a list of wine recommendations for a sample input vector. As expected, this returns a nested Numpy array consisting of the cosine distances between the input vector and its Nearest Neighbors, and the indexes of these Nearest Neighbors.

[[1.37459606e-01 1.42040288e-01 1.46988100e-01 1.54312524e-01

1.56549391e-01 1.62581288e-01 1.62581288e-01 1.62931791e-01

1.63314825e-01 1.65550581e-01]

[91913 24923 74096 26492 77196 96871 113695 874654

100823 14478]]

And there we go! We have trained and hosted a Scikit-Learn Nearest Neighbors model in AWS SageMaker."
Automatically Analyzing Laboratory Test Data,"Tutorial: Automatically Analyzing Laboratory Data to Create a Performance Map

Automatically Analyzing Laboratory Test Data

It’s very common that scientists find themselves with large data sets. Sometimes it comes in the form of gigabytes worth of data in a single file. Other times it’s hundreds of files, each containing a small amount of data. Either way, it’s hard to manage. Hard to make sense of. Hard for your computer to process. You need a way to simplify the process, to make the data set more manageable, and to help you keep track of everything.

That’s what this tutorial is all about. We’re in the process of writing Python scripts that will automatically analyze all of the data for you and store the data with meaningful, intuitive file names. All while using an example taken from actual research, so you know that the skills you’re developing are practical and useful.

The first post in this tutorial introduced the concepts of the tutorial. If the terms “heat pump water heater”, “coefficient of performance (COP)”, and “performance map” don’t mean anything to you, you might want to read it.

The second post introduced the companion data set, and split the data set into multiple files with user-friendly names.

The companion data set is a valuable part of the tutorial process, as it allows you to follow along. You can write the exact same code that I’ll present, run the code, see the results, and compare it to results I present. In that way you can ensure that you’re doing it right.

At the end of the second part of the tutorial we now have three data files, each containing test results at a specified ambient temperature. In each of those files we see electricity consumption of the heat pump, the temperature of water in the storage tank, and the air temperature surrounding the water heater.

The next step is to process those results. We need to write some code that automatically makes sense of the data, calculates the COP of the heat pump, and plots the data so that we can visually understand it."
Why Machine Learning Models Degrade In Production,"After several failed ML projects due to unexpected ML degradation, I wanted to share my experience in ML models degradation. Indeed, there is a lot of hype around model creation and development phase, as opposed to model maintenance.

Assuming that a Machine Learning solution will work perfectly without maintenance once in production is a faulty assumption and represents the most common mistake of companies taking their first artificial intelligence (AI) products to market.

The moment you put a model in production, it starts degrading.

Why Do ML Models Degrade With Time?

As you may already know, data is the most crucial component of a successful ML system. Having a relevant data set that provides you with accurate predictions is a great start, but how long will that data continue to provide accurate predictions?

In all ML projects, it is key to predict how your data is going to change over time. In some projects, we underestimated this step and it became hard to deliver high accuracy. In my opinion, as soon as you feel confident with your project after the PoC stage, a plan should be put in place for keeping your models updated.

Indeed, your model’s accuracy will be at its best until you start using it. This phenomenon is called concept drift, and while it’s been heavily studied in academia for the past two decades, it’s still often ignored in industry best practices.

Concept drift: means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.

The key is that, in contrast to a calculator, your ML system does interact with the real world. If you’re using ML to predict demand and pricing for your store, you’d better consider this week’s weather, the calendar and what your competitors are doing."
Introduction to Latent Matrix Factorization Recommender Systems,"Latent Factors are “Hidden Factors” unseen in the data set. Let’s use their power. Image URL: https://www.3dmgame.com/games/darknet/tu/

Latent Matrix Factorization is an incredibly powerful method to use when creating a Recommender System. Ever since Latent Matrix Factorization was shown to outperform other recommendation methods in the Netflix Recommendation contest, its been a cornerstone in building Recommender Systems. This article will aim to give you some intuition for when to use Latent Matrix Factorization for Recommendation, while also giving some intuition behind why it works. If you’d like to see a full implementation, you can go to my Kaggle kernel: Probabilistic Matrix Factorization.

Before starting, let’s first review the problem we’re trying to solve. Latent Matrix Factorization is an algorithm tackling the Recommendation Problem: Given a set of m users and n items, and set of ratings from user for some items, try to recommend the top items for each user. There are many flavors and alternate deviations of this problem, most of which add more dimensions to the problem, like adding tags. What makes Latent Matrix Factorization powerful is that it yields really strong results from the core problem, and can be a good foundation to build from.

When working with an User-Item matrix of ratings, and reading an article on matrix factorization, the first place to look is in linear algebra. That intuition is correct, but its not exactly what you’d expect.

Sparse and Incomplete Matrix Algebra:

Traditional Linear Algebra is the bedrock of Machine Learning, and that is because most machine learning applications have something Recommender Systems do not: a data-set without NaNs(incomplete data entries). For example, whenever you’re constructing a model, NaNs, or missing data, are pruned in the data pre-processing step, as most functions cannot work with unfilled values. Functions like Principal Component Analysis are undefined if there are missing values. However, Recommender Systems cannot work if you get rid of NaNs. Those NaNs exist for a reason: not every user has rated every item, and its a bit nonsensical to expect them to. Working with Sparse data is something that can be very different — and that’s what makes Recommendation an interesting problem.

Sparsity complicates matters. Singular Value Decomposition, a factorization of a m x n Matrix into its singular and orthogonal values, is undefined if any of the entries in the Matrix are undefined. This means we cannot explicitly factorize the Matrix in such a way where we can find which we can find which diagonal(or latent) factors carry the most weight in the data set.

Instead, we’re going to approximate the best factorization of the Matrix, using a technique called Probabilistic Matrix Factorization. This technique is accredited to Simon Funk who used this technique in his FunkSVD algorithm to get very successful in the Netflix contest. For more reading, check out Simon’s original post.

The Approach:

I’ll explain the algorithm, then explain the intuition.

We’ll first initialize two matrices from a Gaussian Distribution(alternatively, randomly initialize them). The first one will be a m x k matrix P while the second will be a k x n matrix Q. When these two matrices multiply with each other, they result in an m x n matrix, which is exactly the size of our Rating matrix in which we are trying to predict. The dimension k is one of our hyper-parameters, which represents the amount of latent factors we’re using to estimate the ratings matrix. Generally, k is between 10–250, but don’t take my word for it — use a line search(or grid search) to find the optimal value for your application.

With our Matrices P, Q, we’ll optimize their values by using Stochastic Gradient Descent. Therefore, you’ll have two more hyper-parameters to optimize, learning rate and epochs. For each Epoch, we’re going to iterate through every known rating in our original m x n matrix.

Then, we’ll get a error or residual value e by subtracting the original rating value by the dot product of the original ratings’ user’s row in P and its item’s column in Q.

In normal Stochastic Gradient Descent fashion, we’ll update both of the matrices P and Q simultaneously by adding the current row for P and Q by the learning rate times the product of the error times the other Matrix’s values.

Here it is in python. View it fully in my Kaggle Kernel.

#randomly initialize user/item factors from a Gaussian

P = np.random.normal(0,.1,(train.n_users,self.num_factors))

Q = np.random.normal(0,.1,(train.n_items,self.num_factors))

#print('fit') for epoch in range(self.num_epochs):

for u,i,r_ui in train.all_ratings():

residual = r_ui - np.dot(P[u],Q[i])

temp = P[u,:] # we want to update them at the same time, so we make a temporary variable.

P[u,:] += self.alpha * residual * Q[i]

Q[i,:] += self.alpha * residual * temp self.P = P

self.Q = Q self.trainset = train



Now that we have the algorithm, why does it work and how do we interpret it’s results?

Latent factors represent categories that are present in the data. For k=5 latent factors for a movie data-set, those could represent action, romance, sci-fi, comedy, and horror. With a higher k, you have more specific categories. Whats going is we are trying to predict a user u’s rating of item i. Therefore, we look at P to find a vector representing user u, and their preferences or “affinity” toward all of the latent factors. Then, we look at Q to find a vector representing item i and it’s “affinity” toward all the latent factors. We get the dot product of these two vectors, which will return us a sense of how much the user likes the item in context of the latent factors."
Factor Analysis 101,"Great! We survived the theoretical part, so let’s look at an example in R.

Alright, so what did we just spend 10 minutes reading about?

Dealing with multidimensional data can be tough, so if we can “cut” down the number of dimensions, we can make the data way easier to look at and we can subjectively try to intepret the underlying factors.

We will look at the Olympic decathlon data (Johnson and Wichern p.499) that you can get with

od.data <- read.table(""http://static.lib.virginia.edu/statlab/materials/data/decathlon.dat"")

Edit: If you are trying this out for the first time, you will get an error if you are not adding a matrix translation. This is done by:

od.data <- as.matrix(od.data)

rownames(od.data) <- colnames(od.data) <- c(""100m"",""LJ"",""SP"",""HJ"",""400m"",""100mH"",""DS"",""PV"",""JV"",""1500m"")

First let’s try 3 factors. The output is:

By default, factanal provides ML estimates and Varimax rotation.

First is the uniqueness. It represents ^Ψ from earlier. If the uniqueness is high, the variance cannot be described by any of the factors, thus the name uniqueness or specific variance. It cannot be explained by some underlying factor. This is the case for the 1500m run and vica versa for SP (Shot put). If we subtract the uniqueness from 1, we get the communality and it will then tell how much variance is explained by the 3 factors.

The loadings represent ^L from earlier. I have sorted the data and we can easily see that shot put (SP) and discus (DS) have high loadings relative to the other variables in factor 1, implying thename “arm strength” for this factor. Specifically it means that e.g. shot put has correlation of 0.915 with factor 1 and a smaller correlation with the other two factors.

We are also interested in “Cumulative var”, the cumulative proportion of variance explained, and it should be a “high” number, where high is subjective. We should probably try fitting 4 factors, as I don’t think that 0.554 is that high and we might be able to do better. The low p-value in the end also rejects the hypothesis that 3 factors are sufficient.

The output is now:

This looks more promising. 4 factors are sufficient (according to the p-value) and we have succesfully reduced the number of variables from 10 to 4. The interpretation of the factors is subjective, we cannot explicitly name them, but a good guess could be “arm strength”, “leg strength”, “speed/acceleration” and “running endurance” by looking at where the correlation is high for the variables. Javelin and 100m hurdles have high uniqueness and does not fit well into any of the factors. Maybe a little bit in factor 1 and 2, respectively.

The most important part is finding the number of factors. From here you can play around with estimation and rotation methods and sharpen the focus of the microscope, but it’s not that crucial."
Testing your machine learning (ML) pipelines,"When it comes to data products, a lot of the time there is a misconception that these cannot be put through automated testing. Although some parts of the pipeline can not go through traditional testing methodologies due to their experimental and stochastic nature, most of the pipeline can. In addition to this, the more unpredictable algorithms can be put through specialised validation processes.

Let’s take a look at traditional testing methodologies and how we can apply these to our data/ML pipelines.

Testing Pyramid

Your standard simplified testing pyramid looks like this:

Image by author

This pyramid is a representation of the types of tests that you would write for an application. We start with a lot of Unit Tests, which test a single piece of functionality in isolation of others. Then we write Integration Tests which check whether bringing our isolated components together works as expected. Lastly we write UI or acceptance tests, which check that the application works as expected from the user’s perspective.

When it comes to data products, the pyramid is not so different. We have more or less the same levels.

Image by author

Note that the UI tests would still take place for the product, but this blog post focuses on tests most relevant to the data pipeline.

Let’s take a closer look at what each of these means in the context of Machine Learning, and with the help of some sci-fi authors.

Unit tests

“It’s a system for testing your thoughts against the universe, and seeing whether they match” Isaac Asimov.

Most of the code in a data pipeline consists of a data cleaning process. Each of the functions used to do data cleaning has a clear goal. Let’s say, for example, that one of the features that we have chosen for out model is the change of a value between the previous and current day. Our code might look somewhat like this:

def add_difference(asimov_dataset):

asimov_dataset['total_naughty_robots_previous_day'] =

asimov_dataset['total_naughty_robots'].shift(1)



asimov_dataset['change_in_naughty_robots'] =

abs(asimov_dataset['total_naughty_robots_previous_day'] -

asimov_dataset['total_naughty_robots']) return asimov_dataset[['total_naughty_robots',

'change_in_naughty_robots', 'robot_takeover_type']]

Here we know that for a given input we expect a certain output, therefore, we can test this with the following code:

import pandas as pd

from pandas.testing import assert_frame_equal

import numpy as np def test_change():

asimov_dataset_input = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3],

'robot_takeover_type': ['A', 'B', np.nan, 'A']

}) expected = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3],

'change_in_naughty_robots': [np.nan, 3, 1, 2],

'robot_takeover_type': ['A', 'B', np.nan, 'A']

}) result = add_difference(asimov_dataset_input)



assert_frame_equal(expected, result)

For each piece of independent functionality, you would write a unit test, making sure that each part of the data transformation process has the expected effect on the data. For each piece of functionality you should also consider different scenarios (is there an if statement? then all conditionals should be tested). These would then be ran as part of your continuous integration (CI) pipeline on every commit.

In addition to checking that the code does what is intended, unit tests also give us a hand when debugging a problem. By adding a test that reproduces a newly discovered bug, we can ensure that the bug is fixed when we think that is fixed, and we can ensure that the bug does not happen again.

Lastly, these tests not only check that the code does what is intended, but also help us document the expectations that we had when creating the functionality.

Integration tests

Because “The unclouded eye was better, no matter what it saw.” Frank Herbert.

These tests aim to determine whether modules that have been developed separately work as expected when brought together. In terms of a data pipeline, these can check that:

The data cleaning process results in a dataset appropriate for the model

The model training can handle the data provided to it and outputs results (ensurign that code can be refactored in the future)

So if we take the unit tested function above and we add the following two functions:

def remove_nan_size(asimov_dataset):

return asimov_dataset.dropna(subset=['robot_takeover_type']) def clean_data(asimov_dataset):

asimov_dataset_with_difference = add_difference(asimov_dataset)

asimov_dataset_without_na = remove_nan_size(

asimov_dataset_with_difference) return asimov_dataset_without_na

Then we can test that combining the functions inside clean_data will yield the expected result with the following code:

def test_cleanup():

asimov_dataset_input = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3],

'robot_takeover_type': ['A', 'B', np.nan, 'A']

})

expected = pd.DataFrame({

'total_naughty_robots': [1, 4, 3],

'change_in_naughty_robots': [np.nan, 3, 2],

'robot_takeover_type': ['A', 'B', 'A']

}).reset_index(drop=True) result = clean_data(asimov_dataset_input).reset_index(drop=True)



assert_frame_equal(expected, result)

Now let’s say that the next thing we do is feed the above data to a logistic regression model.

from sklearn.linear_model import LogisticRegression def get_reression_training_score(asimov_dataset, seed=9787):

clean_set = clean_data(asimov_dataset).dropna()

input_features = clean_set[['total_naughty_robots',

'change_in_naughty_robots']]

labels = clean_set['robot_takeover_type']



model = LogisticRegression(random_state=seed).fit(

input_features, labels) return model.score(input_features, labels) * 100

Although we don’t know the expectation, we can ensure that we always result in the same value. It is useful for us to test this integration to ensure that:

The data is consumable by the model (a label exists for every input, the types of the data are accepted by the type of model chosen, etc)

We are able to refactor our code in the future, without breaking the end to end functionality.

We can ensure that the results are always the same by providing the same seed for the random generator. All major libraries allow you to set the seed (Tensorflow is a bit special, as it requires you to set the seed via numpy, so keep this in mind). The test could look as follows:

from numpy.testing import assert_equal def test_regression_score():

asimov_dataset_input = pd.DataFrame({

'total_naughty_robots': [1, 4, 5, 3, 6, 5],

'robot_takeover_type': ['A', 'B', np.nan, 'A', 'D', 'D']

})

result = get_reression_training_score(asimov_dataset_input,

seed=1234)

expected = 50.0 assert_equal(result, expected)

There won’t be as many of these kinds of tests as unit tests, but they would still be part of your CI pipeline. You would use these to check the end to end functionality for a component and would therefore test more major scenarios.

ML Validation

Why? “To exhibit the perfect uselessness of knowing the answer to the wrong question.” Ursula K. Le Guin.

Now that we have tested our code, we need to also test that the ML component is solving the problem that we are trying to solve. When we talk about product development, the raw results of an ML model (however accurate based on statistical methods) are almost never the desired end outputs. These results are usually combined with other business rules before consumed by a user or another application. For this reason, we need to validate that the model solves the user problem, and not only that the accuracy/f1-score/other statistical measure is high enough.

How does this help us?

It ensures that the model actually helps the product solve the problem at hand

It ensures that the values produced by the model make sense in terms of the industry

It provides an extra layer of documentation of the decisions made, helping engineers joining the team later in the process.

It provides visibility of the ML components of the product in a common language understood by clients, product managers and engineers in the same way.

This kind of validation should be ran periodically (either through the CI pipeline or a cron job), and its results should be made visible to the organisation. This ensures that progress in the data science components is visible to the organisation, and ensures that issues caused by changes or stale data are caught early.

Conclusion

After all “Magic’s just science that we don’t understand yet” Arthur C. Clarke.

ML components can be tested in various ways, bringing us the following advantages:

Resulting in a data driven approach to ensure that the code does what is expected

Ensuring that we can refactor and cleanup code without breaking the functionality of the product

Documenting functionality, decisions and previous bugs

Providing visibility of the progress and state of the ML components of a product

So don’t be afraid, if you have the skillset to write the code, you have the skillset to write the test and gain all of the above advantages 🙂

So long and thanks for all the testing!"
Sentiment Analysis: a practical benchmark,"Sentiment Analysis: a practical benchmark

With hands-on practical Python code, we demonstrate limitations of simple recurrent neural networks and show how embeddings improve fully connected neural networks and convolutional neural networks for the classification of sentiment.

We show how to work with sequence data, by doing sentiment classification on a movie review dataset. Sentiments are basically feelings which include emotions, attitude and opinions written in natural language.

IMDB movie reviews dataset

We start by loading the IMDB dataset using Keras API. The reviews are already tokenized. We want to have a finite vocabulary to make sure that our word matrices are not arbitrarily small. We also want to have a finite length of reviews and not have to process really long sentences. Our training dataset has 25,000 customer reviews, together with their correct labels (either positive or negative)."
Multi-Armed Bandits and Reinforcement Learning,"Multi-Armed Bandits and Reinforcement Learning

Photo by Carl Raw on Unsplash

Multi-armed bandit problems are some of the simplest reinforcement learning (RL) problems to solve. We have an agent which we allow to choose actions, and each action has a reward that is returned according to a given, underlying probability distribution. The game is played over many episodes (single actions in this case) and the goal is to maximize your reward.

An easy picture is to think of choosing between k-many one-armed bandits (i.e. slot machines) or one big slot machine with k arms. Each arm you pull has a different reward associated with it. You’re given 1,000 quarters, so you need to develop some kind of strategy to get the most bang for your buck.

One way to approach this is to select each one in turn and keep track of how much you received, then keep going back to the one that paid out the most. This is possible, but, as stated before, each bandit has an underlying probability distribution associated with it, meaning that you may need more samples before finding the right one. But, each pull you spend trying to figure out the best bandit to play takes you away from maximizing your reward. This basic balancing act is known as the explore-exploit dilemma. Forms of this basic problem come up in areas outside of AI and RL, such as in choosing a career, finance, human psychology, and even medical ethics (although, I think my favorite proposed use case relates to the suggestion that, due to its richness, it be given to Nazi Germany during WWII, “as the ultimate form of intellectual sabotage.”).

TL;DR

We introduce multi-armed bandit problems following the framework of Sutton and Barto’s book (affiliate link) and develop a framework for solving these problems as well as examples. In this post, we’ll focus on ϵ−greedy, ϵ−decay, and optimistic strategies. As always, you can find the original post here (which properly supports LaTeX).

Problem Setup

To get started, let’s describe the problem in a bit more technical detail. What we wish to do, is develop an estimate Qt​(a):"
Introduction to AIC — Akaike Information Criterion,"Photo by Lukasz Szmigiel on Unsplash

Introduction to AIC — Akaike Information Criterion

In this article, I will cover the following topics:

What is AIC? When should you use it? How should the results be interpreted? Pitfalls of AIC

Note: This article should be considered a quick introduction to AIC. I include external links that explore tangents in greater detail.

1. What is AIC?

The Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. — Wikipedia

In plain words, AIC is a single number score that can be used to determine which of multiple models is most likely to be the best model for a given dataset. It estimates models relatively, meaning that AIC scores are only useful in comparison with other AIC scores for the same dataset. A lower AIC score is better.

AIC is most frequently used in situations where one is not able to easily test the model’s performance on a test set in standard machine learning practice (small data, or time series). AIC is particularly valuable for time series, because time series analysis’ most valuable data is often the most recent, which is stuck in the validation and test sets. As a result, training on all the data and using AIC can result in improved model selection over traditional train/validation/test model selection methods.

AIC works by evaluating the model’s fit on the training data, and adding a penalty term for the complexity of the model (similar fundamentals to regularization). The desired result is to find the lowest possible AIC, which indicates the best balance of model fit with generalizability. This serves the eventual goal of maximizing fit on out-of-sample data.

AIC equation, where L = likelihood and k = # of parameters

AIC uses a model’s maximum likelihood estimation (log-likelihood) as a measure of fit. Log-likelihood is a measure of how likely one is to see their observed data, given a model. The model with the maximum likelihood is the one that “fits” the data the best. The natural log of the likelihood is used as a computational convenience. For more details on log-likelihood, here is a useful (and slightly cheesy) introductory video for Maximum Likelihood Estimation, and another video on how maximum likelihood estimation is applied to logistic regression, to get a more intuitive feel for what maximizing a model’s log-likelihood looks like.

AIC is low for models with high log-likelihoods (the model fits the data better, which is what we want), but adds a penalty term for models with higher parameter complexity, since more parameters means a model is more likely to overfit to the training data.

The overfit model maximizes log-likelihood, since all data points fall exactly on the model’s prediction. Penalizing parameter complexity counterbalances this, and leads to better fit.

2. When should you use AIC?

AIC is typically used when you do not have access to out-of-sample data and want to decide between multiple different model types, or for time convenience. My most recent motivation to use AIC was when I was quickly evaluating multiple SARIMA models to find the best baseline model, and wanted to quickly evaluate this while retaining all the data in my training set.

(SARIMA Note: AIC makes an assumption that all models are trained on the same data, so using AIC to decide between different orders of differencing is technically invalid, since one data point is lost through each order of differencing.) For a list of other technical “Facts and Fallacies of AIC” that apply across contexts, check out Rob Hyndman’s blog post.

You must be able to fulfill AIC’s assumptions. AIC makes assumptions that you:

Are using the same data between models Are measuring the same outcome variable between models Have a sample of infinite size

That last assumption is because AIC converges to the correct answer with an infinite sample size. Often a large sample is good enough to approximate, but since using AIC often means that you have a small sample size, there is a sample-size adjusted formula called AICc, which adds a correction term that converges to the AIC answer for large samples, but gives a more accurate answer for smaller samples.

As a rule of thumb, always using AICc is safest, but AICc should especially be used when the ratio of your data points (n) : # of parameters (k) is < 40. (StackExchange article discussing this in greater mathematical detail, and a youtube video giving more conceptual understanding of AIC vs AICc, starting at 17:25).

Once the assumptions of AIC (or AICc) have been met, the biggest advantage of using AIC/AICc is that your models do not need to be nested for the analysis to be valid, unlike other single-number measurements of model fit like the likelihood-ratio test. A nested model is a model whose parameters are a subset of the parameters of another model. As a result, vastly different models can be compared mathematically with AIC.

3. How should the results be interpreted?

Once you have a set of AIC scores, what do you do with them? Pick the model with the lowest score as the best? You can do that, but AIC scores are themselves actually a probabilistic ranking of the models that are likely to minimize the information loss (best fit the data). I’ll explain via the formula below.

Assume you have calculated the AICs for multiple models and you have a series of AIC scores (AIC_1, AIC_2, … AIC_n). For any given AIC_i, you can calculate the probability that the “ith” model minimizes the information loss through the formula below, where AIC_min is the lowest AIC score in your series of scores.

“exp” means “e” to the power of the parenthesis

Wikipedia has a great example on this, with two sample AIC scores of 100 and 102 leading to the mathematical result that the 102-score model is 0.368 times as probable as the 100-score model to be the best model. An AIC of 110 is only 0.007 times as probable to be a better model than the 100-score AIC model. While this means that you can never know when one model is better than another from AIC (it is only using in-sample data, after all), there are strategies to handle these probabilistic results:

Set an alpha level below which competing models will be dismissed (alpha = 0.05, for instance, would dismiss the 110-score model at 0.007). If you find competing models above your alpha level, you can create a weighted sum of your models in proportion to their probability (1 : 0.368, in the case of the 100 and 102-scored models)

Or, if the precision of your answer is not of the utmost importance, and you want to simply select the lowest AIC, know that you are more likely to have picked a suboptimal model if there are other AIC scores that are close to the minimum AIC value of your experiments (100 vs 100.1 may leave you indifferent between the two models, compared to 100 vs 120, for example).

4. Pitfalls of AIC

As a reminder, AIC only measures the relative quality of models. This means that all models tested could still fit poorly. As a result, other measures are necessary to show that your model’s results are of an acceptable absolute standard (calculating the MAPE, for example).

AIC is also a relatively simple calculation that has been built upon and surpassed by other more computationally complicated — but also typically more accurate — generalized measures. Examples of these include DIC (Deviance Information Criterion), WAIC (Watanabe-Akaike Information Criterion), and LOO-CV (Leave-One-Out Cross-Validation, which AIC asymptotically approaches with large samples).

Depending on how much you care about accuracy vs. computational strain (and convenience of the calculation, given your software package’s capabilities), you may opt for AIC, or one of the newer, more complicated calculations. Ben Lambert gives an excellent, succinct video overview of the differences between AIC, DIC, WAIC, and LOO-CV.

Conclusion

For most calculations where one has enough data, the best (and easiest) way to accurately test your model’s performance is to use a train, validation, and test set, in good machine learning practice. But if circumstances arise where that is not possible (with small data, or time series analysis), AIC may be a better go-to test of performance."
Six Recommendations for Aspiring Data Scientists,"Six Recommendations for Aspiring Data Scientists

Data science is a field with a huge demand, in part because it seems to require experience as a data scientist to be hired as a data scientist. But many of the best data scientists I’ve worked with have diverse backgrounds ranging from humanities to neuroscience, and it takes demonstrated experience to stand out. As a new grad or analytics professional making the jump to a data science career, it can be challenging to build a portfolio of work to demonstrate expertise in this space. I’ve been on both sides of the hiring process for data science roles and wanted to call out some of the key experiences that can help land a job as a data scientist:

Get hands-on with cloud computing Create a new data set Glue things together Stand up a service Create a stunning visualization Write a white paper

I’ll expand on these topics in detail, but the key theme in data science is being able to build data products that add value to a company. A data scientist that can build these end-to-end data products is a valuable asset, and it’s useful to demonstrate these skills when pursuing a data science career.

Get hands-on with cloud computing

Many companies are looking for data scientists with past experience in cloud computing environments, because these platforms provide tools that enable data workflows and predictive models to scale to massive volumes. It’s also likely that you’ll be using a cloud platform, such as Amazon Web Services (AWS) or Google Cloud Platform (GCP) in your everyday work.

The good news is that many of these platforms provide free tiers for becoming familiar with the platform. For example, AWS has free-tier EC2 instances and free usage of services such as Lambda for low volume requests, GCP offers $300 of free credit to try out most of the platform, and Databricks provides a community edition that you can use to get hands on with the platform. With these free options you won’t be able to work with massive data sets, but you can build experience on these platforms.

One of my recommendation is to try out different features on these platforms, and see if you can use some of the tools to train and deploy models. For example, for my model services post I leveraged a tool I was already familiar with, SKLearn, and investigated how to wrap a model as a Lambda function.

Create a new data set

In academic courses and data science competitions, you’re often provided a clean data set where the focus of the project is on exploratory data analysis or modeling. However, for most real-world projects you’ll need to perform some data munging in order to clean a raw data set into a transformed data set that is more useful for an analysis or modeling task. Often, data mungling requires collecting additional data sets in order to transform data. For example, I worked with Federal Reserve data in a past role in order to better understand the asset allocation of affluent households in the US.

This was an interesting project, because I worked with third-party data in order to measure the accuracy of first-party data. My second recommendation is to actually go a step further and build a data set. This can include scraping a website, sampling data from an endpoint (e.g. steamspy), or aggregating different data sources into a new data set. For example, I created a custom data set of StarCraft replays during my graduate study, which demonstrated my ability to perform data munging on a novel data set.

Glue things together

One of the skills that I like to see data scientists demonstrate is the ability to make different components or systems work together in order to accomplish a task. In a data science role, there may not be a clear path to productizing a model and you may need to build something unique in order to get a system up and running. Ideally a data science team will have engineering support for getting systems up and running, but prototyping is a great skill for a data scientists to move quickly.

My recommendation here is to try to get different systems or components to integrate within a data science workflow. This can involve getting hands on with tools such as Airflow in order to prototype a data pipeline. It can involve creating a bridge between different systems, such as the JNI-BWAPI project I started to interface the StarCraft Brood War API library with Java. Or it can involve gluing different components together within a platform, such as using GCP DataFlow to pull data from BigQuery, apply a predictive model, and store the results to Cloud Datastore.

Stand up a service

As a data scientist, you’ll often need to stand up services that other teams can use within your company. For example, this could be a Flask app that provides results from a deep learning model. Being able to prototype services means that other teams will be able to use your data products more quickly.

My recommendation is to get hands on experience with tools such a Flask or Gunicorn, in order to setup web endpoints, and Dash in order to create interactive web applications in Python. It’s also useful practice to try setting up one of these services in a Docker instance.

Create a stunning visualization

While great work should stand on its own, it’s often necessary to first get your audience’s attention before explaining why an analysis or model is important. My recommendation here is to learn a variety of visualization tools in order to create compelling visualizations that stand out.

Creating visualizations is also a useful way of building up a portfolio of work. The blog post below shows a sample of the different tools and datasets I explored over 10 years as a data scientist.

Write a white paper

One of the data science skills that I’ve been advocating for recently is the ability to explain projects in the form of a white paper that provides an executive summary, discusses how the work can be used, provides details about the methodology, and results. The goal is to make your research digestible by a wide audience and for it to be self explanatory, so that other data scientists can build upon it.

Blogging and other forms of writing are great ways of getting experience in improving your written communication. My recommendation here is to try writing data science articles for broad audiences, in order to get experience conveying ideas at different levels of detail.

Conclusion

Data science requires hands on experience with a number of tools. Luckily, many of these tools are becoming more accessible and it’s becoming easier to build out a data science portfolio."
Response Formats and the Moon Landing,"In July 1969, the Apollo 11 spaceflight launched and landed humans on the Moon. Fifty years later, some people believe that the Moon landings did not really happen.

This article looks at two recent survey estimates of the belief that the Moon landings were faked, and the differences between those two surveys.

True or false?

Back in February 2019, YouGov asked 2,039 British adults in its internet panel, for its podcast with Yahoo:

To what extent, if at all, do you think the following statement is true or false? The moon landing was staged

People could answer along a scale from definitely true to definitely false, or that they don’t know. The survey estimated that 16% believe it is probably (12%) or definitely true (4%) that the moon landing was staged.

A few weeks ago, Number Cruncher Politics — which is not a British Polling Council member— asked 1,000 British adults about several conspiracies. Respondents could tick all that they believed were true.

Previously, Number Cruncher Politics has used a mixture of internet panels and internet ‘river sampling’. River sampling involves inviting people to answer questions via promotional ads on selected websites.

In the survey, only 5% ticked “The moon landings between 1969 to 1972 were faked and humans have not really been to the moon”.

It has been fifty years since the first Moon landings. (Photo: ABC News Australia)

So, which is it: 5% or 16%?

Differences

There are some differences in wording, comparing ‘faked’ and ‘staged’, and specifying the time of multiple Moon landings.

However, the key difference is the response format.

In YouGov’s survey, people had to select an option for each statement: definitely true, probably true, probably false, definitely false, or don’t know. This is called the forced-choice format.

The Number Cruncher Politics survey was, as the article suggested, of the select-all-that-apply format. If you have taken surveys on the internet…"
Logistic Regression from Scratch in R,"Introduction

In statistics and data science, logistic regression is used to predict the probability of a certain class or event. Usually, the model is binomial, but can also extend to multinomial. It probably is one of the simplest yet extremely useful models for a lot of applications, with its fast implementation and ease of interpretation.

This post will focus on the binomial logistic regression (with possible follow up on a multinomial model). I will discuss the basics of the logistic regression, how it is related to linear regression and how to construct the model in R using simply the matrix operation. Using only math and matrix operation (not the built-in model in R) will help us understand logistic regression under the hood.

Finally, I will use the constructed model to classify some generated data and show the decision boundary.

Logistic regression

We can think logistic regression is a generalized linear model, with a binominal distribution and a logit link function. This similarity with linear regression will help us construct the model. However the difference between the two models is that: in linear regression, the range of predicted value is (-∞, ∞), while in logistic regression, it is the probability p ranging [0, 1]. That’s why we need to use the logit link function.

Instead of predicting p directly, we predict the log of odds (logit):

which has a range from -∞ to ∞. When p → 0, logit(p) → -∞ and when p → 1, logit(p) → -∞. As a result, the logit function effectively maps the probability values from [0, 1] to (-∞, ∞). Now the linear relationship is:

where the superscript denotes the ith example, and the subscript denotes the feature or predictors x1, x2 etc ( x0 is 1 as bias). For total of m training examples, the shape of the predictor matrix X will be m×(D+1), where D is the dimensionality of the predictor variables ( x1, x2, …, xD). Adding 1 is to include the bias column x0.

And (θ0, θ1, …, θD) is a (D+1)×1 column vector. To vectorize the calculation, the right hand side (RHS) can be written as transpose(θ)⋅X or X⋅θ. Next the task is to find θ, which best represents the variation in p with varying X amongst m training examples.

To find θ, we need to define a cost function. The cost function is such that every incorrect prediction (or further away from the real value) will increase its value. In logistic regresion, the cost function is defined as:

where h(x) is the sigmoid function, inverse of logit function:

For every example, y is the actual class label 0 or 1, and h(x) is the predicted probability of getting the value of 1. If y = 1 (the second term with (1-y) will be 0), J(i) = -y⋅log(h(x)). When h(x) → 1, J(i) → 0 since log(1) = 0; when h(x) → 0, J(i) → ∞. If y = 0, J(i) = -log(1-(h(x))). When h(x) → 0, J(i) → 0, when h(x) → 1, J(i) → ∞. As h(x) furthers from y, the cost function increases rapidly.

This is the basic process to construct the model. Surprisingly it is simpler than I thought when I start coding.

Model construction in R

Now that we have the math part, let’s build our logistic regression. First I will define helper functions such as the sigmoid function, cost function J and the gradient of J. Note %*% is the dot product in R. All the functions below uses vectorized calculations.

Next is the logistic regression fuction, which takes training data X, and label y as input. It returns a column vector which stores the coefficients in θ. One thing to pay attention to is that the input X usually doesn’t have a bias term, the leading column vector of 1, so I added this column in the function.

Finally, I can write two prediction functions: first one predicts the probability p with X and θ as input, the second one returns y (1 or 0) with p as input.

Classification and decision boundary

The training data is generated such that it has two classes (0, 1), two predictors (x1, x2) and can be separated by a linear function.

There is some slight overlap so no such line will perfectly separate the two classes. However, our model shall still be able to find the best line.

Now I can train the model to get θ.

A grid is also created, which can be seen as a test set. The trained model will be applied to this grid, and predict the outcome Z. This can be used to create a decision boundary.

In the plot below, the model predicts a boundary that separates most of the two classes. Some data points are not correctly predicted as expected. However, a model that makes 100% prediction on the training data may not be a good one most of the time, as it overfits the data. In fact based on how I generated the data, the analytical solution should be x/3 + y = 2. And my decision boundary is very close to this analytical line.

Conclusion

There you have it, it is not that hard for ourselves to build a regression model from scratch. If you follow this post, hopefully by now, you have a better understanding of logistic regression. One last note, although logistic regression is often said to be a classifier, it can also be used for regression: to find the probability as we see above."
Data Visualization — Which graphs should I use? (Seaborn Examples),"Member-only story Data Visualization — Which graphs should I use? (Seaborn Examples)

Working with datasets on a daily basis has made it easier for me to read and understand table statistics. However, while numeric statistics might give you the essence of your data, a graph or visualization can uncover a whole new dimension of underlying information within your dataset.

When it comes to presenting your data, especially to clients, it’s always good to use visualization tools that can help bring out the scope and purpose of your work. You wouldn’t want just to show data files or code, rather a neat set of graphs to make your story seem more plausible and lucid.

However, creating a scatter plot for any set of data doesn’t mean you’re good to go. When visualizing data, it is important to understand 3 things.

What are the different types of plots you can use?

How many should you use and how would you explain them?

Can you tell a story using just these plots? What do they tell you?

Below, you will find the visualization types, different kinds of plots, when to use them and when not to. I tried to include examples wherever I could, but if you have any questions unanswered here, feel free to post in the comments.

Additionally, if you want to perfect the art of Data Visualization, you need to have a deep understanding of the different visualization types and plots. I’ll add some resources at the end of this read for those interested.

Visualization Types

Temporal Visualization

Typically done for one-dimensional data, showing some sort of linear relationship between data points. Such datasets usually involve time as an independent variable and thus, time-series data is visualized in this way.

Plot-types: Scatter-plots, Gantt charts, Timelines, Time-Series Line plots.

Network Visualization

As the name suggests, Network Visualization is about connecting multiple datasets with each other and showing how they relate with one another in a network where each variable is…"
Classifying Gravitational Waves with Convolutional Neural Networks (CNNs),"We all have built, trained, evaluated and inspected neural networks on a few common image datasets such as MNIST or CIFAR10. Similar AI techniques have been used for recognizing videos of cats on YouTube channel, for detecting a cancerous tumor or for understanding how climate change affects things.

Now it’s time to try machine learning on a real-world dataset in space exploration.

We invite you to consider the problem of classifying images as either gravitational waves or non-detection events using images from two detectors.

What are Gravitational Waves?

A gravitational wave is a stretch and squash of space and so can be found by measuring the change in length between two objects.

LIGO (Laser Interferometer Gravitational-Wave Observatory), the world’s largest gravitational wave observatory is able to record them extremely precisely. Currently, the data archive holds over 4.5 Petabytes of data. It is expected to grow at a rate of 800 terabytes per year. We were provided with two files containing data in the form of NumPy objects, prepared by Prof. Pavlos Protopapas’s team:"
Is your algorithm confident enough?,"Is your algorithm confident enough?

When machine learning techniques are used in “mission critical” applications, the acceptable margin of error becomes significantly lower.

Imagine that your model is driving a car, assisting a doctor or even just interacting directly with an (perhaps easily annoyed) end user. In these cases, you’ll want to ensure that you can be confident in the predictions your model makes before acting on them.

Measuring prediction uncertainty grows more important by the day, as fuzzy systems become an increasing part of our fuzzy lives.

Here’s the good news: There are several techniques for measuring uncertainty in neural networks and some of them are very easy to implement! First, let’s get a feel for what we’re about to measure.

Photo credit: Juan Rumimpunu.

Putting a number on uncertainty.

When you make models of the world, your models cannot always provide accurate answers.

This is partly due to that fact that models are simplifications of a seriously complicated world. Since some information is unknown, the predictions from your model are subject to some degree of uncertainty.

Parts of our world (and the ways we measure it) are simply chaotic. Some things happen randomly, and this randomness is also a source of uncertainty in your model’s predictions.

Prediction uncertainty can be divided into 3 categories:

1. Model uncertainty.

Model uncertainty comes from “ignorance” of the problem. That is, model uncertainty quantifies the things that could be correctly captured by the model but isn’t.

Yoel and Inbar from Taboola provide a fun example:

You want to build a model that gets a picture of an animal, and predicts if that animal will try to eat you. You trained the model on pictures of lions and giraffes. Now you show it a zombie. Since the model wasn’t trained on pictures of zombies, the uncertainty will be high. If trained on enough pictures of zombies, this uncertainty will decrease.

Sometimes it is also referred to as epistemic or structural uncertainty. Measuring model uncertainty is an area of statistics which is considered to be particularly challenging. One reason for this, is that principled techniques like Bayesian model averaging become very costly as models grow more complex.

2. Model misspecification.

If your model produces good predictions during training and validation but not during evaluation (or in production), it might be misspecified.

Model misspecification uncertainty captures scenarios where your model is making predictions on new data with very different patterns from the training data.

3. Inherent noise.

This is uncertainty produced by noise present in the dataset. It could be attributed to imperfect measurement techniques or an inherent randomness in the thing being measured.

Imagine your dataset contains 2 images of cards facing down. You’re feeling optimistic and you want to build a model to predict the suit and value of each card. The first card is labeled as ace of spades and the other is labeled as 8 of hearts. Here, the exact same features (an image of a card facing down) can be linked to different predictions (either ace of spades or 8 of hearts). Therefore, this dataset is subject to lots of inherent noise.

Inherent noise is also sometimes called aleatoric or statistical uncertainty. The amount of inherent noise is linked to the Bayes error rate which the lowest achievable error rate of a given classifier. As you can imagine, the lowest possible error rate that a model can achieve is tightly linked to the amount of error produced by noise in the data itself."
Kohonen Self-Organizing Maps,"P roperties

A Self-Organising Map, additionally, uses competitive learning as opposed to error-correction learning, to adjust it weights. This means that only a single node is activated at each iteration in which the features of an instance of the input vector are presented to the neural network, as all nodes compete for the right to respond to the input.

The chosen node — the Best Matching Unit (BMU) — is selected according to the similarity, between the current input values and all the nodes in the grid.

The node with the smallest Euclidean difference between the input vector and all nodes is chosen, along with its neighbouring nodes within a certain radius, to have their position slightly adjusted to match the input vector.

By going through all the nodes present on the grid, the entire grid eventually matches the complete input dataset, with similar nodes grouped together towards one area, and dissimilar ones separated.

A Kohonen model with the BMU in yellow, the layers inside the neighbourhood radius in pink and purple, and the nodes outside in blue.

Variables

t is the current iteration

n is the iteration limit, i.e. the total number of iterations the network can undergo

λ is the time constant, used to decay the radius and learning rate

i is the row coordinate of the nodes grid

j is the column coordinate of the nodes grid

d is the distance between a node and the BMU

w is the weight vector

w_ij(t) is the weight of the connection between the nodes i,j in the grid, and the input vector’s instance at the iteration t

x is the input vector

x(t) is the input vector’s instance at iteration t

α(t) is the learning rate, decreasing with time in the interval [0,1], to ensure the network converges.

β_ij(t) is the neighbourhood function, monotonically decreasing and representing a node i, j’s distance from the BMU, and the influence it has on the learning at step t.

σ(t) is the radius of the neighbourhood function, which determines how far neighbour nodes are examined in the 2D grid when updating vectors. It is gradually reduced over time.

Algorithm

Initialise each node’s weight w_ij to a random value Select a random input vector x_k Repeat point 4. and 5. for all nodes in the map: Compute Euclidean distance between the input vector x(t) and the weight vector w_ij associated with the first node, where t, i, j = 0. Track the node that produces the smallest distance t. Find the overall Best Matching Unit (BMU), i.e. the node with the smallest distance from all calculated ones. Determine topological neighbourhood βij(t) its radius σ(t) of BMU in the Kohonen Map Repeat for all nodes in the BMU neighbourhood: Update the weight vector w_ij of the first node in the neighbourhood of the BMU by adding a fraction of the difference between the input vector x(t) and the weight w(t) of the neuron. Repeat this whole iteration until reaching the chosen iteration limit t=n

Step 1 is the initialisation phase, while step 2–9 represent the training phase.

Formulas

The updates and changes to the variables are done according to the following formulas:

The weights within the neighbourhood are updated as:

The first equation tells us that the new updated weight w_ij (t + 1) for the node i, j is equal to the sum of old weight w_ij(t) and a fraction of the difference between the old weight and the input vector x(t). In other words, the weight vector is ‘moved’ closer towards the input vector. Another important element to note is that the updated weight will be proportional to the 2D distance between the nodes in the neighbourhood radius and the BMU.

Furthermore, the same equation 3.1 does not account for the influence of the learning being proportional to the distance a node is from the BMU. The updated weight should take into factor that the effect of the learning is close to none at the extremities of the neighbourhood, as the amount of learning should decrease with distance. Therefore, the second equation adds the extra neighbourhood function factor of βij(t), and is the more precise in-depth one.

The radius and learning rate are both similarly and exponentially decayed with time.

The neighbourhood function’s influence β_i(t) is calculated by:

The Euclidean distance between each node’s weight vector and the current input instance is calculated by the Pythagorean formula.

The BMU is selected from all the node’s calculated distances as the one with the smallest.

Further Reading"
Quick Code to Spruce Up Your Histograms & Scatterplots,"Simple graphs are essential visual tools for data analysis. If you are starting to learn how to make visualizations in Python, there are small adjustments to your graph parameters that will make them stand out. To get started, import the Pyplot and Seaborn libraries.

I. Choose Matplotlib colors that brighten up your graph.

II. Outline your histogram bins with a chosen edgecolor.

import seaborn as sns

from matplotlib import pyplot as plt

%matplotlib inline plt.rcParams[""patch.force_edgecolor""] = True

plt.figure(figsize=(8,8)) #adjust the size of your graph sns.distplot(df[""glucose""], bins= 20,color ='tomato',

hist_kws=dict(edgecolor=""k"", linewidth=2))

#modify colors, number of bins, and linewidth for custom looks

III. Create a grid of subplots for a quick visual overview.

sns.set_style('darkgrid',{'axes.edgecolor': '.9'},)

f, ax = plt.subplots(2,3,figsize = (16,7))

plt.rcParams[""patch.force_edgecolor""] = True



vis1 = sns.distplot(df[""pregnancies""],bins=10,color='mediumturquoise',

hist_kws=dict(edgecolor=""magenta"", linewidth=2.5),ax= ax[0][0]) vis2 =

sns.distplot(df[""glucose""],bins=10,color='mediumturquoise', hist_kws=dict(edgecolor=""magenta"",linewidth=2.5),ax=ax[0][1]) vis3 = sns.distplot(df[""bloodpressure""],bins=10,color='mediumturquoise', hist_kws=dict(edgecolor=""magenta"", linewidth=2.5),ax=ax[0][2]) vis4 = sns.distplot(df[""skinthickness""],bins=10,color='mediumturquoise', hist_kws=dict(edgecolor=""magenta"", linewidth=2.5), ax=ax[1][0]) vis5 =

sns.distplot(df[""insulin""],bins=10,color='mediumturquoise', hist_kws=dict(edgecolor=""magenta"", linewidth=2.5),ax=ax[1][1]) vis6 =

sns.distplot(df[""bmi""],bins=10,color='mediumturquoise', hist_kws=dict(edgecolor=""magenta"", linewidth=2.5),ax=ax[1][2])

IV. Choose marker shapes, sizes, and colormaps to increase the readability of scatterplots and regression plots. Determine a “hue” variable input that will add clarity to each data point or use “size” in the same way.

plt.figure(figsize = (8,8)) ax = sns.scatterplot(x = df.insulin, y = df.glucose, hue= df.bmi, size=df.bmi, sizes=(0,200), marker = ‘h’, palette=’plasma’, data=df)"
New in Hadoop: You should know the Various File Format in Hadoop.,"New in Hadoop: You should know the Various File Format in Hadoop.

A few weeks ago, I wrote an article about Hadoop and talked about the different parts of it. And how it plays an essential role in data engineering. In this article, I’m going to give a summary of the different file format in Hadoop. This topic is going to be a short and quick one. If you are trying to understand, how’s Hadoop work and it’s a vital role in Data Engineer, please visit my Article on Hadoop here or happy to skip it.

The file format in Hadoop roughly divided into two categories: row-oriented and column-oriented:

Row-oriented:

The same row of data stored together that is continuous storage: SequenceFile, MapFile, Avro Datafile. In this way, if only a small amount of data of the row needs to be accessed, the entire row needs to be read into the memory. Delaying the serialization can lighten the problem to a certain amount, but the overhead of reading the whole row of data from the disk cannot be withdrawn. Row-oriented storage is suitable for situations where the entire row of data needs to be processed simultaneously.

Column-oriented:

The entire file cut into several columns of data, and each column of data stored together: Parquet, RCFile, ORCFile. The column-oriented format makes it possible to skip unneeded columns when reading data, suitable for situations where only a small portion of the rows are in the field. But this format of reading and write requires more memory space because the cache line needs to be in memory (to get a column in multiple rows). At the same time, it is not suitable for streaming to write, because once the write fails, the current file cannot be recovered, and the line-oriented data can be resynchronized to the last synchronization point when the write fails, so Flume uses the line-oriented storage format."
Practical Machine Learning with C++ and GRT,"Photo by Franck V. on Unsplash

This will be the first in a series of tutorials that explain the basics of machine learning from a programmer’s perspective. In part 1 I will show how you can incorporate basic machine learning into a C++ project using the GRT library.

What is machine learning?

Machine learning is an approach to computing that enables programs to generate predictable output based on a given input without using explicitly defined logic.

For example, using traditional logic-based programming we could write a function that classifies fruit, which takes colour and dimensions as input and outputs the name of the fruit. Something like this:

string classifyFruit(Colour c, Dimensions d)

{

if (c.similar({255, 165, 0})) // green

{

if (d.similar({10, 9, 11})) // round-ish

{

return ""Apple"";

}

else

{

return ""Pear"";

}

}

if (c.similar({255, 255, 0})) // yellow

{

return ""Banana"";

}

if (c.similar({255, 165, 0})) // orange

{

return ""Orange"";

}



return ""Unknown"";

}

It can be seen that this approach has all sorts of problems. Our function only knows about four types of fruit, so if we want to extend it to also classify Clementines, we would need additional statements differentiating them from Oranges. We would also get pears and apples mixed up depending on the exact shape of the fruit and the definition of our similar() method. To create a function that would classify a wide range of fruit with a good degree of accuracy things would get very complex.

Machine learning solves this by representing the relationship between input and output as state rather than through logical rules. This means that instead of programming our classifier using if / then / else statements, we can use given / then examples to express our intent. So the code for determining the behaviour of our function could look more like this:"
Detecting Abnormal Weather Patterns With Data Science Tools,"Around the world, extreme weather events are becoming more intense and frequent.

There’s no missing some of these major weather abnormalities, such as flash floods or prolonged droughts, which can affect large swathes of a country’s population.

But not every weather outlier can be easily observed, particularly in Singapore where the seasonal changes are less obvious to the naked eye. Yet, these “milder” anomalies could be just as important in understanding future changes in the weather pattern.

Data visualisation, such as the range of charts in my earlier Medium post on the subject, provide a quick way to spot such outliers in a dataset. The classic Seaborn pair plot is one way to do this.

But when you have 36 years of weather data, it won’t be easy or efficient to rely on charts to accurately pick out the outliers.

In this third of a multi-part data science project using historical weather data from Singapore, I’ll use Scikit-learn’s Isolation Forest model as well as the PyOD library (Python Outlier Detection) to try to pinpoint outliers in the dataset.

It will be interesting to see the precise dates where these abnormal weather patterns took place. This is also important pre-work for Part IV of the project — time series forecasting, where removal of the outliers would be key to more accurate predictions.

DATA AND REPO

The original source of the weather data in this project can be found here. The Jupyter notebook for this post is here, and you can download the interactive Plotly charts here to explore the outlier data in greater detail.

OVERVIEW OF RESULTS

For the benefit of those who might have no interest in plowing through the detailed data workflow, I’ll start with a quick overview of the results."
A Deeper Dive into the NSL-KDD Data Set,"Have you ever wondered how your computer/network is able to avoid being infected with malware and bad traffic inputs from the internet? The reason why it can detect it so well is because there are systems in place to protect your valuable information held in your computer or networks. These systems that detect malicious traffic inputs are called Intrusion Detection Systems (IDS) and are trained on internet traffic record data. The most common data set is the NSL-KDD, and is the benchmark for modern-day internet traffic.

The NSL-KDD data set is not the first of its kind. The KDD cup was an International Knowledge Discovery and Data Mining Tools Competition. In 1999, this competition was held with the goal of collecting traffic records. The competition task was to build a network intrusion detector, a predictive model capable of distinguishing between “bad’’ connections, called intrusions or attacks, and “good’’ normal connections. As a result of this competition, a mass amount of internet traffic records were collected and bundled into a data set called the KDD’99, and from this, the NSL-KDD data set was brought into existence, as a revised, cleaned-up version of the KDD’99 from the University of New Brunswick.

This data set is comprised of four sub data sets: KDDTest+, KDDTest-21, KDDTrain+, KDDTrain+_20Percent, although KDDTest-21 and KDDTrain+_20Percent are subsets of the KDDTrain+ and KDDTest+. From now on, KDDTrain+ will be referred to as train and KDDTest+ will be referred to as test. The KDDTest-21 is a subset of test, without the most difficult traffic records (Score of 21), and the KDDTrain+_20Percent is a subset of train, whose record count makes up 20% of the entire train dataset. That being said, the traffic records that exist in the KDDTest-21 and KDDTrain+_20Percent are already in test and train respectively and aren’t new records held out of either dataset.

These data sets contain the records of the internet traffic seen by a simple intrusion detection network and are the ghosts of the traffic encountered by a real IDS and just the traces of its existence remains. The data set contains 43 features per record, with 41 of the features referring to the traffic input itself and the last two are labels (whether it is a normal or attack) and Score (the severity of the traffic input itself)."
Subspace clustering,"Subspace clustering

This post addresses the following questions:

What are the challenges of working with high dimensional data? What is subspace clustering? How to implement a subspace clustering algorithm in python

High dimensional data consists in input having from a few dozen to many thousands of features (or dimensions). This is a context typically encountered for instance in bioinformatics (all sorts of sequencing data) or in NLP where the size of the vocabulary if very high. High dimensional data is challenging because:

it makes the visualization and thus understanding of the input difficult, it often requires applying a dimensionality reduction technique beforehand. It leads to the ‘curse of dimensionality’ which means that the complete enumeration of all subspaces becomes intractable with increasing dimensionality

most underlying clustering techniques depend on the results and the choice of the dimensionality reduction technique

many dimensions may be irrelevant and can mask existing clusters in noisy data

one common technique is to perform feature selection (remove irrelevant dimensions) but there are cases when identifying redundant dimensions is not easy

What is subspace clustering?

Subspace clustering is a technique which finds clusters within different subspaces (a selection of one or more dimensions). The underlying assumption is that we can find valid clusters which are defined by only a subset of dimensions (it is not needed to have the agreement of all N features). For example, if we consider as input patient data observing the gene expression level (we can have more than 20000 features), a cluster of patients suffering from Alzheimer can be found only by looking at the expression data of a subset of 100 genes, or stated differently, the subset exists in 100D. Stated differently, subspace clustering is an extension of traditional N dimensional cluster analysis which allows to simultaneously group features and observations by creating both row and column clusters.

The resulting clusters may be overlapping both in the space of features and observations. Another example is shown in the figures below, taken from the paper. We can notice that points from 2 clusters can be very close which can confuse many traditional clustering algorithms analyzing the entire feature space.

Further more, we can see that subspace clustering manages to find a subspace (dimensions a and c) where the expected clusters are easily identifiable.

Types of subspace clustering

Based on the search strategy, we can differentiate 2 types of subspace clustering, as shown in the figure below: bottom up approaches start by finding clusters in low dimensional (1 D) spaces and iteratively merging them to process higher dimensional spaces (up to N D). Top down approaches find clusters in the full set of dimensions and evaluate the subspace of each cluster. The figure below, taken from the same paper provides an overview of the most common subspace clustering algorithms.

Clique algorithm

In order to better understand subspace clustering, I have implemented the Clique algorithm in python here.

In a nutshell, the algorithm functions as follows: for each dimension (feature) we split the space in nBins(input parameter) and for each bin we compute the histogram (number of counts). We only consider dense units, that is the bins with a count superior to a threshold given as second input parameter. A dense unit is characterized by the following:

the dimension it belongs to (e.g. feature 1)

the index (or the position) of the bin (from 0 to nBins)

the observations lying in the bin

In my implementation I have generated 4 random clusters in a 2D space and I have chosen 8 bins and 2 points as minimal density threshold. The figure below shows the resulting grid applied to the input space.

Input space split in 8 bins per dimension

The intuition behind the clique algorithm is that clusters existing in a k dimensional space can also be found in k-1. We start from 1D and for each dimension we try to find the dense bins. If 2 or more dense bins are neighbors, we merge them into one bigger bin. This operation can be easily implemented by transforming all existing dense bins into a graph, where an edge is drawn if 2 dense units belong to the same dimension and the difference between their bin index is no more than 1 ( e.g a dense unit corresponding to feature 3 and bin 4 is neighbor for dense units of the same feature and bins 3 and 5). The dense units to be merged can be identified by calculating the connected components on the graph described above.

The result of this merging operation retrieves the following clusters in 1 D (one plot per cluster) for the first dimension:

and for the second dimension:

Next, we want to calculate all valid clusters in each subspace from 2 to the number of input dimensions. This operation comes down to calculating combinations of dense units in k dimensions and only keeping results having an overlap of dense continuous bins with the size greater than the initial minimal density threshold. Once we calculated the dense units for k-1 dimensions we can extend to k dimensions by computing all combinations of these last k-1 candidates.

Thus, in 2 D we are able to retrieve the clusters shown in the figure below. Note that some points (in purple) outside the 4 clusters because they belong to bins having a density inferior to the arbitrary input of 2.

Clique clustering has been criticized for its high sensitivity to the input parameters (the number of bins and the minimal density) which can lead to very different results. However, it is an essential algorithm in the family of bottom-up subspace clustering. There are multiple ways to optimize the clique algorithm, for instance by using a density adaptive grid as proposed in the MAFIA algorithm.

References

Clique paper

Mafia algorithm

Comparative study of subspace clustering methods

Presentation of subspace clustering methods"
Are we Asking too Much of Algorithms?,"Over the past week Google has been under fire as a former police chief accused the internet giant of pushing extremist content, with a search for “British Muslim spokesperson” returning content from a jailed radical cleric as the top search result.

Last month Facebook was criticised for not being able to guarantee that the recent New Zealand massacre video was not still present on its platform.

Twitter has also been reprimanded for not taking down alleged white supremacist content and failing to implement an algorithm to support this effort.

Increasingly we’re seeing greater levels of media attention and public concern directed at the social media giants owing to their seeming inability to control the algorithms that support the distribution of content their platforms serve up to users.

Leaders and spokespeople for the big firms have argued that policing the internet is not their sole responsibility, but many feel that if they are happy to derive super profits from content, then they also have a moral (and some say perhaps a legal) duty of care to do more to moderate their platforms.

But perhaps rather than asking the tech firms to come up with better algorithms we should be stepping back and asking why we believe algorithms alone are the answer?

Despite the hype we are still a long way (Deepmind’s cofounder has talked in terms of decades) from a general AI that’s able to ape human ability to deal with nuance, morality and ethics. Context and adaptive learning are key to these types of ‘human’ decisions and whilst great strides are being made, we have a tendency to overestimate what technology alone can do today.

Whilst many countries now have, or are developing, a national AI strategy and there is little doubt that adaptive algorithmic technologies are learning at an incredible rate, there remains a long way to go.

Perhaps a more appropriate paradigm is to think in terms of human hybrid solutions – where a mixture of scaled technologies such as adaptive AI and machine learning can help to cut through say 80–90 % of the noise, leaving humans to deal with the thorny cases which require difficult judgement calls.

Many examples are already out there whereby algorithms are being used to sift through vast amounts of machine readable data to help identify cases to overturn convictions (for example, in California for marijuana following legalisation of the drug), or in healthcare to help diagnose diseases (for example, the NHS is looking at how to better treat multiple sclerosis as one example). The edge cases that require judgment calls are then filtered through humans.

In fairness to the likes of Google, Facebook and Twitter, they recognise the limitations of current algorithms and do employ vast numbers of human decision makers, or ‘moderators’, to review content (although this brings its own set of contentious issues).

Perhaps they need to be more forthcoming with their public assessment of the limitations of their technology and the reliance they still have on human interventions? It’s not a failure on the part of the algorithms, rather it’s an admittance that there is a much broader journey of discovery and evolution beyond just the power of the technology.

No doubt there is tremendous potential for algorithms to change our lives, and progress is being made at an incredible rate to automate mundane tasks.

As a result, although algorithms will get better at advising on the most difficult of judgement calls, we need to remain realistic about where the boundaries currently lie and accept that humans still have a critical role to play in decision-making."
Handling Discriminatory Biases in Data for Machine Learning,"Ethics in Machine Learning

Most of the time, machine learning does not touch on particularly sensitive social, moral, or ethical issues. Someone gives us a data set and asks us to predict house prices based on given attributes, classifying pictures into different categories, or teaching a computer the best way to play PAC-MAN — what do we do when we are asked to base predictions of protected attributes according to anti-discrimination laws?

How do we ensure that we do not embed racist, sexist, or other potential biases into our algorithms, be it explicitly or implicitly?

It may not surprise you that there have been several important lawsuits in the United States on this topic, possibly the most notably one involving Northpointe’s controversial COMPAS — Correctional Offender Management Profiling for Alternative Sanctions — software, which predicts the risk that a defendant will commit another crime. The proprietary algorithm considers some of the answers from a 137-item questionnaire to predict this risk.

In February 2013, Eric Loomis was found driving a car that had been used in a shooting. He was arrested and pleaded guilty to eluding an officer. In determining his sentence, a judge looked not just to his criminal record, but also to a score assigned by a tool called COMPAS.

COMPAS is one of several risk-assessment algorithms now used around the United States to predict hot spots of violent crime, determine the types of supervision that inmates might need, or — as in Loomis’s case — provide information that might be useful in sentencing. COMPAS classified him as high-risk of re-offending, and Loomis was sentenced to six years.

He appealed the ruling on the grounds that the judge, in considering the outcome of an algorithm whose inner workings were secretive and could not be examined, violated due process. The appeal went up to the Wisconsin Supreme Court, who ruled against Loomis, noting that the sentence would have been the same had COMPAS never been consulted. Their ruling, however, urged caution and skepticism in the algorithm’s use.

The case, understandably, caused quite a stir in the machine learning community — I doubt anyone would want to be judged by an algorithm, after all, you cannot blame an algorithm for being unethical, can you?"
Principal Component Analysis — Math and Intuition (Post 3),"As promised, this is the third and last post on Principal Component Analysis — Math and Intuition. We had a brief introduction to PCA in Post 1 with a real world example and grasped the intuition. We learned some of the most important concepts in Linear Algebra relevant to PCA and perhaps various other data science applications, in Post 2. With all the hard work done, it is now time to use our solid mathematical framework and connect the dots to really understanding how and why PCA works.

PCA is a dimensionality reduction technique. It simplifies a complex dataset making it computationally amenable. There is no feature elimination involved; rather PCA works by extracting the significant bits of information from all features in the original dataset and creates lesser numbers of new features. In simple words, if you have a data set with n-dimensions (n number of features), applying PCA reduces it to a k-dimensional feature space where k < n.

Let us use an example here. You have a dataset on hotel rooms listing the room areas and respective prices.

The 2-dimensional feature space can be plot as shown below.

The goal is to reduce the 2 dimensions that are represented as x-axis and y-axis respectively, into one.

Note that I have chosen a 2D dataset because it is easy to visualise. In a real world scenario, you may have 1000s or more features and therefore the need for dimensionality reduction. Also, please do not confuse the above plot with linear regression, because although it looks similar it is an entirely different concept. There is no prediction going on here.

Coming back to PCA, we would like to simplify our original dataset which is described on a 2D basis. Recall the concept of basis change that we encountered in Post 2. The question we need to ask here is,"
Reinforcement Learning for Mobile Games,"Reinforcement Learning for Mobile Games

Introduction

Deep Reinforcement Learning has made a lot of buzz since it was introduced over 5 years ago with the original DQN paper, which showed how Reinforcement Learning combined with a neural network for function approximation can be used to learn how to play Atari games from visual inputs.

Since then there have been numerous improvements to the algorithms, continuously beating the previous benchmarks. Research and testing new algorithms typically use the fast and stable Atari Benchmark, as well as custom-tailored environments such as those found in OpenAI Gym and DMLab. In addition, we can run hundreds of classic console games using the Gym Retro library using emulators.

For modern games, the main focus has recently been the hardest competitive games, in particular, Dota2 and Starcraft2. Both have achieved very impressive results from OpenAI and DeepMind, training across a huge distributed cluster, reaching total experience of thousands of years of game-play. In both these cases, the observations/inputs were numeric features and not visual frames, bypassing the need for the AI to learn how to extract these features itself as typically done for Atari games. The computing resources required are also not something most researchers or small companies have available.

Here I attempt to apply RL to some modern mobile android games, using only visual inputs as observations, and a reasonable budget amount of computational resources, on par with what’s typically considered ‘sample efficient’ when learning to play Atari games.

Android Environments

I use a configurable amount of Android Emulators as the environment for the RL agent to learn to play. Each emulator is controlled by an asynchronous driver which gathers experiences for training by grabbing a visual frame, selecting an action using the RL policy, sending that action back to the emulator, and sending the transition data back to be used for training.

Not Really Emulation

At its core, the android emulator is a ‘true emulator’, able to emulate the ARM instruction set and potentially able to be used as a fully emulated RL environment similar to the ALE or gym-retro environments, where the emulation can be paused/resumed as needed when the next action/observation step is needed. In reality though, emulation on the Android Emulator is extremely slow, especially for games, and not feasible for our task.

The only option is to run the emulator with hardware acceleration for the CPU (Using HAXM/KVM on windows/linux). In this case, an x86 android image is used and the android instructions run directly on the host CPU in real-time using virtualization. This lets us reach the stable android performance we need, but has a few drawbacks:

It means the emulator runs in real-time. This means we need to consistently grab frames, select actions and send those actions with minimal latency. The agent cannot for example pause and wait for a training step to complete, so each environment instance must run asynchronously

This also means we are confined to the ‘actual play-time’, for example, 60FPS, compared to emulation environments like the ALE which can speed up emulated play-time by orders of magnitude for much faster training

Because the android emulator is now running with CPU virtualization, it makes it problematic to run the emulators on VM instances from the popular cloud providers. Some of these providers support ‘Nested Virtualization’ for this purpose, but in practice, I could not get this to work well. Either the emulator failed to boot, or it succeeded but ran much slower than on non-VM servers. This limits us to running these environments on custom server builds or ‘bare metal’ cloud servers which are much less common.

Frame Grabbing

The environment driver grabs visual frames from the emulator at a fixed configurable rate and also tracks how many ‘dropped frames’ it had due to latencies in the agent. The main goal was to ensure minimal such frame drops, which could be detrimental to gameplay. This required to limit the number of emulators running on a single machine, in particular when training was also done on the same machine, to ensure enough CPU bandwidth for acting and training. 2 dedicated CPU cores (4 vCPUs) were needed per emulator to get stable gameplay.

I configured the frame grabbing to 30FPS with a frame-skip of either 2 or 4 (Compared to 60FPS with a frame-skip of 4 typically used in Atari training).

The android emulators are run at a 320x480 (Portrait) resolution to minimize the rendering overhead. This was the minimal resolution at which the games still worked normally. This gives us raw observations of size 320x480x3, which were then scaled down to 80x120x3 for the final observation used as the policy input (Compared to 84x84x1 grayscale observations used in most Atari research)

Action Delays

Another issue I encountered was an inherent delay in the Android Emulator from the time I sent an action until it was actually received/executed by the game. The delay was about 60–90ms, which should seemingly be negligible and something the learning should be able to account for (Even recent Starcraft 2 work from DeepMind mentions delays of around 250ms), but in reality, I found these can hurt the ability to optimally learn how to play.

To confirm this point I created a custom wrapper for Atari environments from OpenAI gym which inserted an artificial configurable delay from the time an action was sent by the agent until it was sent to the actual environment. Even with a delay of 60–90ms I saw a non-negligible reduction in final performance when training a tuned RL algorithm with the delay, compared to without the delay.

This isn’t to say we shouldn’t be able to learn how to play with such delays, as these delays can be the common case when applying RL to real-time tasks such as robots or drones. But it might be worthwhile to consider inserting such delays into the commonly used benchmark tasks when testing and comparing RL algorithms, to ensure the algorithms are robust to them.

Detecting and Skipping Screens

In order to provide an environment which can be used for RL we need to handle detecting and tapping through irrelevant screens:

Tapping in the right place to start a new game

Detecting screens which signify the game is over, and tapping the appropriate places to start a new game

Detecting various popup screens and closing them (Messages, new character unlocks, new item, etc..)

The above can probably be automated somehow, maybe even using some sort of AI/learning, but that’s a totally different topic which I did not deal with. For now, I just manually configured the relevant screens per game and where to tap when encountering such a screen.

Rewards

I used a ‘Score Change’ based reward signal. This gives a +1 reward every time the score changes. Instead of dealing with OCR of the actual score value, I found it was enough and easier to just detect the score value changed and give +1 in this case. For most games, this translates to the actual score (Except for some occasional misdetections). Moreover, since many RL algorithms clip the rewards to -1/+1 it is less important to detect the actual score in this case.

Sample Efficient Scalable RL

Because of the slowness and high resource cost of training on the Android emulators, I went looking for a sample efficient RL algorithm which can scale well to multiple environments.

I tried various families of RL algorithms but eventually focused on q-learning variants, in particular due to the fact that the games I was trying to learn had discrete action spaces with near-zero tolerance for errors (i.e. choosing a single wrong action will very oftentimes lead to a game-over). This makes value-based methods like q-learning more suitable than stochastic algorithms such as actor-critic variants (Though it is still possible for stochastic algorithms to converge to a good policy, I initially got fairly good results using PPO, but overall stabler and better with DQN variants).

The current go-to sample-efficient RL algorithm for discrete actions is Rainbow. Rainbow takes the original DQN algorithm, and combines 6 independent improvements into a single agent, reaching state-of-the-art results on the Atari benchmark at the 200M total frames threshold (~38 days of ‘play-time’).

IQN is an improved distributional version of DQN, surpassing the previous C51 and QR-DQN, and is able to almost match the performance of Rainbow, without any of the other improvements used by Rainbow.

Both Rainbow and IQN are ‘single agent’ algorithms though, running on a single environment instance, and take 7–10 days to train. Running a single android emulator agent in real-time at 60fps for 200M frames would take 38 days.

For multi-actor/distributed q-learning, the state-of-the-art is R2D2. R2D2 is a distributed multi-actor algorithm, improving over the previously published APEX with additional improvements, in particular using a ‘recurrent’ version of the DQN model with an added LSTM layer in the middle, to help the agent maintain a ‘memory’ of what happened until now and perform better long-term planning of its actions.

The LSTM is shown to be a big help even on Atari where most games are fully observable (i.e. the full state of the game can be seen on the screen) which should seemingly not benefit from LSTM memory. There is not much research on why it helps, but one possible explanation is it allows the policy to ‘follow a plan’ which allows it to learn and play more efficiently.

R2D2 reaches extremely high scores on almost all Atari games, leaving little room for improvement, however, it does so at the cost of sample-efficiency, requiring to see over 50x more environment frames than sample efficient algorithms. Still though it is able to achieve this in less time (5 Days) than Rainbow/IQN thanks to its highly distributed architecture running 256 asynchronous actors on the high-speed Atari emulations using 256 CPUs. This would not be feasible though for my android setup, where the environment runs ~10x slower and needs 2 CPUs per instance.

In order to maximize my resource utilization and minimize training time, I worked on combining features from IQN/Rainbow/R2D2 into a ‘Recurrent IQN’ training algorithm (full details here), which achieves improved sample-efficient results on the Atari benchmark with the option of running multiple actors in parallel, allowing me to train on the slow android environments efficiently in a reasonable amount of time (Up to a few days).

Training Setup

For training, I used a custom (non-VM) server with 32 CPUs and an NVIDIA 1080ti GPU. I ran 12 Android Instances per training session, each gathering experiences asynchronously in separate processes using a shared-memory copy of the main policy, to avoid latencies when one of the actors had to pause (for example to restart after a game-over). Experiences were retrieved from the processes between training steps to fill the training replay buffer.

I used the same hyperparameters and ‘Recurrent IQN’ algorithm as the one tuned for the Atari Benchmark, with a few modifications:

Reduced from 32 to 12 ENVs

ENVs run asynchronously instead of vectorized, with a shared-memory policy updated every 1000 steps with weights from the training policy

A fixed decaying exploration rate, from 1.0 to 0.001 per actor (The extra low exploration rate is needed as the games I am using are extremely sensitive to ‘wrong actions’, so the common 0.01 rate would make it hard for the training actors to reach longer episodes)

Visual game frames are scaled from 320x480x3 resolution to 80x120x3 RGB observations (Compared to 84x84x1 grayscale observations commonly used in Atari)

Frames are grabbed from the emulator at 30FPS, with a frame-skip of either 2 or 4 depending on the game, without ‘max merging’ (i.e. agent transitions are either 15 or 7.5 steps-per-second, compared to 15 typically done in Atari learning). The lower rate is needed in some games where the actions would otherwise take longer than the step-time (For example swipe actions)

Games and Results

I use the following 3 games to focus testing on:

Flappy Bird: A very hard and frustrating game for human players, requiring very fast and precise reaction time, but has a very simple mechanic and constant visuals, so the overall policy to learn should be a relatively simple one

Subway Surfers: The popular endless arcade game, dodging trains by swiping between 3 lanes, jumping or crouching above and under obstacles, and trying to collect coins along the way

Crossy Road: The infamous chicken hopper game requires the agent to plan ahead and get moving at the right time while considering the oncoming traffic and car trajectories, floating water logs, and fast passing trains

All games are evaluated using the final training policy, with no random-action epsilon.

Flappy Bird

A high score for Flappy Bird. Reached the 30-minute time limit without dying

Flappy Bird was trained at 30FPS with a frame-skip of 2 (15 Steps-Per-Second) for a total of 25M steps (Equivalent to about half the total ‘gameplay time’ used in sample-efficient Atari training). This takes around 40 hours to train using 12 emulators.

The action space has a single action (Tap) and an additional ‘noop action’. The reward is +1 each time the score changes (i.e. each time the bird passes a pipe)

Training chart for Flappy Bird, average reward of last 100 episodes

This game is notorious for taking time to start learning because the initial reward is received only after passing the first pipe, which can be extremely hard and random for an untrained agent using random exploration. There are hundreds of thousands of initial steps with 0 reward, which means the agent has no feedback at all and no way to learn. Only after we get some experiences of passing the first pipe (Which is purely random), the agent finally starts to figure it out, from that point it fairly steadily improves, but it takes over 2M steps to reach that point.

This game is extremely sensitive to wrong actions, for example performing evaluation with a random-action epsilon of 0.001 results in a mean reward of 143 instead of 420, as seen also in the training chart final results (Training also uses 0.001 for the final exploration rate)

Evaluation Score: Average 420, Max 1363 (10 Eval Episodes)

Subway Surfers

2 high scoring Subway Surfer games

For this game, I used the coin collection count as the reward, to encourage the agent to collect coins, which in itself is not necessary in order to progress, but is an important ability to learn. I also added a negative (-1) reward upon game over to encourage the agent to learn not to lose, regardless of coin collection.

The game is trained at 30FPS with a frame-skip of 4 (7.5 steps-per-second compared to 15 commonly used in Atari, due to the swipe gesture taking longer than 66ms), for 25M steps (Equivalent in total play-time to common sample-efficient Atari training using 200M frames at 60FPS). This takes around 90 hours to train using 12 emulators.

The action space here has 4 actions (Swipe up/down/left/right) and the ‘noop action’.

Interestingly the agent almost never chooses the noop action and prefers to always either jump or slide down even when it doesn’t really need to. This sort of makes sense as there is no downside to doing that and it can only help in some cases (Not all training runs are like this, some had many more noop actions). We can maybe try to mitigate this by adding a small negative reward on each action taken (e.g. -0.1).

Training chart for Subway Surfers, average reward of last 100 episodes

Evaluation Score: Average 142, max 434 (30 Eval Episodes)

Crossy Road

3 high scoring Crossy Road games

Crossy Road is trained at the same frame/step rate as Subway Surfers. The reward is +1 on every score change (Which is every time the chicken moves a step forward).

The agent clearly has a preference to try to only move forward, but it knows to move left/right and even back when needed.

Training chart for Crossy Road, average reward of last 100 episodes

Evaluation Score: Average 41, Max 139 (30 Eval Episodes)

Conclusion

The results look promising so far, although still not what we would consider ‘superhuman’ (Except maybe Flappy Bird). Still, it’s promising to see RL able to learn modern casual games in real-time from visual inputs, using the same hyperparameters as Atari, within a reasonable amount of time, frames and resources.

It’s also pretty clear from the charts that learning hasn’t plateaued, and it would be interesting to see what results we can get going to 50M or even 100M steps and beyond, and with some hyperparameter tuning."
Word Embeddings for NLP,"Word Embeddings for NLP

In this article, we will understand how to process text for usage in machine learning algorithms. What are embeddings and why are they used for text processing?

word2vec and GloVe word embeddings

Natural Language Processing(NLP) refers to computer systems designed to understand human language. Human language, like English or Hindi consists of words and sentences, and NLP attempts to extract information from these sentences.

A few of the tasks that NLP is used for

Text summarization: extractive or abstractive text summarization

Sentiment Analysis

Translating from one language to another: neural machine translation

Chatbots

Machine learning and deep learning algorithms only take numeric input so how do we convert text to numbers?

Bag of words(BOW)

Bag of words is a simple and popular technique for feature extraction from text. Bag of word model processes the text to find how many times each word appeared in the sentence. This is also called as vectorization.

Steps for creating BOW

Tokenize the text into sentences

Tokenize sentences into words

Remove punctuation or stop words

Convert the words to lower text

Create the frequency distribution of words

In the code below, we use CountVectorizer, it tokenizes a collection of text documents, builds a vocabulary of known words, and encodes new documents using that vocabulary."
The Basics: Linear Regression,"Data Science From the Ground Up

The Basics: Linear Regression

Linear regression models are for many the first predictive models covered. While conceptually simple, they have some key features that make them flexible, powerful and explicable. While newer and more conceptually complicated models may outperform a linear regression, linear models continue to see wide usage, particularly in social science and policy spheres where data collection can be expensive and there is substantial value in highly interpretable models. Extensions of the linear regression like Ridge and Lasso can help avoid overfitting in feature-rich models and even perform feature selection. Logistic regression fits the linear framework to classification problems. First, let’s look at how a plane-vanilla linear regression works.

What

Linear regression models an output variable as a linear combination of input features. What does that mean exactly? Let’s start with the simplest case and get a sense for how the model works and then think about how it scales it up to more complicated cases with more features.

A linear model attempts to find the simplest relationship between a feature variable and the output as possible. Often this is described as ‘fitting a line’. You may remember from algebra class that any given line can be expressed as some form of the equation:

Where y is your dependent variable/output, m is a slope and x is your input/independent variable. For each unit you increase x by, y increases by m units (or decreases if m is negative). The term b is an intercept term which shifts your line up or down without changing the slope. Linear regression tries find a similar relationship between an input feature and dependent variable, and ends up creating a similar formula:

In one variable it looks just like a line, except we’ve renamed the coefficient m to the greek letter beta. Let’s visualize a simple example. Here’s a collection of data points:

Some points to consider

This is, like every real world data set, a little noisy, but there’s clearly a trend: as you increase x, y increases as well. Perhaps this relationship can be well estimated with a line. How do you choose which…"
An intuitive explanation of Hypothesis Testing and P-Values,"A few years ago I did my first freelance gig in statistics for a company in the supply chain of fruit and vegetables. 24 hours a day, incoming products from farmers underwent quality control before being sent onwards to supermarkets, based on random sampling by quality controllers.

In the year over year quality report comparison, they noticed less quality this year than the previous year, something in the order of half a point on a 1 to 10 scale.

Then I came in and the question for me to answer was:

Is a difference of 0.5 a real difference?

Without any statistical background, this question may seem weird. But don’t worry: the goal of this article is to show you how this question can be answered using hypothesis testing, also called statistical inference.

It’s a numbers game: the impact of 1 different apple

Imagine you’re checking whether an apple is good or bad, using a random selection of apples from a very large box of apples.

In the below image, we see a clear effect of sample size on the measurements: the effect of one different apple is very large for small samples and the effect of 1 apple becomes smaller and smaller the larger our sample becomes.

The impact of 1 different apple depends on the sample size

Understanding the effect of sample size is the first basis towards an understanding of Hypothesis Testing. We could start arguing that 0.5 on 2 apples, would be a 1 apple difference: very likely to happen. But on 100 apples, 0.5 would represent a 50 apple difference: an extremely strong difference!

On a small sample, 0.5 is not a big difference, but on a large sample 0.5 is a big difference.

How large should the sample be: Hypothesis Testing and significance as an answer

There are different ways to approach this question, but in this article, I am going into statistical inference, or hypothesis testing to solve this question.

Hypothesis Testing is a family of statistical methods used to identify whether a sample of…"
RecoTour II: neural recommendation algorithms,"This is the second of a series of posts on recommendation algorithms in python. In the first of the series, that I wrote quite a while ago, I quickly went through a number of algorithms that I implemented and tried using Kaggle’s Ponpare dataset. You can find all the related code in the repo. In this post I will use the Amazon Reviews dataset [1] [2], in particular 5-core Movies and TV reviews, to illustrate the use of two DL-based recommendation algorithms: Neural Collaborative Filtering [3] and the more recent Neural Graph collaborative Filtering [4].

Before I move forward, let me emphasise the following. What I have done here is, after reading the papers by the authors, implementing their original solutions (in Keras and Tensorflow respectively) using Pytorch. As with all other algorithms in the RecoTour repo, I have coded a number of notebooks with a lot of details ranging from how to prepare the data to how one trains and validates the results. I have also included a number of additional functionalities and adapted the code to my preferences. However, and of course, all credit to the authors, for their nice papers and for releasing the code, which is always appreciated. Once that is clear, and without further ado, let’s move onto the algorithms

1. Neural Collaborative Filtering (NCF)

Let me start by clarifying that the authors refereed to NCF as a general framework, which is formulated in the section 3 in their paper as consisting in a series of so called NCF layers that receive the user and item embeddings.

1.1 The algorithm

The specific algorithm that they eventually implement within the context of that general framework is called Neural Matrix Factorisation (NeuMF). This algorithm is relatively simple, so I do not intend to spend too much time here. There are a number of posts online where you can perhaps find more information and as always, I strongly recommend reading the paper, where you can find all details. Moreover, in the first few sections you will find the rationale that lead the authors to the implementation of the algorithm (you know…the scientific justification).

NeuMF consists of two parts, one that the authors refer as General Matrix Factorisation (GMF) and a second one which is a Multi-Layer Perceptron (MLP). GMF is literally the element-wise product between the user and item embeddings. I guess is so simple that the authors did not even consider including a figure just for GMF. Nonetheless, I did one and here it is:

Fig 1. GMF model

The corresponding forward pass is simply:

GMF forward pass

where out=nn.Linear(in_features=n_emb, out_features=1) .

The MLP is not more complex really. The Figure 2 in their paper shows the NCF general frame:

Fig 2. MLP model. (Figure 2 in their paper)

If we think of the Neural CF layers as linear layers with some activation, we have the MLP model. The corresponding forward pass:

MLP forward pass

Where mlp is just a Sequential stack of linear layers and out is the same as in GMF with the corresponding number of input features.

Once we have all the building blocks, NeuMF is the combination of both. As illustrated in their Figure 3:

Fig 3. NeuMF model (Figure 3 in their paper.)

Formally, NeuMF can be defined as:

Equation(s) 1. NeuMF, all the math you need

where Φ-GMF is the element-wise product between the MF embeddings. Φ-MLP is the result of passing the so called MLP embeddings through a series of linear layers with some activation a, and y_hat is just the result of the concatenation of the Φ-GMF and Φ-MLP, passed through a linear layer with a sigmoid activation (σ).

In code (i.e. the forward pass):

NeuMF forward pass

And regarding to the NeuMF algorithm itself, this is it, really.

1.2 Training/Validation strategy and results

Before we move on to the next algorithm le me briefly comment on the authors’ training/validating strategy, and the results I obtained using that strategy for the Amazon Revies dataset .

For a given user, the authors split the dataset so that all but one of the rated items by that user are used for training and the remaining rated item is used for testing. Then, during training, the negative implicit feedback is represented by including N negative items (never rated) per positive item. For example, if a user has rated 10 movies and N=4, 50 observations will be used for that user during training (10 positives + 40 negatives). Note that the N negative samples are randomly sampled every epoch, so that the algorithm has a more comprehensive view of the types of items that users don’t like (see here in the repo). During training, the prediction score is the result of applying the sigmoid function to the output layer and the loss is the Binary Cross Entropy (BCE) loss.

During testing, a set of 100 items are used per user. One of them is the positive item that was not used during training, and the remaining ninety-nine are randomly selected negative items. We then predict the score for these 100 items and compute the ranking metrics. In the case of NeuMF, these are the Hit Ratio (HR) and Normalised Discounted Cumulative Gain (NDCG) at k recommendations (with k=10 in this example).

NeuMF can be run with or without GMF or MLP pre-trained weights, with varying number of embeddings, MLP layers, etc…I have run a number of experiments (which can be found in the script run_experiments.sh in the repo), including the use of Cyclic learning rates [5], and a discussion of the results can be found in this notebook. Let me briefly summarise here.

Fig 4. Ranking metrics vs Embedding dimension for the GMF and MLP models

Figure 4 shows the ranking metrics obtained for the GMF and the MLP models plotted against the number of embeddings (8, 16, 32 and 64). It is interesting to see that better ranking metrics are obtained with the simplest model (i.e. GMF and low number of embeddings). This suggests that good recommendations for this particular dataset can be attained with simple algorithms which, in general, is not an unusual result. Nonetheless it is worth mentioning that the best results are obtained with the NeuMF algorithm using pre-trained weights, but the improvement is marginal. Of course, in “the real world” one might wonder if the added complexity compensates that marginal increase in ranking metrics.

Fig 5. Ranking metrics vs BCE Loss for the GMF and MLP models

Another interesting finding is shown in Fig 5, where the ranking metrics during testing are plotted against the BCE Loss obtained during training. Here one can see that the BCE Loss and the ranking metrics are correlated, while a priori one might expect the opposite behaviour (i .e an anti-correlation, the lower the BCE Loss the higher the ranking metric). This is not extremely unusual and deserves perhaps a bit more attention. I will come back to this result towards the end of the post, once I have described Neural Graph Collaborative Filtering and its corresponding results.

2. Neural Graph Collaborative Filtering

This algorithm is a bit more complex that the previous one, so I will describe it in more detail. Nonetheless, trying to keep the size of this post readable, I will limit the content to what I consider the minimum necessary to understand the algorithm. As always, please, read the paper and in this case the references therein. In particular Semi-Supervised Classification with Graph Convolutional Networks [6] and Graph Convolutional Matrix Completion [7]. The code release by Thomas Kipf and co-authors for their paper is fantastic. Let’s move on to the algorithm: NGCF.

2.1. The Algorithm

Fig 6. Illustration of the user-item interaction up to order 3 (Figure 1 in their paper)

The left-side in Figure 6 is an illustration of the user-item interaction graph, and what the authors call the high-order connectivity. The node/user u₁ is the target user to provide recommendations for. We can see that at first order (l-hop, with l=1), one would capture the information that the user interacted with items 1, 2 and 3. At 2nd order (2-hop), one would capture u₁ ← i₂ ← u2 and u₁ ← i₃ ← u₃, and so on.

To the right we have the model scheme. In the author’s words: “we design a neural network method to propagate embeddings recursively on the graph. This is inspired by the recent developments of graph neural networks […], which can be seen as constructing information flows in the embedding space. Specifically, we devise an embedding propagation layer, which refines a user’s (or an item’s) embedding by aggregating the embeddings of the interacted items (or users). By stacking multiple embedding propagation layers, we can enforce the embeddings to capture the collaborative signal in high-order connectivities.”

Let’s try to add clarity to the previous paragraph through some math and code. The first building block one needs is, of course, representing the graph, i.e. building the Laplacian matrix, defined as:

Eq 2. Laplacian Matrix

where D is the diagonal degree matrix, A is the adjacency matrix and R is the rating matrix, in this case a binary matrix with 1/0 indicating whether the user rated or not a movie. Building the Laplacian matrix happens in load_data.py , within the utis module. There you will see that, following the authors original implementation, I build a number of different adjacency matrices to explore different scenarios (e.g. in some cases we consider self-connections, or use different decay factors between connected nodes). Once the Laplacian matrix is built, let’s move to the model.

Schematically, the model described in quotes above can be drawn like this:

Fig 7. Illustration of NGCF model architecture (Figure 2 in their paper).

The first mathematical operation within the embedding propagation layers (grey squares in Figure 7) is the “embedding aggregation” (i.e. aggregating the embeddings of all the interacted items/users per user/item). The result of that aggregation is then passed through a series of linear layers with LeakyRelu activations and concatenated with the initial embeddings. Finally, we directly compute the loss (in this case, the BPR loss, see below). This is, the output of the forward pass in this particular implementation will be directly the loss value. All this will be clearer once we go through the math and the code.

Formally, the model consists of two pieces: message construction and message aggregation.

Message construction is defined as:

Eq 3. NCGF message construction.

where ⊙ denotes element-wise multiplication. 1/√ (|Nᵤ||Nᵢ|) is the graph Laplacian norm. Nᵤ and Nᵢ are the first-hop neighbours of user u and item i. If you look at the code in load_data.py , you will see that this factor (decay factor between two connected nodes) is already accounted by in our Laplacian matrix by construction. This means that once we reach the forward pass in our Pytorch implementation, we can focus only the two terms within the parenthesis in Equation 3.

It is also worth reminding that here eᵢ or eᵤ are not the initial embeddings, but the aggregated embeddings, i.e. for user 1, eᵢ would be the aggregated embeddings of all the items that that user interacted with. Programatically, this will be simply achieved by multiplying the initial embeddings by the Laplacian matrix. We’ll see later in the forward pass.

Message aggregation is defined as:

Eq4. NGCF message aggregation

Which is simply a LeakyRelu activation applied to the result of the summation of all constructed messages.

One the messages are constructed and aggregated, we repeat the process as many times (i.e. layers) as you might want. If you want to know more about the reasoning behind the formulation of the message construction and aggregation, please, go to the paper. Is easy to read and understand. For what I need for this post Eq 2, 3 and 4 will be enough.

Let’s now focus on the loss function used during training, the so called Bayesian Personalized Ranking loss [8]. In the NGCF paper is defined as:

Eq 5. BPR Loss

Where O= {(u,i,j)| (u,i) ∈ ℝ⁺ , (u,j) ∈ ℝ⁻} is the set of training tuples with ℝ⁺ and ℝ⁻ corresponding to observed and unobserved interactions (aka positive and negative) respectively. σ is the sigmoid function and Θ are all training parameters.

In summary, we have seen how to build the Laplacian matrix, how to construct and aggregate the messages and how to evaluate the performance of the algorithm during training. We can now move to the code:

NGCF Forward Pass

The code in this snippet is similar to the one with the repo with a few extra components removed. Let’s comment a bit on this code to see how it relates to the mathematical expressions shown before:

Line 2: simple concatenation of the initial embeddings row-wise. This will result in a matrix of dimensions (# users+# items, #embeddings). The authors call the result of such concatenation ego_embeddings . This is because formally, if we only use these embeddings we would be only considering information passed from a given node (aka focal node) to the nodes directly connected to it, i.e. ego-networks.

. This is because formally, if we only use these embeddings we would be only considering information passed from a given node (aka focal node) to the nodes directly connected to it, i.e. ego-networks. Lines 9–10: per every hop (layer or connectivity order), we start by multiplying the Laplacian matrix by the result of the concatenation described before. The result of this matrix multiplication will be the aggregated embeddings mentioned before and referred as eᵤ and eᵢ in Equation 3. Because the Laplacian matrix is…BIG, we cannot do this at once, so we break it into “folds” (partitions by rows) and multiply sequentially.

Line 13: the first term within the parenthesis in equation Equation 3 is just line 13.

Line 15–16: this is the second term within the parenthesis in Equation 3.

And from there in advance is rather simple. We normalise the embeddings and concatenate them before computing the BPR loss.

Note that we also apply what the authors called “message dropout”, which is the usual nn.Dropout applied directly to the embeddings. They make a distinction between message dropout and node dropout. The authors implement the later by using tf.sparse_retain (see here) on the Laplacian Matrix, which retains locations. Personally, I would call this edge dropout, since node dropout would be, to my understanding, zeroing an entire row in the Laplacian matrix. Nonetheless, whether edge or node dropout, these are computationally very expensive. Therefore, even though I have implemented them and are included in the code in my repo, I have not explored their effect when running experiments.

At this stage, I would like to pause for a second and focus in the NGCF forward pass in the snippet above. As we can see, down until line 24, the size of the batch does not matter. Down to that line, one has to build and execute the entire graph for all users and items. In this scenario static (aka declarative) frameworks, such as Tensorflow, are more suitable. When using static frameworks the graph is built once and is then executed end-to-end as the data flows through the graph (e.g. as we call the loss function). On the other hand, when using dynamic frameworks (aka imperative) the graph is built and executed in every forward pass. Therefore, while the second have the advantage of being more flexible than the first ones, in problems like the one described here, they perform notably slower. For example, the only way for Pytorch to be “competitive” when running NGCF is using huge batch sizes, so one reduces to the minimum the number of forward passes per epoch. Still, the original TF implementation by the authors is faster.

2.2 Training/Validation strategy and results

In the case of the NGCF, training follows a strategy that is standard when using the BPR loss. Every batch is comprised by triplets like (user, positive item, negative item), i.e. a batch size of 256 will contain 256 of these triplets, and the output of the forward pass is directly the BPR loss that we need to minimise.

Then, during testing, we rank for each user all items that that user never rated In this case, the ranking score is simply the dot product between the user and the items(s) embeddings. Note that since in this case we are rating a large amount of items per user (all the items that the user did not interact with during training) the ranking metrics obtained here are significantly smaller than those obtained for NeuMF, which were computed within groups of 100 items per user.

In the case of NCGF, I have only run 15 experiments. Therefore, this is by no means an exhaustive exploration of the parameter space. I have tried using the RAdam optimiser [9], which is meant to lead to SOTA solutions in a number of problems. The truth is that when using RAdam, I obtained the best BPR loss in the least number of epochs. However, as I will discuss just below, in these types of problems (recommendation algorithms), the best loss value does not always imply the best ranking metrics. Nonetheless I find this results really promising and encouraging and I am looking forward to trying RAdam in other projects.

In general, a summary of the results and a brief discussion can be found in this notebook. Let me include here just the following figure.

Fig8. Ranking metrics vs BPR Loss

Figure 8 shows a trend for the ranking metrics to improve as the BPR loss decreases, as one might expect. However, this is by far not a smooth trend. In fact, the second best result in terms of ranking metrics is obtained for the 6th best BPR loss value (see the notebook). This relates to my earlier comment about “loss vs ranking metrics” evaluation, and deserves perhaps a dedicated paragraph or two.

In general, when building a recommendation algorithm you can normally evaluate its performance as a classification/regression problem, or as a ranking problem. The later is more related to information retrieval effectiveness and is normally my preference. In the first place, because I believe is a more robust measure of how the recommendation algorithm performs, and secondly, because sometimes ratings can be a bit “erratic”. For example, they might be influenced by the mood of the user that day or because something happened during the movie (internet failed, or the site failed).

Also, you do not want to get “too good” predicting ratings. In general you want your algorithm to have good coverage (i.e. covering as much as possible the item space) and diversity (i.e. recommending items as diverse as possible that are likely to be liked by the user). This also relates to the notion of novelty (i.e. recommending items that the user was not aware of) and serendipity(recommending unexpected items to the user). If your recommendations rely completely on achieving the best loss when predicting explicit ratings, you will end up reducing all coverage, diversity, novelty and serendipity, and ultimately, engagement.

If you want more details on how to evaluate recommendation algorithms, make sure you check, for example, Chapter 7 in this fantastic book [10].

And that’s it for now. In both cases, NCF and NGCF, I have included a script called get_embeddings.py where I used the learned embeddings and KNN to show that these learned embeddings “make sense”. This is, that given a certain movie the closest movies in the embedding space would be sensible recommendations.

3. Future Work

When I find sometime I intend to review the notebooks related to Factorisation Machines [11] and Field Aware Factorisation Machines [12]using the latest release of the xlearn library. I believe they have solved a lot of the issues I faced when using the package back in the days and back then it was already a very promising package. Therefore I think it is worth giving it another go. In terms of adding more algorithms, the next in line is Mult-VAE [13]. As described in the excellent work of Ferrari Dacrema et al [14], Mult-VAE seems to be the only Deep Learning-based recommendation algorithm that actually performs better than simpler, non DL techniques.

As always, any ideas or comments, please email me: jrzaurin@gmail.com

References:

[1] J. McAuley, C. Targett, J. Shi, A. van den Hengel. Image-based recommendations on styles and substitutes. SIGIR, 2015

[2] R. He, J. McAuley. Modeling the visual evolution of fashion trends with one-class collaborative filtering. WWW, 2016

[3] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua. Neural Collaborative Filtering. arXiv:1708.05031v2. 2016

[4] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng and Tat-Seng Chua. Neural Graph Collaborative Filtering. SIGIR 2019. arXiv:1905.08108

[5] Leslie N. Smith. Cyclical Learning Rates for Training Neural Networks. WACV 2017. arXiv:1506.01186

[6] Thomas N. Kipf and Max Welling: Semi-supervised classification with

graph convolutional networks. ICLR 2017. arXiv:1609.02907

[7] Rianne van den Berg, Thomas N. Kipf, Max Welling. KDD 2018. arXiv:1706.02263

[8] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In UAI. 452– 461.

[9] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han. On the Variance of the Adaptive Learning Rate and Beyond. arXiv:1908.03265

[10] Recommender Systems: The Textbook. Charu C. Aggarwal. Springer 2016

[11] Steffen Rendle. Factorization Machines. ICDM ’10 Proceedings of the 2010 IEEE International Conference on Data Mining

[12] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun, Quan Lu. Field-weighted Factorization Machines for Click-Through Rate Prediction in Display Advertising. WWW 2018. arXiv:1806.03514

[13] Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara: Variational Autoencoders for Collaborative Filtering. WWW 2018. arXiv:1802.05814

[14] Maurizio Ferrari Dacrema, Paolo Cremonesi, Dietmar Jannach: Are We Really Making Much Progress? A Worrying Analysis of Recent Neural Recommendation Approaches. Proceedings of the 13th ACM Conference on Recommender Systems (RecSys 2019). arXiv:1907.06902"
How to Ease the Pain of Working with Imbalanced Data,"How to Ease the Pain of Working with Imbalanced Data

A summary of methods and resources for creating a model using an imbalanced dataset Andrea Brown · Follow Published in Towards Data Science · 5 min read · Jul 18, 2019 -- Listen Share

You’ve finally collected and cleaned your data and have even completed some exploratory data analysis (EDA). All that hard work has finally paid off — time to start playing with models! However, you quickly realize that 99% of your binary labels are of the majority class while only 1% are of the minority class. Using accuracy as the primary evaluation metric, your models are classifying all predictions as the majority class and are still 99% accurate. You do a little Googling to find the right search terms for Stack Overflow, and then you sigh deeply as you read about the pains of working with imbalanced data…

What is Imbalanced Data?

Fraud detection is commonly used as an example of imbalanced datasets. Maybe only 1 out of every 1,000 transactions is fraudulent, which means only 0.1% of labels are of the minority class. Disease data is also likely to be imbalanced. In environmental health, a typical threshold for an elevated risk of developing cancer is 1 in 10^6, or one in a million people. For my project, I was working with lawsuit data, and the companies that had been sued were minimal compared to the dataset as a whole. In summary, imbalanced data is exactly what it sounds like: the labels for the minority class are far and few between, making the accuracy metric somewhat useless for evaluating model results.

What Should I do Differently?

Use Different Metrics

Another key difference is to use alternative metrics to the traditional accuracy metric. Since I was primarily interested in identifying True Positives and avoiding False Negatives rather than maximizing accuracy, I used the area under the curve receiving operator characteristic (AUC ROC) metric. This TDS post has a great explanation of the AUC ROC metric. To summarize, AUC ROC = 0.5 essentially means the model is equivalent to a random guess and AUC ROC = 1 implies the model can perfectly differentiate between the minority and majority classes.

I also relied on the confusion matrix to maximize the number of True Positives and False Positives and minimize the number of False Negatives. Since I was working with lawsuit data, I decided to consider a False Positive result as an indication of a high likelihood of being sued in the future. Therefore, False Positive results were also very important to me. I liked the code in this Kaggle kernel to visualize the confusion matrix.

Summary of True Positives and False Negatives using the Confusion Matrix

I also calculated the more traditional precision and recall metrics (equations below for quick reference) to compare different learning algorithms, and I added the less traditional miss rate (false positive rate) to the list of metrics to compare as well.

Precision = TP / (TP + FP) Recall (True Positive Rate) = TP / (TP + FN) Miss Rate (False Positive Rate) = FN / (FN +TP)

Methodology

There are two primary classes of methodologies to working with imbalanced data: (1) at the algorithm level and (2) at the data level. I’ll summarize the methods for each in the following sections.

Algorithm-Related Methods

Cost-Sensitive Learning

Considering solutions to an imbalanced dataset at the algorithm level requires an understanding of algorithms that enable cost-sensitive learning. In my case (and likely for disease and fraud detection as well), identifying True Positives was the primary goal of the model, even at the expense of selecting some False Positives. This is where cost-sensitive learning comes in handy. Cost-sensitive learning takes into account the different types of misclassification (False Positives & False Negatives).

Logistic Regression

A classic logistic regression algorithm is a robust model for imbalanced datasets. The logistic regression algorithm includes a loss function that calculates the cost for misclassification. Using SciKit-Learn, the loss function can be manipulated with a penalty weight that includes either ‘L1’ or ‘L2’ Regularization depending on the solver used.

Support Vector Machines

In SciKit-Learn, the Support Vector Classifier includes a ‘class_weight’ parameter that can be used to give more weight to the minority class. “The ‘balanced’ mode “uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.” This paper further details developing a cost-sensitive SVM.

Naive Bayes

SciKit-Learn includes a Complement Naive Bayes algorithm that is a cost-sensitive classifier that “uses statistics from the complement of each class to compute the model’s weights.” Optimizing the model weights is an effective way of handling imbalanced datasets.

Ensemble Method — Boosting

Boosting algorithms are ideal for imbalanced datasets “because higher weight is given to the minority class at each successive iteration.” For example, “AdaBoost iteratively builds an ensemble of weak learners by adjusting the weights of misclassified data during each iteration.”

Data-Related Methods

Re-sampling

Solving a class imbalance problem at the data level typically involves manipulating the existing data to force the dataset used to train the algorithm to be balanced. This method is called re-sampling, and typical re-sampling techniques include:

Over-sampling the minority class,

Under-sampling the majority class,

Combining over- and under-sampling, or

Creating ensemble balanced sets.

Over-sampling the Minority Class

Over-sampling involves balancing the dataset by creating synthetic data to increase the number of outcomes in the minority class. A common method for over-sampling is called the Synthetic Minority Oversampling Technique (SMOTE), which uses k-nearest neighbors to create synthetic data.

Taking this a step further, SMOTEBoost combines over-sampling with boosting. SMOTEBoost is “an over-sampling method based on the SMOTE algorithm that injects the SMOTE method at each boosting iteration.”

Under-sampling the Majority Class

Under-sampling involves decreasing the data included in the majority class to balance the training dataset. Note that this decreases the size of the dataset. A common technique for under-sampling is random undersampling (RUS), which randomly and selects a subset of the majority class. This Medium post goes into more detail on the various under-sampling methods.

RUSBoost combines under-sampling with boosting. Similar to SMOTEBoost, “RUSBoost achieves the same goal by performing random undersampling (RUS) at each boosting iteration instead of SMOTE.”

Ensemble Method — Bagging

Bagging is an example of an ensemble technique at the data level. Bagging involves “building multiple models (typically of the same type) from different subsamples of the training dataset.” Bagging can reduce variance and prevent overfitting in algorithms such as the Random Forest Decision Trees algorithm.

Conclusion

For my dataset, the best approach was to use combine an ensemble method of boosting with over-sampling of the minority class (SMOTEBoost) and use the logistic regression algorithm to achieve an AUC ROC value of 0.81. With additional time spent on feature engineering, I could increase the AUC ROC value even more."
Ensemble learning: A case study from the 1994 US Census database,"# fit grid search

best_model = grid.fit(X_train,y_train)

This configuration allow the algorithm uses all the available cores, which will test 960 different configuration of decision trees, random forests and bagging classifiers(with decision trees as internal model) in a parallel way.

After this process, the best model produced by the GridSearchCV was a random forest model with the following configuration:

Before we define if this is the best model, let’s check the accuracy of the model to train and test datasets. The purpose of this comparison is verify if the model is underfitted or overfitted.

As we can see above, the accuracy for both sets are good, and more than that, the values of train and test accuracy are very close. Thus, this results show us that the best model produced by GridSearchCV was well generalized.

In a random forest classifier we can set a hyperparameter called bootstrap, which defines weather samples will be trained with replacement or not. Although the best model was selected with this parameter as false, which tries to help the model to minimize the chance of overfitting, several other models present similar results when the parameter was set as true, as we can see in the image below.

So, for this dataset, we achieved good results regardless of the value of the bootstrap variable. However, the worst results due to possible overfitting came with bootstrap equal to true.

Understanding the feature importance

Via Giphy

Now, lets check the importance of each feature of the dataset for our model. For this task, we used two tools: The feature importance, from random forest classifier and the library SHAP (SHapley Additive exPlanations), which is a unified approach to explain the output of any machine learning model."
Interactive Visualization with Dash and Plotly,"Interactive data visualization has an important impact on exploratory data analysis. Before applying any descriptive or predictive algorithm to a dataset we must first understand how the features are related with each other and how they are distributed inside. It is obvious that many visualization libraries provide numerous types of charts that satisfy this requirement. But another obvious thing is that it is a hard job to do the same plotting work for each feature and scroll over each chart to compare findings for each feature.

For the last couple of weeks, I had to do this job so much that I had to find a shortcut for this. Yes, it is true that I am a lazy man, and yes it is true that laziness is a key to creativity. That is how I met dash and plotly as a solution to my problem. In this post you will find how this couple would be a good solution for exploratory data analysis.

Let me first explain what dash and plotly are for whom did not hear before. Plotly is a data analytics and visualization company. In this writing, we are interested with the two python libraries of this company; plotly.py and dash. Plotly.py library provides interactive visualization for python applications. As indicated on their website, you can “Create interactive, D3 and WebGL charts in Python. All the chart types of matplotlib and more.”

Dash is also another product of the same company, providing a framework for building web based applications for Python. If you are working with a team or just want share your work with others, a web application is the simplest way, eliminating the library version or interface issues. We will see how convenient sharing our findings over the web is, during the rest of this writing.

So, let’s start coding…

A Simple Dash App

Below is a simple dash web application consisting of six lines of code. Just write it down into a .py file and call the file and your app is up and running, that’s all.

#this is the dash_test.py file import dash

import dash_html_components as html app = dash.Dash(__name__) app.layout = html.H1('hello dash') if __name__ == '__main__':

app.run_server(debug=True, port=8080)

Call the file from command prompt as follows, with the exact path of your file. You will see a console windows that tells the server is running.

python ""c:\users\alper\documents\dash_test.py""

We can now open a web browser and navigate to the localhost url with the given port number: 127.0.0.1:8080.

In the first two lines of the code, we simply import the required dash libraries. The third line initializes the dash app, fourth line prepares the page layout with a header tag which we will be displaying on the page, and the last two lines run the server with debug and port options. (See the detailed explanation on stackoverflow for the “if __name__ … ” line)

Yes, we are far from both interactivity and visuality but be patient, we are on the way. First, we place the required elements. For this, we will modify app.layout and insert a button and a label element into a div. Note that the two elements are placed in a list as the children of the div element. Dash stores html elements in dash_html_components library, you can find the whole list on their website and github repo.

app.layout = html.Div(

[

html.Button('create random number', id='button1'),

html.Label('...', id='label1')

]

)

When we save the file, we will see a new line on the console window with a new debugger pin. If there is a problem in the code, then we will see the error message instead. In this case, we need to call the file again and refresh the browser.

Now, lets add some styling to the elements we had insert. I can’t say I am good at styling, but I’m sure you can do better. We can add style to an element with style attribute accepting a dictionary of css tags.

html.Button('create random number',

id='button1',

style={'display':'block', 'background-color':'#aabbcc'}

),

html.Label('...',

id='label1',

style={'display':'inline-block', 'margin':'10'}

)

And it’s time to go a step further and add some responsiveness. First we import the required libraries

from dash.dependencies import Input, Output

import random

Then we add the callback decorator and the function we want to execute on callback.

update_output function simply generates a random number and returns it as result.

@app.callback decorator binds the button click event to the update_output function, and the result of the function to the label1 element. This is the core part of the responsiveness. There will be another post on callbacks and state parameters.

Adding a Simple Chart

Since we covered the interactivity enough for introduction, it is time to add some charts. First, we will keep it simple and put a bar chart with random values on each button click. So, we need to add a graph object to our layout:

app.layout = html.Div(

[

html.Button(‘create random number’,

id=’button1',

style={‘display’:’block’, ‘padding’:’5', ‘background-color’:’#aabbcc’}),

html.Label(‘…’,

id=’label1',

style={‘display’:’inline-block’, ‘margin’:’10'} ),

dcc.Graph(id=’graph1') # this is the graph we add

]

)

And we need to modify our callback function to produce the chart:



Output(component_id='graph1', component_property='figure'),

[Input(component_id='button1', component_property='n_clicks')]

)

def update_output(input_value):

random_x = [i for i in range(5)]

random_y = [random.random() for _ in range(5)]

figure = {

'data': [

{'x':random_x, 'y':random_y, 'type':'bar', 'name': 'Series1'}

],

'layout': {

'title': 'Dash Data Visualization'

}

}

return figure @app .callback(Output(component_id='graph1', component_property='figure'),[Input(component_id='button1', component_property='n_clicks')]def update_output(input_value):random_x = [i for i in range(5)]random_y = [random.random() for _ in range(5)]figure = {'data': [{'x':random_x, 'y':random_y, 'type':'bar', 'name': 'Series1'}],'layout': {'title': 'Dash Data Visualization'return figure

In the callback decorator, we first replace the label in our Output statement with the graph object we recently added to our layout. Then inside the function we crate x and y values for the chart, and the figure object. That is all. The result is an interactive bar chart inside your browser.

Some more complexity

If the above chart is not fancy enough for you, don’t worry, here is another example for you. Let’s get some deeper.

Was it too fast? Ok, let’s look at the code then.

# coding=utf8

import pandas as pd

import dash

from dash.dependencies import Input, Output

import dash_html_components as html

import dash_core_components as dcc

import plotly.graph_objs as go

app = dash.Dash(__name__)

names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']

data = pd.read_csv('

app.layout = html.Div(

[

html.Div([

dcc.Dropdown(

id='ddl_x',

options=[{'label': i, 'value': i} for i in names],

value='sepal-width',

style={'width':'50%'}

),

dcc.Dropdown(

id='ddl_y',

options=[{'label': i, 'value': i} for i in names],

value='petal-width',

style={'width':'50%'}

),

],style={'width':'100%','display':'inline-block'}),

html.Div([

dcc.Graph(id='graph1')

],style={'width':'100%','display':'inline-block'})

]

)

.callback(

Output(component_id='graph1', component_property='figure'),

[

Input(component_id='ddl_x', component_property='value'),

Input(component_id='ddl_y', component_property='value')

]

)

def update_output(ddl_x_value, ddl_y_value):

figure={

'data': [

go.Scatter(

x=data[data['class'] == cls][ddl_x_value],

y=data[data['class'] == cls][ddl_y_value],

mode='markers',

marker={ 'size': 15 },

name=cls

) for cls in data['class'].unique()

],

'layout':

go.Layout(

height= 350,

hovermode= 'closest',

title=go.layout.Title(text='Dash Interactive Data Visualization',xref='paper', x=0)

)



}

return figure

if __name__ == '__main__':

app.run_server(debug=True, port=8080) import randomimport pandas as pdimport dashfrom dash.dependencies import Input, Outputimport dash_html_components as htmlimport dash_core_components as dccimport plotly.graph_objs as goapp = dash.Dash(__name__)names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']data = pd.read_csv(' https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data' , names=names)app.layout = html.Div(html.Div([dcc.Dropdown(id='ddl_x',options=[{'label': i, 'value': i} for i in names],value='sepal-width',style={'width':'50%'}),dcc.Dropdown(id='ddl_y',options=[{'label': i, 'value': i} for i in names],value='petal-width',style={'width':'50%'}),],style={'width':'100%','display':'inline-block'}),html.Div([dcc.Graph(id='graph1')],style={'width':'100%','display':'inline-block'}) @app .callback(Output(component_id='graph1', component_property='figure'),Input(component_id='ddl_x', component_property='value'),Input(component_id='ddl_y', component_property='value')def update_output(ddl_x_value, ddl_y_value):figure={'data': [go.Scatter(x=data[data['class'] == cls][ddl_x_value],y=data[data['class'] == cls][ddl_y_value],mode='markers',marker={ 'size': 15 },name=cls) for cls in data['class'].unique()],'layout':go.Layout(height= 350,hovermode= 'closest',title=go.layout.Title(text='Dash Interactive Data Visualization',xref='paper', x=0)return figureif __name__ == '__main__':app.run_server(debug=True, port=8080)

The code structure is exactly the same as the previous. After initializing the app,

we added two lines for data reading.

in the app.layout section, we added two dropdown lists, and fill the options with a loop of data columns.

in the @app.callback decorator, we added these two dropdowns as input components

and in the update_output function, we draw a scatter plot graph with the data and the columns selected by the dropdown lists. Here, there is a tricky part. We draw the scatter plot for each class. You see, there is a for loop at the end of the go.Scatter() function and inside the ‘data’ list. And this for loop, also called as list comprehension, returns Scatter() objects n times, where n is the number of unique records in the ‘class’ column of the data. And following line is for the layout properties of the chart.

The code is ready to run. Just;

save it into a file with .py extension, -> “c:\…\dash_test.py”

call it via command prompt using python -> python “c:\…\dash_test.py”

open a browser and navigate to the app -> http://localhost:8080

Your interactive data visualization application is ready in 60 lines of code."
Starbucks: Analyze-a-Coffee,"Introduction

Starbucks is a global coffee company selling coffee, tea, espresso drinks, bakery, and grab-and-go offerings in 75 countries. One of the company’s values is “Creating a culture of warmth and belonging, where everyone is welcome.” Therefore, it utilizes many channels to market its products from social media to TV spots and ads. Starbucks executes its extraordinary marketing strategy by deploying a combination of marketing media channels, where it creates brand recognition. Starbucks does not only understand its products and customers, but also keeps up with how its customers use technology. Starbucks App enables customers to keep track of the available offers and happy hour deals at participating stores. It allows customers to earn and collect stars (collect two stars per $1) that can be redeemed in-store or via the app.

“With nearly 100 million customers in our stores every week, we’re looking for more opportunities to engage directly and personally, providing them with special benefits and offers that are meaningful” — Matt Ryan, executive vice president and chief strategy officer for Starbucks.

Here, we are going to investigate and analysis three files that simulate how people make purchasing decisions and how promotional offers influence those decisions. A sneak of the final data after being cleaned and analyzed: the data contains information about 8 offers sent to 14,825 customers who made 26,226 transactions while completing at least one offer. Below are two examples of the types of offers Starbucks sends to its customers through the app to encourage them to purchase products and collect stars."
Everything you need to know about “Activation Functions” in Deep learning models,"This article is your one-stop solution to every possible question related to activation functions that can come into your mind that are used in deep learning models. These are basically my notes on activation functions and all the knowledge that I have about this topic summed together in one place. So, without going into any unnecessary introduction, let’s get straight down to business.

Contents

What is an activation function and what does it do in a network? Why is there a need for it and why not use a linear function instead? What are the desirable features in an activation function? Various non-linear activations in use Notable non-linear activations coming out of latest research How (and which) to use them in deep neural networks

What is an activation function?

Simply put, an activation function is a function that is added into an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. That is exactly what an activation function does in an ANN as well. It takes in the output signal from the previous cell and converts it into some form that can be taken as input to the next cell. The comparison can be summarized in the figure below.

Source: cs231n by Stanford

Why is there a need for it?

There are multiple reasons for having non-linear activation functions in a network.

Apart from the biological similarity that was discussed earlier, they also help in keeping the value of the output from the neuron restricted to a certain limit as per our requirement. This is important because input into the activation function is W*x + b where W is the weights of the cell and the x is the inputs and then there is the bias b added to that. This value if not restricted to a certain limit can go very high in magnitude especially in case of very deep neural networks that have millions of parameters. This will lead to computational issues. For example, there are some activation functions (like softmax) that out specific values for different values of input (0 or 1). The most important feature in an activation function is its ability to add non-linearity into a neural network. To understand this, let’s consider multidimensional data such as shown in the figure below:

A linear classifier using the three features(weight, Systolic Blood Pressure and Age in this figure) can give us a line through the 3-D space but it will never be able to exactly learn the pattern that makes a person a smoker or a non-smoker(the classification problem in hand) because the pattern that defines this classification is simply not linear. In come the artificial neural networks. What if we use an ANN with a single cell but without an activation function. So our output is basically W*x + b. But this is no good because W*x also has a degree of 1, hence linear and this is basically identical to a linear classifier.

What if we stack multiple layers. Let’s represent nᵗʰ layer as a function fₙ(x). So we have:

o(x) = fₙ(fₙ₋₁(….f₁(x))

However, this is also not complex enough especially for problems with very high patterns such as that faced in computer vision or natural language processing.

In order to make the model get the power (aka the higher degree complexity) to learn the non-linear patterns, specific non-linear layers (activation functions) are added in between.

Desirable features of an activation function

Vanishing Gradient problem: Neural Networks are trained using the process gradient descent. The gradient descent consists of the backward propagation step which is basically chain rule to get the change in weights in order to reduce the loss after every epoch. Consider a two-layer network and the first layer is represented as f₁(x) and the second layer is represented as f₂(x). The overall network is o(x) = f₂(f₁(x)). If we calculate weights during the backward pass, we get o`(x) = f₂(x)*f₁`(x). Here f₁(x) is itself a compound function consisting of Act(W₁*x₁ + b₁) where Act is the activation function after layer 1. Applying chain rule again, we clearly see that f₁`(x) = Act(W₁*x₁ + b₁)*x₁ which means it also depends directly on the activation value. Now imagine such a chain rule going through multiple layers while backpropagation. If the value of Act() is between 0 and 1, then several such values will get multiplied to calculate the gradient of the initial layers. This reduces the value of the gradient for the initial layers and those layers are not able to learn properly. In other words, their gradients tend to vanish because of the depth of the network and the activation shifting the value to zero. This is called the vanishing gradient problem. So we want our activation function to not shift the gradient towards zero. Zero-Centered: Output of the activation function should be symmetrical at zero so that the gradients do not shift to a particular direction. Computational Expense: Activation functions are applied after every layer and need to be calculated millions of times in deep networks. Hence, they should be computationally inexpensive to calculate. Differentiable: As mentioned, neural networks are trained using the gradient descent process, hence the layers in the model need to differentiable or at least differentiable in parts. This is a necessary requirement for a function to work as activation function layer.

Various non-linear activations in use

Sigmoid: The sigmoid is defined as:

This activation function is here only for historical reasons and never used in real models. It is computationally expensive, causes vanishing gradient problem and not zero-centred. This method is generally used for binary classification problems.

Softmax : The softmax is a more generalised form of the sigmoid. It is used in multi-class classification problems . Similar to sigmoid, it produces values in the range of 0–1 therefore it is used as the final layer in classification models.

: The softmax is a more generalised form of the sigmoid. It is used in . Similar to sigmoid, it produces values in the range of 0–1 therefore it is used as the final layer in classification models. Tanh: The tanh is defined as:

If you compare it to sigmoid, it solves just one problem of being zero-centred.

ReLU: ReLU (Rectified Linear Unit) is defined as f(x) = max(0,x):

This is a widely used activation function, especially with Convolutional Neural networks. It is easy to compute and does not saturate and does not cause the Vanishing Gradient Problem. It has just one issue of not being zero centred. It suffers from “dying ReLU” problem. Since the output is zero for all negative inputs. It causes some nodes to completely die and not learn anything.

Another problem with ReLU is of exploding the activations since it higher limit is, well, inf. This sometimes leads to unusable nodes.

Leaky ReLU and Parametric ReLU: It is defined as f(x) = max(αx, x)

the figure is for α = 0.1

Here α is a hyperparameter generally set to 0.01. Clearly, Leaky ReLU solves the “dying ReLU” problem to some extent. Note that, if we set α as 1 then Leaky ReLU will become a linear function f(x) = x and will be of no use. Hence, the value of α is never set close to 1. If we set α as a hyperparameter for each neuron separately, we get parametric ReLU or PReLU.

ReLU6: It is basically ReLU restricted on the positive side and it is defined as f(x) = min(max(0,x),6)

This helps to stop blowing up the activation thereby stopping the gradients to explode(going to inf) as well another of the small issues that occur with normal ReLUs.

The idea that comes into one’s mind is why not combine ReLU6 and a LeakyReLU to solve all known issues that we have with previous activation functions. Popular DL frameworks do not provide an implementation of such an activation function but I think this would be a good idea.

Notable non-linear activations coming out of latest research

Swish: This was proposed in 2017 by Ramachandran et.al. It is defined as f(x) = x*sigmoid(x).

It is slightly better in performance as compared to ReLU since its graph is quite similar to ReLU. However, because it does not change abruptly at a point as ReLU does at x = 0, this makes it easier to converge while training.

But, the drawback of Swish is that it is computationally expensive. To solve that we come to the next version of Swish.

Hard-Swish or H-Swish: This is defined as:

The best part is that it is almost similar to swish but it is less expensive computationally since it replaces sigmoid (exponential function) with a ReLU (linear type).

How to use them in deep neural networks?

Tanh and sigmoid cause huge vanishing gradient problems . Hence, they should not be used.

. Hence, they should not be used. Start with ReLU in your network . Activation layer is added after the weight layer (something like CNN, RNN, LSTM or linear dense layer) as discussed above in the article. If you think the model has stopped learning, then you can replace it with a LeakyReLU to avoid the Dying ReLU problem. However, the Leaky ReLU will increase the computation time a little bit.

. Activation layer is added after the weight layer (something like CNN, RNN, LSTM or linear dense layer) as discussed above in the article. If you think the model has stopped learning, then you can replace it with a LeakyReLU to avoid the Dying ReLU problem. However, the Leaky ReLU will increase the computation time a little bit. If you also have Batch-Norm layers in your network, that is added before the activation function making the order CNN-Batch Norm-Act . Although the order of Batch-Norm and Activation function is a topic of debate and some say that the order doesn’t matter, I use the order mentioned above just to follow the original Batch-Norm paper.

. Although the order of Batch-Norm and Activation function is a topic of debate and some say that the order doesn’t matter, I use the order mentioned above just to follow the original Batch-Norm paper. Activation functions work best in their default hyperparameters that are used in popular frameworks such as Tensorflow and Pytorch. However, one can fiddle with the negative slope in LeakyReLU and set it to 0.02 to expedite learning.

THAT’S ALL FOLKS 😃

I have tried to solve every possible question related to activation functions, however, if I have missed something please comment down below.

You can look at more things related to deep learning on my Github and follow me on Linkedin.

Some of my previous articles:"
Read Text from Image with One Line of Python Code,"Dealing with images is not a trivial task. To you, as a human, it’s easy to look at something and immediately know what is it you’re looking at. But computers don’t work that way.

Photo by Lenin Estrada on Unsplash

Tasks that are too hard for you, like complex arithmetics, and math in general, is something that a computer chews without breaking a sweat. But here the exact opposite applies — tasks that are trivial to you, like recognizing is it cat or dog in an image are really hard for a computer. In a way, we are a perfect match. For now at least.

While image classification and tasks that involve some level of computer vision might require a good bit of code and a solid understanding, reading text from a somewhat well-formatted image turns out to be a one-liner in Python —and can be applied to so many real-life problems.

And in today’s post, I want to prove that claim. There will be some installation to go though, but it shouldn’t take much time. These are the libraries you’ll need:

OpenCV

PyTesseract

I don’t want to prolonge this intro part anymore, so why don’t we jump into the good stuff now.

OpenCV

Now, this library will only be used to load the images(s), you don’t actually need to have a solid understanding of it beforehand (although it might be helpful, you’ll see why).

According to the official documentation:

OpenCV (Open Source Computer Vision Library) is an open source computer vision and machine learning software library. OpenCV was built to provide a common infrastructure for computer vision applications and to accelerate the use of machine perception in the commercial products. Being a BSD-licensed product, OpenCV makes it easy for businesses to utilize and modify the code.[1]

In a nutshell, you can use OpenCV to do any kind of image transformations, it’s fairly straightforward library.

If you don’t already have it installed, it’ll be just a single line in terminal:

pip install opencv-python

And that’s pretty much it. It was easy up until this point, but that’s about to change.

PyTesseract"
Easy Machine Learning in the Browser: Real-time Image Classification,"Photo by Kasya Shahovskaya on Unsplash

Training a Realtime Image Recognition Model in the Browser with Tensorflow.js KNN module and Angular

Deploying a machine learning application into a production environment used to be a daunting process as it required handling complex operations in many different environments. With the introduction of Tensorflow.js, it is super easy to develop, train and deploy machine learning applications in the web browser using javascript.

To demonstrate this, I will create a simplified version of Teachable Machine demo application using Angular. This demo app teaches your computer to recognize images using your webcam, real-time in the web browser.

To achieve this, I will use the KNN module provided by Tensorflow.js. This module creates a classifier using the K-Nearest Neighbors algorithm. Instead of providing a model with weights, it creates a KNN model using activations from another model.

For this, the mobilenet image classification model is a very good choice as it is lightweight and ready to use in Tensorflow. This process in machine learning is called transfer learning as we are using representations of another machine learning model.

Photo by Andrii Podilnyk on Unsplash

Before diving into the details, let’s first better understand what will be the steps followed in the app;

Step1:

The user should provide and label the 3 sets of input images for the training. The webcam and the buttons which are available on the browser are to be used for this purpose.

Step2:

Once all input images are provided with the labels, the activation tensors will be predicted by dumping the input images into the mobilenet model. Those activation tensors will be then used as input to the KNN classifier to create a data-set with labels assigned to each activation tensor provided. The training process will be completed with this step.

Step3:

For the prediction, images captured from the webcam are to be fed real-time into the mobilenet model to get the activation…"
Knowledge Graph: The Perfect Complement to Machine Learning,"Amount of information available today on the web is astounding and it is ever-expanding. For example, there are more than 1.94 billion websites that are linked with the World Wide Web and search engines (e.g., Google, Bing, etc.) can go through those links and serve useful information with great precision and speed. In most of those successful search engines, the most important denominator is the use of Knowledge Graphs. Not only search engines, social network sites (e.g., Facebook, etc.), e-commerce sites (e.g., Amazon, etc.) are also using Knowledge Graphs to store and retrieve useful information.

A Brief History

In 1960, Semantic Networks were invented to address the growing need for a knowledge representation framework that can capture a wide range of entities — real-world objects, events, situations or abstract concepts and relations and in the end can be applied to extended English Dialogue tasks. The main idea behind Semantic Networks was to capture a wide range of issues which includes the representation of plans, actions, time, individuals’ beliefs and intentions, and be general enough to accommodate each issue.

According to Wikipedia, in late 1980, two Netherlands universities started a project called Knowledge Graph which was kind of a semantic network, but with some added restrictions to facilitate algebraic operations on the graph.

In 2001, Tim Berners-Lee coined the term Semantic Web which is an application of Semantic Network combined with the Web.

Tim Berners-Lee stated that “The Semantic Web is an extension of the current web in which information is given well-defined meaning, better enabling computers and people to work in cooperation.”

In 2012, Google named its Knowledge Graph as Knowledge Graph.

An Undefined Definition

Every Company/Group/Individual creates their own version of the Knowledge Graph to limit complexity and organize information into data and knowledge. For example, Google’s Knowledge Graph, Knowledge Vault, Microsoft’s Satori, Facebook’s Entities Graph, etc.

So, there is no formal definition of Knowledge Graph. In a broader perspective, a Knowledge Graph is a variant of semantic network with added constraints whose scope, structure, characteristics and even uses are not fully realized and in the process of development.

An Example of Knowledge Graph

Source: Maximilian Nickel et al. A Review of Relational Machine Learning for Knowledge Graphs: From Multi-Relational Link Prediction to Automated Knowledge Graph Construction

Why Should You Get Excited?

With every passing year, Machine Learning and Knowledge Representation Learning on Knowledge Graphs are advancing rapidly, both in scale and depth, but in different directions. On one hand, Machine Learning techniques are getting better at performing various tasks (e.g., Classification, Generation, etc.) on a variety of datasets with great precision and recall. On the other hand, Knowledge Representation brings the ability to represent entities and relations with high reliability, explainability, and reusability. Recent advances in Knowledge Representation Learning include mining logical rules from the graph.

Source: Bishan Yang et al. Embedding Entities and Relations for Learning and Inference in Knowledge Bases.

However, bringing knowledge graphs and machine learning together will systematically improve the accuracy of the systems and extend the range of machine learning capabilities. For example, results inferred from Machine Learning models will have better explainability and trustworthiness.

Below are some of the opportunities that can be availed by bringing Knowledge Graph to Machine Learning:

Data Insufficiency

Having a sufficient amount of data to train a machine learning model is very important. In the case of sparse data, Knowledge Graph can be used to augment the training data, e.g., replacing the entity name from original training data with an entity name of a similar type. This way a huge number of both positive and negative examples can be created using Knowledge Graph.

Zero-Shot Learning

Today, the main challenge with a Machine Learning model is that without a properly trained data it can not distinguish between two data points. In Machine Learning, this is considered as Zero-Shot Learning problem. This is where knowledge graphs can play a very big role. The induction from the Machine Learning model can be complemented with a deduction from the Knowledge Graph, e.g., with pictures where the type of situation did not appear in the training data.

Explainability

One of the major problems in machine learning industry is explaining the predictions made by machine learning systems. One issue is the implicit representations causing the predictions from the machine learning models. Knowledge Graph can alleviate this problem by mapping the explanations to some proper nodes in the graph and summarizing the decision-taking process.

Note: The above opportunities are explained in more detail in the seminar report of Knowledge Graphs: New Directions for Knowledge Representation on the Semantic Web (Dagstuhl Seminar 18371)

Some Use Cases

Question — Answering is one of the most used applications of Knowledge Graph. Knowledge Graphs contain a wealth of information and question answering is a good way to help end-users to more effectively and also more efficiently retrieve information from Knowledge Graphs.

is one of the most used applications of Knowledge Graph. Knowledge Graphs contain a wealth of information and question answering is a good way to help end-users to more effectively and also more efficiently retrieve information from Knowledge Graphs. Storing Information of Research is another useful application Knowledge Graph. Recently, a lot of companies are using Knowledge Graph to store information generated from various stages of research which can be used for building accessible models, risk management, process monitoring, etc.

is another useful application Knowledge Graph. Recently, a lot of companies are using Knowledge Graph to store information generated from various stages of research which can be used for building accessible models, risk management, process monitoring, etc. Netflix uses a Knowledge Graph to store a vast amount of varied information for its Recommendation System which helps in finding relationships between movies, TV shows, persons, etc. Later, these relationships can be used to predict what customers might like to watch next.

which helps in finding relationships between movies, TV shows, persons, etc. Later, these relationships can be used to predict what customers might like to watch next. Supply Chain Management is also being benefited from the use of Knowledge Graph. Companies can easily keep track of inventories of different components, personnel involved, time, etc which allows them to move items more swiftly and cost-effectively.

and many more…

Open Challenges

A coherent set of best practices, that can be applied during the creation of knowledge graphs, will be helpful in understanding and reuse of Knowledge Graphs amongst engineers, developer, and researchers. Given a set of unstructured data and Knowledge Graph, the problem of knowledge integration is to identify whether the entities mentioned in the data match with the real world entities present in Knowledge Graph. Although this problem can be solved using machine learning algorithms, the outcome of those algorithms directly depends on the quality of the training data. Given a wide variety of dataset, knowledge integration becomes quite difficult. Knowledge is not static, but constantly evolving. For example, if a Knowledge Graph keeps track of patients’ health, the data stored at a particular moment could be false for some later moment. So, how do we capture this evolving nature of knowledge? How to evaluate a Knowledge Graph? Which quality improvement (e.g., completeness, correctness, linkage, etc.) is more important?

Acknowledgments"
Modeling News Coverage with Python. Part 3: News site interactions with Google Search trends,"Fitting models of Google Search to Search Trends and News Articles

This post integrates data from a limited sample of newspaper coverage with Google Search trends to model interactions between the two. In these examples, the preliminary analysis finds news coverage useful for forecasting search trends but small and mixed results in the other direction. These examples were selected to be done with limited resources and time in a blog post, but everything’s general purpose enough that you can swap in your own topics or data and it should be enough to get you started.

This is the third post in a series of blog posts demonstrating modeling news with Python. The first post of this series looked at what time series were and interactions within a medium can be modeled. The second post looked at news coverage of a few publications and themes to forecast coverage with different methods.

Loading Articles

As before, all the necessary code for the models is provided in this post. It’s formatted to be copied and pasted into a Jupyter Notebook file. The code will acquire the article data, but you will want to follow the instructions and link to download the Google Search trend data or just download it yourself if you’re familiar with it.

We’ll start by going and pulling 60 days of articles from GDELT using a Python package (https://www.gdeltproject.org/data.html#rawdatafiles). These articles are labelled with topics, personalities, and locations. This checks if the files are already downloaded and if not, downloads them, and formats them into a pandas dataframe so that we can manipulate the series.

So now we’ll run it through a SARIMA model with parameters like we eyeballed last time. In the last post, we used the mean square error to evaluate accuracy but this time we’ll use the mean absolute percentage error (MAPE). This lets us normalize error across time series of different values. We’ll keep track of the MAPE values in the mape_df dataframe to be able to compare results.

The first model we test will use both coverage of other countries within the publication and coverage of that country in other publication to fit against as done previously to make sure everything works again.

We can then change the exogenous series and only select articles from within the same publication to fit the model with instead, saving the results to the mape_df dataframe column “WithinNewspapersOnly_predicted”.

We can compare the difference in the mape_df betweeen the two:

mape_df[""news_dif""] = mape_df[""WithinNewspapersOnly_predicted""] - mape_df[""NewspapersOnly_predicted""] for country in myCountries:

print(""{} mean hat change of {}"".format(country, mape_df[mape_df.index.map(lambda x: x.find(country) > -1)][""news_dif""].mean())) >>> dprk mean hat change of -10.75

>>> ukraine mean hat change of -1.5

>>> russia mean hat change of 6.5

>>> iran mean hat change of -3.0

>>> china mean hat change of -13.75

It looks as though using only the same publication outperforms using coverage from other publications most of the time. That being said, that’s for this dataset and a small period of coverage. Alright, so what happens with Google Search trends? In the next step we fit a model to the search trends, and then after that fit the news model to the news data and the search trends.

Google Search Trends

We first need to get the Google search data. Google publishes their search trends at https://trends.google.com. There’s lots of fun features there if you aren’t familiar with it. To speed things up, I went and searched for the trends on the relevant countries (North Korea, China, Russia, Iran, and Syria) from 8/8/2019 to 10/6/2019 at this link.

The page should look as follows:

Google Trend Search Results

Google gives search trends as percents relative to the highest searched term in the search queries as opposed to raw search numbers. Here, Ukraine received the highest amount of search result so all the other data points follow as a percent of Ukraine’s maximum search. We can download this in CSV format by clicking the down arrow to the right of the “Interest over time” label.

We can then put the CSV into our current working directory (or some other location if you want to change the code) and load it in with pandas. Note: The first row of Google’s CSV is some sort of metadata label, so we skiprows it to get it the columns formatted properly when loading.

gdata = pd.read_csv(""multiTimeline.csv"", skiprows=2)

gdata.plot()

Visual inspection shows it looks the same as the original data on the website, so it seems to have loaded properly. Now, we can format it to be the same as our news media data frame before and put them together, creating a copy of the original time_series df as original just in case we mess something up and need to go back.

gdata.columns = [x.replace("": (United States)"", """").replace(""North Korea"", ""dprk"").lower() for x in gdata.columns] gdata.day = pd.to_datetime(gdata.day) gdata.set_index(""day"", inplace=True, drop=True) original = time_series.copy() time_series = time_series.join(gdata)

And so now we can just go ahead and plot both the newspaper coverage and the Google Search trends to visually check how much they line up.

We can make a few observations to begin with. There’s a clear weekly seasonality with some of the search trends (e.g. China), but with the others (e.g. Ukraine, Iran), the seasonality isn’t as important as those big spikes that show up with increased news coverage. This variance is going to be a bit problematic because the search seems to ignore news coverage trends for DPRK, react to the second spike with Ukraine, and react to the third spike with Iran.

We can check quickly if there’s an effect from the news on search. To just establish baseline error rates, we can go ahead and run the same SARIMA models on the search results as the news sites, using only previous Google Search trends and news media coverage of the same topic for the models.

North Korea and Ukraine have the most error at this point. As we’d seen earlier, the Google search had diverged from the news results. Sure enough, the model learned from the first Ukraine spike and predicted that the search trends would spike again when they didn’t. Also, the search trends kind of ignore the news content as well for North Korea. We don’t really have anything to predict that sort of behavior in this data, so not going to bother doing decomposition but longer periods of data might let us know how normal that is.

Since total article count per day will generally be around the same for each publication, covering one country might lead to a decrease in coverage of another country (except where both are covered in the same article), so we can see if just including everything improves the model.

It looks like throwing in all the other time series improved the predictions on North Korea and Ukraine while having no effect on Russia and negative effects on Iran and China. A longer amount of time would definitely help massage out what kind of data we really want but I don’t want to get into building a database in these blog posts and it gets the point across enough.

It looks like news trends help us predict search trends. Does the reverse hold?

Adding Google Search back into the News Model

So far we’ve looked at how news predicts Google search trends. But what about the other way around?

Since we already added the Google Search trends into the time_series dataframe, we can just re-run the original code, modifying it slightly to only include newspapers and then the dataframe column save the MAPE values into .

We can check how the new model differs in accuracy from the original,

mape_df[""overall_change""] =mape_df[""WithSearch_predicted""] - mape_df[""NewspapersOnly_predicted""] for country in myCountries:

print(""{} mean hat change of {}"".format(country, mape_df[mape_df.index.map(lambda x: x.find(country) > -1)][""overall_change""].mean())) >>> dprk mean hat change of 0.25

>>> ukraine mean hat change of -2.5

>>> russia mean hat change of 7.0

>>> iran mean hat change of 5.0

>>> china mean hat change of -2.25

With these parameters and this data, adding in the Google Search trends has mixed but small impacts on the model. That’s fine though, with 60 days of data we’re not expecting great results anyway.

In Closing

We saw that sometimes the news can help predict Google Search Trends but not always. SARIMA models did worse on the topics where search trends diverged from news coverage trends, which should be expected. Without previous examples for the Ukraine spike, we wouldn’t expect the model to really guess that the coverage would diverge that much.

We did not get great results from adding the search results back into the news models. That’s fine. The parameters on the models weren’t tuned to the specifics of any of the search terms and the period of time covered is pretty short. It would be interesting to see how both the news -> search and search -> news relationships hold up over longer periods of time. Furthermore, these are pretty broad subjects. In the next post, we’ll look at the trends in the sub-themes publications cover within each of these issues."
CNN Sentiment Analysis,"CNN Sentiment Analysis

Convolutional neural networks, or CNNs, form the backbone of multiple modern computer vision systems. Image classification, object detection, semantic segmentation — all these tasks can be tackled by CNNs successfully. At first glance, it seems to be counterintuitive to use the same technique for a task as different as Natural Language Processing. This post is my attempt to explain the intuition behind this approach using the famous IMDb dataset.

After reading this post, you will:

Learn how to preprocess text using torchtext Understand the idea behind convolutions Learn how to represent text as images Build a basic CNN Sentiment Analysis model in PyTorch

Let’s get started!

Data

The IMDb dataset for binary sentiment classification contains a set of 25,000 highly polar movie reviews for training and 25,000 for testing. Luckily, it is a part of torchtext, so it is straightforward to load and pre-process it in PyTorch:

# Create an instance that turns text into tensors

TEXT = data.Field(tokenize = 'spacy', batch_first = True)

LABEL = data.LabelField(dtype = torch.float) # Load data from torchtext

train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)

train_data, valid_data = train_data.split() # Select only the most important 30000 words

MAX_VOCAB_SIZE = 30_000 # Build vocabulary

TEXT.build_vocab(train_data,

max_size = MAX_VOCAB_SIZE,

# Load pretrained embeddings

vectors = ""glove.6B.100d"",

unk_init = torch.Tensor.normal_) LABEL.build_vocab(train_data)

The data.Field class defines a datatype together with instructions for converting it to Tensor. In this case, we are using SpaCy tokenizer to segment text into individual tokens (words). After that, we build a vocabulary so that we can convert our tokens into integer numbers later. The vocabulary is constructed with all words present in our train dataset. Additionally, we load pre-trained GloVe embeddings so that we don’t need to train our own word vectors from scratch. If you’re wondering what word embeddings are, they are a form of word representation that bridges the human understanding of language to that of a machine. To learn more, read this article. Since we will be training our model in batches, we will also create data iterators that output a specific number of samples at a time:

# Create PyTorch iterators to use in training

train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(

(train_data, valid_data, test_data),

batch_size = BATCH_SIZE,

device = device)

BucketIterator is a module in torchtext that is specifically optimized to minimize the amount of padding needed while producing freshly shuffled batches for each new epoch. Now we are done with text preprocessing, so it’s time to learn more about CNNs.

Convolutions

Convolutions are sliding window functions applied to a matrix that achieve specific results (e. g., image blur, edge detection.) The sliding window is called a kernel, filter, or feature detector. The visualization shows six 3×3 kernels that multiply their values element-wise with the original matrix, then sum them up. To get the full convolution, we do this for each element by sliding the filter over the entire matrix:

CNNs are just several layers of convolutions with activation functions like ReLU that make it possible to model non-linear relationships. By applying this set of dot products, we can extract relevant information from images, starting from edges on shallower levels to identifying the entire objects on deeper levels of neural networks. Unlike traditional neural networks that simply flatten the input, CNNs can extract spatial relationships that are especially useful for image data. But how about the text?

CNNs for NLP

Remember the word embeddings we discussed above? That’s where they come into play. Images are just some points in space, just like the word vectors are. By representing each word with a vector of numbers of a specific length and stacking a bunch of words on top of each other, we get an “image.” Computer vision filters usually have the same width and height and slide over local parts of an image. In NLP, we typically use filters that slide over word embeddings — matrix rows. Therefore, filters usually have the same width as the length of the word embeddings. The height varies but is generally from 1 to 5, which corresponds to different n-grams. N-grams are just a bunch of subsequent words. By analyzing sequences, we can better understand the meaning of a sentence. For example, the word “like” alone has an opposite meaning compared to the bi-gram “don’t like”; the latter gives us a better understanding of the real meaning. In a way, by analyzing n-grams, we are capturing the spatial relationships in texts, which makes it easier for the model to understand the sentiment. The visualization below summarizes the concepts we just covered:

Source: Lopez et al. (2017) Link: https://arxiv.org/pdf/1703.03091.pdf

PyTorch Model

Let’s now build a binary CNN classifier. We will base our model on the built-in PyTorch nn.Module:

class CNN_Text(nn.Module):

''' Define network architecture and forward path. '''

def __init__(self, vocab_size,

vector_size, n_filters,

filter_sizes, output_dim,

dropout, pad_idx):



super().__init__()



# Create word embeddings from the input words

self.embedding = nn.Embedding(vocab_size, vector_size,

padding_idx = pad_idx)



# Specify convolutions with filters of different sizes (fs)

self.convs = nn.ModuleList([nn.Conv2d(in_channels = 1,

out_channels = n_filters,

kernel_size = (fs, vector_size))

for fs in filter_sizes])



# Add a fully connected layer for final predicitons

self.linear = nn.Linear(len(filter_sizes) \

* n_filters, output_dim)



# Drop some of the nodes to increase robustness in training

self.dropout = nn.Dropout(dropout)







def forward(self, text):

'''Forward path of the network.'''

# Get word embeddings and formt them for convolutions

embedded = self.embedding(text).unsqueeze(1)



# Perform convolutions and apply activation functions

conved = [F.relu(conv(embedded)).squeeze(3)

for conv in self.convs]



# Pooling layer to reduce dimensionality

pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2)

for conv in conved]



# Dropout layer

cat = self.dropout(torch.cat(pooled, dim = 1))

return self.linear(cat)

In the init function, we specify different layer types: embedding, convolution, dropout, and linear. All these layers are integrated into PyTorch and are very easy to use. The only tricky part is calculating the correct number of dimensions. In the case of the linear layer, it will be equal to the number of filters you use (I use 100, but you can pick any other number) multiplied by the number of different filter sizes (5 in my case.) We can think of the weights of this linear layer as “weighting up the evidence” from each of the 500 n-grams. The forward function specifies the order in which these layers should be applied. Notice that we also use max-pooling layers. The idea behind max-pooling is that the maximum value is the “most important” feature for determining the sentiment of the review, which corresponds to the “most important” n-gram is identified through backpropagation. Max-pooling is also useful for reducing the number of parameters and computations in the network.

Once we specified our network architecture, let’s load the pre-trained GloVe embeddings we imported before:

# Initialize weights with pre-trained embeddings

model.embedding.weight.data.copy_(TEXT.vocab.vectors) # Zero the initial weights of the UNKnown and padding tokens.

UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token] # The string token used as padding. Default: “<pad>”.

PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token] model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)

model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)

model = model.to(device)

The second part of this code chunk sets the unknown vectors (the ones that are not present in the vocabulary) and the padding vectors (used in case the input size is smaller than the height of the largest filter) to zeros. We’re now ready to train and evaluate our model.

You can find the full training and evaluation code in this notebook:

Before training the model, we need to specify the network optimizer and the loss function. Adam and binary cross-entropy are popular choices for classification problems. To train our model, we get the model predictions, calculate how accurate they are using the loss function, and backpropagate through the network to optimize weights before the next run. We perform all these actions in the model.train() mode. To evaluate the model, don’t forget to turn the model.eval() mode on to make sure we’re not dropping half of the nodes with the dropout (while improving the robustness in the training phase, it will hurt during evaluation). We also don’t need to calculate the gradient in the evaluation phase so that we can turn it off with the help of the torch.no_grad() mode.

After training the model for several epochs (use GPU to speed it up), I got the following losses and accuracies:

Losses and Accuracies

The graph indicates signs of overfitting since both training loss and accuracy keep improving while the validation loss and accuracy get worse. To avoid using the overfitted model, we only save the model in case the validation loss increased. In this case, the validation loss was the highest after the third epoch. In the training loop, this part looks as follows:

if valid_loss < best_valid_loss:

best_valid_loss = valid_loss

torch.save(model.state_dict(), 'CNN-model.pt')

The performance of this model on the previously unseen test set is quite good: 85.43%. Finally, let’s predict the sentiment of some polar reviews using the CNN-model. To do so, we need to write a function that tokenizes user input and turns it into a tensor. After that, we get predictions using the model we just trained:

def sentiment(model, sentence, min_len = 5):

'''Predict user-defined review sentiment.'''

model.eval()

tokenized = [tok.text for tok in nlp.tokenizer(sentence)]

if len(tokenized) < min_len:

tokenized += ['<pad>'] * (min_len - len(tokenized))

# Map words to word embeddings

indexed = [TEXT.vocab.stoi[t] for t in tokenized]

tensor = torch.LongTensor(indexed).to(device)

tensor = tensor.unsqueeze(0)

# Get predicitons

prediction = torch.sigmoid(model(tensor))

return prediction.item()

In the original dataset, we have labels “pos” and “negs” that got mapped to 0 and 1, respectively. Let’s see how well our model performs on positive, negative, and neutral reviews:

reviews = ['This is the best movie I have ever watched!',

'This is an okay movie',

'This was a waste of time! I hated this movie.']

scores = [sentiment(model, review) for review in reviews]

The model predictions are 0.007, 0.493, and 0.971 respectively, which is pretty good! Let’s try some tricker examples:

tricky_reviews = ['This is not the best movie I have ever watched!',

'Some would say it is an okay movie, but I found it terrific.',

'This was a waste of time! I did not like this movie.']

scores = [sentiment(model, review) for review in tricky_reviews]

scores

Unfortunately, since the model has been trained on polar reviews, it finds it quite hard to classify tricky statements. For example, the first tricky review got a score of 0.05, which is quite confident ‘yes’ even though negation is present in the sentence. Try playing around with different n-grams to see whether some of them are more important then others, maybe a model with bi-grams and 3-grams would perform better than a combination of different n-grams we used.

Table of reviews and their sentiment scores

Conclusion

In this post, we went through the concept of convolutions and discussed how they can be used to work with text. We also learned how to preprocess datasets from PyTorch and built a binary classification model for sentiment analysis. Despite being fooled by tricky examples, the model performs quite well. I hope you enjoyed reading this post and feel free to reach out to me if you have any questions!

References

Britz, D. (2015). Understanding Convolutional Neural Networks for NLP. Retrieved from: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/

Lopez, M. M., & Kalita, J. (2017). Deep Learning applied to NLP. arXiv preprint arXiv:1703.03091. Retrieved from: https://arxiv.org/pdf/1703.03091.pdf

Trevett, B. (2019). Convolutional Sentiment Analysis. Retrieved from: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb"
Leveraging Data for Social Good — A Practical Example,"Leveraging Data for Social Good — A Practical Example

The story of how a five-membered team used empirical data to do good for a village tucked away in a rural corner of Northern India Ramshankar Yadhunath · Follow Published in Towards Data Science · 17 min read · Dec 30, 2019 -- 1 Share

Using Data Science to make common lives better

NOTE: All terminology and theory that has been the backbone of our team’s studies on this project are attributed to the preparatory workshop conducted by the Live in Labs Team at Amrita Vishwa Vidyapeetham, India.

In the summer of 2018, I had an opportunity to be a part of a team that visited a highly backward village in Northern India. This visit was a part of our curriculum at the college — The Live in Labs Program(Read more about it here) and our task for the program was as follows :

Identify a Key Indicator in the village that needed immediate attention, validate our find and propose a solution that would help improve the state of the same.

Fig 1. Key Indicators of a village

Through this blog post, I shall provide a gist of our work at the village and elucidate how I was able to put to use, my love for data science in doing social good.

All the code that I have written for this project can be found at https://github.com/ry05/Live-in-Labs-Analysis.

The Village — Basic Information

Some preliminary information about the village that we visited is as given below :

Official Name of the Village : New Thariyal

New Thariyal Location : Northern Punjab, India

Northern Punjab, India Number of Houses in the Village : 58

58 Number of People in the Village : 300+ occupants

300+ occupants Major Occupation of the People : Unskilled and seasonal labour

The Resource Map

A resource map is a rough sketch of the village’s geographical landscape with two major goals :"
Making 3 Easy Maps With Python,"In working with geospatial data, I’ve often needed to visualize this data in the most natural way possible: a map. Wouldn’t it be nice if we could use Python to quickly and easily create interactive maps of your data? We’ll be using a data set on all Starbucks locations in Los Angeles County for this tutorial. By the end of this introductory post you will be able to create:

A basic point map of all Starbucks locations in LA County

of all Starbucks locations in LA County A choropleth map which shades in zip codes in LA County based on how many Starbucks are contained in each one

which shades in zip codes in LA County based on how many Starbucks are contained in each one A heatmap which highlights “hotspots” of Starbucks in LA County

Let’s do it!

You will need …

The Python package pandas. This is used to easily manipulate data in Python

This is used to easily manipulate data in Python The Python package folium . This is used to very easily create maps

. This is used to very easily create maps A spreadsheet with latitude/longitude of Starbucks in LA County (on my GitHub here )

) A GeoJSON (basically a json which describes a complex shape) of LA County (here) and a GeoJSON of all the zip codes in LA County (here)

To get familiar with the data, here’s a snapshot of the first few rows:

We only need to worry about the latitude, longitude, and zip fields for this analysis.

Here are the needed Python imports, loading the Starbucks data, and loading the LA County GeoJSON:"
How to Make A.I. That Looks into Trade Charts (And Use It for Trading),"We are living in a world most of the things are increasingly depending on computer vision and deep learning. From tagging your summer photos automatically to facial detection by security cameras, it feels like we are living in a dystopian future.

While AI revolution is still happening around us, spring of 2019 was interesting times for me. After finishing a deep learning course, I began tinkering with many different use cases of deep learning such as image classification to Natural Language Processing (NLP). After spending around a couple hours using Python and Keras libraries, I trained a simple Convolutional Neural Network (CNN) which was able to distinguish between a cat and a dog image. Sounds simple enough, some years ago that was a huge task to do and I was having a hard time to believe how simple neural networks solved a complex problem! Normally if you want to do image recognition using CV libraries, you have to do feature engineering, develop your own filters and hard code many features into the code. Even after many attempts, you would be left with an algorithm maybe %60–70 accurate which is far away from what we are able to do with machine learning today.

Deep Learning Methods Looks into Pictures as Matrices

I was completely blown away by the simplicity of deep learning. First I defined a very simple CNN architecture, then labelled my dataset with cat and dog images. After that start the training and watch training accuracy and validation accuracy to go up until a satisfactory metric is reached. That’s it!! Next up, load your model and weights file then run the model.predict command with the file you want to predict and boom! Result is there with the accuracy score!

The simplicity and the accuracy of the deep learning was just beautiful!

Around the same time, I developed an interest over economics and how this day trading thing works. I started reading the Life and Work Principles by Ray Dalio. (If you haven’t read it yet, I can only recommend it )"
Trust and interpretability in machine learning,"Do machine learning models always need to be interpretable? Given a choice between an interpretable model that is inaccurate and a non-interpretable model that is accurate, wouldn’t you rather choose the non-interpretable but accurate model? In other words, is there any reason for sacrificing accuracy at the altar of interpretability?

Before going any further we should try to clarify what makes a model interpretable. Often, interpretability is equated with simplicity. This definition is obviously ambiguous; what is simple for one person may not be so for another. More importantly, advanced machine learning is most useful for modeling complex systems. These systems are called complex for a reason — they are not simple! Demanding that useful models for such systems should be simple, by any measure, makes very little sense.

Perhaps, we can go further if we consider how we think about interpretability in a traditional modeling setting. Consider the mathematical model of an internal combustion engine in a car. I doubt if anyone would consider that to be simple. But, on the other hand, I also doubt if anyone would consider the model of an internal combustion engine to be non-interpretable, either. This is primarily because we can derive this model in a deductive manner from well established physical theories such as thermodynamics and fluid dynamics.

Why not use this as our definition of interpretability? A model should be considered to be interpretable if it can be derived (or at least motivated) from a trustworthy theory. This definition of interpretability serves the dual purpose of understanding and trust. It helps us understand the model because we tend to understand things in a deductive manner — by going from the known to the unknown. Also, with such a definition, the trust in the model is derived from the trust that we place in the underlying theory.

Indeed, there are situations where both understanding and trust are necessary — scenarios where we are interested in determining the causal factors behind the behavior of a system. In such scenarios, we must insist that the corresponding models must be interpretable according to the above definition. Most models in the realm of physical sciences belong to this category. One can argue that purely inductive blackbox models are not suitable for such scenarios.

However, there are many other situations where understanding might be nice to have, but by no means is it a must have. In these situations what really matters is the ability to make trustworthy predictions. In these situations, if we could provide an alternate source of trust, then our models need not be bound by the definition of interpretability given above. This is a common argument, and there is merit to it. Remember, machine learning is a way of systematically building models from (preferably) large amounts of data using inductive reasoning. Constraining these models to be interpretable in a deductive manner can seriously limit their accuracy.

So then the question becomes how can we generate trust in a blackbox model where we have little to no insight into its inner workings. A credible basis for trust could be testing. After all, testing forms the basis of our trust in regular software. But to test a model we need be able to formalize our expectations about it. If we could formalize our expectations completely then that would correspond to a complete specification of the model itself. In that case, we would not really need machine learning or any other modeling methodology. What we really need to be able to do is to formalize our expectations about the aspects of the model that we consider important. This is not easy either, because many of the concepts that we care about, such as fairness, do not lend themselves to a convenient mathematical treatment.

It is worth pointing out that significant progress has been made in developing testing methodologies for testing machine learning models. I personally find the idea of using metamorphic relations for formalizing expectations to be particularly promising. But, we are still a long way from having concrete methodologies that will allow us to perform comprehensive testing of blackbox models, and this inability of ours contributes to a trust deficit in blackbox models.

One could question the efficacy of such expectation-based comprehensive testing. After all, the goal of machine learning is to find undiscovered patterns in data. By insisting that the models meet our expectations amounts to pre-defining the model, which defeats the whole purpose. Following this line of reasoning, one would argue that as long as the data is representative and our algorithms are powerful enough to capture the patterns, there is little reason not to trust the model — we should expect the model results to generalize to the overall population, and the extent to which we should expect them to generalize is encapsulated in the model’s performance (accuracy) scores.

Thus, in essence we are asked to delegate our trust to the trifecta of data, algorithms and performance scores. We first need to dissuade ourselves from the notion that a single performance (accuracy) score can form sufficient basis for trusting the model. A performance score is usually a point estimate of how a model is expected to generalize on an average over a population given the current data. Trust, on the other hand, is a nuanced multidimensional concept that cannot be encapsulated in such a single coarse grained score. One can imagine defining more granular performance scores— e.g. by population segments. But, that would require a certain level of understanding of the population and determining what we consider important — this is not very different from forming expectations.

Let us examine the data aspect of this argument. It is, indeed, quite easy to convince oneself that if the data is representative of the population we are interested in, then it should contain all the relevant patterns and no spurious ones. Unfortunately, that is rarely the case. The degree to which the data can be non-representative depends quite acutely on the situation. Nonetheless, we can identify certain high level scenarios.

In the first scenario, we would have a good understanding of the population and complete control over the data collection mechanism. In this scenario, we can choose our data to be representative, and with a high degree of confidence we can expect our resulting model’s predictions to be applicable to the overall population. However, note that having a good enough understanding of the population to be able to draw a representative sample for the task at hand means that we already have some understanding of which features are important for the prediction. Hence, in this case it is debatable if blackbox models are terribly useful. Opinion polling for predicting election results is a good example of this scenario.

In the second scenario, we do not have complete control over the data collection, but our predictions do not affect the data collected. In this scenario, if we assume that the data collection mechanism is unbiased then were we to wait long enough, we would have a representative sample of the population. Of course, there are a lot of ifs and buts that go with this assumption. Firstly, one does not know how long is long enough. Thus one needs to assume that the time scale over which the data is collected is long enough to produce a representative sample. Furthermore, the population itself might change in the meantime. Thus, an additional assumption is that the time scale over which population changes is much longer than the time scale over which a representative sample is generated. As long as we can justify those assumptions, then the estimated performance will be reliable. A model for predicting the stock prices is an example of such a scenario — as long as we are not making investments that are large enough to tip the market as a whole, the decisions that we make as the result of the predictions should not affect the stock prices.

The third scenario is one where the data collection is impacted by the predictions, but we have a moderate to high risk appetite for wrong predictions. An example of this is a product recommender system. The model for a recommender systems will be trained on data consisting of ordered lists of products that different users have bought/clicked on. Based on this data the model will predict what a user is most likely to buy/click on and based on the model’s predictions the system will decide what the user gets to see, which limits what (s)he can buy/click-on. Thus the prediction biases the data collection. In product recommender systems, one can circumvent this problem, somewhat, by keeping an exploration budget — for a fraction of the cases the system shows the user a random set of products regardless of the prediction of the model. The observations resulting from these randomized predictions can then be used to estimate the performance of the model. One still has to address the concerns of the aforementioned second scenario in order to access the reliability of these estimates.

In the fourth and final scenario, the data collection is impacted by the predictions, but we have little to no risk appetite for wrong predictions. For example, suppose we have to build a model to predict whether someone will default on their mortgage loan payments. The mortgage loan will be approved or not based on the prediction. If the prediction is that the person will default, then the loan will not be approved, and in that case there is no way of knowing whether this person would have actually defaulted or not. It is difficult to imagine a situation where an institution would randomly approve (or otherwise) a loan for the sake of data exploration. In these situations, it is very difficult to gauge the reliability of the estimated performance of the resulting model without additional information.

Thus, it is not such a great idea to blindly expect the data to be representative of the population. In most scenarios, given the constraints of the problem at hand, it simply might not be possible to get an unbiased representative sample. Understanding the limitations of one’s data collection mechanism, being able to deduce the implications of those limitations, and having the honesty to report those as a part of the model’s results goes a long way in building trust.

Let us now consider the algorithm aspect of the argument. It is a widespread belief that the more flexible an algorithm is the better it is, because flexibility equips an algorithm to capture more complex patterns. But if the history of the actual successful applications of machine learning are anything to go by, then this belief would appear to be utterly misplaced. In computer vision, success came when we were able to encode the symmetries in pictures into models in the form of convolutional neural networks. In natural language processing we are now able to build extremely accurate cross-purpose language models because we could encode our knowledge about languages, including structure and word context, into these models. In recommender systems — most collaborative filtering algorithms including matrix factorization methods, make strong assumptions about the affinity of a user towards an item.

Whether we would like to slap the label of interpretability on these models or not, it is an objective fact that we build better models when we understand the domain and the context in which the model needs to operate. The best models do not come from the most flexible algorithms, they come from algorithms that are well constrained by domain knowledge and have just the right amount of flexibility to capture the relevant patterns in the data.

We have seen the word understanding being used quite a few times in the above discussion. What we should have realized by now is that it is difficult to build trust without understanding. In the end, it boils down to how one perceives machine learning. Yes, machine learning is an incredibly powerful inductive modeling technique. When combined with big data and big compute, it allows us to model systems and solve problems that were previously out of our reach. But the entry of machine learning should not imply the exit of everything else, including common sense. Machine learning is one element in the wider modeling family that includes deductive modeling as well as domain knowledge. The better we understand and leverage the interconnections between these elements, the further we will go towards robust complex system modeling.

Trust is contextual and trust can have multiple sources, but eventually it flows from knowledge and integrity; specifically in our trust in the knowledge and integrity of the individuals who are building the models. Trust as well as adoption of models will come, in my opinion, only when the wider audience is convinced that the modelers have the knowledge to understand the limitations of their models (machine learning or otherwise), and the integrity to report them."
Why is Boosting Fitting Residual,"Boosting Explained

Boosting or Forward Stagewise Additive Modeling is an ensemble learning method that combines many weak learners to form a collection that acts like a strong learner. It can generically described as follow.

Boosting algorithm (Source: ESLII)

Before explain every steps in detail, let’s clarify the symbols.

x, y: training data and actual value

fᵢ(x): collection of weak learners at i iteration

M: number of trees to add

N: number of training data points

β: expansion coefficient or the ‘weight’ of subsequent weak learner in the collection

b(x; γ): weak learner function characterized by a set of parameters γ

L(y, ŷ): loss function

Now let’s take a look at each step.

Initialize the base collection to predict for 0 for all the training data For every subsequent weak learners adding into the collection:

a) Find the optimal expansion coefficient [β] and set of parameters [γ] that minimize the loss function over the training data. Note that the loss function takes in the actual value and the sum of outputs from both previous collection [fₘ₋₁(x)] and current weak learner function [b(x; γ)]

b) After finding the optimal β and γ, add the coefficient with the weak learner to the collection

The most important step is 2a, which searches for the optimal weak learner function to add to the collection. When squared error is chosen as loss function, formula in 2a can be rewritten as:

Square Error Loss Function (Source: ESLII)

In this expression, fₘ₋₁ represents predicted value from the previous trees. yᵢ represents actual value. yᵢ - fₘ₋₁ will output residual from the previous learners, rᵢₘ. Thus, to minimize the square losses, every new weak learner will fit the residual.

For other common loss functions for regression problems, for example, absolute error and Huber loss, this statement still holds as residual is always computed.

So, what about classification problems?

For a 2-class classification problem, AdaBoost, a famous boosting algorithm introduced by Freund and Schapire in 1997, uses exponential loss. At every iteration, training samples will be re-weighted based on prediction errors made by previous trees. Incorrectly classified samples will be given higher weight. Subsequent tree will fit the re-weighted training sample. Although new tree does not fit on the prediction error or the deviation directly, prediction error plays an important part in fitting new trees.

Exponential Loss (Source: ESLII)

Thanks for reading!"
Python vs Excel — Compound Annual Growth Rate (CAGR),"One of my greatest frustrations with Microsoft Excel (or Google Sheets) is the lack of an inbuilt function to calculate the compound annual growth rate or CAGR (XIRR is the closest but it’s not the same). This means that in every case where I needed to conduct a quick Excel CAGR analysis, I would need to write the Excel formula for CAGR.

Every. Single. Time.

If you don’t know already, the Excel formula for CAGR is as follows:

= (End Value / Start Value) ^ (1 / Number of Years) -1

where the End Value is the latest value of the given time period, the Start Value is the first value of the given time period, and the Number of Years is the number of years over which you would want to calculate the annual growth rate.

In an Excel sheet it may look something like this:

CAGR formula to calculate growth rate between 2010 and 2018

It’s a rather simple formula that can be easily be relied upon… except when the table grows longer with more years!

Just add one more year, and you now need to specify the correct cells for the formula again

Datasets tend to grow, and this presents a problem for an analyst who has to ensure formulas are all correct at all times! (I’ve been there. I had to fix a much more complicated formula for a much bigger dataset, correcting a mistake that has gone unnoticed for an entire year because the formula was buried beneath the sea of data!)

This frustration is a reason why I’ve moved from spreadsheet to programming (specifically Python with the Pandas library) for data analysis. I’m still relatively new to Python but already I’m experiencing its tremendous versatility and efficiency over Excel (like not having to wait a few hours for an Excel sheet to fill all missing values).

Here you’ll see why it’s much better and more efficient to program the CAGR function and apply it to a dataframe converted from a spreadsheet.

Let’s start by defining the CAGR function in Python:

def cagr(start_value, end_value, num_periods): return (end_value / start_value) ** (1 / (num_periods - 1)) - 1

And that’s it! Two lines of code to have your very own CAGR function. You will notice here that there is a slight difference with the Excel function. In this function, there is a minus 1 after num_periods. This allows me to correctly calculate the CAGR by specifying the total number of time periods as an argument rather than the number of compounding periods (which is always minus 1 of the total). I do this because I have, in the past, made repeated mistakes of incorrectly specifying the number of compounding periods on Excel.

Now that you have your CAGR function, you can either save it into a separate Python file (from which you can import into any other Python file) or just code it into the file where you will be loading the Excel table into a dataframe and apply it there.

To apply your CAGR function, first import the Pandas library:

import pandas as pd

Pandas is an open-source, easy to use Python library that can convert any csv or Excel file into a dataframe for data analysis. It’s a must-have for any data analyst programming in Python.

Next, load the Excel into a Pandas dataframe:

ExcelFile = 'ExcelTable.xlsx' #name of my Excel file

df = pd.read_excel(ExcelFile, index_col='Year')

Notice that I’ve added a parameter index_col to assign the ‘Year’ column as the index, leaving ‘Yearly Income (RM)’ as the only column in the dataframe.

Do a print function on your dataframe to see if the Excel table was successfully converted into a dataframe:

print(df)

Looks like it came out right

You now have a dataframe to which you can apply the CAGR function.

The keys to the CAGR function in Python, and why it is much easier to use in Python, are the variables below:

start_value = float(df.iloc[0])

end_value = float(df.iloc[-1])

num_periods = len(df)

These variables will extract the arguments you need for your CAGR function, and in this case, the output returns:

36000.00

102000.00

9

The great thing about these variables is that the code does not have to change even as your dataset grows. Using the .iloc method together with list indexing ensures that the function will always use the first row as the start value and the last row as the end value, and the len function will always count correctly the total number of time periods.

Even if you add an extra year(s), the code works the same. Say for example, you add a row to the dataframe:

df.loc[2019] =[84000]

Running a print function on the same variables will return a different output in accordance with the new data:

36000.00

84000.00

10

If you want to specify a time period, say 5 years between 2012 and 2016, for the CAGR calculation, you can also use the .loc method to easily do so:

start_value = float(df.loc[2012])

end_value = float(df.loc[2016])

num_periods = len(df.loc[2012:2016])

Slicing and dicing

Now let’s try to apply the CAGR function to this sliced dataset by using the variables as input arguments:

result = cagr(start_value, end_value, num_periods)

print(result) 0.12801507993497308

But wait! The output shows the result in a float type with too many decimals. Though I personally prefer this kind of output, it is generally not friendly to many eyes. We will need string formatting to present the output in percentages:

print(""{:.2%}"".format(result)) 12.80%

Much more human-friendly.

And that’s all you need to perform CAGR analysis in Python!"
Using the Pandas “Resample” Function,"Member-only story Using the Pandas “Resample” Function

The next best thing to changing the past — aggregating it. A technical introduction to the pandas resample function. Jeremy Chow · Follow Published in Towards Data Science · 8 min read · Sep 11, 2019 -- 4 Share

This article is an introductory dive into the technical aspects of the pandas resample function for datetime manipulation. I hope it serves as a readable source of pseudo-documentation for those less inclined to digging through the pandas source code!

If you’d like to check out the code used to generate the examples and see more examples that weren’t included in this article, follow the link here.

Let’s get started!

What is resampling?

Resampling is necessary when you’re given a data set recorded in some time interval and you want to change the time interval to something else. For example, you could aggregate monthly data into yearly data, or you could upsample hourly data into minute-by-minute data.

The syntax of resample is fairly straightforward:

<DataFrame or Series>.resample(arguments).<aggregate function>

I’ll dive into what the arguments are and how to use them, but first here’s a basic, out-of-the-box demonstration. You will need a datetime type index or column to do the following:"
Machine Learning Models as Micro Services in Docker,"One of the biggest underrated challenges in machine learning development is the deployment of the trained models in production that too in a scalable way. One joke on it I have read is “Most common way, Machine Learning gets deployed today is powerpoint slides :)”.

Why Docker?

Docker is a containerization platform which packages an application & all its dependencies into a container.

Activating this container results in the application being active.

Docker is used when you have a lot of services which work in an isolated manner and serve as a data provider to a web application. Depending on the load, the instances can be spun off on demand on the basis of the rules set up.

Why Docker for Machine Learning models?

Production deployment of regular software applications is hard. If that software is a Machine Learning pipeline, it’s worse! And in today’s scenario, you can’t get away from machine learning, as it is the most competitive edge you can get in the business. In production, a Machine Learning powered application would be using several models for several purposes. Some major practical challenges in Machine Learning models deployment that can be handled through docker are:

Ununiform environments across models.

There can be cases where for one model you need LANG_LEVEL set to ‘c’ while for another LANG_LEVEL should be ‘en_us.UTF-8’. Put different models in different containers so that isolated environments for different models will be obtained.

2. Ununiform library requirements across models.

You have developed a text summarizer using tensorflow 1.10. Now we want to have a sentiment analysis using transfer learning which is supported by tensorflow2.0(suppose). Putting them in different containers will not break the app.

Another major use case is, you develop ML models in python. But the application you want to make in Go language (for some technical advantages), then exposing the ml model through docker to the app will solve it.

3. Ununiform resource requirements across models.

You have a very complex object detection model which requires GPU, and you have 5 different neural networks for other purposes which are good to run on CPU. Then on deploying the models in containers, you get the flexibility of assigning resources as per requirement.

4. Ununiform traffics across models.

Suppose you have a question identifier model and answer generation mode.w The former is called frequently while the latter one is not called that frequent. Then you need more instances of question identifier than answer generator. This can be easily handled by docker.

Another scenario is, at 10 am you have 10000 requests for your model whereas at 8 pm it is only 100. So you need to spin off more serving instances as per your requirements, which is easier in docker.

5. Scaling at model level

Suppose you have a statistical model which serves 100000 requests per second, whereas a deep learning model capable of serving 100 requests per second. Then for 10000 requests, you need to scale up only the deep learning model. This can be done by docker.

Now let’s see how to create a container of a deep learning model. Here the model I have built is a question topic identifier on the question classifier dataset available at http://cogcomp.org/Data/QA/QC/. Google’s Universal Sentence Encoder is used for word embedding.

While creating a container for a model, the workflow normally has to be followed is:

Build and train the model. Create an API of the model. (Here we have put it in a flask API). Create the requirements file containing all the required libraries. Create the docker file with necessary environment setup and start-up operations. Build the docker image. Now run the container and dance as you are done :)

Build and train the model.

To build and train the model, a basic workflow is to get the data, do the cleaning and processing of the data and then fed the data to the model architecture to get a trained model.

For example, I have built a question intent classifier model on the TREC dataset available at http://cogcomp.org/Data/QA/QC/. The training data has 6 intents with the number of instances of each is as follows:

Counter({'DESC': 1162,

'ENTY': 1250,

'ABBR': 86,

'HUM': 1223,

'NUM': 896,

'LOC': 835})

The model creation can be seen at https://github.com/sambit9238/QuestionTopicAnalysis/blob/master/question_topic.ipynb

The processing steps followed here are:

Dealing with contractions like I‘ll, I‘ve etc. Dealing with hyperlinks, mail addresses etc. Dealing with numbers and ids. Dealing with punctuations.

For embedding, Google's universal sentence encoder is used from tensorflow-hub.

The model architecture followed is a neural network with 2 hidden layers each with 256 neurons. To avoid overfitting, L2 regularization is used.

Layer (type) Output Shape Param #

=================================================================

input_1 (InputLayer) (None, 1) 0

_________________________________________________________________

lambda_1 (Lambda) (None, 512) 0

_________________________________________________________________

dense_1 (Dense) (None, 256) 131328

_________________________________________________________________

dense_2 (Dense) (None, 256) 65792

_________________________________________________________________

dense_3 (Dense) (None, 6) 1542

=================================================================

Total params: 198,662

Trainable params: 198,662

Non-trainable params: 0

_________________________________

The model is stored in .h5 file for reuse. Label encoder is stored in the pickle file for reuse.

Create an API of the model. (Here we have put it in a flask API).

The stored model is put in Flask api so that it can be used in production (https://github.com/sambit9238/QuestionTopicAnalysis/blob/master/docker_question_topic/app.py.)

The API expects a list of texts, as multiple sentences will come while using in real time. It goes through cleaning and processing to fed for prediction. The predicted results are scaled to represent the confidence percentage of each intent. The scaled results are then sent in JSON format.

For example,

input: [ “What is your salary?”]

output: {‘ABBR’: 0.0012655753, ‘DESC’: 0.0079659065, ‘ENTY’: 0.011016952, ‘HUM’: 0.028764706, ‘LOC’: 0.013653239, ‘NUM’: 0.93733364}

That means the model is 93% confident that the answer should be a number for this question.

Create the requirements file containing all the required libraries.

To create a Docker image to serve our API, we need to create a requirement file with all the used libraries along with their versions.

Create the Dockerfile with necessary environment setup and start-up operations.

A Dockerfile is a text document that contains all the commands a user could call on the command line to assemble an image.

For our example, the pre-built python-3.6 image is taken as a base image. Then the pre-trained Universal Sentence Encoder model files have been downloaded followed by the installation of required libraries. The 5000 port of docker is exposed, this is the port where flask app will run as it is in the default configuration.

Build the docker image. and run the container

Now we have the Dockerfile, flask API and trained model files in a directory. Hence we need to create the docker image out of it. The command for that can be:

docker build -t question_topic .

#the last option is location of the directory. Since I am in the directory so '.' is put, which represents current directory in unix.

Now the docker image is created, we need to run the image in a container docker run -p 8888:5000 --name question_topic question_topic

It will make the created docker image run. The port 5000 in the docker is mapped to 8888 port of host machine. Hence, the API will receive and response requests at port 8888. If you want to run the docker in the background and detach it from the command prompt (which will be the case in real time), run it with ‘-d’ option.

To check the outputs of the docker let’s send a post request using curl.

input:

To supply the input, curl — request POST

— url http://0.0.0.0:8888/predict_topic

— header ‘content-type: application/json’

— data ‘{“rawtext_list”:[“Where do you work now?”, “What is your salary?”]}’

output:

{

“input”: “[‘Where do you work now?’, ‘What is your salary?’]”,

“output”: “[ {‘ABBR’: 0.0033528977, ‘DESC’: 0.0013749895, ‘ENTY’: 0.0068545835, ‘HUM’: 0.7283039, ‘LOC’: 0.25804028, ‘NUM’: 0.0020733867},

{‘ABBR’: 0.0012655753, ‘DESC’: 0.0079659065, ‘ENTY’: 0.011016952, ‘HUM’: 0.028764706, ‘LOC’: 0.013653239, ‘NUM’: 0.93733364} ]”

}

It seems the docker is running fine :)

Notes:

The mentioned example is not production ready. But it can be production ready by following few things like:"
Integrating Python & Tableau,"Integrating Python & Tableau

When performing in-depth analyses on large and unstructured datasets, the power of Python and relevant machine learning libraries cannot be overstated. Matplotlib serves as a great tool to help us visualize results, but it’s stylization options are not always optimal for use in presentations and dashboards. Sure, you can make it work, but visualization software like Tableau is much more suited to helping you tell your story.

To illustrate this, consider a simple example from the King County Housing Dataset. In your exploratory analysis, you are likely to find that location is a strong predictor of price, but you want to know whether to use zip code or GPS coordinates as a predictor. By creating a scatter plot with latitude and longitude coordinates, creating a color map based on price, we can get a sense of where prices are highest.

ax = housing_data.plot.scatter('long', 'lat',

c = 'log_price',

s = housing_data['log_price'],

colormap = 'magma',

figsize=(15, 10),

title='Location-Price Map')

ax.set(xlabel='Longitude', ylabel='Latitude')

plt.show()

Based on this, we get a sense that the relationship between latitude and longitude with price is somewhat parabolic, and from here, you can decide how you want to move forward with the analysis. However, if you wanted to give senior managers a clearer idea of where prices are highest, this visualization wouldn’t be ideal. Using Tableau, we can create something like this:

Can something like this be created with libraries in Python? I would assume so, but software dedicated to data visualization can help you get those results with much more efficiency. What’s more, this software makes it easier to compare different styles with much greater…"
Correlation does not imply causation,"Correlation does not imply causation

Photo by Sajjad Zabihi on Unsplash

Smoking cigarettes, as we know today, causes lung cancer. However, that fact was not entirely clear in the 1950s, when the first studies showing a correlation between smoking and lung cancer were published. One of the skeptics was statistician R.A. Fisher, who reasoned that the causality could be the other way around:

“Is it possible then, that lung cancer — that is to say, the pre-cancerous condition which must exist and is known to exist for years in those who are going to show over lung cancer — is one of the causes of smoking cigarettes? I don’t think it can be excluded.”

To be clear, Fisher was not only a statistician but also a heavy smoker, so his view was probably biased. Nevertheless he had a point: correlations alone are not enough to establish causation. What else can explain correlation?

Cigarettes cause lung cancer

Generally, if two variables A and B are correlated, there are at least four possible explanations:

A causes B B causes A A and B are both caused by a third variable, C. Chance (the correlation is spurious).

So how was the causal link between cigarettes and lung cancer established? In the 50s and 60s a large number of studies came out that confirmed the correlation. Furthermore, studies also showed that heavier smokers suffered more cancer than lighter smokers, and that pipe smokers developed more lip cancer while cigarette smokers developed more lung cancer. All that evidence taken together made the case clear. In 1964, the U.S. surgeon general, Luther Terry, made the causal connection official:

“In view of the continuing and mounting evidence from many sources, it is the judgment of the Committee that cigarette smoking contributes substantially to mortality from certain specific diseases and to the overall death rate.”

Smoking, Terry concluded, is a health hazard.

Correlation caused by a third variable

Sometimes correlations appear between two variables simply because both of them are caused by a third, unobserved…"
Getting started with neo4j in 10 minutes,"Getting started with neo4j in 10 minutes

In this article we’ll create our first graph database with neo4j.

After installing neo4j, we’ll explore 2 ways to create the database: Félix Revert · Follow Published in Towards Data Science · 5 min read · Sep 11, 2019 -- 7 Listen Share

manually typing CYPHER queries

generating CYPHER queries using an Excel formula. This is very useful if you start from an Excel file

Before diving into the code, let’s first understand what is neo4j and why it’s important to know this technology.

What’s neo4j?

Screenshot of neo4j browser (source: neo4j website)

To put it in a simple way, neo4j is the mySQL of the graph databases. It provides a graph database management system, a language to query the database, a.k.a CYPHER, and a visual interface with the neo4j browser.

Neo4j is an open source project, maintained by a private company. It comes with a free version that we will use here. Note that I don’t work or receive anything from them.

For me the main advantages of neo4j lie in the following:

The free version is great for prototyping. I’ve never needed the premium version

The neo4j browser gives you a lot of freedom to customize visualizations. It can be used by non data scientist people

The language CYPHER, which is very similar to SQL, allows both simple and complex queries, to get exactly what you want. It takes some time to grasp all the concepts, but after some practice it becomes powerful

The LOAD CSV function, which simplifies the task of creating the graph from a csv file

Another great thing is the multiple APIs available in scripting languages, like Python. But that will come in another article.

I. Setup neo4j

Download neo4j Desktop, and install it

You have to fill the form to download neo4j

2. After installation, click “New Graph” under My Project section, then “create a Local Graph”. Write down the name of the graph, and the password.

3. Run the database, clicking the “play” sign.

Now open the neo4j browser with the new graph you created. A bar at the top will appear and you’ll type queries there

II. Create your neo4j graph database

Let’s say your data is about people and organisations. People are one kind of ‘node’ and Organisations are another kind. They can be connected through a specific relationship. Like someone has worked for a company, someone is the chairman of the company, or a company is the parent of another company.

1. Writing CYPHER queries in the neo4j browser

CYPHER is a flexible language, you can create nodes, relationships and even both together at the same time.

Let’s start the easy and intuitive way: create nodes then relationships.

Create a node with the following syntax:

CREATE ( x: LABEL { ATTRIBUTE_NAME: ATTRIBUTE_VALUE })

Where

x: is a variable name of the node, you could call it whatever. It’s useful when you reuse it for instance to plot the node at the end of the query using RETURN x.

LABEL: is the label of the node. In our case we’ll have 2 labels: Org and Person

For instance

CREATE (p: Org { Name: “Superrobotics Limited” })

Hit enter and you’ll get a confirmation (or an error) that the node was created. In my case, it worked, as mentioned by “Added 1 label, created 1 node…”

By the way, by running the above query we’ve implicitly created a label called Org.

Let’s create a person who is the chairman of this organisation and associate him to the organisation.

CREATE (p: Person { Name: “Patrick Moon” })

Then

MATCH (p:Person),(o:Org)

WHERE p.Name = “Patrick Moon” AND o.Name = “Superrobotics Limited”

CREATE (p)-[r:CHAIRMAN]->(o)

RETURN p,r,o

Adding the RETURN at the end of the query will plot the newly created relationship. Note that neo4j has a color coding that differentiates labels automatically.

Now we kind of understand how CYPHER works: either you create something new and start with CREATE, or you try to find existing things in the graph and you start with MATCH. Nodes are in brackets (), relationships in square brackets [], attributes in curly brackets {}.

____

Note: 2 queries useful to restart the graph from scratch if you mess it up:

Delete all nodes and relationships: match (n)-[r]-() delete n,r

Delete all nodes: match (n) delete n

____

Alright now let’s say we have an Excel with 100+ people linked to organisations, we don’t want to write 100 times the same query. Let’s do it the old classy way in Excel.

2. Pasting CYPHER queries from Excel with an Excel formula

For this example, I copy and pasted data from Wikipedia’s article on the list of CEOs of notable companies — 184 exactly.

What we want is to make a CREATE query for each row, creating at the same time:

a node for the Company

a node for the CEO

a relationship between the two

Let’s do it:

=”CREATE (:PERSON {Name: “””&B2&”””})-[:CEO {Since: “””&D2&”””, Notes: “””&E2&”””}]->(:ORG {Name: “””&A2&”””})”

Note I deleted the first variable within the nodes (p and o) and relationship (r). This is because we’ll send 184 CREATE queries to neo4j at the same time, so we need to make sure it understands that each node (p or o) and relationship (r) is different from the one created one line above.

Once you do copy and paste the formula for all the rows, you’ll get something like this:

Now let’s copy and paste the CREATE formulas into neo4j. Show all the nodes in the graph with

MATCH (n) RETURN n

Conclusions

In this article we’ve gone through the initial steps of installing neo4j and creating your first graph, and it took what, 10–15 minutes? Rather straightforward.

Next step

Searching through the graph for interesting patterns…"
Yet Another Full Stack Data Science Project,"Introduction

I came across the term “Full Stack Data Science” for the first time a couple of years back when I was searching for Data Science meetups in Washington D.C. region.

Coming from a software development background, I am quite familiar with the term Full Stack Developer, but Full Stack Data Science sounded mystical.

With more and more companies incorporating Data Science & Machine Learning in their traditional software applications, the term Full Stack Data Science makes more sense now than at any point in history.

Software development methodologies are meticulously developed over the years to ensure high-quality software applications in low turn around time. Unfortunately, traditional software development methodologies do not work well in the context of Data Science applications.

In this blog post, I am going to emphasize on Cross-industry standard process for data mining (CRISP-DM) to develop a viable full stack Data Science product.

I firmly believe in the proverb, “the proof of the pudding is in the eating.” so I have implemented Starbucks challenge by applying CRISP-DM methodology as a sister project for this post, and is referred at multiple places in this blog post."
Boost your efficiency and process Excel-files with Python,"If you work with data, you will get in touch with excel. Even if you don´t use it by yourself, your clients or colleagues will use it. Excel is great for what it is made: table calculation for smaller data sets. But I always hated the kind of excel sheets with one million rows and hundreds of columns. This workbooks are slow and tend to crash after some calculation. So I started to use python for handling large excel files which offer another big advantage: You create code which is reproducible and provide documentation as well. Let´s jump in!

Reading Excel-files with Python

The file we want to process contains nearly 1 million rows and 16 columns:"
Live Prediction of Traffic Accident Risks Using Machine Learning and Google Maps,"Live Prediction of Traffic Accident Risks Using Machine Learning and Google Maps

Here, I describe the creation and deployment of an interactive traffic accident predictor using scikit-learn, Google Maps API, Dark Sky API, Flask and PythonAnywhere. Meraldo Antonio · Follow Published in Towards Data Science · 10 min read · Oct 14, 2019 -- 6 Share

Traffic accidents are extremely common. If you live in a sprawling metropolis like I do, chances are that you’ve heard about, witnessed, or even involved in one. Because of their frequency, traffic accidents are a major cause of death globally, cutting short millions of lives per year. Therefore, a system that can predict the occurrence of traffic accidents or accident-prone areas can potentially save lives.

Although difficult, traffic accident prediction is not impossible. Accidents don’t arise in a purely random manner; their occurrence is influenced by a multitude of factors such as drivers’ physical conditions, car types, driving speed, traffic condition, road structure and weather. Studying historical accident records would help us understand the (potentially causative) relationships between these factors and road accidents, which would in turn allow us to build an accident predictor.

Fortunately, several of such accident records are publicly available! For instance, the UK government has published detailed records of traffic accidents in the country dating back to 2002. With this data set, my teammates and I have created and deployed a machine learning model that predicts with high accuracy as to when and where accidents are likely to occur in Greater London.

In this article, I will take you through the process of completing this project. The complete code can be found in my GitHub repository, while the live model can be accessed at this website.

Objective

Before embarking on this project, we set ourselves a clear objective: we wanted to create an interactive traffic accident predictor that would be easily accessible…"
How AI Enables Smarter Claims Processing & Fraud Detection?,"AI technologies have well and truly reformed information systems by making them far more adaptive to humans while significantly improving the interaction between humans and computer systems.

With this, Artificial Intelligence within the Insurance industry has overhauled the claims management process by making it faster, better, and with fewer errors. Insurers now have the option of achieving far better claims management by utilizing the technology in the following ways:

Facilitate a real-time Q&A service for a first notice when it comes to losing.

Pre-assess claims while automating the damage evaluation process.

Automate claims fraud detection through rich data analytics.

Predicting patterns of claim volume.

Augment loss analysis.

From smart chatbots that offer quick customer service round the clock to the array of machine learning technologies that spruce up the functioning of any workplace through its automation power, the expanding potential of Artificial Intelligence in Insurance is already being used in many ways.

With increased awareness and resources about the game-changing influence of AI in the Insurance industry, the initial hesitations and shallow discomfort around its implementation are now fading quickly as it begins to trust in the caliber and numerous opportunities brought forward by Artificial Intelligence and Machine Learning. The only question that remains is — how far can we push its capabilities?

The role of AI in the Insurance industry

In 2017, Artificial Intelligence has shown its substance in various business verticals by rapidly creating controlled, digitally enhanced automated environments for maximum productivity.

Apparently, Insurance companies, in particular, have a lot to gain from investing in AI-enabled technology that can not only automate the scheduling of executive-level tasks but can also enrich service quality by helping agents make right decisions and irrefutable judgments.

A glimpse of AI-enabled innovations and solutions

Insurance companies, as on today, face 3 major challenges:

Reaching out to prospective customers at the right time.

2. Providing the right set of products that suit customer requirements.

3. Fastest claim support to loyal customers and rejection of spurious claims.

Insurance companies are striving for a technologically advanced system that helps keep all their employees synchronized. These employees vary from agents, brokers, claim investigators to market and support team. These group of employees coupled with redundant processes create layers of confusion in the Insurance ecosystem.

To make the system more refined and efficient, they should opt for stable and consistent AI-powered solutions that can penetrate the layers of confusion and propel a clear value proposition towards customers. AI in the Insurance industry offers several promising technology-enabled solutions:

The uninterrupted flow of business information

Numerous industries have already adapted to the changing environment of digital technology and have creatively integrated automation and robotics to reshape their productive channels and unsynchronized structures. Some of the industries that have experienced and leveraged the power of Artificial Intelligence are Hospitality, Healthcare, Customer Service, E-commerce and more.

The fact that insurers and insurance companies are surrounded by piles of data and many other scattered management segments isn’t exactly new.

Leveraging AI’s data processing capability, insurers can enable a strategically built sophisticated environment where information pertaining to business and customer interactions can flow from one specific department to another on a common platform without any chain breakers. Thus, insurance companies not only organize task management for their employees but in many ways, it helps elevate the quality of the end-to-end information management system.

Automated claim support

AI-based chatbots can be implemented to improve the current status of the claim process run by multiple employees. Driven by Artificial Intelligence, the touchless insurance claim process can remove excessive human intervention and can report the claim, capture damage, update the system and communicate with the customer all by itself. Such an effortless process will have clients filing their claims without much hassle.

For e.g., an AI-powered claims chatbot can review the claim, verify policy details and pass it through a fraud detection algorithm before sending wire instructions to the bank to pay for the claim settlement.

This is the best example of how claims with standard documentation can minimize human efforts and can be reviewed by bots, thus saving on the workforce for Insurance giants and deliver instant customer assistance. Additionally, AI-powered automated claim support system can liberate companies from expensive fraudulent claims, human errors and resultant inaccuracies by identifying data patterns in claim reports.

Interactive power of Insurance chatbots

Because of lengthy documents, complex policies and tedious instructions, customers often develop a phobia and feel confused and daunted at the idea of settling for Insurance policy. They need human-like interactions that enable both smooth transaction and education.

Intelligent chatbots exceed the capability of Insurance agents and serve as a virtual assistant in messaging apps on customers’ devices. For an in-depth understanding of customer queries, chatbots should have NLP support along with sentiment analysis to assess a customer’s reaction and resolve issues accordingly.

Customers can either type or use their voice to communicate their concerns pertaining to different policies which chatbots can process to deliver personalized solutions. Starting with fundamental questions related to claims, chatbots can do a lot more such as product recommendations, promotions, lead generation or customer retention. These bots can be integrated with the channel of your choice (Website, Facebook, Slack, Twitter, etc.) to guide customers with quotes, policy explanations and purchase of insurance covers.

Advanced underwriting

IoT and tracking devices yield an explosion of valuable data which can be utilized to make the process of determining insurance premium upright and regulated. Fitness and vehicle tracking system in both health and auto insurance sector give rise to the dynamic, intelligent underwriting algorithms that cleverly control the way premium is dictated. Using Artificial Intelligence and Machine Learning, insurers can save a lot of time and resources involved in the underwriting process and tedious questions and surveys, and automate the process.

Insurance bots can automatically explore a customer’s general economy and social profile to determine their living patterns, lifestyle, risk factors, and financial stability. Customers who are more regular in their financial patterns are qualified to feel safe through low premiums. Since AI is more capable of strict scrutiny of gathered data, it can predict the amount of risk involved, protect companies from frauds and give justified insurance amount to customers.

MetroMile, a US-based start-up, has established such a dynamic underwriting system known as ‘pay-per-mile’ where usage of a car determines the insurance premium. Here, an AI-based device installed on the vehicle by the company uses a special algorithm to monitor miles, jerks, collisions and frictions, speed patterns and other car struggles on the road, and it collects detailed data essential to decide whether or not drivers deserve low premiums.

Predictive Analytics for proactive measures

Predictive Analytics backed by Machine Learning is now perhaps the heart of intelligent services across many business verticals that have adopted AI-powered solutions. However, this smart capability is not just aimed at driving future insight into customer’s preferences and tailoring relevant products. Health insurance companies are coming up with rewarding pre-emptive care that is focused on encouraging customers to look after their personal well being. If a person remains healthy, companies don’t need to invest in claim payment and management process.

For instance, Aditya Birla Health Insurance has planned wellness benefits to encourage customers to stay healthy. AI’s predictive algorithms scan past year’s claim activities and hospitalization data to provide incentives to customers to improve health & wellness. This way, health risks will be minimized and so will be the company’s resources.

Thus, nowadays, start-ups leverage AI’s unique potential to scour through piles of claim data and coverage patterns to be more proactive and anticipate health risks at the individual level before they actually transpire.

Marketing and Relevant Products

Marketing is another action gear for insurance companies that wish to enhance their reach and secure higher customer acquisition. Being a part of the competitive market, insurers need to capitalize on a vital marketing strategy which goes beyond the traditional cold calling approach.

The old blanket methods are on the verge of extinction since digital disruption has already shaken the grounds of the insurance field. Customers today seek sophisticated, luxurious and extremely personalized services with custom sales tactics. Using the combined power of predictive analytics, NLP and AI in the insurance industry, agents can gain access to the full profile of customers and prospects. This data can be further analyzed to generate mature insight, accurate predictions on customer preferences and what exact products or offers should be added in their marketing activities.

A quick look at AI in the Insurance industry today

According to a survey Accenture, as on today, 74% of customers would like to interact with modern technology and appreciate the computer-generated system of insurance advice.

Companies who have been early to adopt automation of some aspects of their claims process can experience a significant fall in processing time and cost, and a good increase in service quality.

Talking about early adopters, Allstate Business Insurance has also recently developed ABIe in partnership with EIS. ABIe (spoken as Abbie) is an AI-based virtual assistant application designed to cater to Allstate insurance agents looking for information on ABI’s commercial insurance products. Hopefully, as time goes by, we will get to hear more such breakthroughs of AI investments in insurance companies.

The power combination of Machine Learning, advanced analytics, and IoT sensors enables insurers to reach prospect clients, study their real-time needs, develop insight from their profile on risk magnitude, and ultimately create bespoke solutions.

The future of AI in the Insurance industry

While challenges appear to dismay the present market, insurers still like to view the potential of AI in the Insurance industry with optimistic eyes. To reap the full range of benefits, insurance companies need to devise an enterprise-level strategy to implement AI in such a way that it offers more than just customer experience.

At Maruti Techlabs, we are already working on multiple applications of AI in the Insurance industry when it comes to claims management, damage analysis through image recognition, automated self-service guidance, and others.

When it comes to image recognition, the overall damage analysis, cost estimation, and claim settlement would be carried out by bots that scan through pictures and videos. This way, with time, companies can rely completely on image recognition technology for first level claim automation and subsequently, settle claims or resolve fraud detection in insurance automatically. By working on smart automation of existing workflows, we aim to reduce the time and resources spent on managing or monitoring claims, increasing process efficiency and enhancing the customer experience.

With new AI tools constantly reinventing the claims management, the payoff is bound to include smarter fraud detection, faster settlements, and better customer service.

Rapid advances in technologies over the next 10 years will lead to disruptive changes in the insurance industry. Companies that adopt new-age tech to develop innovative products, harness cognitive learning insights from a myriad of data points, streamline processes, and more importantly, personalize the entire customer experience will be the winners in the AI dominated insurance space."
Missing Data and Imputation,"Missing data can skew findings, increase computational expense, and frustrate researchers. In recent years, dealing with missing data has become more prevalent in fields like biological and life sciences, as we are seeing very direct consequences of mismanaged null values¹. In response, there are more diverse methods for handling missing data emerging.

This is great for increasing the effectiveness of studies, and a bit tricky for aspiring and active data scientists keep up with. This blog post will introduce you to a few helpful concepts in dealing with missing data, and get you started with some tangible ways to clean up your data in Python that you can try out today.

Photo by Carlos Muza on Unsplash

Why do anything at all?

You may be asking yourself — why do I need to deal with missing data at all? Why not let sleeping dogs lie? Well, first of all, missing values (termed NaN, Null or NA) cause computational challenges because. Think about it — if you’re trying to sum up a column of values and find a missing one, what is 5 + NA? If we don’t know the second term in the equation, our outcome is itself NA. So we really can’t derive anything meaningful from missing values, plus it confuses most programs that expect to be handling non-empty cases.

Aside from this, there are three main problems that missing data causes:

Bias More laborious processing Reduced efficiency in outcomes

These are all pretty serious (if not just irritating) side effects of missing data, so we’ll want to find something to do with our empty cells. That’s where “imputation” comes in.

Imputation

Webster’s Dictionary shares a “financial” definition of the term imputation, which is “the assignment of a value to something by inference from the value of the products or processes to which it contributes.” This is definitely what we want to think of here — how can we infer the value that is closest to the true value that is missing?

As an aside— it is interesting to reflect on and consider that this term is likely derived from its theological context. Here, it means “the action or process of ascribing righteousness, guilt, etc. to someone by virtue of a similar quality in another,” as in “the writings of the apostles tell us that imputation of the righteousness of Christ is given to us if we receive Christ.” Just some food for thought as we move along.

Missing Data Mechanisms

When researching imputation, you will likely find that there are different reasons for data to be missing. These reasons are given terms based on their relationship between the missing data mechanism and the missing and observed values. They help us unlock the appropriate data handling method, so they’re really helpful to have a basic understanding of. Below are 3 of the 4 most typical, and you can read more about them on “The Analysis Factor” .

Missing Completely at Random (MCAR)

This one may be the easiest to think about — in this instance, data goes missing at a completely consistent rate. For example, a dataset that lacks 5% of responses from a youth survey. This is because 5% of all students were out sick the day that the survey was administered, so the values are missing at a consistent rate across the entire data set.

2. Missing at Random (MAR)

Despite the name similarities, MAR values are a bit more complex — and more likely to find than MCAR. These are instances that data the rate of missing data can be perfectly explained if we know another variable. For example, imagine the above dataset lacks 10% of responses from girls and 5% of responses from boys. This is because the illness spread at the school was 2x more likely to affect young women than young men. This gets more complex, and more realistic, as multiple variables influence the rate of missing values in a dataset.

3. Missing Not at Random (MNAR)

In this case, the missing-ness of a certain value depends on the true value itself. This one is pretty cyclic, but I like the example given in this video of rates of missing values in a survey of library-goes that collects their names and number of un-returned library books. As the number of hoarded books increases, so does the percentage of missing values from this survey question. The problem with this one is that because the value missing is dependent on the value itself, we have a very difficult time deriving the rate it is missing.

Practical Exploration and Visualization in Python

When dealing with data in Python, Pandas is a powerful data management library to organize and manipulate datasets. It derives some of its terminology from R, and it is built on the numpy package. As such, it has some confusing aspects that are worth pointing out in relation to missing data management.

The two built-in functions, pandas.DataFrame.isna() and pandas.DataFrame.isnull() actually do exactly the same thing! Even their docs are identical. You can even confirm this in pandas’ code.

This is because pandas’ DataFrames are based on R’s DataFrames. In R na and null are two separate things. Read this post for more information. However, in python, pandas is built on top of numpy, which has neither na nor null values. Instead, numpy has NaN values (which stands for “Not a Number”). Consequently, pandas also uses NaN values².

Additionally, the Python package named missingno is a very flexible, missing data visualization tool built with matplotlib, and it works with any pandas DataFrame. Just pip install missingno to get started, and check out this Github repo to learn more.

A “missingno” visualization of cyclist dataset — with Sparkline on the side

Adequately visualizing your missing data is a great first step in understanding which missing data mechanism you are handling, along with the scale of missing data and hot spots to work with.

Methods for Single Imputation:

Starting from the simplest and moving toward more complex, below are descriptions of some of the most common ways to handle missing values and their associated pros and cons.

(Note that one item or row in a dataset is referred to as an “observation.”)

Row (Listwise) Deletion: Get rid of the entire observation.

Simple, but can introduce a lot of bias.

An example of listwise deletion

2. Mean/Median/Mode Imputation: For all observations that are non-missing, calculate the mean, median or mode of the observed values for that variable, and fill in the missing values with it. Context & spread of data are necessary pieces of information to determine which descriptor to use.

Ok to use if missing data is less than 3%, otherwise introduces too much bias and artificially lowers variability of data

3. Hot or Cold Deck Imputation

“Hot Deck Imputation” : Find all the sample subjects who are similar on other variables, then randomly choose one of their values to fill in.

Good because constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive

“Cold Deck Imputation” : Systematically choose the value from an individual who has similar values on other variables (e.g. the third item of each collection). This option removes randomness of hot deck imputation.

Positively constrained by pre-existing values, but the randomness introduces hidden variability and is computationally expensive

Example of basic hot deck imputation using mean values

4. Regression imputations

“Regression Imputation” : Fill in with the predicted value obtained by regressing the missing variable on other variables; instead of just taking the mean, you’re taking the predicted value, based on other variables.

Preserves relationships among variables involved in the imputation model, but not variability around predicted values.

“Stochastic regression imputation” : The predicted value from a regression, plus a random residual value.

This has all the advantages of regression imputation but adds in the advantages of the random component.

Challenges of Single Imputation

These are all great methods for handling missing values, but they do include unaccounted-for changes in standard error. Again, “The Analysis Factor” explains this trade-off perfectly below:

“Since the imputed observations are themselves estimates, their values have corresponding random error. But when you put in that estimate as a data point, your software doesn’t know that. So it overlooks the extra source of error, resulting in too-small standard errors and too-small p-values.”

Additionally, values found in single imputation might be biased by the specific values in the current data set, and not represent the total values of the full population. So how do we reduce the impact of these two challenges?

Multiple Imputation

Multiple imputation was a huge breakthrough in statistics about 20 years ago because it solved a lot of these problems with missing data (though, unfortunately not all). If done well, it leads to unbiased parameter estimates and accurate standard errors.

While single imputation gives us a single value for the missing observation’s variable, multiple imputation gives us (you guessed it) multiple values for the missing observation’s variable and then averages them for the final value.

To get each of these averages, a multiple imputation method would run analyses with 5–10 unique samples of the dataset and run the same predictive analysis on each**. The predicted value at that point would serve as the value for that run; the data signature of these samples change each time, which causes the prediction to be a bit different. The more times you do this, the less biased the outcome will be.

Once you take the mean of these values, it is important to analyze their spread. If they’re clustering, they have a low standard deviation. If they’re not, variability is high and may be a sign that the value prediction may be less reliable.

While this method is much more unbiased, it is also more complicated and requires more computational time and energy.

Conclusion

In closing, we looked at:

The importance of handing missing values in a data set The meaning (and root) of “imputation” Different reasons that data could be missing (missing data mechanisms) Ways to explore and visualize your missing data in Python Methods of single imputation An explanation of multiple imputation

But this is just a beginning! Please look into the linked resources on this post, and beyond, for further information on this topic."
Constructing HTTP data pipelines with Flask,"A lot of Data-Science work tends to focus on the attributes that are quite obvious, training models, validation, statistics… However, without obtaining data to work with, all of these skills can end up feeling like a sprint in the mud. One of the biggest tools used today to transfer data over the web is Flask. Flask is a fantastic tool that is commonly used to create data-driven web-applications and pipelines in Python. Flask has a ton of built in functionality for use with SQL, Json, and HTML that make it extremely viable for back-end use on the web.

Request and Return

In order to have a completely cloud-based pipeline, with its respective returns, we need to understand Flask’s request and return system. For a calculative or informative returning pipeline, typically the route is left to the default, or:

This implies that the Flask-app’s directory has not been altered, and allows us to execute a function when the route is accessed. This can be compared to “ Index.html.”

So sure, we know where we’re at, but how can we apply hyper-text transfer protocol to Flask in order to receive data, and transfer results? In order to do this, we need to add requests to our project:

from flask import Flask, render_template, request

For this particular example, I was working on a project with some of my peers; full-stack developers, UI/UX developers, and front-end developers. As always, I have a source link to the Github repo. I was tasked with creating an algorithm that would take certain properties of housing, and return a prediction that the user could then save to their account. The greatest challenge was getting things setup in a particular way so that their requests, and my returns could line up properly. If you would like to see the entire project’s Github, it’s here.

With that out of the way, I constructed my Sklearn pipeline, and Joblibbed it out. What made this difficult was the sheer number of observations that the data contained… It was hard to work with, and especially hard to read in the first place. If you’re uncertain how something like that is done, you can checkout the notebook here."
Effective Ways How Data Analytics Help to Make a Better Entrepreneur,"Effective Ways How Data Analytics Help to Make a Better Entrepreneur

Check out how Business Intelligence (BI) and data analytics remove uncertainty in business and provide insights that help in decision making and forecasting. MILA JONES · Follow Published in Towards Data Science · 4 min read · Sep 11, 2019 -- Listen Share

Business Intelligence and data analytics are an integral part of any successful business venture. Business analytics has its dedicated market in the industry and is often a sought-after method to skip the guesswork and accelerate the pace of growth. With data analytics, you get invaluable insights into your business. This can help you tweak your strategies and investments so that your ROI remains exceptionally attractive.

The rise of Big Data and Analytics

The market value of big data and data analytics is increasing each day and it will continue to grow.IDC pegs the total digital data created worldwide to be 163 zettabytes (a billion TB) by 2025. Around 60% of this data will be created and managed by enterprise organizations. The massive deluge of data will be useless if data analytics and BI don’t convert these mere numbers in actionable insights. This explains the immense importance of these two fields in modern-day business practices.

The analytical results and findings help to make you intelligent business decisions. It makes information gathering, processing and analyzing more accurate and easier. You can also take the help of technologies like automated software to do the analytical work and fast-track the insight generation process.

There are a lot of ways that data analytics helps you become a successful entrepreneur you are. This article will put some light on these plus points-

Fast Upselling and Cross-Selling Opportunities

Business Intelligence provides you with excellent cross-selling opportunities. The BI tools can help you to gather data and analytics can help you to analyze the collected info. You can attract your customers properly and more dramatically after analyzing the data. The data gives you clear information about your potential customers, their reactions and expectations. This helps you to formulate a strategy that is perfect for your business. The whole process results in better cross-selling tactics.

If you want to explore the area of cross-selling, you first need to know the demands and expectations of your customers. B2B business holders need to have a careful look at their customer reviews. They need to discuss the points that attracted their existing customers and why the clients still do business with the company.

In the case of B2C companies, the process is slightly different. They connect to retail customers directly via options like surveys, social media campaigns, social listening, and other consumer review programs to know the expectation of the customers and their demands.

The simple motto of these whole processes is to allow the customers better service so they continue a fruitful business relationship with the company. To do this you can offer special services to your clients. It applies to both B2B and B2C companies. This will make your customers satisfied and you will be able to gather more revenue without a lot of effort.

Enables Amazing Efficiency in Business

Business intelligence and analytics is a savior in many cases. It effectively answers all of the doubts that you have in your mind regarding your business. This helps you to gain a clear insight into what is happening so that you can make proper decisions quickly rather than being blindsided by incorrect guesswork. If you do not want to read long reports about your business then you can surely take help from BI.

Proper BI allows you to get an accurate visual representation of a large amount of data in just a few minutes. It also organizes and summarizes the collected data. Hence, you can get a correct picture of the latest trends in front of you. The BI uses governance, cloud warehousing and other tools and methods to provide useful data in time. You can access the collected data from anywhere on any device. This saves a lot of time from your side.

For entrepreneurs who wish to monitor their KPIs at one glance on a frequent basis, a feature-packed and visually appealing BI system will prove to be a boon.

Cutting down the cost

Cost is one of the essential factors in any business venture regardless of the size. Each business is different and needs the right investment into resources at the right time. For example, a gift shop may see a rise in sales during the festival seasons like Christmas or New Year.

On the other hand, jewelry and flower shops get extra business during marriage season or on valentine's Day. This means, each business needs flexible labor force during a certain time and also extra effort and investment at a certain time of the year.

BI can help you to determine a perfect strategy so that you can invest during the right time into resources like manpower with modeling and staffing forecasts. This alleviates the risk of cost overheads and better utilization of resources to maximize ROI with the help of data analytics and BI.

Learning customer behavior pattern to serve them better

Your customers’ behavior can be a great tool to generate revenue. If you can know what they are thinking then you will be able to serve them better with bespoke solutions. Business intelligence can help you in this case.

You can use Microsoft365 dynamics for customer service to generate helpful reports that add immense prowess to insight generation in the business. These reports contain detailed data about the customers’ demands, their expectations, what they want, what they buy, how they buy, etc.

It helps you to understand their mindset so that you to think about a customer-oriented business strategy. This way, you can give them proper service and reap rich dividends in the process.

Conclusion

In order to grow, your business cannot rely on crystal ball gazing. What you need is a more comprehensive means of making accurate predictions and framing growth-oriented strategies. This is exactly where data analytics and Business Intelligence (BI) come into the picture.

The collected data can be transformed into rich insights and help you to make faster decisions which help you to attract your customers for a better ROI. BI also helps in making smarter decisions and investments and also saves manpower, time and money."
A gentle introduction to Recommendation Systems,"Member-only story A gentle introduction to Recommendation Systems

Introduction to Recommendation Systems

If you are here, reading about Recommendation Systems, surely you already know what we’ll be talking about, so maybe you can just jump over this brief chapter. But if you came here attracted by the cover image, or if you want to know more about how Recommendation Systems emerged and grew up in the last years, then stay tunned with this section of the article.

Let’s go back in time and try to picture this: it’s Friday night and you want to rent a videotape in your closest Blockbuster. You head over there with your girlfriend, already talking about which movie are you going to rent. Maybe a comedy? Or perhaps an action movie? A thunderstorm is coming, what settles the perfect mood for a horror movie or a thriller.

While you’re thinking, your girlfriend tells you: ‘did you search for any good recommendation on the Internet?’. Unfortunately, the Internet was down all day at your job, and anyway, you’re still trying to get yourself around with all that mumbo jumbo around the World Wide Web.

You finally arrive there, but after almost an hour of deliberations, you still don’t know what to chose. You already saw all the popular movies, and the premier section of the store is full of crap. You don’t want to be nitpicking, but there’s just nothing good. Your girlfriend asks the Blockbuster mate behind the desk, but he seems to know less about movies than your grandpa, and he is really not down with the kids. It’s already getting late and you’re knackered, so long story short, you decide to rent a good old movie you already saw. Is better to keep it sound and safe, than winging about your Friday’s night plan.

This example may seem old, but in so many aspects of life, people keep making decisions in a similar way. I can assure you that my grandpa doesn’t use Google when he wants to buy a book for himself. However, and luckily for the youngsters, the decision making today in…"
Region of Interest Pooling,"Region of Interest Pooling

The major hurdle for going from image classification to object detection is fixed size input requirement to the network because of existing fully connected layers. In object detection, each proposal will be of a different shape. So there is a need for converting all the proposals to fixed shape as required by fully connected layers. ROI Pooling is exactly doing this.

Region of Interest (ROI) pooling is used for utilising single feature map for all the proposals generated by RPN in a single pass. ROI pooling solves the problem of fixed image size requirement for object detection network.

The entire image feeds a CNN model to detect RoI on the feature maps. Each region is separated using a RoI pooling layer and it feeds fully-connected layers. This vector is used by a softmax classifier to detect the object and by a linear regressor to modify the coordinates of the bounding box. Source: J. Xu’s Blog

ROI pooling produces the fixed-size feature maps from non-uniform inputs by doing max-pooling on the inputs. The number of output channels is equal to the number of input channels for this layer. ROI pooling layer takes two inputs:

A feature map obtained from a Convolutional Neural Network after multiple convolutions and pooling layers.

‘N’ proposals or Region of Interests from Region proposal network. Each proposal has five values, the first one indicating the index and the rest of the four are proposal coordinates. Generally, it represents the top-left and bottom-right corner of the proposal.

ROI pooling takes every ROI from the input and takes a section of input feature map which corresponds to that ROI and converts that feature-map section into a fixed dimension map. The output fixed dimension of the ROI pooling for every ROI neither depends on the input feature map nor on the proposal sizes, It solely depends on the layer parameters.

Layer Parameters: pooled_width, pooled_height, spatial scale.

Pooled_width and pooled_height are hyperparameters which can be decided based on the problem at hand. These indicate the number of grids the feature map corresponding to the proposal should be divided into. This will be the output dimension of this layer. Let us assume that W, H are the width and height of the proposal and P_w,P_h are pooled width and height. Then the ROI will be divided into P_w*P_h blocks, each of dimensions (W/P_w, H/P_h)."
5 Steps to correctly prepare your data for your machine learning model.,"The simplest way to describe any machine learning project is that it’s a program that when given pieces of data it hasn’t seen before, it will process them based on previous experience and tell you something(s) you did not already know.

“Data is the new oil.” — Clive Humb — Chief Data Scientist and Executive Director of Starcount

Data is at the core of nearly every business decision made.

Human resources directors are gathering data from online resources to determine the best people to recruit and confirm details about them. Marketing departments are lasering in on market segmentation data to find consumers who are ready to buy, speeding up the sale-closing process whenever possible.

Business executives must examine bigger trends in the market, such as changes in pricing of resources, shipping or manufacturing.

Your project is only as powerful as the data you bring.

Step 1: Gathering the data

The choice of data entirely depends on the problem you’re trying to solve.

Picking the right data must be your goal, luckily, almost every topic you can think of has several datasets which are public & free.

3 of my favorite free awesome website for dataset hunting are:

Kaggle which is so organized. You’ll love how detailed their datasets are, they give you info on the features, data types, number of records. You can use their kernel too and you won’t have to download the dataset. Reddit which is great for requesting the datasets you want. Google Dataset Search which is still Beta, but it’s amazing. UCI Machine Learning Repository, this one maintains 468 data sets as a service to the machine learning community.

The good thing is that data is means to an end, in other words, the quantity of the data is important but not as important as the quality of it. So, if you’d like to be independent and create your own dataset and begin with a couple of…"
Gliding into Model-Based,"Reinforcement Learning (RL) can be a daunting space to those new to the field due to terminology and complex mathematics formula. However, the principles underlying it are more intuitive than first imagined. Let’s imagine RL in the form of a new, yet unreleased, the game of Zelda, which takes place far in the future, the year 2119.

Link’s mission in 2119 is to save humanity from Rittenhouse, a secret agency based in 2019 which has a strong AI and a time machine. Because Rittenhouse is evil, they devise a plan to destroy humanity in 2119 and send their agents forward in time to complete the task.

As the game begins, Link drops down onto a mostly desolate island where he must find the first sign of human civilization and warn them of the impending arrival of Rittenhouse’s agents. As he lands on the top of a tower, an old wise man appears and gives him a paraglider as a gift. Link’s mission is now to paraglide and finds a town with human civilization to warn them of Rittenhouse.

Link is a cyber-alien that can install any software in his mind. He does not know how to paraglide. Before Ritten-House’s agents get there, your goal is to write a program to teach Link paragliding and all the tricks he reaches human-civilization as fast as before possible and avoids Ritten-House as much as possible. Your plan is to send this simulation to Link before Rittenhouse’s agent come and stop him, so he would know to protect himself.

Your plan is to send you this program through light-waves in the future.

In order to design this program, you need to know something called Reinforcement Learning. In Reinforcement Learning, there is an agent that is interacting with an environment.

We model the agent’s interactions with an environment based on the mathematical framework called the Markov Decision Process (MDP). Each agent starts in the state “X” and at each time step takes an action “A”, gets the reward “R”, and land in the next state “Xt+1,”, and this cycle repeats until the agent reaches the target state “X`”.

Now, in the simulation that you are making, Link as the paraglider is an agent in the initial state X in the sky. At each time step Link takes an action from sets of possible actions. Here steering his kite to the left or right are considered as his possible actions, and he goes to a new state or new place in sky. His goal is to land in the target state X`, which is a few states in the future and where he can find human civilization.

At each time step, based on each action he takes, he will be in a different position in the space. For example, if he steers his kite to right, he would be in a different place than when he steers his kite to left.

However, not all of these possible actions are identically favorable. His goal is to find a sequence of optimal actions in order to get to his goal.

This means you want Link to take a route that is most efficient to get to humans. In order to do this, it is better if we use the model-based method. In the model-based approach, in order to take optimal actions, Link also needs to predict the ideal future states, so can choose the best route to get there.

When Link is paragliding, he wants to know how he should steer his kite in order to not fall (i.e., find an optimal action). But, also he wants to avoid the enemy and land in an optimal location like where the town is (i.e., predict the next optimal state).

The prediction of future states and where the right place is to land will, in turn, affect how Link steers his kite in the present moment. You want to find optimal actions based on his prediction of future states.

Link might predict the enemy will be at certain spots and to avoid that enemy, he should steer his kite in other directions.

The prediction of the future state accounts for how the environment is changing. This change of the environment with respect to the current state and action is described as a function, and we refer to this function as the model.

Your goal is to teach Link to lean this model.

The input of the model is the current state x and action u , and the goal is to predict future state x t+1. We can write this as: x t+1 =f(xt, ut)

We call the process of selecting the sequence of actions until the episode ends as the policy. Intuitively, policy means how Link chooses the best way to steer his kite at each time step until he finally reaches to the town to save humans. We can describe the policy with the following notation: ut= лθ(xt). This means at every state xt , policy лθ tell Link what the optimal action ut is.

Cost or reward function

We use the cost function or reward function to find an optimal policy or in another word, optimal actions over the trajectory. Depending on the setting, we use either cost or reward function.

Note that reward is just negative of the cost function. We are trying to minimize cost or maximizing reward. In this setting, we use a cost-function c(xt,ut).

How is finding optimal action related to the prediction of the future state? To put it another way, how is optimal policy related to the model?

Before answering this question, I want you to imagine a world in which the only one of these functions works.

What do you think it would happen?

The first scenario, the model is capable of predicting the next state but incapable of taking good actions.

Here green is the optimal future state and the model has the prediction of both green and red.

This means, even if Link is capable of predicting future state, he knows where is the enemy is, but he is not capable of taking good actions based on his knowledge of the future state. He does not know how to steer his paraglide and falls off or wear off towards Rittenhouse because he does not know how to steer his kite.

Now, imagine the opposite scenario, Link is capable of taking good actions and is an expert at paragliding but he is incapable of predicting where his action will take him. He knows how to steer his kite but he does not know where to go. This might also put him in a bad spot.

This means he does not have any prediction about the future. He might spend lots of time taking actions based on his current circumstance and need to encounter with the enemy and not based on the prediction of the future state, also it might search randomly through the whole area until he finds where is the town with a human. This is similar to lots of model-free environments. The prediction makes Link a wise person, he can predict the consequence of his actions and make decisions based on those predictions. This is what rational human would do too.

Again with the paragliding example, Link is capable of steering his kite at a given moment to not fall but could not remember where the enemy was and as a result, he constantly encountering them and as the result not having a clear idea about how the environment is changing, he can not know know exactly which direction would take him to the town.

Link not only needs to know how to steer his kite but also predict where he should go and steer his kite based on his prediction. Now, you might have a better intuitive sense of why these two functions are related.

Let’s elaborate more on this mathematically.

Here is the formula for the total loss, which we are trying to minimize. Note that this loss consists of two functions.

1. cost function: c(xt,ut).

2., our model ~ transition function: f(xt-1, ut-1)

Instead of xt in c(xt,ut), we can write x as the transition function of its previous state and its action. c(f(xt-1, ut-1),ut). We can rewrite the above formula as the following:

Since we have a sequence of steps, we are trying to minimize the cost at each step. Therefore the total loss is the sum of cost at each time step.

This intuitively means that, Link wants to take a good action at any moment but also towards the whole territory until he reaches humans.

The promise of the model is based around the relationship between how we find the optimal policy лθ based on transition function f(xt, ut) for predicting the future state x t+1.

So, now we want to optimize the total loss function for the entire sequence that includes two functions: cost function at each time step, and a transition function. We want to find an optimization technique that minimizes the loss for these two functions. We can use either linear or nonlinear optimization. We can use the Neural Network to minimize the total cost function.

If our environment is differentiable, we can optimize via back-propagation. in order to calculate the total loss, we require the following derivatives:

This part is what we mean by model. We want to understand the model with respect to state and action:

This part is our cost. We want to understand cost with respect to state and action:

World-Models

So far you learned about model-based method, We can use one of the model-based methods, World-Model, as the core mechanism behind software for the simulation for Link.

Here, the MD-RNN is similar to transition function x t+1 =f(xt, ut) for predicting the future state. However, it is only slightly different from what we learned so for. The MD-RNN adds its own hidden state to predict future state and ht+1.

As you might remember, earlier we discussed depending on the setting, we use either cost or reward function. World-Models uses a reward instead of the cost function. Here, the controller network acts as the reward function and it aims to find the policy that maximizing cumulative reward through the entire roll-out. The inputs of the controller are zt and ht and the output is an optimal action at. ht is an additional variable that controller network uses to predict optimal action. I encourage you reading world-model by yourself.

Thanks for saving Link ❤"
The Unexpected Lesson Within A Jelly Bean Jar,"A good way to start this comparison is probably by providing a definition of what neurons do in Artificial Neural Networks. I found this description to be rather compelling and simple to understand⁴:

Each neuron receives one or more input signals x 1, x 2, …, x m and outputs a value y to neurons of the next layer and so forth. The output y is a nonlinear weighted sum of input signals.

Under this point of view then, neurons in an ANN are the individuals of a collective thinking. In fact, the de facto architecture of ANN’s is a collection of connected individual regressors³. The output of a neuron with n input neurons is defined by⁵ :

Each output h then is a function with parameters W and b of the sum of individual linear regressions from all inputs x, which in turn will be the input (after an activation function, usually non-linear³ ⁶) of the next layer. The neurons collectively and only collectively, solve tasks. Try building an ANN classifier for a complex task with one neuron, you must probably going to fail. This will be like Galton asking one single person to give an estimate of the ox’s weight. The estimation is probably going to be wrong. It is here where ANN’s really work collectively. This concept can be visualized in the next example:

Taken from here

In the image above the trained NN is taking as input 784 features from the image of a “2"" and will classify it accordingly. The complexity of the system increases drastically with each added neuron, but in turn increases the amount of possible feature permutations that effectively pushes up the performance of the classifier. Add too many though and you will be a victim of overfitting⁷. I recommend you to visit this Google Playground to understand these and other concepts better where you can see the effect each added (or removed) neuron has on a simple classifier. Try training the model with only the first two features (X¹ and X²) and see the results. Now do it with more. can you find the minimum amount of neurons needed to get good results? Do you need many neurons/layers to do simple tasks? The answer is no. Will get back to this in a moment.

Going back to oxen and jelly beans, this will be like finding the minimum amount of individuals required for a very good estimation. Surely asking 10,000 people about the weight of the ox will reduce the error, but at 800 we are already 99% around the ground truth. Increasing the complexity of an algorithm is useful only when the desired output has not been satisfied. From here, computationally speaking will be best to reduce the amount of estimators to find the minimum required to reach the desired performance. The vox populi reduces the cost of the computation once this balance is found. To understand this, we can look at the next figure I quickly made in Python:

Distribution plots with μ = 1 and σ = 0.1 from 10 to 1000 points.

We can create a set of random normal distributions with μ = 1 and σ = 0.1 while increasing the amount of samples from 10 to 1000. Because we know that the mean ground truth is by design equal to 1, we can then compute the average across these distributions and see how close it gets to μ. As you might have guessed, the more data we have the better, meaning that our estimation gets closer and closer to our ground truth. After infinite samples we reach μ, but this is unpractical for obvious reasons. It might even be that that 1000 samples is too costly for whatever reason and we decide to use the set with 500 points for our analysis, which yields an error that satisfy our needs. It is our sweet spot: general enough to maximize performance, but specific enough to minimize error. Artificial Neural Networks follow a similar (albeit not identical, mind you) principle."
Street Segmentation (out of the box),"Street Segmentation (out of the box)

If you quickly need a model for Street Segmentation, this is the way to do it. shafu.eth · Follow 4 min read · Jul 6, 2019 -- 1 Listen Share

The street segmentation task is something that is very important for many different people in the industry and research that can be quite challenging. It can be very hard to design and train your own network architecture from the ground up with your own dataset. Therefore I want to show you here the best “out of the box” solutions for Street Segmentations and the available training sets.

What is Street Segmentation?

If you have an image and you want to know which pixels of the image are street and which are not, you have a Street Segmentation task.

More formally:

For a given high-resolution image, categorize each pixel into a set of semantic labels.

Those labels could be anything like a street sign, sidewalk, tree, person, car and of course street.

Available training data

If you want to train your own algorithm but you don’t have training data (HINT: Look at the licenses of the training data before using them) there are available datasets you can use:

Cityscapes: This dataset is created by the German car company Daimler. There are 5k images where the segmentation is nearly perfect and another 20k images with good segmentation masks. This dataset is unfortunately not available for you to download directly. You have to sign up on their website and request access. I would say it’s worth it though because this dataset is perfect for street segmentation.

CamVid

ADE20k: This data is provided by MIT. It wasn’t created specifically for street segmentation, but there some images with street in the dataset.

Pre-trained models

If you don’t want to train your own architecture you can simply use pre-trained models. The best I found is the TensorFlow DeepLab Model Zoo networks. You can find here models that are trained on the Cityscapes or the ADE20k dataset. The one I found to work very good for me is the Xception71 model trained on Cityscapes. You can download the model here.

Run pre-trained model

Load the model:

The load graph method takes as input the full path to your .pb file that you downloaded before. It returns a Tensorflow graph that we will use for inference.

2. Run the model:

We will run the model on one image with the segment method. The method takes as input the graph we created with the load_graph method and a full path to your image.

Then we have to initialize the input and output nodes of the model. After that we create a session load the image, resize it. You can resize your image to any size (I use this size to make inference faster). After that, we run the model and we get the prediction. The prediction is the segmentation mask.

You can’t open this segmentation mask just yet, because it is not a RBG image. It just assigns a label index for each pixel. So you have a number between 1–20 for each class. You will need to parse this prediction to view it as an image.

3. Parse the prediction:

We will assign each class a specific color. To create some RGB colors I use the following method.

This will create n evenly spaced RGB colors for us. Don’t ask me how this function used ;) I found it here. A big thanks go to Reed Oei for this function. It saved me some time copying and pasting random RGB values. Now we need to parse the prediction.

It takes as input the prediction that we created with the segment method. We then get all unique values from the prediction and create an empty numpy array with the same dimensions of the prediction but with 3 channels. Remember, we create an RGB image here. Then we get the RGB colors. Now we iterate over every unique value and get the indexes where the prediction matches this value. After that we assign that value an RGB code and fill the empty array for all the indexes we found with this color and repeat that for all unique values.

I know that you are confused right now. I could have written a function that would be much easier to read but would be very very slow. The numpy operations make this function super fast.

At the end, we save the array as a uint8 array and then create a PIL.Image object that we can open or save.

The whole code as one script:

Results

If you want to see how this model could perform, check this awesome youtube video out:

Conclusion

I hope this tutorial was helpful and understandable. I think street segmentation and the semantic segmentation task, in general, is a very cool field and I hope you could learn something here. If you have any questions, leave them in the comments below.

Thank you for reading and keep up the learning!

If you want more and stay up to date you can find me here:"
Initializing neural networks,"Notice that we normalize the validation set with train_mean and not valid_mean to keep the training and the validation sets on the same scale.

Since the mean (or sd) will never exactly be 0 (or 1), we also define a function to test if they are close to 0 (with some threshold)."
"In the New Era of Knowledge, Connection Beats Collection Every Time","Photo by fabio on Unsplash

“Look at the data — the numbers don’t lie.” It’s an often given piece of advice, but a less often understood one. Because what the person giving the advice really means is “Look at the data, and think about what it means for the situation we’re facing. Once you consider it in the broader context, you’ll see — and intuitively know — what to do next.” That’s a very different challenge to meet, but an infinitely more valuable one when you’re trying to make sense of a complex situation. And it’s possible only when you realise that connecting your data is even more important than collecting it.

Why Collection is no Longer Enough

Data collection is now mainstream. Storage is cheap, source are abundant and most products being built today are done so with the expectation that organisations will want access to their data in ways outside of the product’s offerings, via bulk export or automated delivery. Despite this helpful (and rapidly becoming expected) type of access, most data is still viewed within the single perspective that it was collected. Sure — graphs, charts and dashboards are possible and certainly helpful, but they’re often created using data that’s already been actioned, and usually to just re-tell the same story that’s already been told. The innovation factor is low, because the connection factor is also low.

But apart from these technical considerations, there is another more fundamental reason why disconnected data isn’t as helpful as it could be. Human beings are evolutionary creatures, and at our most basic level we are instinctively wired to consider the entirety of a situation before responding to it. Early human tribes didn’t seek out short-term gain if it compromised their chances of long-term survival (though it could be argued that the modern world has reduced our ability for this type of thinking) because to do so would contradict our unique awareness for considering relationships in our environment. The same holds true for data collected from technical systems — a broader context is lacking by default, and context is key to understanding.

What Connection Makes Possible

Data is enriched through connection, which offers a far more realistic representation of complex environments. When we look at data that is connected through a relevant framework, we engage the part of our mind that understands consequences, dependencies and how the quality of relationships affect the outcome we are seeking.

The argument is sometimes made that this type of thinking is still possible with disconnected data, it just requires us to put the pieces together in our head. This is the same argument that says human beings can multitask, but multiple studies have confirmed that we don’t — rather, we single-task with quick switching. This switching comes at a mental cost though, which can lead to poor outcomes through reluctance to gather all of the disconnected pieces every time a decision needs to be made.

Connecting data into a seamless, holistic perspective removes the need for switching. It enables us to understand the contributing factors of an event and act appropriately. Over time, this heightened awareness leads to the development of new knowledge, which if delivered back into the framework creates a unique feedback loop whereby insight gives rise to further insight. As knowledge is shared, unexpected opportunities for collaboration emerge. The innovation factor rises in response to the rising connection.

Connection is Always Relevant — Especially When Humans Are Involved

Understanding how to connect data in an organisation can sometimes be difficult. Over time people tend to specialise in a given area, and that specialisation can lead to the belief that one type of data has limited relevance to another. But if looked at closely, these same areas will usually be found to have unseen ties and concerns to others. Connecting the data of all areas in an organisation helps to generate a wider context that speaks to our evolutionary understanding of nature and our place in it, where no one entity truly exists in isolation.

This is even more true for organisations that work with human networks. We are deeply connected beings, with our behaviours often influenced far more by those around us (and even those around them) than we believe. These social ties often vary in type and strength, both of which affect the flow of influence and change. Proper consideration of these diverse relationships is fundamental to making informed decisions, and only possible when data is properly connected.

This article first appeared on InsightableTimes.org at https://insightabletimes.org/in-the-new-era-of-knowledge-connection-beats-collection-everytime"
Selenium and SQL Combined- Top Premier League Players,"Selenium and SQL Combined- Top Premier League Players

Source: Jannes Glas via unsplash

The Premier League 2019/20 season is scheduled to begin on Saturday 10th August 2019. Whilst the anticipation grows for the new season, I thought I would investigate the best performers of the Season just gone.

This tutorial piece will demonstrate how we can simply scrape data on the Top Performers of the 2018/19 season, and tabulate that data into a Database Management System (DMS), MySQL. The data gathered can then be queried using SQL.

To begin this tutorial, the first objective is to find a suitable source with key statistics on the Top Premier league players. A quick search leads me to the ‘Premier League Top Scorers’ page found on the BBC Football tab and shown below.

Data Mining

Key statistical metrics I would like to gather data on for each player are: their respective name and team they compete for, goals scored and assists provided, their shot accuracy and finally their minutes per goal ratio.

To begin, I need to find a wrapper that contains all the statistics for each player I am keen on scraping. I right-click in my chrome browser on the page of interest, and hover over the web element that I am searching for. Clearly the wrapper, ‘div.top-player-stats__item.top-player-stats__item — no-image’ contains all the information of interest (as highlighted in the image below).

Before writing any Python code, a working habit I encourage is to switch across to the console tab in the developers tool and search for the elements of interest. This approach is considered best practice because we can visually inspect if we have found our elements of interest.

To use this feature type 2 dollar signs, $$, then input the tag and identifier name in quotations within parentheses."
The Easiest Python Numpy Tutorial Ever,"The Easiest Python Numpy Tutorial Ever

Pie

Want to be inspired? Come join my Super Quotes newsletter. 😎

Python is by far one of the easiest programming languages to use. Writing programs is intuitive and so is reading the code itself — it’s almost like plain English!

One of Python’s greatest strengths is its endless supply of powerful libraries. Many of these libraries are written at least partially in C / C++ for speed and a Python wrapper on top for easy usage!

Numpy is one such Python library.

Numpy is mainly used for data manipulation and processing in the form of arrays. It’s high speed coupled with easy to use functions make it a favourite among Data Science and Machine Learning practitioners.

This article will be a code tutorial — the easiest one ever — for learning how to use Numpy!

Creating arrays"
Support Vector Machines (SVMs),"Support Vector Machines (SVMs)

A Brief Overview Afroz Chakure · Follow Published in DataDrivenInvestor · 3 min read · Jul 6, 2019 -- Listen Share

Introduction

Support Vector Machines (SVMs) are a set of supervised learning methods which learn from the dataset and can be used for both regression and classification. An SVM is a kind of large-margin classifier: it is a vector space based machine learning method where the goal is to find a decision boundary between two classes that is maximally far from any point in the training data.

Source : Towards Data Science

Support Vectors :

The term Support Vectors refers to the co-ordinates of individual observation. Support Vector Machine is a frontier which best segregates the two classes using a hyperplane/ line.

Source: bahrainpavilion2015.com

Working of SVM :

An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.

Identification of a perfect hyperplane. It should be the one that is perfectly segregating two classes.

In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.

Non-Linear Decision Boundary

Kernel Method :

Kernel method is used by SVM to perform a non-linear classification. They take low dimensional input space and convert them into high dimensional input space. It converts non-separable classes into the separable one, it finds out a way to separate the data on the basis of the data labels defined by us.

Performing Non-Linear Classification using Kernel Method

Features and Advantages of SVMs :

They maximize the margin of the decision boundary using quadratic optimization techniques which find the optimal hyperplane. It has the ability to handle large feature spaces. SVM’s are very good when we have no idea about our data. Works well with even unstructured and semi-structured data like text, Images and trees. The kernel trick is real strength of SVM. With an appropriate kernel function, we can solve any complex problem. It scales relatively well to high dimensional data. SVM models have generalization in practice, the risk of over-fitting is less in SVM.

Limitations of SVM :

It is sensitive to noise. The extension of classification to more than two classes is problematic. Choosing a “good” kernel function is not easy. Long training time for large datasets. Difficult to understand and interpret the final model, variable weights and individual impact. Since the final model is not so easy to see, we can not do small calibrations to the model hence its tough to incorporate our business logic. The SVM hyper parameters are Cost -C and gamma. It is not that easy to fine-tune these hyper-parameters. It is hard to visualize their impact

Some applications of SVM :

Source : DataFlair"
Hypothesis testing visualized,"Hypothesis testing visualized

In this article, we’ll get an intuitive, visual feel for hypothesis testing. While there are many articles online that explain it in words, there aren’t nearly enough that rely primarily on visuals; which is surprising since the subject lends itself quite well to exposition through pictures and movies.

But before getting too far ahead of ourselves, let’s briefly describe what it even is.

What is

Best to start with an example of a hypothesis test before describing it generally. The first thing we need is a hypothesis. For example, we could hypothesize that the average height of men is greater than the average height of women. In the spirit of ‘proof by contradiction’, we first assume that there is no difference between the average heights of the two genders. This becomes our default, or null hypothesis. If we collect data on the heights of the two groups and find that it is extremely unlikely to have observed this data if the null hypotheses were true (for example, “if the null is true, why do I see such a big difference between the average male and female heights in my samples?”), we can reject it and conclude there is indeed a difference.

For a general hypothesis testing problem, we need the following:

A metric we care about (average height in the example above). Two (or more) groups which are different from each other in some known way (males and females in the example above). A null hypothesis that the metric is the same across our groups, so any difference we observe in our collected data must merely be statistical noise and an alternate hypothesis which says there is indeed some difference.

We can then proceed to collect data for the two groups, estimate the metric of interest for them and see how compatible our data is with our null and alternate hypothesis. The last part is where the theory of hypothesis testing comes in. We’ll literally see how it works in the proceeding sections.

How to reject

Now that we’ve formed our hypothesis and collected our data, how do we use it to reject our null? The general framework is as follows:"
Zalando Dress Recommendation and Tagging,"In Artificial Intelligence, Computer Vision techniques are massively applied. A nice field of application (one of my favorite) is the fashion industry. The availability of resources in terms of raw images allows for developing interesting use cases. Zalando knows this (I suggest to take a look at their GitHub repository) and frequently develops amazing AI solutions, or publishes juicy ML research studies.

In the AI community, Zalando research team is also known for the release of Fashion-MNIST, a dataset of Zalando’s article images, which aims to replace the traditional MNIST dataset in the study of machine learning. Recently they released another interesting dataset: the Feidegger. A dataset composed of dress images and related textual descriptions. Like the previous one, this data was donated by Zalando to the research community to experiment with various text-image tasks such as captioning and image retrieval.

In this post I make use of this data to build:

a Dress Recommendation System based on image similarity;

based on image similarity; a Dress Tagging System based only on the textual description.

THE DATASET

The dataset itself consists of 8732 high-resolution images, each depicting a dress from the available on the Zalando shop against a white-background. For each of the images were provided five textual annotations in German, each of which has been generated by a separate user. The example below shows 2 of the 5 descriptions for a dress (English translations only given for illustration, but not part of the dataset).

source Zalando

At the beginning the dataset stores for each singular description the related image (in URL format): we have for a singular dress plus entries. We start to merge the description of the same dress to easily operate with images and reduce duplicates."
DeepPiCar — Part 6: Traffic Sign and Pedestrian Detection and Handling,"Modeling Training

There are a few steps involved in model training.

Image collection and labeling (20–30 min) Model selection Transfer learning/model training (3–4 hours) Save model output in Edge TPU format (5 min) Run model inferences on Raspberry Pi

Image Collection and Labeling

We have 6 object types, namely, Red Light, Green Light, Stop Sign, 40 Mph Speed Limit, 25 Mph Speed Limit, and a few Lego figurines as pedestrians. So I took about 50 photos similar to the above and placed the objects randomly in each image.

Then I labeled each image with the bounding box for each object on the image. It seemed like it would take hours, but there is a free tool, called labelImg (for Window/Mac/Linux), which made this daunting task felt like a breeze. All I had to do was to point labelImg to the folder where you stored the training images, for each image, dragged a box around each object on the image and chose an object type (if it was a new type, I could quickly create a new type). So if you are using the keyboard shortcuts, you would only spend about 20–30 sec per picture, and so it took me only 20 min to label about 50 photos (with about 200–300 object instances). Afterward, I just randomly split the images (along with its label xml files) into training and test folders. You can find my train/test dataset in DeepPiCar’s GitHub repo, under models/object_detection/data .

Labeling Images with LabelImg Tool

Model selection

On a Raspberry Pi, since we have limited computing power, we have to choose a model that both runs relatively fast and accurately. After experimenting with a few models, I have settled on the MobileNet v2 SSD COCO model as the optimal balance between speed and accuracy. Furthermore, for our model to work on the Edge TPU accelerator, we have to choose the MobileNet v2 SSD COCO Quantized model. Quantization is a way to make model inferences run faster by storing the model parameters not as double values, but as integral value, with very little degradation in prediction accuracy. Edge TPU hardware is optimized and can only run quantized models. This article that does a deep dive on the hardware and performance benchmark of Edge TPU, for those interested.

Transfer Learning/Model Training/Testing

For this step, we will use Google Colab again. This section is based on Chengwei’s excellent tutorial on “How to train an object detection model easy for free”. Our twist is that we need to run on Raspberry Pi, with the Edge TPU accelerator. As the entire notebook along with its output is quite long, I will present the key parts of my Jupyter Notebook below. The full Notebook code can be found on my GitHub, which contains a very detailed explanation for each step.

Set up Training Environment

The above code chooses MobileNet v2 SSD COCO Quantized model, and downloads the trained models from TensorFlow GitHub. This section is designed to be flexible in case we want to choose a different detection model.

Prepare Training Data

The above code converts the xml label files generated by LabelImg tool to binary format (.record) so that TensorFlow can process quickly.

Download Pre-trained Model

'/content/models/research/pretrained_model/model.ckpt'

The above code will download the pre-trained model files for ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03 model and we will only use the model.ckpt file from which we will apply transfer learning.

Train the Model

This step takes 3–4 hours, depending on the number of steps you train (aka epochs or num_steps ). After the training is done, you will see a bunch of files in the model_dir . We are looking for the latest model.ckpt-xxxx.meta file. In this case, since we ran 2000 steps, we will use the model.ckpt-2000.meta file from model_dir .

!ls -ltra '{model_dir}/*.meta' -rw------- 1 root root 17817892 Apr 16 23:10 model.ckpt-1815.meta

-rw------- 1 root root 17817892 Apr 16 23:20 model.ckpt-1874.meta

-rw------- 1 root root 17817892 Apr 16 23:30 model.ckpt-1934.meta

-rw------- 1 root root 17817892 Apr 16 23:40 model.ckpt-1991.meta

-rw------- 1 root root 17817892 Apr 16 23:42 model.ckpt-2000.meta

During training, we can monitor the progression of loss and precision via TensorBoard (source code in my GitHub link above). We can see for the test dataset that loss was dropping and precision was increasing throughout the training, which is a great sign that our training is working as expected.

Total Loss (lower right) keeps on dropping

mAP (top left), a measure of precision, keeps on increasing

Test the Trained Model

After the training, we ran a few images from the test dataset through our new model. As expected, almost all the objects in the image were identified with relatively high confidence. There were a few images that objects in them were further away, and were not detected. That’s fine for our purpose because we only wanted to detect nearby objects so we could respond to them. The further away objects would become larger and easier to detect as our car approached them.

Save Model Output in Edge TPU Format

Once the model is trained, we have to export the model meta file to the inference graph in Google ProtoBuf format, then to a format that Edge TPU accelerator can understand and process. This seems like a simple step, but at the time of writing, the online resources (including Edge TPU’s own site) were so scarce that I spend hours researching the right parameters and commands to convert to a format that the Edge TPU can use.

Luckily, after many hours of googling and experimenting, I have figure out the below commands to do this.

In the end, we get a road_signs_quantized.tflite file, which is suitable for mobile devices and Raspberry Pi CPU to perform model inference, but not yet for the Edge TPU accelerator.

!ls -ltra '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/*.tflite' root root 4793504 Apr 16 23:43 road_signs_quantized.tflite

For the Edge TPU accelerator, we need to perform one more step. That is to run road_signs_quantized.tflite through the Edge TPU Model Web Compiler.

After you upload road_signs_quantized.tflite to the web compiler, you can download another tflite file, save it as road_signs_quantized_edgetpu.tflite . Now you are done! There will be some warnings, but that can be safely ignored. Note that 99% of the model will on the Edge TPU, which is great!

The difference between the _edgetpu.tflite file and regular .tflite file is that, with the _edgetpu.tffile file, all (99%) model inferences will run on the Edge TPU, instead of Pi’s CPU. For practical purposes, this means you can process about 20 320x240 resolution images per sec (aka. FPS, Frames Per Sec) with the Edge TPU, but only about 1 FPS with the Pi CPU alone. 20 FPS is (near) real-time for DeepPiCar, and this is worth the $75. (Well, I think this $75 cost should be lower considering the entire Raspberry Pi — CPU + circuit board is only $30!)

Planning and Motion Control

Now that DeepPiCar can detect and identify what objects are in front of it, we still need to tell it what to do with them, i.e. motion control. There are two approaches for motion control, i.e. rule-based and end-to-end. The rule-based approach means we need to tell the car exactly what to do when it encounters each object. For example, tell the car to stop if it sees a red light or pedestrian, or drive slower if it sees a lower speed limit sign, etc. This is akin to what we did in Part 4, where we told the car how to navigate within a lane via a set of code/rules. The end-to-end approach simply feeds the car a lot of video footage of good drivers, and the car, via deep-learning, figures out on its own that it should stop in front of red lights and pedestrians, or slow down when the speed limit drops. This is similar to what we did in Part 5, end-to-end lane navigation.

For this article, I chose the rule-based approach, because 1) this is how we, humans, learn how to drive, by learning the Rules of the Road, and 2) it is simpler to implement.

Since we have six types of objects (Red lights, Green lights, 40 Mph Limit, 25 Mph Limit, Stop Sign, Pedestrians), I will illustrate how to handle a few object types, and you can read my full implementation in these two files on GitHub, traffic_objects.py and object_on_road_processor.py .

The rules are pretty simple: if no object is detected, then drive at the last known speed limit. If some object is detected, that object will modify car’s speed or speed limit. For example, stop when you detect a red light that is sufficiently close, and go when you don’t detect a red light.

First, we will define a base class, TrafficObject , which represents any traffic signs or pedestrians that can be detected on the road. It contains a method, set_car_state(car_state) . The car_state dictionary contains two variables, namely speed , and speed_limit , which will be changed by the method. It also has a helper method, is_close_by() , which checks if the object detected is close enough. (Well, since our single camera can’t determine distance, I am approximating distance with the height of the object. To determine distance accurately, we would need a Lidar, or its little cousin, an ultrasonic sensor, or a stereo vision camera system as in a Tesla.)

The implementations for Red Light and Pedestrian are then trivial, which simply set the car speed to 0.

Both 25 Mph and 40 Mph speed limits can use just one SpeedLimit class, which takes speed_limit as its initialization parameter. When the sign is detected, just set the car speed limit to the appropriate limit.

The green light implementation is even simpler, as it does nothing but printing a green light is detected (code not shown). The stop sign implementation is a bit more involved, as it needs to keep track of states, meaning it needs to remember that it has stopped at the stop sign for a few seconds already, and then move forward even if the subsequent video images contain a very large stop sign as the car moves past the sign. For details, please refer to my implementation in traffic_objects.py .

Once we have defined the behavior for each traffic sign, we need a class to tie them together, which is the ObjectsOnRoadProcessor class. This class first loads the trained model for Edge TPU, then detects objects in the live video with the model, and lastly calls each traffic object to change the car’s speed and speed limit. Here are the key parts of objects_on_road_processor.py .

Note that each TrafficObject just changes the speed and speed_limit in the car_state object, but doesn’t actually change the speed of the car. It is ObjectsOnRoadProcessor that changes the actual speed of the car, after detecting and processing all traffic signs and pedestrians.

Putting it together

This is the final outcome of this project. Notice that our DeepPiCar stopped at stop signs and red lights, waited for no pedestrians and moved forward. Also, it slowed down at 25 miles/hour sign and sped up at 40 miles/hour sign.

The full source code of interest is on DeepPiCar’s GitHub:

What’s Next

In this article, we taught our DeepPiCar to recognize traffic signs and pedestrians, and respond to them accordingly. This is not a small feat, as most cars on the road can’t do this yet. We took a shortcut and used a pre-trained object detection model and applied transfer learning on it. Indeed, transfer learning is prevalent in the AI industry when one can’t gather enough training data to create a deep learning model from scratch or don’t have enough GPU power to train models for weeks or months. So why not stand on the shoulders of giants?

In future articles, I have quite a lot of ideas. Here are some fun projects to try in the future.

Hey, Jarvis, Start Driving!

I want to put a microphone on the car, and train it to recognize my voice and my wake word, so that I can say, “Hey, Jarvis, start driving!” or “Hey Jarvis, turn left!” We did a wake-word project in Andrew Ng’s Deep Learning Course, so wouldn’t it be SUPER COOL to be able to summon DeepPiCar, like Ironman or Batman?

JARVIS, Tony Stark’s (Ironman) AI Assistant

Adaptive Cruise Control

Another idea is to install an ultrasound sensor on DeepPiCar. This is similar to a lidar on a real-life vehicle, whereas we can sense the distance between DeepPiCar and other objects. For example, with an ultrasound sensor, we would be able to implement ACC (Adaptive Cruise Control), an essential feature of an autonomous vehicle, or we can enhance our object detection algorithm by only detect for objects when they are within a certain range.

End-to-End Deep Driving

I also want to experiment with full end-to-end driving, where I will remote control the car for a while, save down the video footage, its speed, and steering angle, and just apply deep learning. Hopefully, deep learning will teach it how to drive, i.e. both follow lanes as well as respond to traffic signs and pedestrians, by mimicking my driving behavior. That would be a super cool project if it actually works. This means we don’t have to tell the car ANY rules, it just figures EVERYTHING out by watching!! This would be the Holy Grail of autonomous cars, which is similar to how AlphaZero learns to play Go or Chess, without any human inputted heuristics.

Thank you!

If you have read this far (or built/code along with me), BIG KUDOS to you! If everything worked so far, you should have a deep-learning, self-driving robotic car running in your living room, and be able to stop when it observes a stop sign or a red light! If not, please post a message down below, and I will try my best to help. I really enjoyed sharing this wonderful journey with you. Indeed, it took me a lot longer writing these articles than writing code, training models, and testing DeepPiCar in my living room combined. This also happened to be my first blogging experience, which was both frightening and exciting at the same time. I now realize that I love building things as much as showing others how to build things.

As usual, here are the links to the whole guide, in case you need to refer to a past article. Now I need to find myself a job as a Machine Learning Engineer in an Autonomous Car company! See you in a while!

Part 1: Overview

Part 2: Raspberry Pi Setup and PiCar Assembly

Part 3: Make PiCar See and Think

Part 4: Autonomous Lane Navigation via OpenCV

Part 5: Autonomous Lane Navigation via Deep Learning

Part 6: Traffic Sign and Pedestrian Detection and Handling (This article)"
How to Code Effectively Without Dying in the Attempt,"Programming for non-programmers

How to Code Effectively Without Dying in the Attempt

Image from Émile Perron available at Unsplash

Coding might seem like an easy task. But coding effectively might not. Nowadays many programmers encounter multiple situations that have significant impacts on their work’s productiveness; some help them achieve their objectives in a faster and more efficient way while some others just contribute to stress them and stuck them on their tasks. Everyone has a different strategy to deal with those situations, either because they have been through them or because someone else has told them how to deal with them. Based on my colleagues’ and personal experiences, and looking forward in helping new and experienced programmers on their careers, I would like to share the following 5 advices I have compiled for coding effectively that have worked for me so far in my professional career. Hope you find them useful.

1. Find a comfortable working space

Most programming and coding jobs are flexible enough that allow to work from home, a common space, a library or even a coffee shop, without having to be at an office 8 hours per day 5 days per week. However, the working environment will always have a highly significant impact on your work productiveness. Find a place where you feel comfortable working at, away from distractions, noises and situations that can potentially slow your working pace. Identify the factors that make you more productive (e.g. a cup of coffee or tea, background music, illumination, amenities) and use them in your advantage. Think about it: there is a reason behind why Google and other companies’ working spaces are getting away from the traditional working cubicles. Since they are very aware of how stressful spending 8 hours per day coding in front of a computer can be, they are constantly looking for ways of making their employees feel comfortable at their facilities. There is no surprise on why they are considered as the best companies to work at.

2. Use help resources when in doubt

It is impossible to know the syntax of all the libraries and functions that exist out there. For that reason, there are plenty of documentation and help resources that…"
4 Steps to Break Into Data Science in 2020,"4 Steps to Break Into Data Science in 2020

2020 is almost here, which means it’s that time of the year when you take a piece of paper and make a list of goals you want to accomplish in the next year. Dario Radečić · Follow Published in Towards Data Science · 6 min read · Dec 30, 2019 -- 1 Share

Photo by Jude Beck on Unsplash

Nothing wrong with that, but you probably know that it’s very easy to make an exhaustive list of near-impossible, time-consuming goals that will only make you feel overwhelmed, and very likely not motivated — because there’s so much to do.

If you’re planning to enroll in data science in the next year, I’d say you’ve made a great decision. The field is widely accepted, there are jobs everywhere, salaries are great, and even the management is slowly figuring out why data science is needed.

But before we start, let me to slightly demotivate you (yes, it’s necessary) — one year isn’t enough to learn the entire field.

Don’t get me wrong, 1 year is enough for you to land your first job, but the chances are you won’t go from 0 to data science team lead in a year (if you manage to do so please share your story in the comment section).

With that being said, let’s explore all the skills you’ll need and how to learn just enough of them to get you started.

1. Brushing up the Math skills

You’ve most likely heard of harsh math prerequisites of data science. The amount of math you’ll need to know will vary much depending on the job role, but as a general answer to how much math you will need to get started, I would say: less than you think."
Fire Your BI Team,"Fire Your BI Team

The term “Business Intelligence” gained widespread popularity in the 1990s and was originally defined as “concepts and methods to improve business decision making by using fact-based support systems.” Clearly, this sounds like a strategically important function for any company.

So why the harsh headline? While the description above may capture what BI teams did in the 1990s, that’s not what BI teams have been up to in the last decade. As adoption and sophistication of data analytics changed, BI teams became the bottom-feeders of the analytical ecosystem while data scientists received all the fun and glory. BI teams ended up being order-takers executing precise requests prescribed by people who do not do analytics for a living. This has severely reduced their impact.

But it doesn’t have to be like this. And you don’t need to fire your BI team. We can restore BI’s place in the analytical ecosystem by changing their mandate and how they interact with stakeholders. This will lead to more impactful insights while ensuring that the analysts themselves feel professional ownership and purpose.

Let’s walk through why and how.

To see what’s wrong with BI today just consider the common, ticket-based engagement model: The BI analyst receives a ticket, executes the request, returns the answer, gets feedback through more tickets, and the cycle continues. A conversation may not even take place.

Now, if you’re someone who works with BI teams you might think this type of engagement model is exactly what you want. Fair enough — it does sound efficient and organized.

But trust me, it’s not what you want.

Just think about the nature of the work required to extract meaningful and actionable insights. Even if the goal is to keep the analysis simple, you always end up on an analytical journey that involves slicing the data by segments, creating baselines, constructing derived metrics, and often more. This requires ownership and creativity from the analyst.

But in the BI engagement model, the analyst is merely following someone else’s train of thought. Ownership and creativity are lost.

So what’s the solution here? First, always kick off any “BI” analysis or dashboard with these two questions: (1) what’s the problem we’re trying to solve? and (2) what actions are we looking to take? From there, the onus is on the analyst to seek the question behind the question and share insights as they surface. Tickets will be replaced with face-to-face meetings and a symbiotic partnership will be formed. And if people can’t find time for that, then the question wasn’t important to begin with.

If you’ve gotten this far, you might think I’m suggesting to convert all BI analysts into data scientists. But that’s not the case. While ETL and data exploration are core to any team that turns data into cashflows, there are some key differences:

Data scientists should (mainly) be focused on efforts that lead to “data products” based on advanced methods. This could be ML models that drive product recommendations, or pricing and matching algorithms.

The focus of the analysts, on the other hand, might be to deliver a slide deck with strategic recommendations or a dashboard. In fact, I favor the term “strategic analytics” for these teams because it describes the purpose of their roles.

Both functions — data science and strategic analytics — are tremendously important, have clear purposes, require different competencies, and offer rewarding career paths. Most companies need both teams, but they don’t need a BI team. So if you’re managing a BI team, it’s time to change their mandate and the stakeholder engagement model. And let’s retire the term “Business Intelligence” once and for all."
Costa Rican Household Poverty Level Prediction in R,"Costa Rican Household Poverty Level Prediction in R

Authors: Chaithanya Pramodh Kasula and Aishwarya Varala

A map of Costa Rica

Introduction: The current report details the process of answering several research questions related to the poverty levels of Costa Rican households. It is comprised of data sources, exploratory data analysis through visualization, model development, fine-tuning, approaches to tackle data imbalance problems, performance metrics and visualization of results.

Background: In a state or a locality, it is important for the government or banks to identify the right households which are in need of aid for their social welfare programs. It was observed that people living in the economically backward areas, do not have the necessary knowledge or cannot provide the necessary documents such as income/expense records to prove that they qualify for the aid.

In Latin America, a popular method, called the Proxy Means Test, is used to make that decision. Agencies look at a family’s observable attributes such as the material of the ceiling, number of rooms in the household, number of people in the household, etc, to conclude about a family’s qualification for the aid. However, accuracy remains a problem. Hence, the Inter-American Development Bank has provided a data set to the Kaggle community to come up with new methods that could effectively direct them towards the households that are in need of social welfare assistance.

Data Source: The data set used in the project is extracted from Kaggle. The URL of the data source is: https://www.kaggle.com/c/costa-rican-household-poverty-prediction/overview

Data Description: The data folder consists of train.csv and test.csv with 9557 rows and 23856 rows respectively. However, the test.csv does not contain the ‘target’ column which determines the poverty level. Hence, train.csv alone is used as the data set whose size is 3.08 MB. The number of columns is 143. Each record is associated with a single person. The descriptions of 143 columns are found in the URL of the data source mentioned above. The descriptions for a few columns are provided below.

Target: denotes poverty level 1 = extreme poverty, 2 = moderate poverty, 3 = vulnerable households, 4 = non-vulnerable household

Idhogar: A unique identifier for each household. People belonging to a single household are identified by this column.

v2a1: Monthly rent paid by each household.

rooms: the number of rooms in the house.

escolari: years of schooling etc.

Associated Research Questions:

R1: Can we construct a model to identify the level of poverty for various Costa Rican households?

R2: Can we identify the most important factors/columns/predictors that determine the level of poverty for a household?

R3: Is there any relationship between the education attained, gender, head of the household, number of persons, number of rooms in the household, dependency, and technology (mobile phones, computer, television, tablet) to the level of poverty for a household?

R4: In the absence of the ‘target’ column, and with the given features in R3, how accurately can K-Means clustering algorithm help in assigning the class label (determining the poverty level/values of the target column) for a person?

Data types of variables: There are four data types associated with the variables in the data set:

Boolean: Integer Boolean (0 or 1), Character Boolean (yes or no). Columns such as paredblolad, noelec, etc. Float data type. For example, meaneduc, overcrowding, etc. Integer data type. For example, age, rent, rooms, tamviv, etc. Alpha-numeric. For example, Id, idhogar.

Data Exploration Class distribution: The poverty level distribution (class distribution) is very imbalanced as shown in Fig. 2. The number rows belonging to class ‘four’ form 65.72% of the data set but the number of rows belonging to class ‘one’ account to only 0.074%. The same uneven distribution has been observed in the household-level data set as depicted in Fig. 3. For a detailed understanding of household-level data set please read the ‘Household data set’ section.



Number of NaN values in monthly_rent_payment column: There are 6860 rows that contain NaN values in the ‘monthly_rent_payment’ column. The columns, ‘own_and_fully_paid_house’, ‘own_paying_in_installments’, ‘rented’, ‘precarious’ and ‘other_assigned_borrowed’ contain binary values that denote 0 (FALSE) or 1 (TRUE). From Fig. 4, it can be inferred that there are 5911 people who own a house and 961 people who own the house but pay installments. This fact can be immensely useful during data pre-processing.

Fig. 2

Fig. 3

Data pre-processing: The original column names have been renamed to their shortened English equivalent descriptions for easy reference, understanding, and communication. They can be found in the ‘changed_column_names.csv’. Henceforth, data will be referenced through the renamed columns.

Missing value treatment and Feature Engineering: From the research questions, R1, R2, and R3, it can be interpreted that the unit of analysis is a household. However, each record in the data set describes the attributes of a single person in the household. People belonging to a single household can be grouped together by the ‘household_identifier’ column as they have the same identifier value assigned to each of them. ‘Household_identifer’ values are unique to every household. Additionally, the same value of the ‘target’ class (poverty level) is assigned to all the people in a single household.

Missing values are found across multiple columns in the data set. With the explanations drawn in the ‘Data Exploration’ section over the NaN values present in the ‘monthly_rent_payment’ column, it is assumed that all the people who own a house do not pay the rent. There are only 7 people left. Since their count is too less, all the rows with NaN values in the ‘monthly_rent_payment’ column are replaced with a zero.

The column ‘number_of_tablets_household_owns’ also contains NaN values. The column ‘owns_a_tablet’ indicates whether a household owns a tablet or not. If the household does not own a tablet, then its value in the ‘number_of_tablets_household_owns’ column is replaced with a zero. For every household, the mean values for the columns ‘years_of_schooling’ and ‘years_behind_in_school’ are computed and assigned to the household head. The character Boolean values (yes or no) in the ‘dependency’ column have been replaced with 1 or 0 respectively. The same operation has been performed for ‘edjefe’ and ‘edjefa’ columns.

Fig. 4

Fig. 5 — Correlation plot between highly correlated features

Duplicate columns are removed from the data set. For example, there are two columns with the same name ‘age_squared’ that are duplicates of each other. Only, one of them is retained. Additionally, there are a lot of columns that are squared values of existing columns such as overcrowding_squared, dependency_squared, etc. All such columns have been removed from the data set as they provide no additional information to the model. Also, the class variable, ‘target’, consists of poverty levels in numeric format 1, 2, 3 and 4. They have been replaced with words one, two, three and four respectively. The columns (if_stepson_or_doughter, if_son_or_doughter_in_law, if_grandson_or_doughter, if_father_or_mother_in_law, if_brother_or_sister,if_brother_or_sister_in_law, if_other_family_member, if_spouse_or_partner, if_son_or_doughter, if_mother_or_father) are unimportant and do not fall under the scope of answering the research questions and therefore have been eliminated. The packages ‘dplyr’ (Wickham et al., 2017), and ‘stringr’ (Wickham, 2019) are used for data pre-processing. The packages ‘ggplot’ (Wickham, 2016) and Tableau (Tableau Software, 2019) software were utilized for data visualization. The package ‘corrplot’ (Wei et al., 2017) was used for plotting the correlation matrix.

Using Correlation to reduce features: A correlation matrix has been constructed for 119 columns which remain after the pre-processing stage. Visualization of such a huge plot is clumsy. Hence, highly correlated features whose correlation value is greater than 0.98 have been extracted from the matrix and plotted separately. As the software cannot plot cannot incorporate lengthy column names, they have been represented with numbers in Fig. 5. The numbers 1 to 14 in the picture correspond to the columns ‘size_of_the_household’, ‘household_size’ , ‘of_total_individuals_in_the_household’, ‘total_persons_in_the_household’, , ‘toilet_connected_to_sewer_or_cesspool’, ‘if_household_head’, ‘region_brunca’, ‘if_widower’, ‘no_main_source_of_energy_used_for_cooking_no_kitchen’,‘if_predominant_material_on_the_outside_wall_is_natural_fibers’, ‘electricity_from_cooperative’, ‘if_predominant_material_on_the_roof_is_natural_fibers’, ‘if_predominant_material_on_the_floor_is_wood’ and ‘if_predominant_material_on_the_roof_is_fiber_cement_mezzanine’ respectively. The purpose of constructing a correlation plot is to remove highly correlated columns from the data set as they do not provide any additional value. From Fig. 5, it is observed that ‘size_of_the_household’, ‘household_size’, ‘of_total_individuals_in_the_household’, ‘total_persons_in_the_household’ are highly correlated to each other. Hence, only one of those columns is included in the data set.

Household data set: The household head is regarded as a representative of each household. Hence, only rows whose ‘if_household_head’ column equals 1 are made a part of this data set. Features such as ‘years_of_schooling’ that are associated with a single person have been appropriately handled to reflect the household during data pre-processing. A total of 2973 rows form this data set and henceforth, it will be referenced as the household dataset.

Modelling using Random Forest: In order to answer the first research question, the household data set is used to train a random forest. For a given instance, whose poverty level/target is unknown, the trained model would predict the class for that row. Random Forest is an ensemble learning technique that builds trees of varying lengths by taking samples from the data set (bootstrap sampling). The left-over data which is not a part of the construction is often called the Out-Of-Bag (OOB) data set. The constructed model then uses the OOB data set as a test set and evaluates its performance on its own. Assuming the rows are independent of each other (as in our case), there is no need to perform cross-validation separately while using Random Forest. It is implicitly done internally with the help of OOB data. The OOB error for each constructed decision tree can be averaged to represent the model’s overall error rate. Random Forests also generalize well and avoid overfitting which is one of the major problems observed in Decision Trees. A package called ‘randomForest’ (Liaw et al., 2002) from the ‘caret’ package (Kuhn et al., 2019) is used for training and testing the data.

Splitting the data set into Train and Test sets: Due to the class imbalance problem, randomly splitting the data set in the ratio of 75:25 for training and test sets does not extract a significant number of rows associated with the minority class in the test set. Hence, the obtained performance metrics are not very reliable. Therefore, 75% of data from each class is made a part of the training set and 25% of data from each class was made a part of the test set. Therefore, 75% and 25% of data from each class constitute the train and test set respectively. The resultant number of rows in the train and test set are 2230 and 744 respectively.

Training: For the first iteration, 112 columns (columns left after pre-processing) were used for training. Alphanumeric features such as ‘id’ and ‘household_identifier’ columns were removed from the training set. The hyperparameters used for training the classifier are: ‘ntree=500’ and ‘mtry=10’. Various ‘mtry’ values were tested but ‘mtry’ equalling 10 resulted in better performance. In order to reduce the number of columns used for training, the MeanDecreaseinGini values are extracted from the trained model. The value of the mean decrease in Gini is directly associated with the importance of a feature. The greater the mean decrease in Gini value for a feature, the more its importance in predicting the target variable. Fig. 6 represents the top 15 important features. In the coming iterations, only these 15 features are used for training. This reduces the cost of training by decreasing the number of features from 112 to 15.

Fig. 6 — Top fifteen important features from the random forest model

Fig. 7 — OOB estimate and Confusion Matrix without Sampling

The OOB error estimate is shown in Fig. 7. It can be noticed that the class error for ‘four’ is very less. Whereas, the class error for ‘one’, ‘two’ and ‘three’ are high. The model was trained well on class 4 because the number of records for class 4 are relatively high and hence low error. But, due to the lesser number of records associated with ‘one’, ‘two’ and ‘three’ classes, the model was not trained well, hence the high error.

Sampling: In order to adjust the distribution of classes in the data set, two popular techniques known as under-sampling and oversampling have been employed. Under-sampling a class involves taking only a fraction of records associated with the majority class. To illustrate, only a few records from class ‘four’ are extracted and made a part of the data set. The records belonging to other minority classes are unchanged. Under-sampling results in loss of data. Oversampling involves synthesizing/duplicating records belonging to the minority classes.

Under-sampling: Random under-sampling has been performed for class ‘four’. 35% of the records belonging to class ‘four’ have been chosen randomly and made part of the data set. So, the number of records belonging to class ‘four’ was reduced from 1954 to 684. No sampling has been performed on the records belonging to other classes. After under-sampling, Fig. 8 and Fig. 9 show the class distribution for train and test sets respectively. Fig. 10 details the OOB error estimate for the under-sampled data set. It can be observed that the OOB error estimate for classes ‘one’, ‘two’ and ‘three’ has not reduced significantly when compared to Fig. 7. The class error for ‘four’ has increased. The model does not perform well over any class due to the lesser number of records available for all the classes. Hence, under-sampling the data set when a lesser number of records are available is a bad approach as it can lead to underfitting.

Fig. 8

Fig. 9

Fig. 10 — OOB estimate and Confusion Matrix during Under-sampling

Fig 11 — OOB estimate and Confusion Matrix during Oversampling

Oversampling: Random oversampling has been performed for classes ‘one’, ‘two’ and ‘three’. The records associated with the mentioned classes have been duplicated appropriately to minimize the difference in class distribution. After oversampling, Fig. 12 and Fig. 13 represent the class distribution in train and test sets respectively. The model has been retrained and the OOB error estimate is represented in Fig. 11.

Fig. 12

Fig. 13

It can be noticed that there is a significant decrease in the over-all OOB error estimate and the respective class errors. Hence, oversampling the records of the minority classes promotes better model training and development. Fig. 14 and Fig. 15 represent the change in the training error rate of different classes with the increase in the number of trees during under-sampling and oversampling. The colors red, green, blue and purple represent classes ‘four’, ‘one’, ‘two’ and ‘three’ respectively. The black line represents the over-all OOB error rate. During oversampling, the error rate for all the classes decreases with the increase in the number of trees.

Testing: The total number of records in the test set is 488. It comprises 25% of records extracted from each class. The columns ‘target’, ‘id’ and ‘household_identifier’ have been removed from the test set. The resultant data is sent to the trained random forest classifier to obtain results. For each record in the test set, the classifier utilizes the knowledge acquired through training to predict the class for that record. The predicted labels are evaluated against their original values in the ‘target’ class which determines the model’s performance.

Performance Metrics: Accuracy is one of the primary metrics for evaluating classification models. It is defined as follows:

Fig. 14

Fig. 15

However, in classification problems, Accuracy alone cannot be used to evaluate a classifier. A closer inspection of the obtained results can be made through a Confusion Matrix. A confusion matrix is a combination of predicted and actual values of different classes. Important metrics such as Precision (Specificity) and Sensitivity (Recall) can be derived from it. Sensitivity is also called as true positive rate. Specificity is also known as the true negative rate. The higher the values of Accuracy, Sensitivity, and Specificity for the existing classes, the better the model.

The function ‘confusionMatrix’ from the caret (Kuhn et al., 2019) package is used to obtain the confusion matrix by providing actual and predicted values as input. Fig. 16 and Fig 17 represent the confusion matrices and the classifier’s performance for the respective techniques. However, the metrics represented in the population must be adjusted as these metrics are for the extracted samples but not for the whole population. Therefore, the obtained metrics must be adjusted so that they reflect the actual population.

For a sample, if C1, C2, C3, and C4 denote a classification metric for classes ‘one’, ‘two’, ‘three’ and ‘four’ respectively, then, its weighted metric value for the whole/original population is denoted by,

where N1, N2, N3, N4 represent the original/actual population size and S1, S2, S3, S4 represent the sample population size. Therefore the weighted Accuracy, Weighted Specificity, Weighted Sensitivity for the classifier trained on under-sampled data are 53.41%, 78.65%, and 18.01% respectively. The Weighted Accuracy, Weighted Specificity, Weighted Recall for classifier trained on oversampled data set are 84.39%, 93.24%, and 84.44%, respectively.

Evaluating the relationship between the features mentioned in R3 and the poverty level: In order to evaluate the relationship between education attained, gender, head of the household, number of persons, number of rooms in the household, dependency, and technology (mobile phones, computer, television, tablet) to the level of poverty, the model is trained with only the below mentioned features. The trained model is then used to predict the records in the test set. The closer the performance metrics of the new classifier, to the performance metrics of the old classifier (model trained with the top 15 important features), the stronger the relationship between the mentioned features and the ‘target’ (poverty level).

Fig. 16

Fig. 17

The new model is trained by oversampling classes ‘one’, ‘two’ and ‘three’ but only using the features ‘edjefa’, ‘edjefe’, ‘years_of_education_of_male_head_of_household_squared’,‘dependency’, ’overcrowding’, ‘meaneduc’, ‘years_of_schooling’, ‘total_females_in_the_household’, ‘total_persons_in_the_household’, ‘no_of_mobile_phones’, ‘total_males_in_the_household’, ‘if_the_household_has_notebook_or_desktop_computer’. Fig. 18 details the obtained performance metrics of the retrained model over the test set.

Fig. 18

Fig. 19

The over-all adjusted Weighted Accuracy, Weighted Sensitivity and Weighted Specificity for the original population are 81.11%, 0.8115 (81.15%), and 0.9059 (90.59%), respectively. The results show that the features exhibit a strong association with the ‘target’ class. Their sorted MeanDecreaseGini values as shown in Fig. 19. Among the selected features, ‘years_of_schooling’ is the most important feature and ‘if_the_household_has_notebook_or_desktop_computer’ is the least important.

Clustering: Clustering techniques facilitate the grouping of records based on a similarity measure. K-Means is a famous clustering technique which uses a distance metric (generally Euclidean) to cluster the data points. The centroid is the most representative point of a K-Means cluster. The packages ‘cluster’ (Rousseeuw et al., 2019), ‘factoextra’ (Kassambara et al., 2017) and ‘purrr’ (Henry et al., 2019) are used for the K-Means algorithm, cluster visualization and elbow plot (Fig. 20) respectively. The features that were used in ‘Evaluating the relationship between the features mentioned in R3 and the poverty level’ section were only used for clustering as they are related to the factors mentioned in R3. K-Means algorithm was applied over the mentioned features only. Here, the unit of analysis is a single person. The input cluster number was provided as 8, as it appears to be the bend on the knee in the elbow graph (Fig. 20). The pre-processed data that was used for random forest is also utilized for K-Means. The ‘target’ column is removed from the data set.

Fig. 20 — Elbow Plot

Fig. 21 — Visualization of K-Means Clusters

Fig. 21 shows the visualization of 8 clusters. The X and Y axes represent the reduced dimensions of records that are represented by two principal components with the highest variance. The dimensionality reduction was performed with PCA through the ‘factoextra’ package for visualization only. In K-Means, each record is assigned to its nearest centroid which is represented by a cluster number. Each color in the plot represents a cluster.

A record and its cluster number are compared for further analysis. The purpose is to find whether K-Means has the ability to cluster records with similar poverty levels together. Table. 1 represents the distribution of individual poverty levels in the resultant clusters. It can be inferred that K-Means clustering is not effective in determining the poverty level as there was no association between the resultant clusters and the original poverty levels. The reason can be attributed to the unequal class distributions and non-linearity of data. While minimizing the within-cluster sum of squares, the algorithm gives more weight to large clusters than the smaller ones. Hence, no clear groups were observed. The poverty levels were distributed across clusters. In order to check the importance of the input cluster number, different values were provided as input and K-Means clusters were generated accordingly. However, an increase in the cluster number did not result in the formation of efficient clusters. As an experiment, the top 15 features (which are all continuous variables), obtained from the trained random forest model were used instead of the features in R3, to generate the K-Means clusters. This also did not improve the result significantly.

Table 1 — Distribution of individual poverty levels in the clusters (when input number of clusters = 8)

Answers to Research Questions:

Answer for R1: Yes. The Random Forest classifier has been constructed to successfully identify the level of poverty for Costa Rican households with good performance metrics. With the availability of more data, the performance of the model can be improved.

Answer for R2: Yes. The fifteen most important columns that determine the level of poverty for a household, in the decreasing order of their importance are, ‘years_of_schooling’, ‘meaneduc’, ‘age_in_years’, ‘dependency’, ‘overcrowding’, ‘number_of_all_rooms_in_the_house’, ‘no_of_mobile_phones’, ‘edjefe’, ‘years_of_education_of_male_head_of_household_squared’, ‘number_of_children_0_to_19_in_household’, ‘monthly_rent_payment’, ‘edjefa’, ‘total_females_in_the_household’, ‘bedrooms’, ‘number_of_persons_living_in_the_household’.

Answer for R3: Yes. There is a strong relationship between the features related to the entities mentioned in the question R3 to the level of poverty for a household.

Answer for R4: K-Means clustering technique does not perform well in clustering the records with the same poverty level together. Hence, in the absence of the ‘target’ column, and with the given features in R3, K-Means clustering algorithm cannot help in assigning the class label for a person/individual.

References:

Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2019). dplyr: A Grammar of Data Manipulation. R package version 0.8.3. https://CRAN.R-project.org/package=dplyr

Hadley Wickham (2019). stringr: Simple, Consistent Wrappers for Common String Operations. R package version 1.4.0. https://CRAN.R-project.org/package=stringr

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016. Tableau Software (2019). Retrieved from https://www.tableau.com/

Taiyun Wei and Viliam Simko (2017). R package ""corrplot"": Visualization of a Correlation Matrix (Version 0. 84). Available from https://github.com/taiyun/corrplot

A. Liaw and M. Wiener (2002). Classification and Regression by randomForest. R News 2(3), 18--22.

Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams, Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, Brenton Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau, Andrew Ziem, Luca Scrucca, Yuan Tang, Can Candan and Tyler Hunt. (2019. caret: Classification and Regression Training. R package version 6.0-84. https://CRAN.R-project.org/package=caret

Maechler, M., Rousseeuw, P., Struyf, A., Hubert, M., Hornik, K.(2019). cluster: Cluster Analysis Basics and Extensions. R package version 2.1.0.

Alboukadel Kassambara and Fabian Mundt (2017). factoextra: Extract and Visualize the Results of Multivariate Data Analyses. R package version 1.0.5. https://CRAN.R-project.org/package=factoextra

Lionel Henry and Hadley Wickham (2019). purrr: Functional Programming Tools. R package version 0.3.2. https://CRAN.R-project.org/package=purrr

Tableau Software (2019). Retrieved from https://www.tableau.com/"
"It’s the Great PumpGAN, Charlie Brown | Blog | Exxact","Get Into The Halloween Spirit of GANs With This Pumpkin Generator Tutorial

(Generative adversarial training. Blue lines indicate flow of inputs, green lines outputs, and red lines error signals.)

Generative Adversarial Networks, or GANs for short, are one of the most exciting areas of deep learning to arise in the last 10 years. That’s according to, among others, Yann LeCun of MNIST and backpropagation fame. The rapid progress since the 2014 introduction of GANs by Ian Goodfellow and others marks adversarial training as a breakthrough idea, complete with the potential to alter society in beneficial, nefarious, and silly ways. GAN training has been used for everything from the predictable cat generators to fictional portraitists “painted” by GANs selling for six-figures at fine art auctions. All GANs are based on the simple premise of dueling networks: a creative network that generates some kind of output data (images in our case) and a skeptical network that outputs a probability that the data are real or generated. These are known as the “Generator” and “Discriminator” networks, and by simply trying to thwart each other they can learn to generate realistic data. In this tutorial we’ll build a GAN based on the popular fully convolutional DCGAN architecture and train it to produce pumpkins for Halloween.

We’ll use PyTorch, but you could also use TensorFlow (if that’s what you’re comfortable with). The experience of using either major deep learning library has grown strikingly similar, given the changes to TensorFlow in this year’s 2.0 release we’ve seen as part of a broader convergence of popular frameworks to dynamically executed, pythonic code with optional optimized graph compilation for speedup and deployment.

To set up and activate a virtual environment for basic PyTorch experiments:

virtualenv pytorch --python=python3 pytorch/bin/pip install numpy matplotlib torch torchvision source pytorch/bin/activate

And if you have conda installed and prefer to use it:

conda new -n pytorch numpy matplotlib torch torchvision conda activate pytorch

And to save you some time guessing, here are the imports we’ll need:

import random import time import numpy as np import matplotlib.pyplot as plt import torch import torch.nn as nn import torch.nn.parallel import torch.optim as optim import torch.nn.functional as F import torch.utils.data import torchvision.datasets as dset import torchvision.transforms as transforms import torchvision.utils as vutils

Our GAN will be based on the DCGAN architecture, and borrows heavily from the official implementation in the PyTorch examples. The ‘DC’ in ‘DCGAN’ stands for ‘Deep Convolutional,’ and the DCGAN architecture extended the unsupervised adversarial training protocol described in Ian Goodfellow’s original GAN paper. It’s a relatively straightforward and interpretable network architecture, and can form the starting point for testing more complex ideas.

The DCGAN architecture, like all GANs, actually consists of two networks, the discriminator and the generator. It’s important to keep these evenly matched in terms of their fitting power, training speed, etc. to avoid the networks becoming mismatched. GAN training is notoriously unstable, and it may take a fair bit of tuning to get it to work on a given dataset architecture combination. In this DCGAN example, it’s easy to get stuck with your generator outputting yellow/orange checkerboard gibberish, but don’t give up! In general I have a strong admiration for the authors of novel breakthroughs like this, where it would be easy to be discouraged by early poor results and a heroic level of patience may be required. Then again, sometimes it’s just a matter of ample preparation and a good idea coming together, and things work out with just a few extra hours of work and computation.

The generator is a stack of transposed convolutional layers that transform a long and skinny, multi-channel tensor latent space into a full-sized image. This is exemplified in the following diagram from the DCGAN paper:

Fully convolutional generator from Radford et al. 2016.

We’ll instantiate as a sub-class of the torch.nn.Module class. This is a flexible way to implement and develop models. You can seed in the forward class function allows the incorporation of things like skip connections that are not possible with a simple torch.nn.Sequential model instance.

class Generator(nn.Module): def __init__(self, ngpu, dim_z, gen_features, num_channels): super(Generator, self).__init__() self.ngpu = ngpu self.block0 = nn.Sequential(\ nn.ConvTranspose2d(dim_z, gen_features*32, 4, 1, 0, bias=False),\ nn.BatchNorm2d(gen_features*32),\ nn.ReLU(True)) self.block1 = nn.Sequential(\ nn.ConvTranspose2d(gen_features*32,gen_features*16, 4, 2, 1, bias=False),\ nn.BatchNorm2d(gen_features*16),\ nn.ReLU(True)) self.block2 = nn.Sequential(\ nn.ConvTranspose2d(gen_features*16,gen_features*8, 4, 2, 1, bias=False),\ nn.BatchNorm2d(gen_features*8),\ nn.ReLU(True)) self.block3 = nn.Sequential(\ nn.ConvTranspose2d(gen_features*8, gen_features*4, 4, 2, 1, bias=False),\ nn.BatchNorm2d(gen_features*4),\ nn.ReLU(True)) self.block5 = nn.Sequential(\ nn.ConvTranspose2d(gen_features*4, num_channels, 4, 2, 1, bias=False))\ def forward(self, z): x = self.block0(z) x = self.block1(x) x = self.block2(x) x = self.block3(x) x = F.tanh(self.block5(x)) return x

is the creative half of our GAN duo, and the learned abilities of to create seemingly novel images is what most people tend to focus on. In fact the generator is hopeless without a well-matched discrimintor . The discriminator architecture will be familiar to those of you that have built a few deep convolutional image classifiers in the past. In this case it’s a binary classifer, attempting to distinguish fake and real, so we use a sigmoid activation function on the output instead of the softmax we would use for multiclass problems. We’re also doing away with any fully connected layers, as they are unnecessary here.

Fully convolutional binary classifer suitable for use as a discriminator D(x).

And the code:

class Discriminator(nn.Module): def __init__(self, ngpu, gen_features, num_channels): super(Discriminator, self).__init__() self.ngpu = ngpu self.block0 = nn.Sequential(\ nn.Conv2d(num_channels, gen_features, 4, 2, 1, bias=False),\ nn.LeakyReLU(0.2, True)) self.block1 = nn.Sequential(\ nn.Conv2d(gen_features, gen_features, 4, 2, 1, bias=False),\ nn.BatchNorm2d(gen_features),\ nn.LeakyReLU(0.2, True)) self.block2 = nn.Sequential(\ nn.Conv2d(gen_features, gen_features*2, 4, 2, 1, bias=False),\ nn.BatchNorm2d(gen_features*2),\ nn.LeakyReLU(0.2, True)) self.block3 = nn.Sequential(\ nn.Conv2d(gen_features*2, gen_features*4, 4, 2, 1, bias=False),\ nn.BatchNorm2d(gen_features*4),\ nn.LeakyReLU(0.2, True)) self.block_n = nn.Sequential( nn.Conv2d(gen_features*4, 1, 4, 1, 0, bias=False),\ nn.Sigmoid()) def forward(self, imgs): x = self.block0(imgs) x = self.block1(x) x = self.block2(x) x = self.block3(x) x = self.block_n(x) return x

We’ll also need a few helper functions for creating the dataloader and initializing the model weights according to the advice in the DCGAN paper. The function below returns a PyTorch dataloader with some mild image augmentation, just point it to the folder containing your images. I’m working with a relatively small batch of free images from Pixabay, so the image augmentation is important to getting better mileage from each image.

def get_dataloader(root_path): dataset = dset.ImageFolder(root=root_path,\ transform=transforms.Compose([\ transforms.RandomHorizontalFlip(),\ transforms.RandomAffine(degrees=5, translate=(0.05,0.025), scale=(0.95,1.05), shear=0.025),\ transforms.Resize(image_size),\ transforms.CenterCrop(image_size),\ transforms.ToTensor(),\ transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\ ])) dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\ shuffle=True, num_workers=num_workers) return dataloader

And to initialize the weights:

def weights_init(my_model): classname = my_model.__class__.__name__ if classname.find(""Conv"") != -1: nn.init.normal_(my_model.weight.data, 0.0, 0.02) elif classname.find(""BatchNorm"") != -1: nn.init.normal_(my_model.weight.data, 1.0, 0.02) nn.init.constant_(my_model.bias.data, 0.0)

That’s it for the functions and classes. All that’s left now is to tie it all together with some scripting (and merciless iteration over hyperparameters). It’s a good idea to group the hyperparameters together near the top of your script (or pass them in with flags or argparse), so it’s easy to change the values.

# ensure repeatability my_seed = 13 random.seed(my_seed) torch.manual_seed(my_seed) # parameters describing the input latent space and output images dataroot = ""images/pumpkins/jacks"" num_workers = 2 image_size = 64 num_channels = 3 dim_z = 64 # hyperparameters batch_size = 128 disc_features = 64 gen_features = 64 disc_lr = 1e-3 gen_lr = 2e-3 beta1 = 0.5 beta2 = 0.999 num_epochs = 5000 save_every = 100 disp_every = 100 # set this variable to 0 for cpu-only training. This model is lightweight enough to train on cpu in a few hours. ngpu = 2

Next we instantiate the models and dataloader. I used a dual-GPU setup to quickly evaluate a few different hyperparameter iterations. It’s trivial in PyTorch to train on several GPUs by wrapping your models in the torch.nn.DataParallel class. Don't worry if all of your GPUs are tied up in the pursuit of Artificial General Intelligence, this model is lightweight enough for training up on CPU in a reasonable amount of time (few hours).

dataloader = get_dataloader(dataroot) device = torch.device(""cuda:0"" if ngpu > 0 and torch.cuda.is_available() else ""cpu"") gen_net = Generator(ngpu, dim_z, gen_features, \ num_channels).to(device) disc_net = Discriminator(ngpu, disc_features, num_channels).to(device) # add data parallel here for >= 2 gpus if (device.type == ""cuda"") and (ngpu > 1): disc_net = nn.DataParallel(disc_net, list(range(ngpu))) gen_net = nn.DataParallel(gen_net, list(range(ngpu))) gen_net.apply(weights_init) disc_net.apply(weights_init)

The generator and discriminator networks are updated together in one big loop. Before we get to that, we need to define our loss criterion (binary cross entropy), define optimizers for each network, and instantiate some lists that we’ll use to keep track of training progress.

criterion = nn.BCELoss() # a set sample from latent space so we can unambiguously monitor training progress fixed_noise = torch.randn(64, dim_z, 1, 1, device=device) real_label = 1 fake_label = 0 disc_optimizer = optim.Adam(disc_net.parameters(), lr=disc_lr, betas=(beta1, beta2)) gen_optimizer = optim.Adam(gen_net.parameters(), lr=gen_lr, betas=(beta1, beta2)) img_list = [] gen_losses = [] disc_losses = [] iters = 0

The Training Loop

The training loop is conceptually straightforward but a bit long to take in in a single snippet, so we’ll break it down into several pieces. Broadly speaking, we first update the discriminator based on the predictions for a set of real and generated images. Then we feed generated images to the newly updated discriminator and use the classification output from D(G(z)) for the training signal for the generator, using the real label as the target.

First we’ll enter the loop and perform a discriminator update:

t0 = time.time() for epoch in range(num_epochs): for ii, data in enumerate(dataloader,0): # update the discriminator disc_net.zero_grad() # discriminator pass with real images real_cpu = data[0].to(device) batch_size= real_cpu.size(0) label = torch.full((batch_size,), real_label, device=device) output = disc_net(real_cpu).view(-1) disc_real_loss = criterion(output,label) disc_real_loss.backward() disc_x = output.mean().item() # discriminator pass with fake images noise = torch.randn(batch_size, dim_z, 1, 1, device=device) fake = gen_net(noise) label.fill_(fake_label) output = disc_net(fake.detach()).view(-1) disc_fake_loss = criterion(output, label) disc_fake_loss.backward() disc_gen_z1 = output.mean().item() disc_loss = disc_real_loss + disc_fake_loss disc_optimizer.step()

Note that we’re also keeping track of the average predictions for the fake and real batches. This will give us a straightforward way to keep track of how balanced our training is (or not) by telling us how the predictions change after each update.

Next, we will update the generator based on the discriminator’s predictions using the real label and binary cross entropy loss. Note that we are updating the generator based on the discriminator’s misclassification of fake images as real. This signal produces better gradients for training than minimizing the ability of the discriminator to detect fakes directly. It’s pretty impressive that GANs can eventually learn to produce photorealistic content based on this type of dueling loss signals.

# update the generator gen_net.zero_grad() label.fill_(real_label) output = disc_net(fake).view(-1) gen_loss = criterion(output, label) gen_loss.backward() disc_gen_z2 = output.mean().item() gen_optimizer.step()

Finally, there’s a bit of housekeeping to keep track of our training. Balancing GAN training is something of an art and it’s not always obvious from just the numbers whether your networks are learning effectively, so it’s a good idea to check the image quality occasionally. On the other hand, if any of the values in the print statement go to either 0.0 or 1.0 chances are your training has collapsed and it’s a good idea to iterate with new hyperparameters.

if ii % disp_every == 0: # discriminator pass with fake images, after updating G(z) noise = torch.randn(batch_size, dim_z, 1, 1, device=device) fake = gen_net(noise) output = disc_net(fake).view(-1) disc_gen_z3 = output.mean().item() print(""{} {:.3f} s |Epoch {}/{}:\tdisc_loss: {:.3e}\tgen_loss: {:.3e}\tdisc(x): {:.3e}\tdisc(gen(z)): {:.3e}/{:.3e}/{:.3e}"".format(iters,time.time()-t0, epoch, num_epochs, disc_loss.item(), gen_loss.item(), disc_x, disc_gen_z1, disc_gen_z2, disc_gen_z3)) disc_losses.append(disc_loss.item()) gen_losses.append(gen_loss.item()) if (iters % save_every == 0) or \ ((epoch == num_epochs-1) and (ii == len(dataloader)-1)): with torch.no_grad(): fake = gen_net(fixed_noise).detach().cpu() img_list.append(vutils.make_grid(fake, padding=2, normalize=True).numpy()) np.save(""./gen_images.npy"", img_list) np.save(""./gen_losses.npy"", gen_losses) np.save(""./disc_losses.npy"", disc_losses) torch.save(gen_net.state_dict(), ""./generator.h5"") torch.save(disc_net.state_dict(), ""./discriminator.h5"") iters += 1

It may take a bit of work to get passable results, but lucky for us slight glitches in reality are actually preferable for amplifying creepiness. The code described here can be improved, but should be good for reasonably plausible jack-o-lanterns at low resolution.

Training progress after about 5000 updates epochs.

Hopefully, the above tutorial has served to whet your appetite for GANs, Halloween crafts, or both. After mastering the basic DCGAN we’ve built here, experiment with more sophisticated architectures and applications. Training GANs is very much still an artful science, and balancing training is tricky. Use the hints from ganhacks and, after getting a simplified proof-of-concept working for your dataset/application/idea, add only small chunks of complexity at a time. Best of luck and happy training.

Originally published at https://blog.exxactcorp.com on October 28, 2019."
Animated Information Graphics,"Animated Information Graphics

Image www.pexels.com

Animated information graphics of various datasets are a popular topic on youtube, for example, the channel

has almost a million subscribers. I will show in this article some examples of such infographics on my dataset of news articles from Austrian newspapers on politics, and how to create the animations with Python and Plotly (https://plot.ly/).

In the article https://medium.com/datadriveninvestor/topics-in-online-news-before-the-austrian-elections-2019-d981facbd0dd, I analyzed the topics before the elections. In this post, I want to look back at the news from the second half of the year 2019, and use animated graphics to analyze the news over time. I collected and analyzed more than 22,000 news articles from 12 Austrian online daily newspapers.

The first video shows day by day how often each party has been named up to that day. It is counted from July until the end of the year. In the end, the ÖVP is just ahead of the FPÖ and “NEOS” is clearly at the end.

In the second video, the number of times each party was mentioned per newspaper per week is shown. This is also shown animated for the entire second half of the year. It is noticeable that individual media have a clear preference for some parties, and for example, NEOS was mentioned very little outside of ORF. The week of the election of the National Council is clearly visible in the increased number of reports and mentions. The period of coalition negotiations after the election can be seen in the increased naming of the “Greens”. Many other aspects can often only be seen after watching the video several times.

In order not only to review the quantity of reporting I have also tried to show the dominant contents in further visualization. The most common words are shown in a “Wordcloud” day by day. The dominating personalities stand out clearly, but also the election period is clearly recognizable by the changing choice of words. Here, words such as “percent” or “election results” dominate."
How to predict the success of your marketing campaign,"Now let us plot the histograms for the numerical features. We’ll use two very handy data visualization libraries, Matplotlib and Seaborn (which builds upon Matplotlib):

import matplotlib.pyplot as plt

import seaborn as sns quan = list(data.loc[:, data.dtypes != 'object'].columns.values)

grid = sns.FacetGrid(pd.melt(data, value_vars=quan),

col='variable', col_wrap=4, height=3, aspect=1,

sharex=False, sharey=False)

grid.map(plt.hist, 'value', color=""steelblue"")

plt.show()

As a final glance we will have a look at basic linear correlations between the numerical features. First, let us visualize those with a Seaborn heatmap:

sns.heatmap(data._get_numeric_data().astype(float).corr(),

square=True, cmap='RdBu_r', linewidths=.5,

annot=True, fmt='.2f').figure.tight_layout()

plt.show()

In addition, we can also output the correlations of each feature with the dependent variable:

>>> data.corr(method='pearson').iloc[0].sort_values(ascending=False)

impressions 1.000000

budget 0.556317

days 0.449491

google_display 0.269616

google_search 0.164593

instagram 0.073916

start_month 0.039573

start_week 0.029295

end_month 0.014446

end_week 0.012436

facebook -0.382057

Here we can see that the number of impressions is positively correlated with the budget amount and the campaign duration in days, and negatively correlated with the binary option to use Facebook as a channel. However, this only shows us a pair-wise linear relationship and can merely serve as a crude initial observation.

Pre-process your data

Before we can begin to construct a predictive model, we need to make sure our data is clean and usable, since here applies: “Garbage in, garbage out.”

We are lucky to be presented with a fairly well-structured dataset in this case, but we should still go through a quick pre-processing pipeline specific to the challenge at hand:

Only keep rows where the dependent variable is greater than zero since we only want to predict outcomes larger than zero (theoretically values equal to zero are possible, but they won’t help our predictions). Check for columns with missing data and decide whether to drop or fill them. Here we’ll drop columns with more than 50% missing data since these features cannot add much to the model. Check for rows with missing values and decide whether to drop or fill them (doesn’t apply for the sample data). Put rare categorical values (e.g. with a share of less than 10%) into one “other” bucket to prevent overfitting our model to those specific occurrences. Encode categorical data into one-hot dummy variables since the models we will use require numerical inputs. There are various ways to encode categorical data, this article provides a very good overview in case you’re interested to learn more. Specify dependent variable vector and independent variable matrix. Split dataset into training and test set to properly evaluate your models’ goodness of fit after training. Scale features as required for one of the models we are going to build.

Here’s the full pre-processing code:

Train your models

Finally, we can proceed to build and train multiple regressors to ultimately predict the outcome (value of the dependent variable), i.e. the number of impressions for the marketing campaign in question. We will try four different supervised learning techniques — linear regression, decision trees, random forest (of decision trees) and support vector regression — and will implement those with the respective classes provided by the Scikit-learn library, which was already used to scale and split the data during pre-processing.

There are many more models we could potentially use to develop regressors, e.g. artificial neural networks, which might yield even better predictors. However, the focus of this article is rather to explain some of the core principles of such regression in a — for the most part — intuitive and interpretable way than to produce the most accurate predictions.

Linear regression

Constructing a linear regressor with Scikit-learn is very simple and only requires two lines of code, importing the required function from Scikit’s linear model class and assigning it to a variable:

from sklearn.linear_model import LinearRegression

linear_regressor = LinearRegression(fit_intercept=True, normalize=False, copy_X=True)

We want to keep the default parameters since we need an intercept (result when all features are 0) to be calculated and we do not require normalization preferring interpretability. The regressor will calculate the independent variable coefficients and the intercept by minimizing the sum of the squared errors, i.e. the deviations of the predicted from the true outcome, which is known as the Ordinary Least Squares method.

We can also output the coefficients and their respective p-values, the probability of the output being independent from (here also uncorrelated to) a specific feature (which would be the null-hypothesis of the coefficient equaling 0), and hence a measure of statistical significance (the lower the more significant).

Having visualized the correlations between the numerical features earlier during our “first glance”, we expect the features “budget”, “days” and “facebook” to carry relatively small p-values with “budget” and “days” having positive and “facebook” a negative coefficient. The statsmodels module provides an easy way to output this data:

model = sm.OLS(self.y_train, sm.add_constant(self.X_train)).fit()

print(model.summary())

The p-value here was calculated using a t-statistic or -score based on the t-distribution. The summary also gives us a first hint at the accuracy or goodness of fit for the whole model, scored by the coefficient of determination R-squared measuring the share of variance in the output explained by the input variables, here 54.6%.

However, in order to compare all models and to suit our particular challenge we will use a different scoring method which I call “Mean Relative Accuracy”, defined as 1 - mean percentage error = 1 - mean(|(prediction - true value) / true value|). This metric is obviously undefined if the true value is 0, but in our case this is not relevant as we check this condition in the pre-processing step (see above) and we will thus obtain good interpretability matching an intuitive definition of accuracy. We will calculate this score for all models using five-fold cross-validation, randomly splitting the dataset five times and taking the mean of each of the scores. Scitkit-learn also provides a handy method for this:

linear_score = np.mean(cross_val_score(estimator=linear_regressor,

X=X_train, y=y_train, cv=5,

scoring=mean_relative_accuracy))

The training score we obtain for the linear regressor is 0.18; ergo the best fit we were able to produce with this model results only in a 18% prediction accuracy. Let’s hope the other models are able to outperform this.

Decision Tree

Next up is a regressor made from a single decision tree. Here we will use a Scikit-learn function with a few more arguments, so-called hyperparameters, than for the linear model, including some we don’t know the desired settings for yet. That’s why we are going to introduce a concept called Grid Search. Grid Search, again available from Scikit-learn, lets us define a grid or matrix of parameters to test when training the prediction model and returns the best parameters, i.e. the ones yielding the highest score. This way we could test all available parameters for the Decision Tree Model, but we will focus on two of those, the “criterion” to measure the quality of a split of one branch into two and the minimum number of samples (data points) for one leaf (final node) of the tree. This will help us to find a good model with the training data while limiting overfitting, i.e. failing to generalize from the training data to new samples. From now on we will also set a random state equal to one for all stochastic calculations so that you will receive the same values coding along. The rest works similar to the linear regression we’ve built earlier:

tree_parameters = [{'min_samples_leaf': list(range(2, 10, 1)),

'criterion': ['mae', 'mse'],

'random_state': [1]}]

tree_grid = GridSearchCV(estimator=DecisionTreeRegressor(),

param_grid=tree_parameters,

scoring=mean_relative_accuracy, cv=5,

n_jobs=-1, iid=False)

tree_grid_result = tree_grid.fit(X_train, y_train)

best_tree_parameters = tree_grid_result.best_params_

tree_score = tree_grid_result.best_score_

The best parameters chosen from the grid we defined include the mean squared error as the criterion to determine the optimal split at each node and a minimum of nine samples for each leaf, yielding a mean relative (training) accuracy of 67% — which is already a lot better compared to the 18% from the linear regression.

One of the advantages of a decision tree is that we can easily visualize and intuitively understand the model. With Scikit-learn and two lines of code you can generate a DOT representation of the fitted decision tree which you can then convert into a PNG image:"
What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur,"What I Learned from (Two-time) Kaggle Grandmaster Abhishek Thakur

Photo by Georgie Cobbs on Unsplash

Quick Bio

Before his many data scientist stints in companies scattered throughout Germany, Abhishek Thakur earned his bachelor’s in electrical engineering at NIT Surat and his master’s in computer science at the University of Bonn. Currently, he holds the title of Chief Data Scientist at Norway’s boost.ai, a “software company that specializes in conversational artificial intelligence (AI).” But I’m most impressed by Abhishek’s Kaggle clout.

You can visit his Kaggle profile here. Here’s a snapshot of his accolades:

Competitions Grandmaster (17 gold medals and an all-time high rank of #3 in the world)

Kernels Expert (he’s well within the top 1% of Kagglers)

Discussion Grandmaster (65 gold medals and an all-time high rank of #2 in the world)

I want to take a look at Abhishek’s tutorial, Approaching (Almost) Any NLP Problem on Kaggle. I’ve selected this kernel of Abhishek’s because I myself have been trying to learn more about natural language processing, and how could I resist learning with Kaggle’s Halloween-themed Spooky Authors dataset?

Abhishek’s Approach to NLP

I would highly encourage you to read this article alongside the kernel. And if you really want a firmer grasp of NLP or data science in general, be sure that you understand every line of Abhishek’s code by writing it yourself as you go through his kernel.

Just so we don’t forget — our task is to identify the author (EAP — Edgar Allen Poe; HPL — H.P. Lovecraft; MWS — Mary Wollstonecraft Shelley) of each sentence in the test set.

1. Exploring the Data and Understanding the Problem

After importing the necessary Python modules and the data, Abhishek calls the head() method on the data to see what the first five rows look like. Since Abhishek is a pro and this is an NLP problem, the exploratory data analysis (you’ll most often see this referred to as EDA) is shallow compared to problems involving numerical data. Data science newcomers might benefit from more thorough EDA. A…"
Face Alignment: Deep multi-task learning,"3. Data Overview

The dataset for this project is provided by the authors of the paper themselves which can be found here.

Data comes with 12295 images in total, out of which 10,000 are training images and 2295 are test images.

Data also comes with two txt files: training.txt and testing.txt . These two files holds the information about the path of images, the co-ordinate positions of facial features and 4 other facial attributes:

1st Attribute: Gender[M/F]

2nd Attribute: Smiling/Not Smiling

3rd Attribute: With Glasses/No Glasses

4th Attribute: Pose Variation

3.1 Loading and Cleaning the data

Let’s load the training.txt file and try to understand and analyse the data. When you’ll read the training.txt files using pandas read_csv function using space as a separator, it will not be loaded correctly and that’s because of the reason there is space at the beginning of each line. So, we need to strip that out.

Training.txt file

The following code will do exactly that.

f = open('training.txt','r')

f2 = open('training_new.txt','w')

for i,line in enumerate(f.readlines()):

if i==0:

continue

line = line.strip()



f2.write(line)

f2.write('

')

f2.close()

f.close()

Now, we’ll use this newly created file training_new.txt in the project. Do the same for testing.txt file.

Reading the cleaned training.txt file.

names = ['Path']+list('BCDEFGHIJK')+['Gender','Smile','Glasses','Pose'] train = pd.read_csv('training_new.txt',sep=' ',header=None,names=names)

train['Path'] = train['Path'].str.replace('\\','/')

Here is the meaning of each attribute in the training file.

Path: the path of the image(absolute path)

B: x co-ordinate of right eye centre

C: x co-ordinate of left eye centre

D: x co-ordinate of nose centre

E: x co-ordinate of extreme right point of mouth

F: x co-ordinate of extreme left point of mouth

G: y co-ordinate of right eye centre

H: y co-ordinate of left eye centre

I: y co-ordinate of nose centre

J: y co-ordinate of extreme right point of mouth

K: y co-ordinate of extreme left point of mouth

Gender: whether the person is male/female, 1: Male, 2: Female

Smile: Whether the person is smiling or not, 1: Smile, 2:Not smile

Glasses: whether the person has glasses or not, 1: Glasses, 2: No Glasses

Pose: [Pose estimation ] , 5 categories.

3.2 Visualize the data

Now, let’s visualize some of the images with the facial keypoints.

Code:

#visualising the dataset

images = []

all_x = []

all_y= []

random_ints = np.random.randint(low=1,high=8000,size=(9,))

for i in random_ints:

img = cv2.imread(train['Path'].iloc[i])

x_pts = train[list('BCDEF')].iloc[i].values.tolist()

y_pts = train[list('GHIJK')].iloc[i].values.tolist()

all_x.append(x_pts)

all_y.append(y_pts)

img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)

images.append(img) fig,axs = plt.subplots(nrows=3,ncols=3,figsize=(14,10)) k =0

for i in range(0,3):

for j in range(0,3):

axs[i,j].imshow(images[k])

axs[i,j].scatter(all_x[k],all_y[k])

k += 1 plt.show()

4. Deep Dive

Now, we get the idea of what facial key point prediction is all about. Let’s dive deep and understand the technical details about it.

Takes a image as input and give the co-ordinate of facial features.

It’s a regression problem, as it predict the continuous values i.e co-ordinate of facial landmarks.

Face Alignment, Magic box?

What’s that magic thing in the box which does all of this?

Let’s deep dive more, and understand it.

As of now there are two approaches through which we can solve this problem, one is vanilla computer vision techniques(like viola and jones for bounding box prediction of face), and other one is deep learning based, especially convolutional neural network based.

But what the heck is this convolutional neural network?

Simply put, it’s technique that is used to extract and detect meaningful information from image. If you’re interested in learning more, head over to here.

We’ll take the second route for this problem, that is deep learning based."
The New Photography — What is it?,"This three-part series about photography and imaging examines many of the recent technical and social developments. In part 1, we look at the 190-year history since photography’s invention, noting the rapid pace of change in the medium and sudden transition from film to digital and rise of The Smartphone.

In this installment, part 2, we’ll survey a number of recent technical developments in an effort to build a larger context for understanding new capabilities and consider what’s next.

The terms Computer Vision and Computational Photography are often used interchangeably. Computer vision, however, is a broader discipline covering a range of digital capture and processing techniques. It refers to the ability of computers to understand imagery, often in a similar way as people. Commonly this is achieved by characterizing the content, color, tonal values, shape or edge data. But it can also apply to other metadata such as embedded location or time data. When very large sets of images are analyzed, patterns and insights are gained and subsequently applied to organizing, verifying or even modifying imagery.

Computational photography is more specifically a discipline involving calculations, analysis, manipulation of imagery using algorithms rather than optical methods. We won’t fret over the distinction between the two disciplines here but rather consider the larger genre of computer vision.

Eyes Robot

This isn’t necessarily a new area. Initial examples of computer vision have been with us for awhile in the likes of:

Optical Character Recognition (OCR) enabling bar- and QR-code scanning and conversion of text-based printouts to machine-readable documents.

High-Dynamic-Range (HDR) imaging where multiple images are combined to depict a high-contrast setting, normally one that exceeds the range of a camera sensor or even the human eye. Recent updates have dramatically improved the quality, helping HDR shed the moody and surreal “Harry Potter” look of its early days.

Panoramic imagery where multiple images are aligned and stitched together with the seams between images automatically blended.

Contextual image replacement, coined Content-Aware Fill in Adobe’s software, where portions of an image are replaced using surrounding data. A common usage would be to remove power lines from a photo. Adobe released this in 2010 but more recently, the feature has seen dramatic improvements in quality and capability with the application of more sophisticated algorithms based on their AI platform.

We’ve seen a raft of new developments in computer vision covering a range of use in photography and video. Recent examples include:

Mechanical Brains

The news has been rife with stories of fake imagery and videos, often involving some of the techniques mentioned above — and it’s only beginning to mature. A basic understanding of the methodology is helpful to understand its capabilities and where it may lead. While it can be used to generate new, synthesized or fake imagery, it can also be used to recognize, categorize, track or creatively modify imagery as in the case of the popular Prisma app which uses a technique known as style transfer achieved through use of a Convolutional Neural Network (CNN). Additionally, these approaches, which are highly adaptive, are a big focus in the effort to create self-driving vehicles.

Generally, achieving good results makes use of neural networks patterned after biological systems where stimuli roll up to higher levels creating more meaningful impulses. At a very elemental level neural networks are optimization methods that train a computer model by finding associations between data. Strong associations are given more importance, weak associations have less value. It’s a bit of a brute force method but computers being fast and tireless can crunch enormous amounts of data to reach surprisingly good results.

One approach pits two neural networks against each other in an optimization scheme known as a Generative Adversarial Network (GAN). One network generates an image based on learnings from a dataset, the other assesses the image to determine if it’s realistic. Rejected images are refined until the discriminator can no longer determine if the image is fake.

Convolutional Neural Networks (CNNs) are commonly used to categorize images or otherwise find patterns. As data is analyzed, convolutional layers transform the data and pass the info to the next layer for further analysis. A number of filters are specified for each layer such as edges, shapes and corners, representing more complex information or objects with each layer. As the data moves further into the network, the more sophisticated layers are able to identify more complex objects like eyes, faces, or cars as data from prior layers is combined.

Perceptual Loss Functions are also used for their speed in training a CNN. This method recognizes that 2 images can look the same to humans but be mathematically different to a computer — such as shifting the same image by a pixel or more. The more data analyzed, the better the results.

These explanations represent the very tip of the iceberg with these technologies. Implementations are still rough around the edges but they are improving rapidly. Yet even with this limited understanding, it’s not hard to see how neural networks can be used to generate impressive, animated models of real people, especially celebrities as we’ve heard many times in the news. For example, high definition video with 24 frames per second can be pulled from YouTube to train a network on how a specific person speaks and moves. These learnings can then be used to generate new or altered imagery such as this example where Jon Snow apologizes for GoT season 8.

These methods are computationally very intensive. Faster processors and the availability of huge amounts of digital imagery that can be used for patterning have allowed the more sophisticated and open-sourced algorithms to proliferate at this time. Interestingly, despite the complexity of image data, ML/AI methodologies have progressed much further than they have with text, due largely to the objective nature of imagery. Words and text, on the other hand, can have varying interpretations based on context, personality, culture and other factors like irony which pose bigger challenges for machines to understand.

The examples we covered above are far from comprehensive. Software and hardware companies continue their aggressive progress while many universities have added the subject to their curriculum and formed computer vision departments. It’s clear we’ll continue to see an increase in the volume and quality of manipulated imagery. Further characterization of large image datasets will naturally bring insights and learnings along with some abuses.

In the final installment of this series, we’ll consider some of the social and ethical challenges with these technologies along with some thoughts on mitigation. We’ll also look at what’s on the horizon."
Reinforcement Learning : Markov-Decision Process (Part 1),"#InsideRL

In a typical Reinforcement Learning (RL) problem, there is a learner and a decision maker called agent and the surrounding with which it interacts is called environment. The environment, in return, provides rewards and a new state based on the actions of the agent. So, in reinforcement learning, we do not teach an agent how it should do something but presents it with rewards whether positive or negative based on its actions. So our root question for this blog is how we formulate any problem in RL mathematically. This is where the Markov Decision Process(MDP) comes in.

Typical Reinforcement Learning cycle

Before we answer our root question i.e. How we formulate RL problems mathematically (using MDP), we need to develop our intuition about :

The Agent-Environment relationship

Markov Property

Markov Process and Markov chains

Markov Reward Process (MRP)

Bellman Equation

Markov Reward Process

Grab your coffee and don’t stop until you are proud!🧐

The Agent-Environment Relationship

First let’s look at some formal definitions :

Agent : Software programs that make intelligent decisions and they are the learners in RL. These agents interact with the environment by actions and receive rewards based on there actions. Environment :It is the demonstration of the problem to be solved.Now, we can have a real-world environment or a simulated environment with which our agent will interact.

Demonstrating an environment with which agents are interacting.

State : This is the position of the agents at a specific time-step in the environment.So,whenever an agent performs a action the environment gives the agent reward and a new state where the agent reached by performing the action.

Anything that the agent cannot change arbitrarily is considered to be part of the environment. In simple terms, actions can be any decision we want the agent to learn and state can be anything which can be useful in choosing actions. We do not assume that everything in the environment is unknown to the agent, for example, reward calculation is considered to be the part of the environment even though the agent knows a bit on how it’s reward is calculated as a function of its actions and states in which they are taken. This is because rewards cannot be arbitrarily changed by the agent. Sometimes, the agent might be fully aware of its environment but still finds it difficult to maximize the reward as like we might know how to play Rubik’s cube but still cannot solve it. So, we can safely say that the agent-environment relationship represents the limit of the agent control and not it’s knowledge.

The Markov Property

Transition : Moving from one state to another is called Transition. Transition Probability: The probability that the agent will move from one state to another is called transition probability.

The Markov Property state that :

“Future is Independent of the past given the present”

Mathematically we can express this statement as :

Markov Property

S[t] denotes the current state of the agent and s[t+1] denotes the next state. What this equation means is that the transition from state S[t] to S[t+1] is entirely independent of the past. So, the RHS of the Equation means the same as LHS if the system has a Markov Property. Intuitively meaning that our current state already captures the information of the past states.

State Transition Probability :

As we now know about transition probability we can define state Transition Probability as follows :

For Markov State from S[t] to S[t+1] i.e. any other successor state , the state transition probability is given by

State Transition Probability

We can formulate the State Transition probability into a State Transition probability matrix by :

State Transition Probability Matrix

Each row in the matrix represents the probability from moving from our original or starting state to any successor state.Sum of each row is equal to 1.

Markov Process or Markov Chains

Markov Process is the memory less random process i.e. a sequence of a random state S[1],S[2],….S[n] with a Markov Property.So, it’s basically a sequence of states with the Markov Property.It can be defined using a set of states(S) and transition probability matrix (P).The dynamics of the environment can be fully defined using the States(S) and Transition Probability matrix(P).

But what random process means ?

To answer this question let’s look at a example:

Markov chain

The edges of the tree denote transition probability. From this chain let’s take some sample. Now, suppose that we were sleeping and the according to the probability distribution there is a 0.6 chance that we will Run and 0.2 chance we sleep more and again 0.2 that we will eat ice-cream. Similarly, we can think of other sequences that we can sample from this chain.

Some samples from the chain :

Sleep — Run — Ice-cream — Sleep

Sleep — Ice-cream — Ice-cream — Run

In the above two sequences what we see is we get random set of States(S) (i.e. Sleep,Ice-cream,Sleep ) every time we run the chain.Hope, it’s now clear why Markov process is called random set of sequences.

Before going to Markov Reward process let’s look at some important concepts that will help us in understand MRPs.

Reward and Returns

Rewards are the numerical values that the agent receives on performing some action at some state(s) in the environment. The numerical value can be positive or negative based on the actions of the agent. In Reinforcement learning, we care about maximizing the cumulative reward (all the rewards agent receives from the environment) instead of, the reward agent receives from the current state(also called immediate reward). This total sum of reward the agent receives from the environment is called returns.

We can define Returns as :

Returns (Total rewards from the environment)

r[t+1] is the reward received by the agent at time step t[0] while performing an action(a) to move from one state to another. Similarly, r[t+2] is the reward received by the agent at time step t[1] by performing an action to move to another state. And, r[T] is the reward received by the agent by at the final time step by performing an action to move to another state.

Episodic and Continuous Tasks

Episodic Tasks: These are the tasks that have a terminal state (end state).We can say they have finite states. For example, in racing games, we start the game (start the race) and play it until the game is over (race ends!). This is called an episode. Once we restart the game it will start from an initial state and hence, every episode is independent. Continuous Tasks : These are the tasks that have no ends i.e. they don’t have any terminal state.These types of tasks will never end.For example, Learning how to code!

Now, it’s easy to calculate the returns from the episodic tasks as they will eventually end but what about continuous tasks, as it will go on and on forever. The returns from sum up to infinity! So, how we define returns for continuous tasks?

This is where we need Discount factor(ɤ).

Discount Factor (ɤ): It determines how much importance is to be given to the immediate reward and future rewards. This basically helps us to avoid infinity as a reward in continuous tasks. It has a value between 0 and 1. A value of 0 means that more importance is given to the immediate reward and a value of 1 means that more importance is given to future rewards. In practice, a discount factor of 0 will never learn as it only considers immediate reward and a discount factor of 1 will go on for future rewards which may lead to infinity. Therefore, the optimal value for the discount factor lies between 0.2 to 0.8.

So, we can define returns using discount factor as follows :(Let’s say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)

Returns using discount factor

Let’s understand it with an example,suppose you live at a place where you face water scarcity so if someone comes to you and say that he will give you 100 liters of water!(assume please!) for the next 15 hours as a function of some parameter (ɤ).Let’s look at two possibilities : (Let’s say this is equation 1 ,as we are going to use this equation in later for deriving Bellman Equation)

One with discount factor (ɤ) 0.8 :

Discount Factor (0.8)

This means that we should wait till 15th hour because the decrease is not very significant , so it’s still worth to go till the end.This means that we are also interested in future rewards.So, if the discount factor is close to 1 then we will make a effort to go to end as the reward are of significant importance.

Second, with discount factor (ɤ) 0.2 :

Discount Factor (0.2)

This means that we are more interested in early rewards as the rewards are getting significantly low at hour.So, we might not want to wait till the end (till 15th hour) as it will be worthless.So, if the discount factor is close to zero then immediate rewards are more important that the future.

So which value of discount factor to use ?

It depends on the task that we want to train an agent for. Suppose, in a chess game, the goal is to defeat the opponent’s king. If we give importance to the immediate rewards like a reward on pawn defeat any opponent player then the agent will learn to perform these sub-goals no matter if his players are also defeated. So, in this task future rewards are more important. In some, we might prefer to use immediate rewards like the water example we saw earlier.

Markov Reward Process

Till now we have seen how Markov chain defined the dynamics of a environment using set of states(S) and Transition Probability Matrix(P).But, we know that Reinforcement Learning is all about goal to maximize the reward.So, let’s add reward to our Markov Chain.This gives us Markov Reward Process.

Markov Reward Process : As the name suggests, MDPs are the Markov chains with values judgement.Basically, we get a value from every state our agent is in.

Mathematically, we define Markov Reward Process as :

Markov Reward Process

What this equation means is how much reward (Rs) we get from a particular state S[t]. This tells us the immediate reward from that particular state our agent is in. As we will see in the next story how we maximize these rewards from each state our agent is in. In simple terms, maximizing the cumulative reward we get from each state.

We define MRP as (S,P, R,ɤ) , where :

S is a set of states,

P is the Transition Probability Matrix,

R is the Reward function, we saw earlier,

ɤ is the discount factor

Markov Decision Process

Now, let’s develop our intuition for Bellman Equation and Markov Decision Process.

Policy Function and Value Function

Value Function determines how good it is for the agent to be in a particular state. Of course, to determine how good it will be to be in a particular state it must depend on some actions that it will take. This is where policy comes in. A policy defines what actions to perform in a particular state s.

A policy is a simple function, that defines a probability distribution over Actions (a∈ A) for each state (s ∈ S). If an agent at time t follows a policy π then π(a|s) is the probability that the agent with taking action (a ) at a particular time step (t).In Reinforcement Learning the experience of the agent determines the change in policy. Mathematically, a policy is defined as follows :

Policy Function

Now, how do we find a value of a state. The value of state s, when the agent is following a policy π which is denoted by vπ(s) is the expected return starting from s and following a policy π for the next states until we reach the terminal state. We can formulate this as :(This function is also called State-value Function)

Value Function

This equation gives us the expected returns starting from the state(s) and going to successor states thereafter, with the policy π. One thing to note is the returns we get is stochastic whereas the value of a state is not stochastic. It is the expectation of returns from start state s and thereafter, to any other state. And also note that the value of the terminal state (if there is any) is zero. Let’s look at an example :

Example

Suppose our start state is Class 2, and we move to Class 3 then Pass then Sleep.In short, Class 2 > Class 3 > Pass > Sleep.

Our expected return is with a discount factor of 0.5:

Calculating the Value of Class 2

Note:It’s -2 + (-2 * 0.5) + 10 * 0.25 + 0 instead of -2 * -2 * 0.5 + 10 * 0.25 + 0.Then the value of Class 2 is -0.5 .

Bellman Equation for Value Function

Bellman Equation helps us to find optimal policies and value functions. We know that our policy changes with experience so we will have different value functions according to different policies. The optimal value function is one that gives maximum value compared to all other value functions.

Bellman Equation states that value function can be decomposed into two parts:

Immediate Reward, R[t+1]

Discounted value of successor states,

Mathematically, we can define Bellman Equation as :

Bellman Equation for Value Function

Let’s understand what this equation says with a help of an example :

Suppose, there is a robot in some state (s) and then he moves from this state to some other state (s’). Now, the question is how good it was for the robot to be in the state(s). Using the Bellman equation, we can that it is the expectation of reward it got on leaving the state(s) plus the value of the state (s’) he moved to.

Let’s look at another example :

Backup Diagram

We want to know the value of state s. The value of state(s) is the reward we got upon leaving that state, plus the discounted value of the state we landed upon multiplied by the transition probability that we will move into it.

Value Calculation

The above equation can be expressed in matrix form as follows :

Bellman Linear Equation

Where v is the value of state we were in, which is equal to the immediate reward plus the discounted value of the next state multiplied by the probability of moving into that state.

The running time complexity for this computation is O(n³). Therefore, this is clearly not a practical solution for solving larger MRPs (same for MDPs).In later Blogs, we will look at more efficient methods like Dynamic Programming (Value iteration and Policy iteration), Monte-Claro methods, and TD-Learning.

We are going to talk about the Bellman Equation in much more detail in the next story.

What is Markov Decision Process ? Markov Decision Process : It is Markov Reward Process with a decisions.Everything is same like MRP but now we have actual agency that makes decisions or take actions.

It is a tuple of (S, A, P, R, 𝛾) where:

S is a set of states,

A is the set of actions agent can choose to take,

P is the transition Probability Matrix,

R is the Reward accumulated by the actions of the agent,

𝛾 is the discount factor.

P and R will have slight change w.r.t actions as follows :

Transition Probability Matrix

Transition Probability Matrix w.r.t action

Reward Function

Reward Function w.r.t action

Now, our reward function is dependent on the action.

Till now we have talked about getting a reward (r) when our agent goes through a set of states (s) following a policy π. Actually, in Markov Decision Process(MDP) the policy is the mechanism to take decisions. So now we have a mechanism that will choose to take an action.

Policies in an MDP depend on the current state. They do not depend on history. That’s the Markov Property. So, the current state we are in characterizes history.

We have already seen how good it is for the agent to be in a particular state(State-value function). Now, let’s see how good it is to take a particular action following a policy π from state s (Action-Value Function).

State-action value function or Q-Function

This function specifies how good it is for the agent to take action (a) in a state (s) with a policy π.

Mathematically, we can define the State-action value function as :

State-action value function

Basically, it tells us the value of performing a certain action(a) in a state(s) with a policy π.

Let’s look at an example of the Markov Decision Process :

Example of MDP

Now, we can see that there are no more probabilities. In fact, now our agent has choices to make like after waking up, we can choose to watch Netflix or code and debug. Of course, the actions of the agent are defined w.r.t some policy π and will get the reward accordingly."
The Complete Guide to Decision Trees,"The Complete Guide to Decision Trees

A complete introduction to decision trees, how to use them for regression and classification, and how to implement the algorithm in a project setting Marco Peixeiro · Follow Published in Towards Data Science · 9 min read · Jul 25, 2019 -- 2 Share

They are… don’t even try something else

Tree-based methods can be used for regression or classification. They involve segmenting the prediction space into a number of simple regions. The set of splitting rules can be summarized in a tree, hence the name decision tree methods.

A single decision tree is often not as performant as linear regression, logistic regression, LDA, etc. However, by introducing bagging, random forests, and boosting, it can result in dramatic improvements in prediction accuracy at the expense of some loss in interpretation.

In this post, we introduce everything you need to know about decision trees, bagging, random forests, and boosting. It will be a long read, but it will be worth it!"
How to Visualize Data on top of a Map in Python using the Geoviews library,"How to Visualize Data on top of a Map in Python using the Geoviews library Christos Zeglis · Follow Published in Towards Data Science · 5 min read · Nov 9, 2019 -- 5 Share

So let’s start with the problem we are about to tackle. Say you have some data that represent a specific figure (e.g. population) which differs from place to place (e.g. different cities) and you want to make a plot to visualize that data. How do you proceed with that?

One way to do that (and the most common one) is to create a bar plot. The y-axis represents the figure(e.g. population) and the x-axis represents the place (e.g. cities). I bet the vast majority of plots on that kind of data is of that type. As a result, there is a countless number of such examples on the web and therefore there is no need for me to add another one on the stack.

Fortunately, there is a better way to visualize that kind of data. Remember, plots have to be intuitive for the viewers to get a better grasp of what’s in front of them. So, in this case, a more intuitive way to visualize that data would be to plot them on a map. What’s more intuitive than an interactive map where you can zoom in, out, and over the place or figure you look for?

For the purposes of this tutorial, we are going to make a plot to visualize the passengers volume for the busiest airports in my country, Greece, and the neighbor country, Turkey, for comparison reasons.

First, we need to import the libraries and the methods we are about to use.

import pandas as pd

import numpy as np

import geoviews as gv

import geoviews.tile_sources as gvts

from geoviews import dim, opts

gv.extension('bokeh')

Our two dataframes, greek_aiports and turkish_airports , consist of the top 10 greek airports and the top 5 turkish airports in passengers volume respectively.

Stats for 2018

To these dataframes we will add an extra column country .

greek_airports['country']= 'GR'

turkish_airports['country']= 'TR'

We will also add the column color . You will see later on why we did that.

greek_airports['color']= '#30a2da'

turkish_airports['color']= '#fc4f30'

Now if we merge these two dataframes into airports"
Goal Setting for Data Scientists,"Goal Setting for Data Scientists

Setting goals, particularly around the end of December can be a great way to reflect and think about what you would like to get out of your career in the new year. In this article, we’ll discuss a framework for data scientists to use to achieve their career goals.

Since data science is an interdisciplinary field, the types of goals you set will be quite varied. A good way to partition the different types of goals is into the following three buckets: Technical, Behavioral and Professional.

For technical goals, you may want to improve your understanding of certain techniques or programming languages. Behavioral goals may focus on your presentation and persuasion skills. Finally, professional goals could focus on achieving career milestones, such as a promotion or landing a new job.

Focus On What You Can Control

Regardless of the type of goal you set, it’s important that you set goals that are within your locus of control. Rather than setting an external goal, such as getting a promotion, instead set internal goals that are within your control. For example, coming in early and pitching in to help with tasks nobody else wants to do.

Below you can see some examples of external goals reframed into internal ones for each of the three buckets:

One of the advantages of setting internal goals is that in the event you don’t achieve your external goal (for reasons within or outside your control), you can still feel a sense of accomplishment for having completed the things that were inside your control.

Be SMART about the goals you set

Originating from management literature in the 1980s the SMART (Specific, Measurable, Attainable, Relevant and Time Based) framework for setting goals and objectives has seen ubiquitous use…"
"Fake-follower calculators misinform users, journalists with dubious statistics.","On the left is SparkToro’s current sampling method and Twitter Audit’s method until May 2017. SparkToro asks Twitter for the latest 100k followers, and samples 2k to run its analysis. Twitter Audit requested 75k followers and sampled 5k.

When I asked Casey Henry, co-founder of SparkToro, if he thought that this method was “random” as their app stated (they have since updated their website to clarify their methodology), he said, “The definition isn’t: ‘random of all followers.’ It’s a random sampling of your followers.”

Semantics aside, the potential for error here makes their results effectively worthless for sufficiently large accounts. “Effectively worthless” meaning if SparkToro returns the number “65 percent fake” for Cardi B, what they’re really saying is that the number is somewhere between 1 and 99 percent. You can answer “How many fake followers did Cardi B gain recently?” But expanding that percentage to her millions of followers is akin to predicting how many people will vote for a national candidate after only conducting a poll in Connecticut.

In May 2017, David Caplan, the co-founder of Twitter Audit, began storing old IDs as a way to get around Twitter’s speed limit (diagram on the right). The process is complicated, but essentially if there are more than 1 million followers, Twitter Audit stores one half of the number of followers in their database. They do not check to see if there are any duplicates. Caplan approved the graphic and tried to explain the process the best he could, but he was not willing to share his code.

Caplan admits that he uses stale data, but he said that, until I had reached out, he was unaware that Twitter returns the latest followers first. He added over email, “Back then, the site was more like a toy so I didn’t really care that much about statistical accuracy. I also didn’t think that many people would use it!”

Granted, this half-of-all-followers, duplicate-laden, stale-data method is better than what Twitter Audit had before they updated in 2017. In the most dramatic cases, however, it still can have over a 50 percentage point error spread.

To reiterate, these error bars don’t come from the methods used to separate “real” from “fake” accounts — though that’s always a difficult process prone to error. These additional error bars come from the sampling process alone.

Twitter Audit’s calculation for @realDonaldTrump, samples 5k from their database of 30 million followers, which answers the question “How many fake followers has the President gained since 2017?” It wouldn’t matter as much if we could assume that fake followers were evenly distributed, but reporting from the New York Times has shown that, were someone to buy followers, those accounts are likely to be highly clustered and earlier in Twitter’s history.

To their credit, SparkToro analyzed every user who follows @realDonaldTrump and estimates that 70.2 percent of his followers are fake. In contrast, Twitter Audit, looking only at the latest IDs, posted that @realDonaldTrump has 11 percent fake followers.

Journalists were misled by quick stats.

Journalists may have been too quick to trust a web app that everyone used. Vanity Fair (with the kicker “Numbers Do Not Lie”) cited Twitter Audit to criticize @realDonaldTrump. And Matthew Ingram mentioned Twitter Audit’s stats for @realDonaldTrump in the Columbia Journalism Review. He wrote to me, “I’ve tried to mention and/or link to skeptical assessments of their value whenever I have mentioned them in writing about things like Donald Trump’s fake followers.”

Having been around since 2012, Twitter Audit has become a quick source for journalists. The list of publications goes on: The Washington Post, Newsweek, Quartz, and The Telegraph have all cited Twitter Audit. To be fair, there’s no way for them to have known the platform’s error — even its co-founder wasn’t entirely aware of how wrong the numbers can be.

Both Caplan and Henry mentioned that they’ve run full analyses for some journalists. When asked for specifics and presented with the stories listed above that have cited the app, however, Caplan wrote, “I’m sorry, I don’t have a record of the full audits so I can’t say for sure.” But since none of the authors from the publications listed above would comment, we don’t know which published numbers are more reliable. Both SparkToro and Twitter Audit have said they are always open to running full audits for journalists who ask.

SparkToro wasn’t sympathetic to a hypothetical journalist who took their numbers at face value. “As a reporter, you should be doing your due diligence to understand how that tool works,” said co-founder Henry. I pushed back, citing the app’s misleading methodology. Henry said, “You can take it however you want to take it. You found an edge case. Congratulations.”

The tools weren’t meant to be used on politicians or Katy Perry.

According to their respective founders, Twitter Audit was a tool built to spot accounts that “were literally are a complete fraud,” and SparkToro is an analytics company that can help advertisers assess the value of influencers, most of whom have fewer than 100k followers.

“We’re not advertising or branding ourselves as where you should analyze and criticize 2020 candidates,” said Henry. “That’s not what the tool is for…We could put a red sign on it that says ‘The tool doesn’t do this.’ But it’s a free tool. It wasn’t a priority for us. This is only a small portion of our product.” (It should be noted, however, that SparkToro made a blog post explicitly calculating the fake followers of various politicians, including three Democratic 2020 presidential candidates.)

Twitter Audit shared that fewer than 1 percent of their audits are on accounts with followers greater than 1 million, and SparkToro shared that fewer than 5 percent of their audits are on accounts with followers greater than 100k. Both claim monthly traffic in the tens of thousands.

Other fake-follower calculators exist, but users should be suspicious of any quick results given on accounts with greater than 75k followers. In a public forum, Twitter staff member Andy Piper wrote, “I am not aware of any API method that enables a random sampling of followers.” Meaning, any other fake-follower calculator is probably falling victim to the same problems as SparkToro or Twitter Audit.

In response to my inquiry, both have considered updating their methodology and explaining more clearly the limitations of their platforms. But even as we get slightly more accurate numbers, should users and journalists continue citing them for high-profile individuals? When an account has many millions of followers — when an individual is at the top of the Billboard charts or the President of the United States — is anyone really questioning their influence?"
Geospatial Indexing with Uber’s H3,"Geospatial Indexing with Uber’s H3

Photo by Jonas Svidras on Unsplash

In this article, I am presenting Uber’s H3 geospatial indexing system. One of the many open-source initiatives from this company, the H3 system allows you to quickly and efficiently index your data for later querying.

Geospatial indexing is essential for aggregating and querying data at scale. This type of data is generally abundant, difficult to index or search, and can be structurally complex. Polygons that discriminate particular areas can be very complex to handle. Think of a city’s boundaries. How many points do you need to efficiently determine if a given vehicle has entered a town or even a gas station? The more points you need, the more calculations you will require from your CPU. Overburdening your hardware translates to slower response times and higher resource usage.

An efficient geospatial indexing system helps you overcome these hurdles. In the case of H3, the solution takes the form of a hashing scheme.

How Does H3 Work?

The H3 algorithm partitions the Earth’s surface into a network of hexagons. You can select the amount of detail each hexagon contains by choosing among the available sixteen levels. You can think of these as “zoom” levels on a map. Every time you dive in a further step, the hexagons become smaller, and you need more of them to cover the same area.

Each hexagon is unique and is identifiable as such. You address individual hexagons through a unique sixty-four-bit identifier, an ideal key for a database table, or an in-memory dictionary. These identifiers are consistent across “zoom” levels, so you can mix and match them as you please.

Your first approach to H3 can be to think about it as a bright geospatial hashing scheme. With it, you can quickly determine area inclusions using simple value lookups. You can easily cluster geospatial observations, and display them on a map.

But there is more. To give you a better understanding of H3’s capabilities, I will present a simple use-case in Python.

Back to the UK Traffic Accidents

My second article here on Medium was on mapping the UK’s traffic accident hotspots. I decided to have a second look at it from H3’s perspective, as an illustration for this article. Back then, my concern was to find the areas of the highest reported traffic accident activity and map them. Mapping areas of significant event frequency is a problem that Uber needs to solve daily, although on a significantly larger scale (and for a different use case, for sure).

Here I decided to use H3 for indexing, aggregation, and display. The code that illustrates this article is available on the GitHub repository, and I suggest you use it to follow along. You only need to open one Jupyter notebook:

uk-accident-h3.ipynb

In this file, we are mostly seeing the same process occurring as in the first version. We load the data, clean it, and cluster it using the same DBSCAN algorithm. Instead of going through the trouble of finding a shape for the layers, we are going to use H3’s ability to group locations, and representing them as clusters of hexagons graphically.

Using H3 to Display Regions

Once we have the results from DBSCAN clustering, we get an extra column for our dataset, the cluster number. Noise points have a cluster number of -1 and are not relevant. Getting from the clustered locations to the map shapes is a matter of hashing them into H3 hexagon keys. And this is as simple as calling a conversion function:

h3_key = h3.geo_to_h3(latitude, longitude, level)

The resulting value is a key to a dictionary where we store the hexagon shape and the number of locations that share that same key. To get the geospatial polygon, we need to call another function:

h3.h3_to_geo_boundary(h3_address=h3_key)

This function accepts an H3 key and returns a list of latitudes and longitudes of the hexagon’s vertices that we can readily use to display on a map. To view the map, we only need to iterate through the dictionary and create the polygon shapes. The relevant function is:

map = create_map(clusters)

To test the solution’s sensitivity, you can tweak the two DBSCAN parameters and the H3 level parameter:

eps_in_meters = 50.0

num_samples = 10

h3_level = 11

Here is what we get at the end:

H3 clusters generated from DBSCAN

Clustering with H3

Now we can go a bit further and use H3 to cluster the data directly. Instead of going through DBSCAN, we will use all the input data and hash it into H3 keys. Instead of displaying all possible hexagons, we will impose a limit on the number of occurrences per hexagon. This restriction will help unclutter the map and make it more responsive.

The resulting map is slightly different than the first one, but this is reasonable. After all, we are using different assumptions. Remember that DBSCAN clusters locations using a density-based criterion. In a sense, this is a structure-revealing algorithm. On the other hand, H3 uses a structure-imposing approach by defining what areas to use as clusters.

Conclusion

This article presented a relatively short and superficial foray into Uber’s H3. It is a handy tool to perform geospatial indexing and analysis. I hope that this has whetted your appetite to dig deeper into the library and add it to your geospatial analysis toolbelt. And have fun with it. I did!

References

[1] H3: Uber’s Hexagonal Hierarchical Spatial Index

[2] H3 Python GitHub repository

[3] UK accidents GitHub repository

Related Articles"
Adversarial Attacks on Intrusion Detection Systems,"Artificial intelligence (AI) has come a long way in a very short period of time. Alan Turing, a pioneer computer scientist, published the first paper on the possibility of machines that can think in 1950. In less than a century, humans have created machines and programs that can compute and comprehend very large amounts of data to learn and mimic the acts of humans themselves.

People, businesses, and governments rely heavily on this newfound technology without even realizing it. One growing sector of AI is security. Intrusion Detection Systems (IDS) are systems used to protect networks or systems from malicious traffic. AI is dynamic by nature with its ability to learn, so it would be ideal for this application so that it can learn and evolve. This makes AI good at detecting good and malicious internet traffic as it doesn’t follow a defined set of rules but instead dynamically creates its own.

Intrusion detection systems (IDS) play a big role in protecting this information saved in a network or system, and AI is being integrated into IDS due to their low maintenance and ability to stay up to date with the latest attacks. Due to the rapid developments in AI, the need to increase the robustness of these systems have been neglected, and current research looks into ways of improving IDS."
Python Pro Tip: Start using Python defaultdict and Counter in place of dictionary,"Photo by Brooke Lark on Unsplash

Member-only story

Python Pro Tip: Start using Python defaultdict and Counter in place of dictionary

How you could use defaultdict and Counter to make your code short and readable Rahul Agarwal · Follow Published in Towards Data Science · 4 min read · Apr 22, 2019 -- 11 Share

Learning a language is easy. Whenever I start with a new language, I focus on a few things in below order, and it is a breeze to get started with writing code in any language.

Operators and Data Types: +,-,int,float,str

Conditional statements: if,else,case,switch

Loops: For, while

Data structures: List, Array, Dict, Hashmaps

Define Function

However, learning to write a language and writing a language in an optimized way are two different things.

Every Language has some ingredients which make it unique.

Yet, a new programmer to any language will always do some forced overfitting. A Java programmer, new to python, for example, might write this code to add numbers in a list.

x=[1,2,3,4,5] sum_x = 0

for i in range(len(x)):

sum_x+=x[i]

While a python programmer will naturally do this:

sum_x = sum(x)

In this series of posts named ‘Python Shorts’, I will explain some simple constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.

This series is about efficient and readable code.

Counter and defaultdict — Use Cases

Let’s say I need to count the number of word occurrences in a piece of text. Maybe for a book like Hamlet. How could I do that?

Python always provides us with multiple ways to do the same thing. But only one way I find elegant.

This is a Naive Python implementation using the dict object."
Tricks in R to Boost Your Productivity (Part 1),"Photo by Anete Lūsiņa on Unsplash

I want to share with you top tricks I use in R and RStudio to boost my productivity. They are a combination of public resources, such as StackOverflow, and my personal best practices. Life is short, stop wasting time by using these tricks. I hope you can benefit from it and share me with your tricks as well. I will continue to append new tricks in the post as I get more!

1. Calculating vector weights

In many cases, we need to distribute a number to a vector according to the weights of a reference vector, where the weight of each entry is calculated as its proportion to the sum of all entries. For example, you have a total sales forecast of 1,000 and the historical sales distribution across 3 channels, say, A, B, C, is (300, 100, 100). Then, the channel level sales forecast can be calculated as (3/5 * 1000, 1/5 * 1000, 1/5 * 1000) for A, B, C, respectively. One edge case is that there might not be any historical channel sales observations. In such a case, even distribution across all channels could be a common choice. If the sum is equal to 0, then all weights should be equal across all entries because they should be all equal to 0. If you constantly need to call such function, you can create a helper function to achieve this as below"
Let’s Underfit and Overfit a Machine Learning Model,"Let’s Underfit and Overfit a Machine Learning Model

A colleague recently started using the term “underfitting” to refer to a named entity recognition (NER) model missing entities it should have labelled.

I had to set the record straight. That’s not actually underfitting, but I can see how someone would get that impression.

What then is underfitting, or overfitting for that matter?

Let’s train some models that under and overfit data!

We’ll start by generating a dataset with sklearn’s “make_classification” function. Each datapoint will have 2 features (so it’s easily plottable) and a label.

from sklearn.datasets import make_classification # We didn't need to display all params but I like to see defaults

# I've edited some of these

X,y = make_classification(

n_samples=30,

n_features=2,

n_informative=2,

n_redundant=0,

n_repeated=0,

n_classes=2,

n_clusters_per_class=2,

weights=None,

flip_y=0.01,

class_sep=1.0,

hypercube=True,

shift=0.0,

scale=1.0,

shuffle=True,

random_state=None

) # Split examples by class (positive/negative) to give diff colors

pos_feat0 = []

pos_feat1 = []

neg_feat0 = []

neg_feat1 = [] for idx,klass in enumerate(y):

if klass == 1:

pos_feat0.append(X[idx][0])

pos_feat1.append(X[idx][1])

else:

neg_feat0.append(X[idx][0])

neg_feat1.append(X[idx][1]) # And plot them

import matplotlib.pyplot as plt

plt.scatter(pos_feat0,pos_feat1, c='blue')

plt.scatter(neg_feat0,neg_feat1, c='red')

Boom. We have data.

Now we’ll walk through the definitions of under and overfitting, then intentionally pick algorithms that will under and over fit the data.

Underfitting

According to wikipedia."
Information Entropy,"Information Entropy

If you were to watch me cross the street, or watch me play Russian roulette, which one would be more exciting? The possibilities are the same- me living or dying, but we can all agree that the crossing of the street is a bit boring, and the Russian roulette… maybe too exciting. This is partially because we pretty much know what will happen when I cross the street, but we don’t really know what will happen in Russian roulette.

Another way of looking at this, is to say we gain less information observing the result of crossing the street than we do from Russian roulette. A formal way of putting that is to say the game of Russian roulette has more ‘entropy’ than crossing the street. Entropy is defined as ‘lack of order and predictability’, which seems like an apt description of the difference between the two scenarios.

When is information useful?

Information is only useful when it can be stored and/or communicated. We have all learned this lesson the hard way when we have forgotten to save a document we were working on. In a digital form, information is stored in ‘bits’, or a series of numbers that can either be 0 or 1. The letters in your keyboard are stores in a ‘byte’, which is 8 bits, which allows for 2⁸ =256 combinations. It is important to know that information storage and communication are almost the same thing, as you can think of storage as communication with a hard disk.

Examples of symbols and their 8 digit codes

Information Storage

The mathematician Claude Shannon had the insight that the more predictable some information is, the less space is required to store it. Crossing the street is more predictable than Russian roulette, therefore you would need to store more information about the game of Russian roulette. Shannon had a mathematical formula for the ‘entropy’ of a probability distribution, which outputs the minimum number of bits required, on average, to store its outcomes.

Entropy

Formula from entropy from Wikipedia

Above is the formula for calculating the entropy of a probability distribution. It involves summing P*log(p) with base 2, for all the possible outcomes in a distribution. Here is a function to do this in Python:

import numpy as np

def entropy(dist):

su=0

for p in dist:

r= p/sum(dist)

if r==0:

su+=0

else:

su+= -r*(np.log(r))

return su/np.log(2)

Example: Russian Roulette

If we were to quantify the crossing the street example as having a 1 in a billion chance of death, and Russian roulette as 1 in 2, we’d get entropy([1, 999_999_999]) ≈ 3.1*10^-8 bits , and entropy([50,50])=1 bit, respectively. This means that if we repeated both experiments a trillion times, it would take at least 31,000 bits to store the results of crossing the street, and 1 trillion bits to store the results of Russian roulette, in line with our earlier intuition.

Some distributions and their entropies

Example: English Language

The English language has 26 letters, if you assume each letter has a probability of 1/26 of being next, the language has an entropy of 4.7 bits. However, some letters are more common than other letters, and some letters appear often together, so through clever ‘guessing’ (i.e. not assigning probabilities of 1/26), we can be much more efficient.

Random guessing on average takes us 13.5 guesses to get the correct letter. Let us say we are given the first letter of every word in this sentence:

H_ _ /A_ _ /Y_ _ /D_ _ _ _ / M_ /F_ _ _ _ _?

It would be very bad if it took us 13.5*16=216 guesses to fill in the 16 blanks. It would likely take us less than an average of two guesses per blank to figure out the sentence is “How are you doing my friend?”. So even if we exhaustively guessed the first letter and it took us 13.5 guesses, it would take us roughly 5.1 guesses/letter to fill in all the blanks, a huge improvement on random guessing.

Experiments by Shannon showed that English has an entropy between 0.6 and 1.3 bits. To put that into perspective, a 3 sided die has an entropy of 1.58 bits, and takes on average 2 guesses to predict. Also, note that the encoding system on your keyboard uses 8 bits per letter. So it could theoretically make all files in only the English language at least 6 times smaller!

Applications

Shannon’s work found uses in data storage, spaceship communication, and even communication over the internet. . Even if we are not working in any of those fields, ‘KL divergence’ is an idea derived from Shannon’s work, that is frequently used in data science. It tells you how good one distribution is at estimating another by comparing their entropies.

Communication and storage of information is what has made humans great, and Shannon’s work revolutionised the way we do so in the digital age."
Understanding Neural Networks,"Neural networks generate a lot of interest. However, it’s not always clear to people outside of the machine learning community the problems they’re suited for, what they are, or how they’re built. We’ll address these topics in this blog post, aiming to make neural networks accessible to all readers. For those with programming experience, I’ve appended a Jupyter Notebook at the end which you can follow to build your own neural network.

Most commercially successful applications of neural networks are in the area of supervised learning. In supervised learning, we are trying to build a model that maps inputs to outputs. Some examples include:

We can represent these models as function that takes an input and produces an output:

y = F(x)

where x is the input, y is the output, and F() is our model. Neural networks are a particularly effective way to build a model (i.e. F() ) for many classes of problems.

Let’s briefly consider a traditional approach for building many models. We can derive models describing many phenomena by applying our understanding of calculus to specific domain knowledge. In physics, this would include Newton’s Laws of Motion, or conservation laws stating that mass, energy, and momentum are conserved in a closed system. This approach let’s successfully build a variety of important models, such as the ideal rocket equations which tell us how much fuel a rocket needs to reach space, or the Boussinesq equations which let us model waves along the coast.

What about problems for which we don’t have intuition into the fundamental dynamics? Say you are building an autonomous vehicle, and want to recognize other cars on the road using a video stream from your dashboard camera. Despite the fact that we’re all quite good at recognizing cars, we haven’t been able to formulate physical principles describing what a car looks like. We can’t point to a combination of wheels, doors, and windows which make up a car. Neural networks provide us a technique which we can use to solve these types of problems effectively.

Neural networks work by learning the mapping from input to output directly from data. The process of having the neural network learn this mapping is known as training. Training requires a dataset of training examples, which are pairs of inputs and the corresponding outputs (x and y). For training to be effective, we need a large dataset, typically tens of thousands to tens of millions of training examples.

During training, we are optimizing the weights (or parameters) of the neural network. For each training example, we run the model on the input, and compare the model output to the target output using a loss function. Using an algorithm called backpropogation (or backprop for short), we update all the weights in the network so that the model output will be closer to the target output. The weights are updated proportionally to how much they contribute to any mismatch. We continue cycling through our training set, iteratively updating the model, until performance no longer increases.

Let’s look at a visualization of a straightforward neural network. On the left hand side we have the input layer. This is our data, such as the pixels of an image, or how many times certain words appear in an email. Next we have two hidden layers. Hidden layers is a term we refer to the layers between the input and the output layers. Finally, we have the output layer. As the input passes through each layer of the neural network, it undergoes a series of computations. Each ‘unit’ (or ‘neuron’) in the hidden layers and the output layer contain a set of weights to be optimized, which control these calculations.

Designing a neural network requires selecting hyper-parameters controlling both the architecture of the network and the training process. We use the term hyperparameters, since the term parameters is an alternative to weights. Hyperparameters related to the architecture include the number of layers, the width of each layer (i.e. number of units), as well the choice of something called the activation function in the units. The training is particularly influenced by the choice of optimization algorithm, the learning rate used by the algorithm, and whether a technique called regularization is implemented.

Unfortunately, there is no way to know beforehand what is the best architecture for your problem, or what the best parameters for training that architecture would be. Practitioners are guided by a combination of experience, intuition, and best practices from the community. As the field is moving rapidly, this requires continuously staying up to date. Often the best way to start on a problem is to look at the machine learning literature to see if someone has solved a problem similar to yours, and take their solution as a starting point. Getting a solution to your particular problem will usually require several iterations of looking at the data, realizing ideas, modifying the model code, and testing.

A key question of any neural network is how well is it able to perform on data it hasn’t seen before. Therefore, before training a neural network, a small portion of the data is set aside in what is commonly referred to as the test set. Following training a neural network, we compare the performance of the neural network on the training set and the test set. One possible scenario is that the model doesn’t perform well even on the data it’s trained on. In case, we say the model has high bias. When the model doesn’t fit the data it has seen well, the hyperparameters should be reevaluated. Another outcome is that the model performs well on the training set, but not very well on the test set. In this situation, we say the model has high variance — that the neural network has been overfit to the training data. Equivalently, the network hasn’t learnt features of the data that generalize well, and perhaps has memorized features specific to the training set. To address high variance, we typically employ a technique called regularization. When feasible, acquiring additional data is also beneficial.

An important detail is that the data you train your neural network has to be similar to the data you apply your neural network to. Statistically, you want your training and test data to come from the same distribution. Intuitively, this means if you train your autonomous vehicle to drive exclusively in sunny weather, you can’t expect it to stay on the road during a snowfall. In practice, this means if you develop a neural network to predict customer behaviour, you’ll need to update your model periodically as your important factors such as your products, customers, and competition evolve.

These are the broad concepts key to understanding how to apply neural networks, and communicate with those who regularly work with them. After reading this, you should be able to confidently navigate a conversation on applying neural networks.

To summarize the key points about neural networks:

They are an effective way to build a model mapping from input to output directly from a training dataset. Neural networks have many hyperparameters, and designing a good network is an iterative process. Your training and test data should come from the same distribution.

Finally, a lot of software developers I speak to are keen to know implementation details of neural networks. The following Jupyter Notebook detail has been designed with you in mind, implementing a neural network to recognize handwritten digits with 98% accuracy. We’ll assume you have python environment setup with PyTorch. If you don’t, Anaconda is the recommended package manager and pretty simple to get started with.

You can download the jupyter notebook to run on your own machine here, as well as follow along with the precomputed version here."
Can You Tell Random and Non-Random Apart?,"Photo by Alejandro Luengo on Unsplash

Apophenia or the “ tendency to mistakenly perceive connections and meaning between unrelated things”. Humans it turns out are quite rubbish at telling things that are random apart from things that are not. In fact we sometimes see patterns where there are none and for others they can say something is random but have no scientific way of explaining it. You see this most often when people think that lottery numbers “1 2 3 4 5 6” are less likely to occur than say “12 3 15 82 90” or that if a sequence of numbers has come up it is less likely to come up again.

Don’t believe me? Then have a go at the question below.

I have two binary strings generated by two processes:

0001011000111110011111000000100101111110110111111111010111111011010000010001000100110000000001100001 1010101001011011001011100001101010101111001000101001010010100101000101011011010101011101010101010101

If I told you one was made by a random process and another by a biased process, can you tell which is which?

Would you believe me if I said there was an easy way to find out?

How about if I told you there was an easy way to tell them apart and in a factual way?

I have in fact given this question to around 60 highly educated consultants and the most common answer was a gut estimate that the top one (string 1) seems less structured than the bottom, so string 2 is the non-random one.

I mean they are not wrong and what they were picking up on was the “Frequency Stability Property” (also another link here), but it seems a bit hard to convince someone with only an experience led decision. For example, if they said it was string 1 and I said it wasn’t how would we tell who was lying or not? We need some harder evidence. This is where this easy method I eluded to earlier comes into play.

If you were to count the number of 1’s and 0’s in the strings you would find they both have the same number of each and so tells you little. However, for any random process any sequence within it (e.g. “11” or “101”) should have equal probability compared to another sequence of the same length. This is because the number before in a string should not…"
Introduction to Natural Language Processing (NLP),"So, What is Natural Language Processing (NLP)?

NLP is an interdisciplinary field concerned with the interactions between computers and natural human languages (e.g. English) — speech or text. NLP-powered software helps us in our daily lives in various ways, for example:

Personal assistants : Siri, Cortana, and Google Assistant.

: Siri, Cortana, and Google Assistant. Auto-complete : In search engines (e.g. Google).

: In search engines (e.g. Google). Spell checking : Almost everywhere, in your browser, your IDE (e.g. Visual Studio), desktop apps (e.g. Microsoft Word).

: Almost everywhere, in your browser, your IDE (e.g. Visual Studio), desktop apps (e.g. Microsoft Word). Machine Translation: Google Translate.

Okay, now we get it, NLP plays a significant role in our daily computer interactions; let’s take a look at some example business-related use-cases for NLP:

Fast-food chains receive a vast amount of orders and complaints daily; manually handling this will be tiresome and repetitive, also inefficient in terms of time, labour and cost. Thanks to recent advancements in conversational AI , they can build virtual assistants that automate such processes and reduce human intervention.

, they can build virtual assistants that automate such processes and reduce human intervention. Brands launch new products and market them on social media platforms; they can measure campaigns’ success rates using metrics such as reach and number of interactions. Still, they can’t understand the consumers’ public sentiment automatically. This task can be automated using sentiment analysis, a text classification task where machine learning models are trained to quantify affective states and subjective information.

NLP is mainly divided into two fields: Linguistics and Computer Science.

The Linguistics side focuses on understanding the structure of language, including the following sub-fields [Bender, 2013]:

Phonetics: The study of the sounds of human language. Phonology: The study of the sound systems in human languages. Morphology: The study of the formation and internal structure of words. Syntax: The study of the formation and internal structure of sentences. Semantics: The study of the meaning of sentences. Pragmatics: The study of the way sentences with their semantic meanings are used for particular communicative goals.

The Computer Science side is concerned with translating linguistic knowledge and domain expertise into computer programs with the help of sub-fields such as Artificial Intelligence."
Creating a comic book recommendation system for non-comic readers,"A passion of mine is comic books. As you may have noticed, even if you are not a reader, you are bound to be exposed to characters and stories that originated in comics being used in television and movies. Netflix, Hulu, Amazon, HBO and many others all have dipped into making content from comics. Summer theaters are bustling with the latest Avengers or Batman tale.

As much as I love the medium, the comic book industry at large is often looked over in favor of the next big movie or tv show using their stories or characters. There are many people who see these shows and movies and are inspired to read, but don’t know where to start. I get it, jumping into comics can be intimidating. I think many people get that vision in their head of the comic shop owner on The Simpsons and want to run the other way. I thought there has to be a way to help people get a foot into comics without starting from scratch.

My main thought was, how can I get people who watch movies and tv shows about comic books into comic books? How about with their preferences in movies and tv shows! How about I recommend you comics from those existing preferences.

In general, this project relies on the idea that there is some intrinsic taste to the content you enjoy. Below is an example of the type of assumption. It may not always be a clearly labeled concept, but there is potentially some underlying aspect to your tastes that can be seen across media you consume.

With all this in mind, let’s build a recommendation system!

Data prep & digging up comics

My goal was to look for people who had rated both movies/tv and comic books/graphic novels. To build my model, I used a large repository of Amazon reviews previously collected in a research project at the University of California, San Diego (~24GB of book and movies/tv reviews from 1996–2014, more information here).

In order to extract the correct users and ratings, I had to spend a good amount of time learning and exploring this dataset. All of the comic books/graphic novels are lumped within all other book reviews with no shortcuts to pull them out. I started with a smaller amount of Amazon IDs (ASIN) for comic books from scraping Amazons bestseller pages. Based on data exploration, I found a pattern in the ids to shortcut getting a few large chunks of ids for comic books. Below is an example of the patterns I found:

With this set of ids, I found any corresponding reviews. I then took the reviewers in that set and found any that also had reviewed movies/tv. After removing items with less than 5 reviews, and dropping any data missing relevant metadata (some were missing titles or linked to items that were no longer listed, making them unusable), I was working with ~84,000 reviews, with ~8,500 distinct users and ~7,400 items (~1,300 comic books/graphic novels, ~6,100 movies/tv).

Modeling & Alternating Least Squares

My approach was to build a Alternative Least Squares (ALS) model to have a collaborative filtering recommender system. What you may be asking yourself is what are these least squares and why in the hell are they alternating?

Alternating Least Squares is a method by which we can decompose a ratings matrix into a matrices representing users and items, with each expressed through latent features. In a very simplistic example, it could be thought of as such:

So I have my ratings matrix with users and their reviews for comics and movies (R). This matrix is going to be fairly sparse, as users will have only rated some things, but we want to fill in those empty slots and guess what they might score based on what we do know about them. We first choose a number of latent features. The latent features are essentially a mathematically derived way of describing users and items. Through non-negative matrix factorization (NMF), we can derive from R the matrices for users (U) and items (P). We hold U constant (populated with random values) and solve for P. Then, we hold P constant and solve for U. This process goes back and forth until there is no reduction in error seen. Once complete, U and P are multiplied together to get our new R estimate, filling in values across the user review matrix.

Evaluation

For evaluation, I optimized the performance of my model based on optimizing for Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE). Tuning the parameters of my model did not prove to offer too much in terms of performance, but I settled on my best model using 50 latent features, regularization parameter at .1, and max number of iterations at 20. My best performing model has an RMSE of 1.17. In general, I would hope to get that number under 1, but think this is a fairly good performance under my current scope.

Up, up, and away!

You can test it for yourself at IntoComics. The application gives you a selection of movies to rate from 1 to 5 (currently a curated list of 60 movies that were most frequently rated, but leaving off all that are based on comic books or graphic novels to more cleanly drive the separation of the two medium). Once you have rated one or more, it returns a listing of the top 5 comic book/ graphic novel recommendations with links to each product on Amazon. I’m going to continue to work on improving the model, but for now, hope you enjoy!"
Convolutional Neural Network: A Step By Step Guide,"Convolutional Neural Network: A Step By Step Guide Shashikant · Follow Published in Towards Data Science · 9 min read · Mar 17, 2019 -- 1 Listen Share

“Artificial Intelligence, deep learning, machine learning — whatever you’re doing if you don’t understand it — learn it. Because otherwise, you’re going to be a dinosaur within three years” — Mark Cuban, a Serial Entrepreneur

Hello and welcome, aspirant!

If you are reading this and interested in the topic, I’m assuming that you are familiar with the basic concepts of deep learning and machine learning.

If not, don’t worry! The tutorial is designed in a way that gets you started with deep learning skills from the beginning to the end―from perceptron to deep learning.

In this tutorial, we’ll touch base on the aspects of neural networks, models, and algorithms, some use cases, libraries to be used, and of course, the scope of deep learning. In addition to it, other important concepts for deep learning will also be discussed.

Step 1: Pre-requisites

Any Tom, Dick, and Harry cannot just hear about deep learning wonders, develop interest and start a tutorial. There has to be a fine way of learning, and that’s why we have laid the foundation work for you. Following are the points that highlight what all you need to do before you fire up your learning process:

Knowledge of R/Python: These are the two most commonly used and preferred languages for deep learning. One of the main reasons is that there is enough support/ community available for both. Before you jump into the world of machine learning, select one of these at your convenience. Undoubtedly, Python is leading the field; however, you can see the comparison here.

These are the two most commonly used and preferred languages for deep learning. One of the main reasons is that there is enough support/ community available for both. Before you jump into the world of machine learning, select one of these at your convenience. Undoubtedly, Python is leading the field; however, you can see the comparison here. Basic understanding of linear algebra, calculus, and probability: There are an incredible amount of online videos and courses are available for each lesson of which, many of which are free. We are not suggesting you hone the skill but just brush it up for a bright understanding of the tutorial. You may try starting with Stanford’s CS231n.

There are an incredible amount of online videos and courses are available for each lesson of which, many of which are free. We are not suggesting you hone the skill but just brush it up for a bright understanding of the tutorial. You may try starting with Stanford’s CS231n. Primary know-how of neural networks and deep learning: As I said earlier, plenty of sources are available online for free as well as paid. Online videos are always helpful anyway. If you want to read through the concept, we suggest you follow Neural Networks and Deep Learning, which is absolutely free. (Also, pay attention to 25 Must-Know Terms & concepts for Beginners in Deep Learning)

As I said earlier, plenty of sources are available online for free as well as paid. Online videos are always helpful anyway. If you want to read through the concept, we suggest you follow Neural Networks and Deep Learning, which is absolutely free. (Also, pay attention to 25 Must-Know Terms & concepts for Beginners in Deep Learning) Set-up requirements: Since deep learning relies heavily on computational concepts, we need faster machines to operate at that level. So, all that you need for now is:

GPU (4+ GB, preferably Nvidia) — It is the heart of deep learning applications today CPU (e.g. Intel Core i3 or above will do) 4 GB RAM or depending on the dataset

Note: (If you want to learn more about hardware requirements, go through this hardware guide, and most importantly, do not install deep learning libraries at this step. You’ll be told further in this tutorial.)

Step 2: Introduction to Concepts and Technical Aspects

How Can You Dive Into Deep Learning? Essentially, it all starts with neural networks, and deep learning is nothing but the logical implementation of those networks to abstract useful information from data. In technical terms, it is a discreet method of classification of unstructured input data such as media which includes images, sound, video, and text.

Firstly, you need to decide which learning medium suits you the best for your research and study deep learning. It could be blogs, books, videos, or online courses approach. We are listing the sources for you to start with the simplest concept that will help you to get a grip on the subject gradually.

Blog Approach

- Fundamentals of Deep Learning — Starting with Artificial Neural Network

- A Deep Learning Tutorial: From Perceptron to Deep Networks

Book Approach

- Neural networks and Deep Learning (A free book by Michael Neilson)

- Deep Learning (An MIT Press book)

Video Approach

- Deep Learning SIMPLIFIED

- Neural networks class — Université de Sherbrooke

Online Course Approach

- Neural Network by (Enroll starts 27 Nov)

- Machine Learning by Andrew Ng (Enroll Starts 27 Nov)

- Machine Learning By Nando de Freitas (contains videos, slides, and a list of assignments)

Dear learners, accept the fact that transformation to becoming a deep learning expert would require plentiful time, many additional resources, and dedicated practice in building and testing models. We, however, do believe that utilizing the resources listed above could set you in motion with deep learning.

Step 3: Select Your Adventure

After you have got the basics, here comes the interesting part―hands-on experience in deep learning state-of-the-art technology. There are numerous exciting applications and opportunities that the field has to offer. Techniques in deep learning will vary based on your interest and purpose, see below:

Computer vision/ pattern recognition: Both are not much different since pattern recognition is also a part of computer vision, many times. However, in broader terms, computer vision includes analyzing only images and is used for object detection, segmentation, vision-based learning, etc. whereas pattern recognition is not restricted to images. It is about the classification of anything which can possess a pattern.

To learn, go here:

Deep Learning for Computer Vision

CS231n: Convolutional Neural Networks for Visual Recognition

For Video and use cases:

Detailed lectures on computer vision

Speech and audio recognition: Ever said “Ok Google”? I’m sure, you did. It comprises a speech recognition system that helps you find what you’re looking for on Google.

Technically, it consists of a type of neural network that involves sequences of inputs to create cycles in the network graph called recurrent neural networks (RNNs). They are called ‘Recurrent’ because they perform the same task for every element of the sequence and perform tasks such as machine translation or speech recognition.

To learn, go here:

The Unreasonable Effectiveness of Recurrent Neural Networks

Recurrent Neural Networks Tutorial

Understanding LSTM Networks (a wildly used RNN variant)

For Videos:

A friendly introduction to Recurrent Neural Networks

Recurrent Neural Networks (RNN)

Natural Language Processing OR NLP: NPL is a way for computers to read, analyze and respond by simulating the human language in a smart and useful manner. Today, technology is widely applied to multiple industry segments such as advertising, customer care, insurance, etc. to automate the process of human-computer interaction.

The NPL layer translates user requests or queries into information and searches for a proper response from its database. An advanced example of NLP would be a language translation―from one human language to another. For instance, English to German.

To learn, go here:

Ultimate Guide to Understand & Implement Natural Language Processing

NLPFORHACKERS.IO

How to Get Started with Deep Learning for Natural Language Processing

For videos:

Introduction to Natural Language Processing

Natural Language Processing with Deep Learning

Reinforcement Learning OR RL: Imagine a robot trained to learn from its previous actions and perform a new task, whenever required, wouldn’t that be great, and automatic! In fact, it is for real.

Reinforce learning introduces a similar concept for a computer agent; whether it fails or succeeds in a particular task, the agent receives rewards and punishments for the action on an object. It gains knowledge on it as part of the deep learning model controlling its actions.

To learn, go here:

A Beginner’s Guide to Reinforcement Learning (for Java)

Simple Beginner’s guide to Reinforcement Learning & its implementation

For Videos:

Deep Reinforcement Learning

Reinforcement Learning

Step 4: Choosing the Right Framework

We discussed many applications and usage of deep learning technologies in step 3. Chances are, for some tasks, traditional machine learning algorithms would be enough. But, if you’re dealing with a large collection of images, videos, text or speech, deep learning is bliss and everything for you. However, in deep learning, which framework will be the right choice for you is a question for many.

Remember, there’s no right framework, there is only a suitable framework. Here’s what your selection criteria should primarily depend on:

Availability of pre-trained models

Open-source

Supported operating systems and platforms

Licensing model

Ease of model definition and tuning

Availability of debugging tools

Ease of extensibility (for example, able to code new algorithms)

Connected to a research university or academia

Supported deep learning algorithmic families and models

To assist you in selecting one, let me take you on a brief tour of Deep Learning frameworks:

(a) TensorFlow: Backed by Google, TensorFlow is an all-purpose deep learning library for numerical computation based on data flow graph representation.

- Try out its introductory tutorial

- To install TensorFlow, visit here

- Refer to its documentation

- Have a look at its whitepaper

(b) Theano: Theano, a math expression compiler, is an actively developed architecture that efficiently defines, optimizes, and evaluates mathematical expressions having multi-dimensional arrays.

- Try out an introductory tutorial

- To install Theano, visit here

- Keep the documentation handy

(c) Caffe: While Theano and TensorFlow can be your “general-purpose” deep learning libraries, Caffe is made by keeping expression, speed, and modularity in mind. The framework, developed by a computer vision group, enables simple and flexible deep learning to organize computation. To its update, Caffe2 is also available.

- To install Caffe, visit here for Caffe and Caffe2

- Familiarize yourself with an introductory tutorial presentation

- Here you’ll find its documentation

(d) Microsoft Cognitive Toolkit: Microsoft Cognitive Toolkit — previously known as CNTK — is a unified deep-learning toolkit that easily realizes and combines popular model types such as CNN, RNN, LTSM and more, across multiple GPUs and servers.

- To install Microsoft Cognitive Toolkit, visit here

- For tutorials, reach here

- Model Gallery collection of code samples, recipes, and tutorials for various use cases.

Please note that the above-listed architectures are not the only popular libraries in use today. We have listed some more with their key features:

- Written in Python; a minimalist and highly modular neural networks library

- Capable of running on top of either Theano or TensorFlow

- Enables fast experimentation.

- A scientific computing framework

- Offers wide support for machine learning algorithms

- Based on the Lua programming language

- Python supported, a flexible and intuitive neural networks library

- designed on the principle of define-by-run

- let you modify networks during runtime

To learn more on criteria-based selection and a detailed review on other frameworks, visit the page- How to Select a Deep Learning Framework (we suggest you bookmark the link as it is updated very often).

Step 5: Exploring Deep Learning

Deep learning is a complex, yet prominent field of artificial intelligence where the real magic is happening right now. The three key points which steer the field of deep learning are:

The availability of huge amounts of training data Powerful computational infrastructure Advances in academia

However, what you need to do to pioneer deep learning is simple:

Repeat from step 2 — step 4 with a different adventure every time

Keep testing your deep learning skills (e.g. Kaggle)

Join Deep Learning communities and ask questions (e.g. Google Group, DL Subreddit)

Follow recent researches/ researchers (e.g. Most Cited Deep Learning Papers)

EndNote

Today, researchers, who were also learners just like us a few years back, are working hard to defy impossibility in the field of technology. In the beginning, you might find difficulty in learning concepts but, tenacity is the key.

You may find yourself baffling with deep learning algorithms and think why it didn’t work as you expected or why am I getting this error ABC?…believe me, that’s normal. If required, try out a sample algorithm you trust would work on a small set of data first.

In this field of learning, give everything a try that makes sense to you. While you gain new skills, try to build something different using your mind. Remember the dialogue from the movie Spiderman — “With great power, comes great responsibility.” The trend for deep learning is rising non-stop. To make a dent in the Deep Learning hall of fame, the universe is open to you. Come out and showcase your talent as many things are still unexplored.

At last, I request you to support (by Clapping) this piece and SHARE it so that we do not leave any aspiring talent behind and miss any opportunity on inventions to come!! Love you all…"
20 Popular Machine Learning Metrics. Part 1: Classification & Regression Evaluation Metrics,"Introduction

Choosing the right metric is crucial while evaluating machine learning (ML) models. Various metrics are proposed to evaluate ML models in different applications, and I thought it may be helpful to provide a summary of popular metrics in a here, for better understanding of each metric and the applications they can be used for. In some applications looking at a single metric may not give you the whole picture of the problem you are solving, and you may want to use a subset of the metrics discussed in this post to have a concrete evaluation of your models.

Here, I provide a summary of 20 metrics used for evaluating machine learning models. I group these metrics into different categories based on the ML model/application they are mostly used for, and cover the popular metrics used in the following problems:

Classification Metrics (accuracy, precision, recall, F1-score, ROC, AUC, …)

Regression Metrics (MSE, MAE)

Ranking Metrics (MRR, DCG, NDCG)

Statistical Metrics (Correlation)

Computer Vision Metrics (PSNR, SSIM, IoU)

NLP Metrics (Perplexity, BLEU score)

Deep Learning Related Metrics (Inception score, Frechet Inception distance)

There is no need to mention that there are various other metrics used in some applications (FDR, FOR, hit@k, etc.), which I am skipping here.

As a side note, it is also worth mentioning that metric is different from loss function. Loss functions are functions that show a measure of the model performance…"
Deep Learning on a Budget,"Introduction

Why?

There are many articles and courses dedicated to the latest ML/AI research aimed at training bigger models and achieving higher classification accuracy. This is great for research and academia and pushing the limits of what AI can do. However, these are not really tailored for poor student practitioners starting off with their first major AI projects or penny conscious entrepreneurs looking to build an MVP of their cool revolutionary idea.

What?

In this work I take a budgeted approach to model training and try to answer the question:

What is the minimum, practical cost to complete a real world AI project?

The problem that I chose for this was an Image Classification problem.This article captures the process I followed and key budgeting lessons learned from each step.

Summary

The answer is roughly $300 → This is the amount it takes to train a well performing Computer Vision model using cloud computing. Incidentally (or not) this is also the amount of credit that Google gives as incentive to get started on the Google Cloud Platform (GCP) [1].

The breakdown of the budget is given below. The two rightmost columns list the instances from AWS and GCP that are most amenable to this task. The cost is an average of the instances listed in these columns. At present there is not a lot separating the two cloud providers in terms of cost."
Collaborative filtering with FastAI,"A collaborative filtering model/recommendation system seeks to predict the rating or preference a user would give to an item given his old item ratings or preferences. Recommendation systems are used by pretty much every major company in order to enhance the quality of their services.

In one of my first articles, I created a book recommendation system using Keras a high-level API build on Tensorflow. In this article, I will show you how to build the same recommendation system using the FastAI library as well as how to build a neural network based model to get even better results.

For our data, we will use the goodbooks-10k dataset which contains ten thousand different books and about one million ratings. It has three features the book_id, user_id and rating. If you want you can get the all the data files as well as the complete code covered in this article from my Github repository.

If you prefer a visual tutorial you can check out my FastAi videos."
How to Send Email with Attachments by using Python,"How to Send Email with Attachments by using Python

Photo by Web Hosting on Unsplash

As a Data Analyst, most often than not I receive requests like “Can you send this report to me on a weekly basis?” or “Can you send this data to me through email every month?”. Sending report is easy but it will be irritating if you have to do the same thing every week. That’s why you should learn how to use python to send email/report and schedule the script on your server.

In this article, I will show you how to extract data from Google BigQuery and send it as a report. You can jump to the Email part if you only want to know how to send the email using Python.

Import Libraries"
A.I. and Humanity’s Self-Alienation,"In the current fervent pursuit of generalized AI, we risk sleepwalking into self-obsolescence. By downplaying the risks of AI by believing that the benefits will outweigh the burdens, if and when a generalized AI is achieved and comes online, it will be too late.

A critical approach to the problem of AI can differ in content from cliché pessimism. Our great fear does not have to be a violent Terminator or Matrix scenario resulting in a tragic life-imitating-art plot twist. Although that is within the realm of possibility, rather than assuming a stance that looks outward, towards the machine, contemplating its power, it behooves us to critique the fetishistic pursuit of AI by turning inward, and asking questions. For example, what does AI (and our pursuit thereof) tell us about ourselves? Where does the dream of AI come from? What is the context of its appeal?

Self-entanglement. | Photo by Artem Kim on Unsplash

Capitalist Modernity

The pursuit of AI comes out of our contemporary history — a late capitalist period of advanced production. From machinery and technology comes the possibility to imagine a utopian world in which production is fully automated, leaving humans with free time — so the dream goes. The realities of production, distribution, and consumption in our time create the real, concrete basis for the extension of our imagination in this way.

Dreams of full automation aside, the period of capitalist modernity engenders forms of social life specific to itself. Quotidian realities are taken for granted — e.g., ideas about individual freedom/entrepreneurship and Darwinesque competition in the marketplace. These truths appear to have universal, transhistorical validity, but are in fact aspects of social life specific to the capitalist epoch. They are slices of the general human experience of creation and refinenment that are necessary for this system’s maintenance and reproduction.

One of the distinguishing features of capitalist modernity is alienation. When people go to work, they necessarily suspend their otherwise individual freedoms to the hierarchical and authoritarian structure of the workplace. People are separated from the product of their labor, be it clothing manufacturing or a line of code. Our labor belongs to another. It is the property of another, which is a basic feature of life most of us take for granted. Recently, Taylor Swift was quite publicly alienated from her labor when music executive Scooter Braun purchased her entire back catalog, despite, as writer Noah Berlatsky notes, being the author of her own lyrics and music and a powerhouse in her own right. According to Berlatsky, what differentiates Swift from everyone else is

[M]ost people not only don’t own their labor but aren’t in a cultural position to even imagine owning it. Swift can say, “I should own my albums,” and it makes instant sense. But if an assembly line worker said, “I should own my cars,” or a Walmart employee said, “I should own my store” they’d receive substantially less public support, presuming they could even find anyone to listen.

These separations — of rights and products of work — are forms of alienated activity in the modern world. Let it be said, however, that people today suffer from far less overt domination than those in previous societies (we are not at the mercy of feudal landlords, although many of us can be evicted by our urban ones). Yet, as the late Marx scholar Moishe Postone argued, “To consider freedom only with reference to questions of personal dependence and independence can serve to veil the existence of a more overarching form of unfreedom…” (2015, p. 8). We are nonetheless compelled as individuals to do what we must in order to survive. It is this form of abstract alienation, characterized by the heteronomous history of modernity, that differentiates capitalism from previous social formations.

Alienation in the twenty-first century plays a disruptive and destructive social role. It is simultaneously concrete and abstract, determined by immediate social activity and experienced as a kind of shadowy, ineffable form of social domination. One of its ramifications is the collective nonresponse surrounding the development of artificial intelligence — perhaps because we imagine a being of our own creation, separate from us, as more complete than the kind of people we have allowed ourselves to become.

In short, alienation is about relations that separate and remove human beings from aspects of their nature. It is human nature to work and to create, yet we are separated from it. It is in our nature to produce, but we are separated from our products, unable to control what we create and what to do with our creations. We become separated from each other, forced into competition and forms of hostility, much of the time between classes (see Office Space for a fictional example, and the fight for a living wage and safe work conditions happening every day). Through such separations and cut-offs, the vast majority of human beings in modernity are reduced to their lowest common denominator, having lost the particular qualities that rendered them human to begin with. By way of thoroughgoing alienated forms of life, humans become “abstractions,” individualized, isolated, and emptied of connection to any kind of social whole beyond superficial commonalities. According to NYU professor of politics Bertell Ollman, alienation is “this process which largely accounts for the power that money has in capitalist societies, the buying of objects which could have never been sold had they remained integral parts of their producer” (1976, p. 135).

Given the configuration of our current world, it is no surprise that people have begun to question the status quo. The freedom we know today is one-sided and shallow. One is free to go hungry if one “chooses” not to work — the choice itself a false one. Rather, we are forced into underpaid labor and told we are free. In such a context it is understandable that there would exist a collective yearning for completely intelligent and capable machinery, an instrument to set us free. We want to endow a synthetic being with powers to point beyond its own limitations and rid us of our drudgery, while we systematically deny ourselves that same power under current conditions. AI as we know it is tantamount to outsourcing humanity because we have alienated ourselves."
Generating Startup names with Markov Chains,"Generating Startup names with Markov Chains

The most interesting applications of Machine Learning are, without a doubt, the generative models.

The idea of finding patterns in data and generating new content that at the same time is similar to your data, but unique in its own way, has always fascinated me.

So I have decided to develop a simple text generator to create Startup names using Markov Chains.

But first of all, a short introduction to Markov Chains 🔗

Markov Chains

A Markov chain is a model of some random process that happens over time.

Markov chains are called this way because they follow a rule called the Markov property. The Markov property says that whatever happens next in a process only depends on how it is right now (the state).

For instance, consider the example of predicting the weather for the next day, using only the information about the current weather. By analysing some real data, we may find these conditions:

Given that today is sunny, tomorrow will also be sunny 90% of the time

Given that today is sunny, tomorrow will be rainy 10% of the time

Given that today is rainy, tomorrow will also be rainy 50% of the time

Given that today is rainy, tomorrow will be sunny 50% of the time

This is modelled via the Markov Chain below, where each circle is a state and the numbers in the arrows represent the probabilities of changing from the current state.

So if we want to make a prediction for tomorrow, we just need to verify the current state we are in (sunny or rainy) and use the transition probabilities to calculate which next state is more likely.

An in-depth explanation of Markov Chains, with some cool animations, can be found here: http://setosa.io/ev/markov-chains/

Applying Markov Chains to text generation

So, how can we apply this idea to generate text? Well, it's quite simple actually.

Names or sentences are basically a sequence of characters and those sequences follow some patterns.

For instance, if I asked you to give words that started with whe__, you would quickly come up with the words when, where, whenever, etc. While wheeziness is a perfectly valid word, it's less frequent considering the initial whe_. In other words, given the state whe, we will most likely change to the states when, where or whenever than to the state wheeziness.

So how can we build a model that capture these probabilities given our data?

For this, I'll show you how to build a simple Markov Chain using the words startup, statistic and artist. First of all, we will list all the states transitions for each tuple of 3 characters:

Startup

sta -> tar

tar -> art

art -> rtu

rtu -> tup Statistic

sta -> tat

tat -> ati

ati -> tis

tis -> ist

ist -> sti

sti -> tic Artist

art -> rti

rti -> tis

ist -> ist

Now, if you pay close attention to the states, you will notice that some of them are shared among different tuples. To better visualize that, let's create a dictionary where each entry is a state, and the values are the next states and its weights.

{

""sta"": {

""tar"": 1,

""tat"": 1

},

""tar"": {

""art"": 1

},

""art"": {

""rtu"": 1,

""rti"": 1

},

""rtu"": {

""tup"": 1

},

""tat"": {

""ati"": 1

},

""ati"": {

""tis"": 1

},

""tis"": {

""ist"": 2

},

""ist"": {

""sti"": 1

},

""sti"": {

""tic"": 1

},

""rti"": {

""tis"": 1

}

}

Ta-da! That's our Markov Chain. Simple, isn't it? (PS: Technically, Markov Chains are defined with a transition matrix with probabilities, not weights. We could easily transform into a transition matrix, but for the problem we have in mind, this is a better way of visualizing it).

Now, how can we generate new data with this?

There are basically four steps, let's go through each one of them:

Step 1: Start with some initial random state

You could select any of the states as a starting position, however, you will most likely generate text that doesn't make any sense. For instance, rtu is a valid initial state, but you won't find a word in real life that starts with those letters (none that I can't think of it, at least)

A better approach is to keep track of the starting states in another dictionary, and selecting the first state from there. In our case, the possible initial states are sta(as both startup and statistic start with sta) and art. For our example, let's select sta.

Step 2: Select randomly one of its transition states, considering its weights

For the tuple sta, you can go to tar or tat, both of them with the same probability (same weight). In a real case scenario, they would have different weights considering the distribution found in your dataset, but as we have just used three words, they have equal weights. Let's ""randomly"" select the tuple tar.

Step 3: Append the new state to your generated text

So far, we have started with the state sta and transitioned to the state tar. So our current generated word is star.

Step 4: Repeat step one using the new state, until a stop character is found, or until you are happy with your result

Now, for our current state tar, the only possible state is art, so our generated word become start.

Now let's continue the algorithm in a faster way. From art, we can go to rti or rtu. Let's select rti. If you continue to apply the algorithm, you will quickly generate our new word: Startist, which is a mix of startup and artist.

Even though the example is quite simple, it demonstrates the potential of Markov Chains.

Now that we have ""implemented"" by hand a Markov Chain, let's do in Python using real data where you can get actual useful results.

Let's code!

Let's start by importing some modules. We will only need two: pandas, to read CSV data, and random to (unsurprisingly) generate random numbers.

import pandas as pd

import random

As a dataset for our Startup name generator, we are going to use a 2015 dump from CrunchBase with around 18k companies.

The dataset is not that big, but you will see that Markov Chains work pretty well even with a database much smaller than this.

Reading our companies data is pretty straightforward: pandas read_csv function accepts a URL as a parameter and returns a data frame. We have also removed symbols and transformed the names to lower case.

As we have discussed previously, the simplest way to model the data structure for a Markov Chain is a dictionary containing the states and transitions weights.

chain = build_markov_chain(companies['name'].tolist(), 3)

print(chain['sta'])

If you run the above code, you will get this result:

{

'tar':290,

'tat':151,

'ta.':52,

'ta ':35,

'tac':55,

'tag':43,

'tal':46,

'tay':34,

'tau':22,

'tad':14,

'tam':19,

'tas':19,

'taq':5,

'tan':92,

'tab':23,

'tap':6,

'tak':8,

'tai':22,

'taf':16,

'tax':5,

'ta™':1,

'tah':2,

'tav':5,

'tae':1,

'taj':1,

'taw':1,

'taa':2,

'taz':1

}

What does this mean? These are the list of next states and weights on the Markov Chain, considering that the current state is the tuple sta. The higher the next state weight, more likely its transition to it.

For instance, if you take the first state tar, it has the largest weight on this state list. Intuitively, it makes sense, as it is probably capturing the occurrence of the word startup.

Now we need to build a function that returns a random tuple from the chain considering its weights.

Finally, here is where the magic happens: let's generate some new words.

Let's go step by step in our generate function.

tuple = select_random_item(chain['_initial'])

result = [tuple]

Remember that we mentioned that is better to keep track of the initial tuples and selecting one of those as the initial state? That's exactly what we are doing here.

while True:

tuple = select_random_item(chain[tuple])

last_character = tuple[-1]

if last_character == '.':

break

result.append(last_character)

This is where we are navigating through our Markov Chain, considering its probabilities. We are selecting a random weighted next state and appending the last character of this state to our result string. However, if the last character is a period, we stop our generation, as this is the ending of our chain.

We could add additional rules such as generating words given a minimum or maximum length, but let's keep it simple for now.

generated = ''.join(result)

if generated not in chain['_names']:

return generated

else:

return generate(chain)

Finally, we join all of the generated characters together, and we do the last verification. As nothing prevents the Markov Chain from generating an already existing name, and as we are interested in creating new names, we will simply generate a new one if the generated name is already in our database.

Results

Here is a couple of examples from our Startup Name Generator™®.

Domos

Hup Online

Vubanky

Acara

Ignaly

iFly

Pretty cool, huh? 😎

More ideas

One final idea, what if we generated Startup names specific for each industry? It would be awesome, don't you think?

Let's do it, it will be extremely easy 🙃

The only thing we have to do is to build our Markov Chain considering only examples from the industry we are interested in.

And here are our results:

Travel Startups

print(generate_amount_by_category('Travel',5))

Pango

Movology

Nextrive

Triptel

Stingi

Technology Startups

print(generate_amount_by_category('Technology',5))

Naco Innovation

Kicksense

NetWatch

Chony

Datars

Try yourself

You can try yourself using this Google Colab link or downloading the source code directly from my GitHub.

What are your thoughts? Any suggestion for new content? Feedbacks? Let me know in the comments.

Hope you have enjoyed it :)"
"What’s the deal with Accuracy, Precision, Recall and F1?","It often pops up on lists of common interview questions for data science positions. Explain the difference between precision and recall, explain what an F1 Score is, how important is accuracy to a classification model? It’s easy to get confused and mix these terms up with one another so I thought it’d be a good idea to break each one down and examine why they’re important.

Accuracy

The formula for accuracy is pretty straight forward.

But when dealing with classification problems we are attempting to predict a binary outcome. Is it fraud or not? Will this person default on their loan or not? Etc. So what we care about in addition to this overall ratio is number predictions that were falsely classified positive and falsely classified negative, especially given the context of what we are trying to predict. A 99% accuracy rate might be pretty good if we are trying to predict something like credit card fraud, but what if a false negative represents someone who has a serious virus that is apt to spreading quickly? Or a person who has cancer? That’s why we have to breakdown the accuracy formula even further.

Where TP = True Positive, TN = True Negatives, FP = False Positives and FN = False Negatives.

Precision & Recall

Before getting into precision and recall, a quick note on Type I and Type II errors. These terms that are not unique to classification problems in machine learning, they’re also extremely important when it comes to statistical hypothesis testing.

Type I Error: False positive (rejection of a true null hypothesis)

Type II Error: False negative (non-rejection of a false null hypothesis)

So with that in mind we can define precision as the percentage of relevant results, while recall is characterized as the percentage relevant results that are correctly classified by the model you’re running. Now obviously these definitions aren’t all that intuitive, so let’s take a look at a few visualizations and see if we can wrap our heads around it.

Ok, so this is starting to make a little more sense I think. When it comes to precision we’re talking about the true positives over the true positives plus the false positives. As opposed to recall which is the number of true positives over the true positives and the false negatives. Below are the formulas and as you can see they are not too complicated.

I think the most intuitive visualization for interpreting the performance of a statistical classification model is a confusion matrix. It’s a two by two table in which each row represents an instance in the predicted class while the columns represent the instances of the actual class.

Below is an actual confusion matrix from a project I did on prediction of hard drug use based on surveyed data using several classification models including Logistic Regression, XGBoost and Random Forest. Here is a link to that project on my GitHub.

F1-Score

Finally we have the F1-score, which takes both precision and recall into account to ultimately measure the accuracy of the model. But what’s the difference between this metric and accuracy? Well as we talked about in the beginning, false positives and false negatives can be absolutely crucial to the study, while true negatives are often less import to what ever problem you’re trying to solve especially in a business setting. The F1 score tries to take this into account, giving more weight to false negatives and false positives while not letting large numbers of true negatives influence your score.

Hopefully this blog clears up any confusion you might have had when it comes to these four metrics and you realize that accuracy is not necessarily the end-all be-all of measurement for machine learning classification models. It’s really going to depend on what kind of problems you are trying to solve."
Optimizing pandas.read_sql for Postgres,"Reading SQL queries into Pandas dataframes is a common task, and one that can be very slow. Depending on the database being used, this may be hard to get around, but for those of us using Postgres we can speed this up considerably using the COPY command. However, there are several ways of using the COPY command to get data from SQL into pandas, with different memory/speed tradeoffs. In this article, we’ll test several different methods against each other.

The test dataset is simply the first five million rows of a sample Triage predictions table, which is just one I had handy. I tried to use all thirteen million rows I had in my local Postgres database, but pandas.read_sql crashed so I decided to bring down the dataset to something it could handle as a benchmark.

Included which each method are three statistics:

Peak memory — the highest amount of memory used during the sql read code. This is the important one to see if your program will crash!

Increment memory — the amount of memory that is still used at the end of the sql read code. In theory, this would be the same for all of the methods, but memory leaks can make different methods retain more memory.

Elapsed time — the clock time used by the program.

The pandas version used here is 0.24.1.

First, a quick rundown of the different methods being tested:

pandas.read_sql — the baseline

tempfile — Using the tempfile module to make a temporary file on disk for the COPY results to reside in before the dataframe reads them in

StringIO — Using a StringIO instead of disk; more memory used, but less disk I/O

Compressed BytesIO, pandas decompress — Using a BytesIO instead of a StringIO, and compressing the data; should use less memory, but take longer

Compressed BytesIO, gzip decompress — Same as the other compressed bytesio, but using GzipFile to decompress rather than pandas

Compressed tempfile — Applying the compression idea to the diskfile; should reduce the disk I/O needed

Compressed BytesIO, low compression level — A lower compression level to try and split the difference between uncompressed and compressed methods

pandas.read_sql

This is the baseline. Nothing fancy here.

Peak memory: 3832.7 MiB / Increment memory: 3744.9 MiB / Elapsed time: 35.91s

Using a temporary file

Here is our first attempt of using the COPY command. The data from the COPY command has to go do a filehandle: what simpler way to do this than using a temporary file?

Peak memory: 434.3 MB / Increment memory: 346.6 MB / Elapsed time: 8.93s

That’s…much better. I’m not surprised that the elapsed time is far quicker than read_sql, but I’m a bit surprised that the memory usage is so much different. Anyway, let’s keep going

Using a StringIO

Disk I/O can be expensive, especially depending on what type disk is available. Can we speed it up by using a StringIO for the filehandle? This would take more memory, of course, but maybe that’s a tradeoff we can make.

Peak memory: 434.2 MB / Increment memory: 346.6 MB / Elapsed time: 9.82s

This is a surprising result. I would have expected this to use more memory and be faster, but it’s neither. My hypothesis would be that the peak memory used by the StringIO ends up being surpassed by a spike during the dataframe creation process.

Also of note: the increment memory was the same as the temporary file version, which probably tells us that the 346.6 MB is a good reference for what the baseline for that memory should be without any memory leaks.

Using a Compressed BytesIO, pandas decompression.

Could we bring down on the memory required for the in-memory option? Given the prior results this may seem like a fool’s errand, but I already wrote the code so I’m not going to cut off the test early! Python’s GzipFile interface wraps a filehandle (in this case, a BytesIO) and handles compression. We let pandas handle the decompression by passing `compression=’gzip’` to read_csv

Peak memory: 613.6 MB Increment memory: 525.8 MB, Elapsed time: 1:30m

Not good! It actually used more memory (and leaked some) compared to the uncompressed versions.

Using a Compressed BytesIO, Gzip decompression

Same as the last one, except we bypass pandas’ decompression routines in case they introduced a problem. GzipFile can handle the decompression for us, too!

Peak memory: 504.6 MB Increment memory: 416.8 MB, Elapsed time: 1:42m

Well, this is better RAM-wise than the pandas decompression, for sure, but this is still worse than the uncompressed versions.

Using a Compressed tempfile

The compression idea can also apply to the tempfile method from before. In this case, compression should help us cut down on disk I/O.

Peak memory: 517.2 MB Increment memory: 429.5 MB, Elapsed time: 1:35m

Similar to the other gzip examples. Not a good option.

Using a compressed BytesIO, low compression level

Since we’re trying things out, we have one more avenue to explore: the gzip compression level. The default for all of the prior examples is 9, the highest compression possible. It’s possible that in doing this, it takes extra memory to do the compression in addition to extra time. What if we flip one of them to the lowest compression level (1)?

Peak memory: 761.5 MB Increment memory: 673.8 MB, Elapsed time: 1:13m

Slightly better on time, but worse on RAM: it appears that the gzipping process is using a bunch of memory no matter what and doesn’t stream well.

Conclusion

What did we learn here?"
Neural Networks Training with Approximate Logarithmic Computations,"Fig 1: Hardware Accelerated Learning

Neural Network training is expensive in terms of both computation and memory accesses — around three to five times computationally expensive from inference. Together these two factors contribute significantly to the net power requirements when training a neural network on edge-devices (devices connected to the edge of the internet — wearables, smartphones, self-driving cars, etc). To make real-time training as well as inference possible on such edge devices, computation reduction is of paramount importance. Although a lot of solutions to the problem posed above has been proposed, such as sparsity, pruning and quantization based methods, we propose yet another — design end-to-end training in a logarithmic number system. Note,

for this to work, all significant Neural Network operations need to be defined in LNS.

In LNS, multiplication reduces to addition. But addition itself becomes computationally expensive.

Hence we resort to Approximate Logarithmic Computations with the intuition that back-propagation noise tolerance would be able to absorb the uncertainty of our log-domain operations.

Fig 2: An LNS-DNN-MLP where the neurons have logarithmic activation functions and the activations and weights are in fixed point LNS

The mapping between real numbers and Logarithmic Numbers are given as,

This kind of logarithmic mapping implies that basic operations of the vector space ℝ needs to be modified, these operations being addition and multiplication. In log-domain, multiplication trades it’s computational complexity with addition.

That is, multiplications are eliminated and replaced with additions. Additions, on the other hand, are tricky. The exact form that addition takes is given below,

The delta term induces non-linearity and a lot of extra calculations while performing addition in log-domain.

A graphical representation of △ w.r.t. d shows the non-linearity clearly.

Fig 3: Correction term △

In a similar fashion subtraction can be defined too,

Since a single multiplication is now just an addition, intuitively, we can deduce that exponentiation would be simpler too in log-domain,"
How We Dumped A/B Testing For Machine Learning,"I was the A/B Testing guy in my previous role. We were driving thousands of visitors from paid channels to our website, so naturally, we wanted to ensure that we were getting the most out of our Ad Spend by optimizing our Conversion Rate. Every other week, I would launch new tests and analyze the results of the previous tests.

On one occasion, when I was wrapping up a Header Image test for one of our Diploma website (I was working for a Private University), I decided to do a simple experiment: I asked my colleagues to rank the variants of the header image by their performance — intending to compare my teammates’ guesses with the actual results. The outcome was surprising, to say the least.

Their guesses were way off. It’s not uncommon; A/B tests are meant to elicit differences between what the customers want and what the marketers think customers want. But some of my teammates were graduates of that Diploma Program, making them our target market. Their strong preference for the statistically worst performing header image got me thinking: maybe there’s a fundamental flaw in the design of A/B tests.

Let me illustrate this: say you decide to A/B test your headline.

After splitting the website traffic equally between Variant A and Variant B, you realize that 70% of total conversions are happening on Variant A and 30% of conversions are coming from Variant B.

As per the methodology of A/B tests — you’ll conclude that Variant A shows superior performance compared to Variant B — therefore it should be made the default headline.

That’s where the fault lies: there’s absolutely no assurance that people who converted on Variant B would have also converted on Variant A. Maybe Variant B’s headline resonated with a subset of your target market represented by that 30% of total conversions. If you decide to go with Variant A, you might end up alienating people who prefer Variant B — thereby hurting your overall Conversion Rate.

Because of my AI startup experience, I have some rudimentary knowledge of Tensorflow, and it seemed like a perfect tool for this problem. The goal was to predict the headline a user will most likely convert on before the page renders. The time restriction for gathering data and making a prediction forced me to rely on client-side attributes for the feature list. Using free services, I was able to find the real IP address of the visitors and subsequently the city they were from. Another API call gave me the current weather conditions in their city.

I had to assign a numeric id to the browsers, OS and city names because Machine Learning models use statistics which makes them incapable of digesting words. Due to the sheer number of cities in the world, I restricted the scope of the ML experiment to Canada — even then, there were hundreds of cities to tag, and it became a mini-project on its own.

In the end, we had the following features to train our model:

timezone — browser name — OS name — browser’s default language — cookie enabled — java enabled — screen width — screen height — screen color depth — the day of the week — the hour of the day — city — min forecast temperature of the day — max forecast temperature of the day — wind speed — wind direction — visibility — atmospheric pressure

To come up with the variations of headlines, we surveyed our colleagues on the “front line”: the Admission Advisers. In essence, we weren’t just trying to predict the best headline, we were trying to guess the intent of a prospect, and in my opinion, no one understands it better than your sales team.

The survey showed that most of our students belonged to two broad categories:

Working professionals who want to get ahead in their careers. Youth who want to enjoy the freedom of studying from anywhere.

To appeal to those two categories, we created the following two website headlines:

BBA For The Working Professional Study for your BBA from Anywhere

The two variants

Now, with our feature set and headline variations locked in, it was time to collect data. The plan was to run each headline until we had 100 data points. Since the Landing Pages were built using Vue.js framework, getting the page to collect data and, at a later stage, make real-time predictions was relatively easy.

It took us four weeks to collect the data — we ran each headline for roughly two weeks and gathered attributes of visitors who converted on those headlines. During that time, I coded the pipeline to make predictions and re-train the model. To keep things simple, I used the Softmax Regression Machine Learning model.

When we had 100 data points for each of the headlines, it was time to train the ML model. I have a confession to make: ideally, at this stage, I should have performed something called Feature Engineering, where you root out insignificant features and combine features that show a strong correlation. But in my eagerness to get the solution out the door as soon as possible, I postponed Feature Engineering to a later date.

The first pass at training the model gave us an accuracy of only 52% — still superior to a coin-flip but we had to do way better than that for the ML model to be viable. Tinkering with hyper-parameters bumped up the accuracy to 67% — not optimum, but we were training the model with 160 data points (40 data points for testing), and we couldn’t possibly expect the performance to be any better than this. Over time, with constant retraining, the accuracy was expected to increase.

Adjusting hyper-parameters to improve the accuracy

There was some housekeeping required before we deployed the model. For example, we had to optimize page load speed to compensate for extra milliseconds added by the real-time prediction algorithm. The strategy was to initially roll out the model for 20% of the total sessions and if things go well, keep increasing that percentage every week by 20% until we achieved a distribution where 80% of the traffic was going to the ML powered page, and 20% was going to the regular page. The reason why we wanted 20% of the traffic to always go to the non-ML powered page was to train the model continuously (our explore exploit strategy).

A month into the deployment, we noticed an increase of 4% in the conversion rate. That number doesn’t look exciting, but for a company that spends millions of dollars on PPC ads, that’s something.

Another metric showed considerable improvement which came as a complete surprise: the time spent by visitors on the page went up by 25%. Since Google’s algorithm rewards websites for user engagement, increase in session duration caused our Cost Per Click to go down by a considerable amount — making our leads cheaper."
Fast and easy way to dynamically import Open Street Map data into a Google Sheet file,"Fast and easy way to dynamically import Open Street Map data into a Google Sheet file Nicola Simboli · Follow Published in Towards Data Science · 5 min read · Oct 14, 2019 -- 3 Listen Share

Problem overview

You work as a business analyst at the company that manages metro underground trains transport in a large Italian city and you are required to provide the marketing&communication department a list of the bars and the cafés in the area, to distribute them maps that can be affixed to the windows or inside the rooms. In addition, the closest bars and cafés to the stations will be contacted by the sales department to propose to join and ticket sales program, for the transport service that your company provides.

This is a periodic request that occurs on a monthly basis.

Data sources

One of the primary data sources for this kind of geography-related problems is Open Street Map.

There are several ways to access the Open Street Map database, but one of the most popular is using the Overpass APIs. In this article, we will see how to use these APIs to generate a dynamic list in Google Sheet, generated by a parameterized query and with values set by the final business user. In this way, other colleagues can get the data on their own and can copy, edit or save the list for future manipulations, for example assigning each bar or cafe to one key account manager that will contacts the structure.

Queries via Overpass API can be made using two languages: Overpass XML or Overpass QL.

This time we will use Overpass QL, an imperative programming language written with a C style syntax. The main “getting started” guide to learn this language is available on this page.

Data extraction

Let’s summarize what we need to extract:

text output, we’ll use the standard CSV then with the header on the first line and with comma as column separator , Google Sheet will appreciate this choice!

, Google Sheet will appreciate this choice! list of bars and cafés in the indicated area;

the premises must be within a radius of X meters from a subway station entrance.

The detail of the attributes to be extracted is very varied and not all the information for each premises is fully compiled, compared to other countries (first and foremost Germany) there could be a good number of detail fields that are still incomplete (for example the telephone number or opening hours).

The timeout will be fixed in 20 seconds. The business user will not be able to easily change this value. It is not appropriate to increase it, as the data we want to extract are not big and the following rule applies:

“If the query runs longer than this time, the server may abort the query with a timeout. The second effect is, this higher value, the server probably rejects the query before executing it. “

Let’s set up the query!

As anticipated, we want to extract the list of bars and cafés in a particular area. These elements are classified within the OSM database with the following tags:

amenity = bar

amenity = cafe

A good rule to follow, when we have to work with Overpass APIs, is to consult the TagInfo pages related to the tags we want to extract:

Here we find various useful information for our purposes. In the “combination” tab we find the list of features concerning that tag, which is a good starting point to make the list of columns we want to extract and which we will specify a little later in our project.

In the “overview” tab, instead, we see how this place is coded in the database. In this case, we can easily see that bars and cafés can be nodes, way or relations. For our purposes, this information is fundamental, either because in the query we will specify to search in all these entities and because we know that the information can be stored as a point (nodes) or a wider area (way). In this project we obviously want to extract only the information related to that bar/cafés once, so the latitude and longitudes coordinates we want to get are those of the center of the area bounded by the lines.

Let’s take an example, in the case of a square bar there will be four way elements inside the OSM database, but we want that in the sheet all those are represented in a single line, considering that the bar is always the same (same name, same telephone number, …). This purpose will be achieved by indicating to Overpass to extract only the center of the area.

The last element to consider in the query is the area we want to extract. We make the choice to extract the data using the boundaries of each municipality, which will be indicated in the input cells filled by the user of the sheet, bearing in mind that some metro stations are located in different Municipalities.

From this useful table, we can learn that the administrative boundaries of Italian municipalities are indicated in the OSM database with level 8.

At this point we have all the elements to build our query:

We considered the query with fixed distance values (50 meters) between the bars/cafés and the entrances of the stations and also a fixed municipality (ISTAT code number 015146 corresponds to the city of Milan) so we have to modify the query to make it usable with Google sheet.

How to use the query into a spreadsheet

We need to follow these steps:

Include the query within the IMPORTDATA formula (read the Google Sheet official guide);

(read the Google Sheet official guide); Replace the single single-double quotation marks with nested-double quotation marks;

Replace the distance value with “&B5&”;

Replace the value of the ISTAT code of the municipality with “&B4&”.

We finally came to the following:

With a data validation linked to a hidden sheet and a VLOOKUP formula to get the ISTAT code, we can make a user-friendly drop-down menu from which the user can choose the name of the municipality and digit the value of the distance to consider.

Each time cell B3 or cell B4 changes, values of list of premises is updated."
Analogies from Word Vectors?,"Another common example of analogies is the derivation of capitals. Let’s see what are the candidates for the capital of Austria:

Candidates for Austria-Germany+Berlin from German Wikipedia corpus

This time we have a hit, indeed Vienna is to Austria as Berlin is to Germany. The list of other candidates also makes sense, we find five capitals of Austrian federal states (Graz, Klagenfurt, Linz, Innsbruck, Salzburg) a capital of a Polish province (Wrocław) … and two cities from the German federal state of Saxony that are in competition over who is the true capital (Dresden, Leipzig).

Hooray? Does this result extend? Let’s write a few lines of Python to test and visualize the analogy exercise for all European countries — minus the city states and Kazakhstan, where the corpus doesn’t contain enough data for its captial to show up in the model:

Test all pairs of countries and capitals for correctly guessed analogy and visualize the result

After some calculations we get 57.25% hits and 42.25% misses, quite worse than actually hoped for:

Result matrix of the country-capital guessing exercise

There seems no pattern behind a country having a good or bad to guess capital. So let’s see the raw frequencies, a complete list is linked below.

Selected countries that have many failed guesses: UK (2,482), Czech Republic (679), Italy (91,024) , Iceland (37,591), Moldova (2,585). In comparison some countries with many correct guesses: Albany (7,067), Denmark (34,079), Ireland (23,024), Portugal (29,124), Romania (22,700). UK and Czech Republic could be somehow explained by the fact that their official names are used rather infrequently and that this maybe doesn’t happen in the same contexts as mentions of their capitals. Also Moldova could suffer from an ambiguity between the name of the country and a river. But this doesn’t explain Italy or Iceland.

Also let’s do the same for cities with many failed guesses: London (114,921), Prague (34,565), Rome (70,952), Rejkyavik (2,890), Chișinău (851) comparted to the cities with a many correct guesses: Tirana (2,240), Copenhaven (18,092), Dublin (9630), Lisboa (11,196), Bucarest (8,350). Again we have no clear case and this time there is also no backing from additional knowledge. Can anyone explain this?"
Introducing the “Banana Test” for Text Classification Models,"Introducing the “Banana Test” for Text Classification Models

Introduction

There are millions of instances in which businesses have collected free-form text in their systems. In order to automate business processes that utilize this data, the free-form text often needs to be bucketed into higher-level categories. Text classification models are capable of classifying such free-form text quickly and effectively… for the most part.

Regardless of the reported validation/test accuracy, there are a number of gotchas that can cause even the most well-trained text classification model to fail miserably at making an accurate classification. In particular, text classification models tend to fail when faced with text they have never seen seen before.

If your goal is maximizing accuracy for a new text classification model, you should consider using the Banana Test. Despite the silly name, it’s a very real technique that you can use to improve the quality of your models.

What is the Banana Test?

The Banana Test is intended to be applied as a deciding factor (usually in combination with the confidence interval) for whether a given classification can be trusted. Since a machine learning model will always classify data by default and the confidence interval it provides is little more than a measure of probability (rather than accuracy), the Banana Test helps ensure that a model only classifies new data when it has adequate training data to make a confident decision. How it works and why it’s important are both very simple concepts.

To explain the name of the test, the word “banana” has never been relevant to any company I’ve worked for. Due to my prior experience as a product owner before branching into data science, I came to believe that defining proper acceptance criteria is essential to developing a good product. So, as a form of acceptance criteria for any new text classification model being built, I enforce that my models should never confidently classify a text string that has the word “banana” in it."
Data Science for Startups: Containers,"Data Science for Startups: Containers

One of the skills that is becoming more in demand for data scientists is the ability to reproduce analyses. Having code and scripts that only work on your machine is no longer sustainable. You need to be able to share your work and have other teams be able to repeat your results. Some of the biggest impacts that I’ve seen in data science organizations is when other teams repurpose old code for new use cases. This blog post is about encouraging the reuse of analyses via containers, which means that your work can transfer.

The idea of a container is that it is an isolated environment in which you can set up the dependencies that you need in order to perform a task. The task can be performing ETL work, monitoring data quality, standing up APIs, or hosting interactive web applications. The goal of a container framework is to provide isolation between instances with a lightweight footprint. Containers are an alternative to virtual machines, which are a great solution to isolation, but require substantial overhead. With a container framework, you specify the dependencies that your code needs, and let the framework handle the legwork of managing different execution environments. Docker is the defacto standard for containers, and there is substantial tooling built around Docker.

In general, using Docker is going to take more work for a data scientist, versus a local deployment. However, there are several benefits to Docker:

Reproducible Research: If you can deliver your analysis as a container, then other data scientists can rerun your work.

If you can deliver your analysis as a container, then other data scientists can rerun your work. Explicit Dependencies: In order to set up your script as a container, you need to understand the dependencies of your code and any additional libraries that may be needed, and their versions.

In order to set up your script as a container, you need to understand the dependencies of your code and any additional libraries that may be needed, and their versions. Improved Engineering Collaboration: If you want to scale up a model you’ve built, providing a dockerfile to your engineering team is going to go much further than handing off an R or Python script. It also calls out the dependencies that the code needs in order to execute.

If you want to scale up a model you’ve built, providing a dockerfile to your engineering team is going to go much further than handing off an R or Python script. It also calls out the dependencies that the code needs in order to execute. Broader Skill Set: Being able to stand up infrastructure as code is a valuable skill set, and using Docker containers can help data scientists to start developing this skill set.

The ideal state for an effective data science organization is that any member of the team can reproduce prior research. As a former academic, I’d like to take this recommendation further and encourage all submissions to arXiv to include reproducible environments. It’d be great to establish a standard framework for research, and as a proof-of-concept I ported one of my prior research papers to a container environment:

There’s a lot of ecosystems that have been built around container environments, such as Kubernetes and Elastic Container Service (ECS). Instead of focusing on scale, as provided by these environments, we’ll focus on taking an existing script and wrapping it in a container.

All of the code that is used in this post is available on github. When working with Docker, I encourage hosting all files in source control in order to ensure that your container can deploy to new environments. In this post, I’ll cover Docker installation, wrapping a simple web app in Docker, and then hosting a deep learning model as a Docker container.

Installing Docker

The first step in using Docker is to set up Docker on a machine where you want to build and test images. For this post, I spun up an EC2 instance with the new AWS AMI (Setup Instructions). You can install and verify a Docker installation with the commands shown below:

# python 3

sudo yum install -y python3-pip python3 python3-setuptools # docker install

sudo yum update -y

sudo amazon-linux-extras install docker

sudo service docker start # test docker setup

sudo docker ps

For AWS, additional details are available here if you’re using a different instance type. For all other environments, see the docker instructions. After running these steps, you can check which containers are running by running the following command: sudo docker ps .

An empty Docker Install

While there are not any active containers yet, this output indicates the Docker is up and running on your instance. We’re now ready to start hosting web apps and Python scripts as Docker containers!

An Echo Service

One of the most common tools for standing up web services in Python is Flask. To start, we’ll stand up a simple echo web service, in which a passed in message is returned to the caller. This is a relatively simple environment. We need to install Python 3, which we already did when installing Docker, and then install Flask as shown below:

pip3 install --user Flask

Now we can write a Flask app to implement this echo service, where a param passed to the service is echoed to the terminal:

This is a simple web app that will return a payload with the msg param echoed to the web response. Since we are using Flask, we can deploy the application with a single command:

python3 echo.py

The result is that we can post messages to the service:

# web call

http://ec2-3-88-9-61.compute-1.amazonaws.com:5000/predict?msg=HelloWorld # result

{""response"":""HelloWorld"",""success"":true}

The majority of the work we’ve done so far is around setting up AWS to allow incoming connections, and installing Python 3 on an EC2 instance. Now we can focus on the containerization of services.

Echo Service as a Container

Since we got the echo service to work on a fresh EC2 instance, we’ve already gone through some of the process of setting up a reproducible environment. We needed to set up Python 3 and Flask before we could execute our simple service. With Docker, we need to do the same process, but in an automated way. To specify how to construct an environment with Docker, you need to create a Dockerfile object in your project, which enumerates the details on setting up your environment. A dockerfile that reproduces the echo service app is shown below, and is on github:

FROM ubuntu:latest

MAINTAINER Ben Weber RUN apt-get update \

&& apt-get install -y python3-pip python3-dev \

&& cd /usr/local/bin \

&& ln -s /usr/bin/python3 python \

&& pip3 install flask COPY echo.py echo.py ENTRYPOINT [""python3"",""echo.py""]

This Dockerfile provides a few entries:

From: Lists a base container to build upon.

Lists a base container to build upon. Run: Specifies commands to run when building the container.

Specifies commands to run when building the container. Copy: Tell Docker to copy files from the EC2 instance to the container.

Tell Docker to copy files from the EC2 instance to the container. Entrypoint: specifies the script to run when the container is instantiated.

We’ll start with an Ubuntu environment, setup Python 3, copy our script into the container, and then launch the script when instantiating the container. I tested out this container using the following script:

# install git

sudo yum -y install git # Clone the repo and build the docker image

git clone

cd StartupDataScience/containers/echo/

sudo docker image build -t ""echo_service"" . git clone https://github.com/bgweber/StartupDataScience cd StartupDataScience/containers/echo/sudo docker image build -t ""echo_service"" . # list the docker images

sudo docker images

I installed git on the EC2 instance, cloned the code from my repo to the local machine, and then built the container. Running the ps command resulted in the following command line output:

Docker Images

We now have a container that we can run! To run it, we need to specify the image name and a port mapping that identifies container port (5000) and external port (80):

sudo docker run -d -p 80:5000 echo_service

sudo docker ps

More details on exposing EC2 ports are available here. When I ran the commands above, I got the following output:

This output indicates that the echo service is now running as a container and is exposed as an endpoint to the web. The result is exactly the same as before, but instead of the port being exposed as a Flask app, the port is exposed as a mapped port to a Docker instance running a Flask app.

# web call

http://ec2-18-204-206-75.compute-1.amazonaws.com/predict?msg=Hi_from_docker # result

{""response"":""Hi_from_docker"",""success"":true}

Functionally, the API call is similar between the initial and dockerized set ups. The key difference is that the dockerized setup uses container-scoped python libraries, while the direct flask setup relies on the system-scoped python libraries. It’s trivial to stand up this service on a new instance with the container approach, but may be non-trivial to reproduce on a new machine if not using Docker.

Hosting a Complex Model

The power of Docker is more apparent when using complicated libraries. In this section, we’ll train a Keras model locally, and then deploy it as a container. To train the model locally, we need to install a few libraries:

# Deep Learning setup

pip3 install --user tensorflow

pip3 install --user keras

pip3 install --user pandas

Next, we’ll train the model by running a Python script locally. The output of this script is an h5 model that we want to host as an endpoint. More details about the training code are available here.

Since we’ve installed the necessary libraries on our host EC2 instance, we can build the model file with the following command:

python3 train_model.py

The result is a games.h5 model that we want to include in our container for predictions. While we could wrap this step into our Docker setup, it’s easier to separate these steps when first setting up a Docker workflow.

Now that we have a model specification, we can host a deep learning model as a Flask app, managed as a Docker container. The code below shows how to set up a Flask app to service this model, and is unmodified from you prior post on hosting deep learning models with Flask:

The next step is to specify a Dockerfile that takes in the code and model when building a container. The script below shows that we’ve added a few more libraries, and also copied a model file from the local machine to the docker image, which means that it can be used when serving predictions:

FROM ubuntu:latest

MAINTAINER Ben Weber RUN apt-get update \

&& apt-get install -y python3-pip python3-dev \

&& cd /usr/local/bin \

&& ln -s /usr/bin/python3 python \

&& pip3 install tensorflow \

&& pip3 install keras \

&& pip3 install pandas \

&& pip3 install flask COPY games.h5 games.h5

COPY keras_app.py keras_app.py ENTRYPOINT [""python3"",""keras_app.py""]

The command line instructions below show how to turn this Dockerfile into a container that we can use to host a deep learning model:

# Clone the repo and build the docker image

git clone

cd StartupDataScience/containers/model/

sudo docker image build -t ""model_service"" . git clone https://github.com/bgweber/StartupDataScience cd StartupDataScience/containers/model/sudo docker image build -t ""model_service"" . # Expose a model endpoint

sudo docker run -d -p 80:5000 model_service

The result of running this container is that we now have a deep learning model exposed as a Flask endpoint that we can pass parameters to in order to get a prediction. The code block below shows how I tested this interface in order to get a prediction result.

# web call

http://ec2-18-204-206-75.compute-1.amazonaws.com/predict?g1=1&g2=0&g3=0&g4=0&g5=0&g6=0&g7=0&g8=0&g9=0&g10=0 # result

{""prediction"":""2.160104e-16"",""success"":true}

The result of all of this work was that we wrapped a Keras model in a Docker container, but maintained the Flask interface to expose the model as an endpoint on the web. The key difference from my initial post on Flask is that the model is now defined within a container scoped environment, rather than an EC2-scoped environment, and it’s trivial to set up this model on a new machine. In addition to designing models that can work in containers, eagerly targeting Docker and cloud tooling means that data scientist projects are easier to share used across an organization.

Conclusion

Data scientists should be able to author model and data workflows that extend beyond their local workspaces. Container environments such as Docker are one way of achieving this goal, and becoming familiar with these types of tools helps build your portfolio with skills such as specifying infrastructure as code. This post showed how to stand up a Keras model as a webpoint using Docker, but was only a glimpse into the capabilities of reproducible research enabled by these tools."
Backpropagation super simplified!,"I won’t say that backpropagation is a very simple algorithm. If you don’t know calculus, linear algebra, matrix multiplication, it could be very daunting. Even if you know some or all of it, it really needs a bit of mental exercise to get ahold of it.

By saying that, I don't mean to discourage you and have you avoid learning it (yes, you can avoid it and still continue your deep learning journey). It is a bit complex, but I won’t say it is super tough, rather it is very intuitive and easy to get hold of. You will be amazed to know, how easy it is as compared to the kind of problem it solves. It is literally the backbone of the deep neural networks. The concepts required are super easy to learn and Khan Academy is a great source for the purpose. I have listed the URLs for the required mathematical concepts, which you can review before reading the post.

1. Chain Rule

2.Gradient Descent

3.Matrices

Enough of backdrop on backpropagation algorithm, let’s get going now. When I first started learning the backpropagation algorithm, I found the representation of nodes and the weights very confusing rather than the algorithm itself. So, I would try to make it as simple as possible. Let’s start with a very simple neural network.

Simple Neural Network"
Introduction to Mesa: Agent-based Modeling in Python,"Python-based alternative to NetLogo, Repast, or MASON for agent-based modeling

Simulation result showing segregation between blue and red agent

Agent-based modeling relies on simulating the actions and interactions of autonomous agents to evaluate their effects on the system. It is often used to predict the projections that we will obtain given a complex phenomena. The main purpose is to obtain explanatory insight on how the agents will behave given a particular set of rules. Agent-based modeling has been extensively used in numerous industry such as biology, social sciences, network and business. This article covers the necessary steps to kick-start your agent-based modeling project using an open-source python module called Mesa. There are 4 sections in this tutorial:

Setup Schelling Segregation Model Visualization Conclusion

1. Setup

Setup is pretty straightforward for Mesa. Make sure to create a new virtual environment. I name the environment as mesaenv. Open up your terminal and change the directory to mesaenv and activate the virtual environment using the following code:

Virtual Environment

Run the following command to activate the virtual environment depending on your use case.

#Anaconda

conda activate mesaenv #Terminal

source bin/activate

Python modules

This tutorial requires three modules:

mesa

matplotlib

jupyter

python3 -m pip install mesa

python3 -m pip install matplotlib

python3 -m pip install jupyter

Base folder

Create a base folder called Mesa that you will use to store all the python files. You should have the following files in the base folder at the end of this sections:

Feel free to download it in case you got lost somewhere in the tutorial. Once you are done, let’s proceed to the next section.

2. Schelling Segregation Model"
Object Detection in the City,"Object Detection in the City

Ta Prohm. Siem Reap, Cambodia. Photo by me.

What’s better than a machine learning model? A deployed one. A model that’s actively producing something. The opposite of it is what I like to call “PowerPoint AI,” models that only live inside a presentation made of 57 slides. This kind of black boxes, as impressive and shiny as they sound, haven’t had the chance to experience what’s out there in the wild. They haven’t tasted good, and real data; they go straight from the text editor, to the repository. Poor them :(. Since I believe that a free model, is a happy one, I decided to load of them on my phone and take it for a walk in the historic and breathtaking city of Siem Reap, Cambodia. In this article, I’ll summarize my findings, explain the overall results, and of course, present a video and several gifs from my city tour. But before we get there, I’ll quickly go through the particulars of the system.

The model in question is a pre-trained object detector — a system that identifies and localize multiple objects in an image– originally trained using TensorFlow Object Detection API, and it runs on an Android app whose code I took (and modified a bit) from the official TensorFlow’s examples repository. To be more specific, I’m using an SSD MobileNet V1 model trained on the COCO dataset…wait, what? What does this even mean? Allow me to explain. SSD, which stands for Single Shot Detector, is the system’s architecture, and it consists of a single neural network that predicts the image’s objects and their position during the same shot. And while this makes the model a speedy one, its accuracy performance is not as good as that of other models.

Not this kind of coco. Photo by Nathalie Seti on Unsplash.

Then, there’s MobileNets, a class of efficient and light models suitable for object detection applications deployed on mobile devices (hence the name), and other low-resource platforms like embedded systems (think of tiny and cheap computers). Within this architecture, MobileNet plays the role of the feature extractor, which is the part of the system that extracts the details from the regions that might contain an object.

Lastly, there’s COCO (Common Objects in Context), which besides being “coconut” in Spanish, it’s the name of the dataset used to train the model. As its names indicate, COCO consists of 80 everyday objects one usually sees or finds around. For example, a person, airplane, apple, bottle, cow, and so on. Nevertheless, as impressive as 80 objects sound in the video I’m about to present, you will mostly see the same ones (after all, we don’t typically see cows and snowboards in every corner). Meaning that either the video had many things not present in the dataset, or it couldn’t detect most of them.

Below you’ll find the video. It’s a bit long, so feel free to skim around it. I’ll see you in a minute.

What did you think? How was the music? Let’s take a look at some of the most interesting cases.

In this screenshot from one of the first frames, we can see a stack of fruits where only two apples are detected. The sad part is that COCO contains orange and a banana label, and while I accept missing or mislabeling oranges, the same can’t be said about bananas. I mean, come on! It’s a banana!

Sadly, there are no bananas here.

I like this one. Even though there are around 7 million bikes on this scene, the model can handle most of them, and with a reasonable and quite acceptable confidence score (most of them are in the high ’60s and ’70s percent) too.

Vroom vroom.

This scene is another of my favorites. Here we can see the model doing what it knows best from a moving car, an awful image quality, and a very unstable video. (Remember my earlier remark about how fast the model? This gif proves it).

I should get a stabilizer.

This one is funny. For starters, these are masseur fishes that eat your dead skin, but only if you dare to put your legs inside the tank. But you know what’s even funnier? That these aren’t fishes. No, according to the magic of AI, some of them are carrots. What a miracle.

Fishes that eat your dead skin.

lol.

These are not umbrellas. Never. The curious thing here is that for about one or two frames (hard to see it in the gif), one of the bags is labeled as “handbag,” (which is correct) but as I approach them, they turned into umbrellas and even ties.

They can be used as umbrellas, though.

Most of the predictions we’ve seen here have the same labels: person, motorcycles, carrots, umbrellas, and so on. But this is not the whole story. Since I knew I was going to get curious about the specific detected objects, I modified the Android program and added a small functionality that logs and write to a file all the detected objects, and their count. To keep this post brief and simple I won’t explain the process, but I’ll link the repo with the modified code, and leave the explanation for another article. Now, with the files at hand, it was just a matter of reading them from R, performing a simple group by and find out the final tally.

In total, across all my footage, the model performed 3698 detections and found 29 unique objects. Of these predictions, 1611 (43.6%) of them were “car” (I didn’t show much of them, though), 1309 (35.4%) “person,” 327 (8.84%) “potted plant,” and 180 (4.87%) “motorcycle.” On the lower end, there was 1 (0.027%) “bottle,” “bowl,” “keyboard,” “kite,” and “oven.” The following two graphs present this information."
13 Essential Newsletters for Data Scientists: Remastered,"If you’re anything like me, you subscribe to newsletters. Lots of newsletters. Like, too many newsletters. I can’t help myself. When done right, newsletters are an excellent form of content curation that can help deliver interesting and insightful information to you with minimal effort on your part. Lucky for us, data newsletters are no exception to this rule.

In this post, I’ll outline why I think newsletters are such an awesome tool for continued growth in data science. More importantly, I’ll lay out the data science and AI-focused newsletters that I look forward to most throughout the week.

It’s also worth noting that this post has been remastered from a previous post. More concretely, I’ve added more newsletters, elaborated on each one with a small blurb, and cleaned up some structural things.

Why Newsletters?

Looking back on when I started out in data science, one of the first and most valuable things that I did was seek out tons of data newsletters. This forced me to stay on top of the latest news in the ever-changing realm of data science.

It helped me learn techniques and technologies, introduced me to new concepts and learning resources that I wouldn’t have known about otherwise and notified me when it came to networking opportunities, public tech talks, and job openings. Most importantly, it did this on a weekly basis.

Consistently delivered content curation is invaluable in our current world of clickbait titles and fake news. The amount of information out there is overwhelming. Newsletters help solve this problem by differentiating between signal and noise for you. Why take the time to weed through thousands of posts and find the best ones when others will gladly do it for you?

The following collection of newsletters is a perfect example of this. Ranging from data science to machine learning to artificial intelligence, the links delivered by these content curators can serve as a game-changer when it comes to your continued growth as a data scientist."
To dance or not to dance? — The Machine Learning approach.,"I love dancing! There, I said it. Even though I may not want to dance all the time, I do find myself often scrolling through my playlists in search of my most danceable songs. And here’s the thing, it has nothing to do with genres — at least not for me. But it has everything to do with the music.

How it all started

I had a question: Can I predict whether or not I can dance to a song based on the song’s attributes? So, I set out to find some answers — but before I share my journey with you, let’s discuss some key concepts that will come out throughout the project.

Danceability describes the degree to which a song is good for dancing based on its audio features, e.g. rhythm, tempo, valence. I chose danceability because it is a direct product of the audio features of a song, and therefore an accurate representation of these features — and since I love dancing, I thought it would be a great option to create playlists and make recommendations.

Many songs already have a danceability index associated with them — and we are going to use this data to predict the danceability level of songs that have not been previously classified.

Audio features are features used to characterize audio signals. In this case, these features are key, mode, time signature, acousticness, energy, instrumentalness, liveness, loudness, speechiness, valence, and tempo.

And so the journey begins…

Enter Machine Learning

Predictive modeling, as discussed by Jason Brownlee from Machine Learning Mastery, refers to the mathematical problem of approximating a mapping function (f) from input variables (X) to output variables (y). This is called the problem of function approximation. In other words, we are using historical data to make predictions about new data.

Generally speaking, we could divide most function approximation problems into two major classes: regression and classification. Since I wanted to practice classification algorithms, I decided to set it up as a classification problem."
3 Reasons Why I’m Ditching SSIS for Python,"Photo by Chris Ried on Unsplash

I’ve been using the Microsoft SQL Server technology stack for more than a decade, and while I continue to be extremely bullish about it, I’ve lately changed my tune on a key component of it, namely SQL Server Integration Services, or SSIS for short. SSIS is a very powerful tool to perform extract, transform, and load (ETL) workflows on data, and can interact with pretty much any format out there. And while I’ve mostly seen it used in the context of loading data into or out of SQL Server, that certainly isn’t its only use.

I’ve authored more than my share of SSIS packages over the years, and while I still feel it’s a tremendous tool to have in your arsenal (and one that in many cases may be the only one available in large enterprises with strict standards around technology usage), I’ve now decided that for reasons I’ll outline below, I’d prefer using Python for most, if not all, ETL needs. This is especially true when combining Python with two modules specifically made for manipulating and analyzing data at scale, namely Dask and Pandas.

Python is free and open source

Python is a completely open source language, and is maintained by the Python Software Foundation. It, and a huge number of its packages, are available completely free of charge, and you can easily contribute to the underlying source code if you see a bug or need a feature. For example, Dask and Pandas combined have had over 25,000 commits and 9,000 forks on GitHub. Both are very active projects and have large, distributed, and active communities behind them. In addition, Python can talk to pretty much any data source using other open source packages; from CSV files, to Kafka, to scraping web sites. Python as a whole is hugely popular and growing, jumping from seventh to fourth place in Stack Overflow’s 2019 Developer Survey.

SSIS, on the other hand, requires you to license any machine running it just as you would any other machine running a full instance of SQL Server. So, if you want to follow good practice and offload your ETL processing on to a machine different from your SQL Server instance, you’ll have to pay fully to license that machine. Assuming you can successfully navigate the rather complex licensing for SQL Server, think about this: many ETL workloads are batch operations, meaning…"
Getting started with Bitcoin data on Kaggle with Python and BigQuery,"What is Blockchain

Getting started with Bitcoin data on Kaggle with Python and BigQuery

Photo by André François McKenzie on Unsplash

In my previous post, I examined the usability of Python on studying BigQuery data on Kaggle. There, I concluded that, as for now, we could not avoid using BigQuery more extensively than I initially assumed.

(This post is part of a series about analyzing BigQuery blockchain data with Python. For an overview of the project and the posts, see this link.)

In this post, I show a simple and straightforward way to run a query of the BigQuery Bitcoin dataset on Kaggle with the help of pandas and Google’s bigquery Python module. You can see and fork the notebook based on which I wrote this post, and you can read more about the dataset on its BigQuery page.

While there are other ways to access Bitcoin data, Google’s BigQuery dataset collection is the most exhaustive and up-to-date data source on Bitcoin, and it also incorporates data on further cryptocurrencies. On the other hand, using BigQuery through the Google Cloud Platform is an entirely separate set of skills that you need to master to start the analysis. Compared to this, using Kaggle can give you a quick headstart, although — as I pointed out in that previous post — you do need to know some BigQuery SQL. Furthermore, through Google Cloud, in the free plan, you can process only 1TB of data a month, while using Kaggle’s license provides you 5TBs of processing power.

Outline

The BigQuery Client object The general structure of BigQuery datasets and tables An example query Running the query and generating a dataframe Plots

First, I will introduce the BigQuery Client object, which we will use to access the data. With the help of this object, we will get some insights into the high-level structure of BigQuery datasets. Following that, we will write a simple query to access the Bitcoin data.

Because the Bitcoin dataset is vast and because using BigQuery can be very costly (both in terms of memory usage and time), we will also assess the size of the query and the resulting dataframe. While it is not our main focus here, in the end, we will generate a few plots with pandas.

🔧 The BigQuery ‘client’ object

Here we use Google’s official Python library and specifically its bigquery module. After importing the library, we initialize a Client object, which will be our primary tool to access and handle the data.

Because we will often examine the objects we create and use, we create a helper function to simplify this process.

We try it out on our client object. This gives us a sense of what kind of other objects it works with (e.g. 'table', 'database', 'schema', 'model', 'rows', 'job') and what are the operations it supposed to execute (e.g. 'list', 'get', 'update', 'delete').

(If you are interested in the details of the client object, you can read more about it in its documentation or its API description)

📩 Accessing BigQuery datasets and tables

To understand the following steps, it helps if we start with an overview image.

(The image is from Kaggle’s tutorial. It uses the example of the Hackernews dataset, but its logic also applies here).

The structure is relatively straightforward. A client can have multiple projects which have its datasets consisting of various tables. We have already created a client object, so, in this section, we continue with the following steps:

Define the project and the dataset Get the table names Define a table reference Get the table schema Examine the first few lines

While demonstrating these steps, we will frequently examine the attributes of the objects we created. I hope this will make this process more understandable and reproducible.

Besides merely accessing the data, whenever we want to do something else, or whenever we need some information beforehand, we need to be able to navigate ourselves around among datasets and tables. The following steps show us the fundamentals way to do this.

Getting information about the data

One of the primary use of the client object is to get information about the data objects we try to access. For example, looking for the term 'list', our function returns the following attributes.

In line with our intentions, we will use list_tables and list_rows .

(We could also try to list datasets, but the Kaggle license does not allow this). We can find the dataset name listed on its page)

First, we want to list tables. Before any of this, however, we need to define a project through which we access a BigQuery dataset. Because we work through Kaggle’s license, we use the ‘bigquery-public-data’ project. The name of the dataset is ‘crypto_bitcoin’.

We pass these parameters to the list_tables method, which, in turn, returns an iterator.

To have a better look, we list the attributes of the first item it returns.

This list shows the attributes of the table objects in the dataset. Based on it, we will use the table_id attribute, which will return the name of the tables in the dataset.

Let’s examine the transactions table. We can do this, again, through the Client object. Here we list the table-related client attributes and methods.

From among these, we use the get_table method. Doing this will create a reference to the table which we can access to examine its internals.

First, we examine the table’s schema."
A Brief Primer on Data Science and What It Can Do for Your Business,"A Brief Primer on Data Science and What It Can Do for Your Business Dheeraj Nallagatla · Follow Published in Towards Data Science · 5 min read · Apr 22, 2019 -- Listen Share

With all of the data available on the internet, a new area of study emerged called data science, which is a spin-off of old business analytics but with a twist of machine learning. It’s not a new concept, but having a data science team that works with developers wasn’t common a decade ago. Now, a data science team is a beneficial part of a group of developers. It’s an expensive addition to your development team, so here is a brief explanation of the data science process and what data models can do for you.

What is Data Science?

A simple definition of data science is that it’s the study of analyzing information and predicting outcomes. The predictions are mainly made using machine learning, but just one model can take months of data extraction, cleanup, coding, and deployment.

Data science requires much larger reservoirs of data than a standard application using basic algorithms. You can’t use a few dozen stored records to analyze data accurately. You need millions of records to build and test a model. The first step for a data scientist to work with any organization is to gather and clean data. You’ve probably heard of “big data” and may even use the technology in your current applications. Big data is unstructured, but it’s perfect for data science.

Unstructured data technologies grab as many records as possible and store them in a database such as MongoDB. This data can be anything, but just as an example consider a website and each of its pages. A crawler finds pages on a website and stores its text, images, and links in an unstructured record. You can scrape an entire site and get its data without worrying about structuring the data as you crawl as long as you use a database that supports unstructured formats.

The next step is to clean the data, which is probably the most tedious part of data science. Most scientists clean the data and load it into a CSV file, which is a comma-delimited list of values. These files are easy to import either into another database or code, and any operating system supports them.

After collecting the data, it’s time to figure out its functions. The data scientist first analyzes the data he has and asks a question. For instance, maybe you want to know what products are more likely to attract customers. You could take data from your e-commerce store and use previous customer orders to determine which products are most popular and which ones could be popular during the holidays to improve your sales and focus marketing efforts. Data science models could answer this question for you and make predictions using machine learning to contribute to improving your sales.

Building a Model

After the data scientist and the business determine the question to be answered, it’s time to build a model. A model is a unit of code that represents the “answer” to the question. The answer is usually represented in a graph to make it easier for the public to consume and understand the information. The visuals are typically a part of a library imported into the project, but the data scientist must ensure that the analysis that transforms data to a graph is accurate.

One of two main programming languages often is chosen to create the models. R is the language of math and statistics, so is the likely choice if your scientists have a mathematics educational background. More people understand Python, which is suitable for other development projects and is more popular among data scientists. Colleges teach Python, and because of its wide use within programming circles, you might find it easier to implement with a smaller learning curve.

The data scientist creates the model with the question in mind. Using the e-commerce example, here’s how it works: The data scientist would review the data and set it up as rows and columns to import into Python code, which then calculates and displays it as a graph. The graph can be any number of plots, charts, and even visualization tools such as Excel or PowerPoint. The visual output is used to present information to the business for them to sign off on the results. Once the analysis is shown to be accurate, the data scientist can move on to the next step, which is creating the model.

The foundation for the model is logistic code that takes the data stored in a CSV file and runs it through the data scientist’s algorithms. The algorithms could be open-source or custom made by the scientist. It’s not uncommon for a developer to also dive into the analytics to better understand what must be deployed.

Although every model is different, you can just think of them as a module of code that represents the answer to a question. The business asks the question, and the data scientist develops a solution in the form of a model.

Model Deployment

One of the toughest parts of the entire data science workflow is deploying a model. The scientist must work with a developer to convert the model (written in R or Python) into the local application’s language, which can be any language such as C#, JavaScript, PHP, or VB. The developer might not be an analytics expert, and it’s up to the scientist to explain the model in a way that makes it easy for the developer to convert it.

Statistically, most models never even make it to the application. Developers are hesitant to deploy machine learning code into an application without fully understanding what the code does, but it’s necessary for the business to take advantage of each model.

A good example of a machine learning module and local applications is in the finance industry. When you go to a cashier and want credit at the department store, you give her some information about yourself without telling your life story. From just your social security number, name and birth date, an algorithm can decide whether or not to give you credit. These financial decisions are made using data science models.

The question that the model answers is: “Are you a trustworthy candidate for credit, and does your data history indicate that you will pay the money back?” You know that the response to a credit application is either yes or no. Data models written by data scientists make this decision, and they use machine learning to pick up on patterns within millions of records.

Integrating Data Science into Business Code

Building a new data team is costly, takes time, and there is a learning curve for your developers. The benefits far outweigh the disadvantages, and you can work with project managers and agencies to help get you started.

If you’ve thought of taking your business analytics to the next level, adding a data scientist to your current IT team is the way to go. Your developers will learn new skills, your business will make more money, and you can take advantage of the latest in code design and database storage technologies."
LSTM-based Handwriting Recognition by Google,"Handwriting is a one of the challenge in NLP task. It is because it can be various among different people. On the other hand, some characters (e.g. English) are quite similar. As a human beginning, we leverage contextualize information, lexicon matching.

Sometimes, “O” can be written as “0” while human begin has the capability to distinguish whether it is “O” or “0” from contextualize information. For example, “0” will be used in phone number while “O” will be used as part of English word. Another skill is lexicon searching. It helps to guess words even though we cannot recognize every single characters.

Handwriting in different language (Carbune et al., 2019)

How can we tackle in via deep learning era? This story will discuss about Fast Multi-language LSTM-based Online Handwriting Recognition (Carbune et al., 2019) and the following are will be covered:

Data

Architecture

Experiment

Data

Carbune et al. leverage both open and close dataset to validate the model. As usual, IAM-OnDB dataset is used to train a model. During the experiment, they use two representations which are Raw Touch Points and Bézier Curves .

Raw Touch Points

Data will be convert to 5-dimensional points which are x coordinate, y coordinate, timestamp of the touchpoint since the first touch, pen-up or pen-down and new stroke or not. Some preprocessing a necessary

Since the size of image can be various, normalization of x and y coordinates are necessary. Surrogate are 20% larger than observed touch points if writing area is unknown

Equidistant linear resampling along the strokes with value of 0.05. In other word, a line of length 1 will have 20 points.

Bézier Curves"
"Applying NLP in Java, all from the command-line","Introduction

We are all aware of Machine Learning tools and cloud services that work via the browser and give us an interface we can use to perform our day-to-day data analysis, model training, and evaluation, and other tasks to various degrees of efficiencies.

But what would you do if you want to do these tasks on or from your local machine or infrastructure available in your organisation? And, if these resources available do not meet the pre-requisites to do decent end-to-end Data Science or Machine Learning tasks. That’s when access to a cloud-provider agnostic deep learning management environments like Valohai can help. And to add to this, we will be using the free-tier that is accessible to one and all.

We will be performing the task of building a Java app, and then training and evaluating an NLP model using it, and we will do all of it from the command-line interface with less interaction with the available web interface — basically it will be an end-to-end process all the way to training, saving and evaluation of the NLP model. And we won’t need to worry much about setting up any environments, configuring or managing it.

Purpose or Goals

We will learn to do a bunch of things in this post covering various levels of abstractions (in no particular order):

how to build and run an NLP model on the local machine?

how to build and run an NLP model on the cloud?

how to build NLP Java apps that run on the CPU or GPU?

most examples out there are non-Java based, much less Java-based ones

most examples are CPU based, much less on GPUs

how to perform the above depending on absence/presence of resources i.e. GPU?

how to build a CUDA docker container for Java?

how to do all the above all from the command-line?

via individual commands

via shell scripts

What do we need and how?

Here’s what we need to be able to get started:

a Java app that builds and runs on any operating system

CLI tools that allow connecting to remote cloud services

shell scripts and code configuration to manage all of the above

The how part of this task is not hard once we have our goals and requirements clear, we will expand on this in the following sections.

NLP for Java, DL4J and Valohai

NLP for Java: DL4J

We have all of the code and instructions needed to get started with this post, captured for you on GitHub. Below are the steps you go through to get acquainted with the project:

Quick startup

To quickly get started we need to do just these:

open an account on https://valohai.com, see https://app.valohai.com/accounts/signup/

install Valohai CLI on your local machine

clone the repo https://github.com/valohai/dl4j-nlp-cuda-example/

$ git clone https://github.com/valohai/dl4j-nlp-cuda-example/

$ cd dl4j-nlp-cuda-example

create a Valohai project using the Valohai CLI tool, and give it a name

$ vh project create

link your Valohai project with the GitHub repo https://github.com/valohai/dl4j-nlp-cuda-example/ on the Repository tab of the Settings page (https://app.valohai.com/p/[your-user-id]/dl4j-nlp-cuda-example/settings/repository/)

$ vh project open ### Go to the Settings page > Repository tab and update the git repo address with https://github.com/valohai/dl4j-nlp-cuda-example/

update Valohai project with the latest commits from the git repo

$ vh project fetch

Now you’re ready to start using the power of performing Machine Learning tasks from the command-line.

See Advanced installation and setup section in the README to find out what we need to install and configure on your system to run the app and experiments on your local machine or inside a Docker container — this is not necessary for this post at the moment but you can try it out at a later time.

You will have noticed we have a valohai.yaml in the git repo and our valohai.yaml file contains several steps that you can use, we have enlisted them by their names, which we will use when running our steps:

build-cpu-gpu-uberjar : build our uber jar (both CPU and GPU versions) on Valohai

: build our uber jar (both CPU and GPU versions) on Valohai train-cpu-linux : run the NLP training using the CPU-version of uber jar on Valohai

: run the NLP training using the CPU-version of uber jar on Valohai train-gpu-linux : run the NLP training using the GPU-version of uber jar on Valohai

: run the NLP training using the GPU-version of uber jar on Valohai evaluate-model-linux : evaluate the trained NLP model from one of the above train-* execution steps

: evaluate the trained NLP model from one of the above execution steps know-your-gpus: run on any instance to gather GPU/Nvidia related details on that instance, we run the same script with the other steps above (both the build and run steps)

Building the Java app from the command line

Assuming you are all set up we will start by building the Java app on the Valohai platform from the command prompt, which is as simple as running one of the two commands:

$ vh exec run build-cpu-gpu-uberjar [--adhoc]



### Run `vh exec run --help` to find out more about this command

And you will be prompted with the execution counter, which is nothing by a number:

<--snipped-->

😼 Success! Execution #1 created. See https://app.valohai.com/p/valohai/dl4j-nlp-cuda-example/execution/016dfef8-3a72-22d4-3d9b-7f992e6ac94d/

Note: use --adhoc only if you have not setup your Valohai project with a git repo or have unsaved commits and want to experiment before being sure of the configuration.

You can watch your execution by:

$ vh watch 1



### the parameter 1 is the counter returned by the `vh exec run build-cpu-gpu-uberjar` operation above, it is the index to refer to that execution run

and you can see either we are waiting for an instance to be allocated or console messages move past the screen when the execution has kicked off. You can see the same via the web interface as well.

Note: instances are available based on how popular they are and also how much quota you have left on them, if they have been used recently they are more likely to be available next.

Once the step is completed, you can see it results in a few artifacts, called outputs in the Valohai terminology, we can see them by:

$ vh outputs 1



### Run `vh outputs --help` to find out more about this command

We will need the URLs that look like datum://[....some sha like notation...] for our next steps. You can see we have a log file that has captured the GPU related information about the running instance, you can download this file by:

$ vh outputs --download . --filter *.logs 1



### Run `vh outputs --help` to find out more about this command

Running the NLP training process for CPU/GPU from the command-line

We will use the built artifacts namely the uber jars for the CPU and GPU backends to run our training process:

### Running the CPU uberjar

$ vh exec run train-cpu-linux --cpu-linux-uberjar=datum://016dff00-43b7-b599-0e85-23a16749146e [--adhoc]



### Running the GPU uberjar

$ vh exec run train-gpu-linux --gpu-linux-uberjar=datum://016dff00-2095-4df7-5d9e-02cb7cd009bb [--adhoc]



### Note these datum:// link will vary in your case

### Run `vh exec run train-cpu-linux --help` to get more details on its usage

Note: take a look at the Inputs with Valohai CLI docs to see how to write commands like the above.

We can watch the process if we like but it can be lengthy so we can switch to another task.

The above execution runs finish with saving the model into the ${VH_OUTPUTS} folder to enable it to be archived by Valohai. The model names get suffix to their names, to keep a track of how they were produced.

At any point during our building, training or evaluation steps, we can stop an ongoing execution (queued or running) by just doing this:

$ vh stop 3

(Resolved stop to execution stop.)

⌛ Stopping #3...

=> {""message"":""Stop signal sent""}

😁 Success! Done.

Downloading the saved model post successful training

We can query the outputs of execution by its counter number and download it using:

$ vh outputs 2

$ vh outputs --download . --filter Cnn*.pb 2

See how you can evaluate the downloaded model on your local machine, both the models created by the CPU and GPU based processes (respective uber jars). Just pass in the name of the downloaded model as a parameter to the runner shell script provided.

Evaluating the saved NLP model from a previous training execution

### Running the CPU uberjar and evaluating the CPU-verion of the model

$ vh exec run evaluate-model-linux --uber-jar=datum://016dff00-43b7-b599-0e85-23a16749146e --model=datum://016dff2a-a0d4-3e63-d8da-6a61a96a7ba6 [--adhoc]



### Running the GPU uberjar and evaluating the GPU-verion of the model

$ vh exec run evaluate-model-linux --uber-jar=datum://016dff00-2095-4df7-5d9e-02cb7cd009bb --model=datum://016dff2a-a0d4-3e63-d8da-6a61a96a7ba6 [--adhoc]



### Note these datum:// link will vary in your case

### Run `vh exec run train-cpu-linux --help` to get more details on its usage

And at the end of the model evaluation we get the below, model evaluation metrics and confusion matrix after running a test set on the model:

Note: the source code contains ML and NLP-related explanations at various stages in the form of inline comments.

Capturing the environment information about Nvidia’s GPU and CUDA drivers

This step is unrelated to the whole process of building and running a Java app on the cloud and controlling and viewing it remotely using the client tool, although it is useful to be able to know on what kind of system we ran our training on, especially for the GPU aspect of the training:

$ vh exec run know-your-gpus [--adhoc]



### Run `vh exec run --help` to get more details on its usage

Keeping track of your experiments

While writing this post, I ran several experiments and to keep track of the successful versus failed experiments in an efficient manner, I was able to use Valohai’s version control facilities baked into its design by

filtering for executions

searching for specific execution by “token”

re-running the successful and failed executions

confirming that the executions were successful and a failure for the right reasons

also, checkout data-catalogues and data provenance on the Valohai platform below is an example of my project (look for the Trace button):

Comparing the CPU and GPU based processes

We could have discussed comparisons between the CPU and GPU based processes in terms of these:

app-building performance

model training speed

model evaluation accuracy

But we won’t cover these topics in this post, although you have access to the metrics you need for it, in case you wish to investigate further.

Necessary configuration file(s) and shells scripts

All the necessary scripts can be found on the GitHub repo, they can be found in:

the root folder of the project

docker folder

resources-archive folder

Please also have a look at the README.md file for further details on their usages and other additional information that we haven’t mentioned in this post here.

Valohai — Orchestration

If we have noticed all the above tasks were orchestrating the tasks via a few tools at different levels of abstractions:

docker to manage infrastructure and platform-level configuration and version control management

java to be able to run our apps on any platform of choice

shell scripts to be able to again run both building and execution commands in a platform-agnostic manner and also be able to make exceptions for the absence of resources i.e. GPU on MacOSX

a client tool to connect with the remote cloud service i.e. Valohai CLI, and view, control executions and download the end-results

You are orchestrating your tasks from a single point making use of the tools and technologies available to do various Data and Machine Learning tasks.

Conclusion

We have seen that NLP is a resource-consuming task and having the right methods and tools in hands certainly helps. Once again the DeepLearning4J library from Skymind and the Valohai platform have come to our service. Thanks to the creators of both platform. Also, we can see the below benefits (and more) this post highlights.

Benefits

We gain a bunch of things by doing the way we did the things above:

not have to worry about hardware and/or software configuration and version control management — docker containers FTW

able to run manual one-off building, training and evaluation tasks — Valohai CLI tool FTW

automate regularly use tasks for your team to be able to run tasks on remote cloud infrastructure — infrastructure-as-code FTW

overcome the limitations of an old or slow machine or a Mac with no access to the onboard GPU — CUDA-enabled docker image scripts FTW

overcome situations when not enough resources are available on the local or server infrastructure, and still be able to run experiments requiring high-throughput and performant environments — a cloud-provider agnostic platform i.e Valohai environments FTW

run tasks and not have to wait for them to finish and be able to run multiple tasks — concurrently and in-parallel on remote resources in a cost-effective manner — a cloud-provider agnostic platform i.e Valohai CLI tool FTW

remotely view, control both configuration and executions and even download the end-results after a successful execution — a cloud-provider agnostic platform i.e Valohai CLI tool FTW

and many others you will spot yourself

Suggestions

using provided CUDA-enabled docker container: highly recommend not to start installing Nvidia drivers or CUDA or cuDNN on your local machine (Linux or Windows-based) — shelve this for later experimentation

highly recommend not to start installing Nvidia drivers or CUDA or cuDNN on your local machine (Linux or Windows-based) — shelve this for later experimentation use provided shell scripts and configuration files: try not to perform manual CLI command instead use shells scripts to automate repeated tasks, provided examples are a good starting point and take it further from there

try not to perform manual CLI command instead use shells scripts to automate repeated tasks, provided examples are a good starting point and take it further from there try to learn as much : about GPUs, CUDA, cuDNN from resources provided and look for more (see Resources section at the bottom of the post)

: about GPUs, CUDA, cuDNN from resources provided and look for more (see section at the bottom of the post) use version control and infrastructure-as-code systems: git and the valohai.yaml are great examples of this

I felt very productive and my time and resources were effectively used while doing all of the above, and above all, I can share it with others and everyone can just reuse all of this work directly — just clone the repo and off you go.

What we didn’t cover and is potentially a great topic to talk about, is the Valohai Pipelines in some future post!

Resources

Valohai resources

Other resources

Other related posts"
How to run RStudio on AWS in under 3 minutes for free,"When it comes to data analytics there are my reasons to move from your local computer to the cloud. Most prominently, you can run an indefinite number of machines without needing to own or maintain them. Furthermore, you can scale up and down as you wish in a matter of minutes. And if you choose to run t2.micro servers you can run for 750 hours a month for free within the first 12 months! After that it’s a couple of bucks per month and server.

Alright, let’s get to it then! Understandably you won’t have time to read a ten minute article about RStudio Sever and Amazon Web Services after clicking a title that promised you a solution in 3 minutes. So I skip the formal introduction and cut to the chase.

Step 1: Log in to your AWS account (or create one if you’re new to AWS)

Step 2: Go to Louis Aslett’s website, choose the region you want your virtual machines to stay, and click on the link next to it (e.g., ami-02bf650155c44b475 for US West, N. California). The link will directly lead you to the EC2 Management Console.

Step 3: Click through the Launch instance wizard. Almost everything is already pre-filled out and your screen should look like the screenshots below. Don’t press “Review and Launch” yet. We need to go through the whole process once just to be sure, so press “Next: …”.

Nothing to change here. Press Next: Configure Instance Details.

Nothing to change here. Press Next: Add Storage."
The 5 Classification Evaluation metrics every Data Scientist must know,"Member-only story The 5 Classification Evaluation metrics every Data Scientist must know

What do we want to optimize for? Most of the businesses fail to answer this simple question.

Every business problem is a little different, and it should be optimized differently.

We all have created classification models. A lot of time we try to increase evaluate our models on accuracy. But do we really want accuracy as a metric of our model performance?

What if we are predicting the number of asteroids that will hit the earth.

Just say zero all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable. What should we do in such cases?

Designing a Data Science project is much more important than the modeling itself.

This post is about various evaluation metrics and how and when to use them.

1. Accuracy, Precision, and Recall:

A. Accuracy

Accuracy is the quintessential classification metric. It is pretty easy to understand. And easily suited for binary as well as a multiclass classification problem.

Accuracy = (TP+TN)/(TP+FP+FN+TN)

Accuracy is the proportion of true results among the total number of cases examined.

When to use?

Accuracy is a valid choice of evaluation for classification problems which are well balanced and not skewed or No class imbalance.

Caveats

Let us say that our target class is very sparse. Do we want accuracy as a metric of our model performance? What if we are predicting if an asteroid will hit the earth? Just say No all the time. And you will be 99% accurate. My model can be reasonably accurate, but not at all valuable.

B. Precision"
Using Data Science to save money on my next trip to Mexico,"Using Data Science to save money on my next trip to Mexico

How am I using basic data work to ensure I am getting a good price on my trip. Marc-Olivier Arsenault · Follow Published in Towards Data Science · 6 min read · Oct 28, 2019 -- 1 Listen Share

It has been 4 years since my wife and I took some vacation in a sunny place. Last time, for our honeymoon, we spent some quality time in Mexico. We enjoyed 10 days in a very nice all-inclusive resort in Riviera Maya. Since then, a house, two kids, a new job and many other things. After some reflexion we decided that it was time to go back on the beach. So, next December (2019) we (my wife, our 3 years old, our 4 months old and I) will be heading to Riviera Maya once again.

Don’t worry, I am not turning my Data blog into a travel and lifestyle blog. I want to share with you how I am making sure I am getting the right price for the trip.

So here is where it is interesting, when we signed the contract this summer (in June) the contract said, if the price go down, I could, once, ask for a price match. Since the trip was +-6 months away, that looked like a very interesting feature. BTW, this is one of the factors that made us pick that travel company. To be upfront with the numbers, we paid 4317 CAD$ for the full family.

The no so easy part

After a couple of weeks, I went back to check the price. It was still the same. Then I realize, how can I track the price. There is no way I can take the time to go, click through a series of web interfaces to query the price. That would represent an annoying 2–3 minutes a day and I just don’t have that time.

Here is what it looks like to get the price update:

The full process of getting the price update.

Obviously, the travel company does not feel like it would be a great feature to track the price. After a couple of time searching for it, there is just no way to do this. At least with a tool on the website.

Then what if it is below my original price, do I take this price or I wait a bit more. Because remember, I can only do a price match once. It would be really useful to have all the historic prices, then if/when it gets below original price, I could look at the trend and take a more informed decision.

So now I would need to spend couple of minutes daily to fetch the price, and then couple of more to copy the value into some sort of spreadsheet. Anyone that did something like this knows that it works kinda of OK for the first few days, maybe weeks. But at some point you start missing days, do copy paste error, etc.

Manual process in data is more or less the equivalent of no process.

Just script it.

My idea was, I can just get the URL, download the HTML from python or something and do some regex magic to extract the price.

Of course, it could not be that easy. First thing, the URL does not really change. All of the price things are some sort of modal on top of the other page. So copying the URL basically brings you back to the home page.

Now when I started to look at Chrome Developer tools, I assumed I could see somewhere the data coming in. Data has to come in right… right…

After a couple of times digging in each of these files, I found the golden nugget.

Seems like we are on the right track. We have what seems to be a JSON file and the custom URL to get it. OBVIOUSLY, when I use this URL directly in a new page it is not working. I receive the equivalent of a page unavailable. Really it seems like they don’t want us to do this. They have set many roadblocks to prevent us from doing it. Thanks Obama.

Then, I found a very neat option in chrome Dev tools.

Copy as cURL. You end up with a very long command you can paste in your terminal and get the JSON.

Finally, something is working. I now have a way to extract the price.

As you can see the query is pretty nasty. Really, it is like if they do not want us to extract the prices automatically. At least, now, we have a pretty nice dictionary to work with.

What to do with this value now

Now that we can access the value, how do we extract it. I have decided to create a AWS Lambda function that run every 6 hours to extract the price. With this price, 4 times a day, we do 3 things:

We check if price is good. Since I paid a bit more than 4300, if price would go below 4k$, I send myself an email. To make sure I can act quickly if needed. I store the value (with the timestamp) in a database. (DynamoDB) I store the value (with the timestamp) in AWS S3

Why 2 and 3, I was not sure how I would use it, since AWS have some rules about what can access what and because storage is shockingly cheap, I stored it twice.

The price

So sadly, the price did not go below 4300$ yet, and to be fair I doubt it will. The trip is now priced at 4700/5000 depending on the days.

To build this view, I have used the fantastic tool call Dash which allows you with fewer than 100 lines of code build this visual. If you are interested, you can go see the app here: https://voyage.coffeeanddata.ca.

Discoveries

For a couple of days I did not collect any new data points. In fact the cURL command was getting back a 404 from the server. Took me a while to realize that I had no new data.

Because I did not implement any validations in my code. The Lambda script would simply silently fail and I would not collect new data points.

So I added a validation in my code that sends me an email when the cookie needed to be updated. I assume the cookie has some expiration encrypted in it. So simply getting a new token seems enough.

Pick wisely

The other interesting points I got from the data up to now is this part of the curve:

For a couple of days in a row, the price surged by 200$ at around 7am in the morning. This is something I will be careful with, next time I look to book a trip.

Conclusion

Sadly, I did not save any money…yet. I am flying for Mexico in more than 2 months so I will keep tracking the price on my website. Travel companies seems to be making some effort to prevent us to automatically extracting the price.

The price looks to be very volatile, I would be curious to understand what influence the price on an hourly basis."
Interactive Choropleth Maps With Plotly,"Interactive Choropleth Maps With Plotly

How To Read Data From An Api, Matching Geo- And Non-Spatial Data And Creating An Interactive Map Benedikt Droste · Follow Published in Towards Data Science · 7 min read · Aug 11, 2019 -- 2 Share

Recently, I wanted to visualize data from the last federal election. I live in Duesseldorf, Germany and wanted to know in which districts which party had relative strengths.

Surprisingly, it was a little harder than expected. Therefore I would like to share my experiences here. We will go through the following steps together:

0. Preparation

1. Reading Data Over An Api In GeoJSON- And JSON-Format

2. Extracting The Relevant Features From The GeoJSON-Data

3. Customizing Our Map With Colorscales and Mouseovers

4. Putting Everything Together And Adding Interactivity

0. Preparation

You can follow this tutorial with your own dataset to create your individual map. The easiest way to do this would be to have the data in GeoJSON or JSON format. Here you can take a look at the data from this tutorial:

https://opendata.duesseldorf.de/api/action/datastore/search.json?resource_id=6893a12e-3d75-4b2f-bb8b-708982bea7b7

https://opendata.duesseldorf.de/sites/default/files/Stadtteile_WGS84_4326.geojson"
10 Reads for Data Scientists Getting Started with Business Models,"10 Reads for Data Scientists Getting Started with Business Models

If you’re getting started with data science, you’re probably focusing your attention on mostly stats and coding. There’s nothing wrong with this, in fact, this is the right move — these are essential skills that you need to develop early on in your journey.

With this being said, the biggest knowledge gap that I’ve encountered during my data science journey doesn’t deal with either of these areas. Instead, upon starting my first full-time role as a data scientist, I realized, to my surprise, that I didn’t really understand business.

I suspect that this is a common theme. If you studied a technical field in college or picked things up using online courses, it’s unlikely that you ever had any reason to deep dive into business concepts like models, strategy, or important metrics. Adding on to this, I didn’t really come across data science interviews that stress-tested this type of understanding. Plenty of them tried to get a sense of product intuition, but I found that it rarely went beyond that.

The fact is that business understanding isn’t taught or evangelized in the data science community to the extent that it’s used in practice. The goal of this post is to help bridge this gap by sharing some of the resources that I found most helpful as I got up to speed on how businesses work from the inside-out.

This article from Andreessen Horowitz is a great place to start if you’re trying to get familiar with the slew of metrics and acronyms that get thrown around in a business, whether it’s a startup or not. On a more general note, their posts are consistently high-quality and are almost always worth your time. If you have a larger appetite, check out their follow-up post on 16 more metrics and the thread below for some additional tips on metrics.

Some helpful tips on misleading metrics

An overall solid resource, the articles at FourWeekMBA are worth exploring at some point. I particularly recommend this for an overview of all the different business models out there. It’s hard to come away from this without learning something new. For a more practical dive into business models, I also found this post going over how Slack makes money interesting.

This one is a bit denser than the previous two, but it’s really excellent. The unmissable Ben Thompson from Stratechery goes over how markets work and why certain companies are dominating their industry. The takeaway from this post is that markets have three components, and the companies that can monopolize two of the three typically win out in a big way. Think Netflix.

A lot of what we’ve seen so far has been conceptual, so let’s look at a specific model and analyze why it does and doesn’t work. Another one of my favorite business writers out there, Andrew Chen looks at the dating industry and why most investors don’t find it attractive. Other great essays from the venture capitalist commonly cover things like growth and metrics.

More from Ben Thompson, here’s another great essay from him. This time on how large companies, particularly Facebook and Google, process data from its raw form to something uniquely valuable. Published in Fall 2018, this provides a good early look into the business side of all of the data privacy and regulation concerns we’re seeing now.

If you’re not familiar with LTV (lifetime value), then you’ll probably have to get familiar with it at some point. There’s plenty of resources out there regarding the metric, but this is probably my favorite go-to on the subject. It clearly explains how to calculate LTV, and why you should think twice before you blindly buy into it without context.

This short post focuses on the SaaS (software as a service) business model. The basic idea is outlined quite simply in the picture below, but I’d still recommend you take the time to read the full write-up. Christoph Janz really does an excellent job of taking a complex question and breaking it down. He also recently updated the chart in a new post.

Co-founder and former CEO of StackOverflow, Joel Spolsky hammers home a crucial part-business, part-economics lesson here: Smart companies try to commoditize their products’ complements. Whether they succeed or not is a very different story, shown here with plenty of examples.

We covered a few ways that companies can make money, but this resource takes the most simplistic (and still accurate) approach. It all started with Jim Barksdale at a trade show. As he was heading out the door to catch a flight, he left the audience with one last pearl of wisdom before departing, one that sums up the post quite nicely.

“Gentlemen, there’s only two ways I know of to make money: bundling and unbundling.”

Last but not least, if you want to take things a step further, I recommend case studies. You can find a ton of them out there from top universities like Stanford and Harvard for cheap or often no cost at all. Once you have a grasp on the fundamentals, this an excellent way to continue to supplement your learning. This is where I’m currently at — I’ve challenged myself to take on one case study every two weeks over the summer. Join me on the ride!

Wrapping Up

That does it for the list. I know all of the above links really helped me out and I hope you take the time to explore them. As you might have noticed, not all of them tie into the day-to-day life of a data scientist — that’s intentional.

I said this in my last post, I’ll say it again — data scientists are thinkers. We do our best work when we understand the systems that surround us. This understanding is what sets us up for the cool stuff: exploratory analysis, machine learning, and data visualization. Lay the foundation first and reap the benefits later. That’s what it’s all about.

The resources selected above were heavily influenced by SVP of Strategy at Squarespace, Andrew Bartholomew’s reading list."
Market Profile: a statistical view on financial markets,"Market Profile: a statistical view on financial markets

A gentle introduction and a short recipe on how to plot market profile in Matplotlib Mario Emmanuel · Follow Published in Towards Data Science · 9 min read · Aug 11, 2019 -- 3 Share

Market profile is a technique used to contextualize current market conditions.

A brief introduction to Market Profile methodology

Market Profile was a technique developed by J. Peter Steidlmayer in the 60s. The methodology represents the statistical distribution of a given market in a given period.

Steidlmayer, son of a wealthy farmer, become a CBOT pit trader in the 60s and eventually become one of the CBOT directors in the early 80s. He merged the concepts of minimum relevant price movement, balance and gaussian distribution to define a methodology that can track how a given market is moving at a given time.

Market profile theory is properly covered in several books and there is also some good material on the Internet. It is a methodology that experienced a great interest during the late 80s and 90s, being Steidlmayer its main promoter—he also was in charge of delivering the first electronic data services at CBOT in the early 80s —. While it is no longer a mainstream analysis technique, it still counts with a base of followers who actively use it.

Market Profile uses time and price to locate the value areas of the trading session (i.e. price areas where the participants consider that the fair price of a given asset/instrument is located). While it is not a trading methodology or system, it is a sound way to analyse the current status of a given market as it helps to clarify if a market is consolidating or trending.

One of the advantages of Market Profile over Volume Profile is that volume data is not required. This is especially interesting for non-regulated OTC markets where volume information is either not available or not significative. It also enables using non-expensive historical data for simulation.

As Market Profile uses the concept of TPO (Time Price Opportunity) to reflect areas of interest, those areas are highly correlated with the high volume areas. So in the end, both methodologies can lead to similar results and it is sometimes actually astonishing to see how similar both profiles are. The explanation is that moving large volumes at a given price in the market requires…"
Machine Learning from Scratch-ish,"How to fail

Tutorials and help articles are great at trying to tell you what you should affirmatively be doing, but they traditionally spend significantly less time explaining what not to do. I believe that setting yourself up for success includes making sure to avoid potential traps and failures, and this journey has more than its fair share of those for the unwary.

You will almost certainly fail in your journey to become a Machine Learning Beginner if:

You lack patience and resolve

I can tell you as a programmer, if not as a Machine Learning practitioner, that learning to program is an exercise in patience. Computers are, in a very important (and yet ironic) sense, extraordinarily dumb machines — they do only what you tell them and literally nothing more.

If you do not have the patience to sit in front of a computer and engage in the iterative process of coding, then debugging, and then researching, this is probably not going to be enjoyable for you.

You lack curiosity

Regardless of the path you eventually choose to take to acquire your Machine Learning knowledge, it is almost certain that you will be given the advice to “type in all the code” or “work along with the exercises” or “go out and build the thing that we just built, but slightly different”. These are all just variants of the same idea, which is that there is value in just “tinkering” — experimenting, reading the documentation when something fails, looking at other peoples’ source code, etc. If you are not fundamentally driven and sustained by your own curiosity and a desire to “build”, you will almost certainly not keep up with those who are.

You overestimate the difficulty of the learning curve

This is, in essence, a re-statement of the initial concern, meaning that this is simply a concern as to whether someone with a non-traditional background can succeed in the world of Machine Learning. If this article accomplishes anything, I hope that it will at least dispel the myth that Machine Learning is completely unapproachable, or otherwise only for the lucky few that happen to have dual PhDs in Computer Science and Applied Mathematics/Statistics.

You underestimate the difficulty of the learning curve

I have posited that this is all do-able, not that it is easy. If it was easy, suffice it to say that everyone would be doing it, and if everyone was doing it, educational background could not, by force of logic, be a relevant factor, which means the operative question that this article attempts to address would be moot.

The point simply is that there will be hard bumps in the road and how you deal with those will likely be more of a determining factor in the outcome of your success. Are you going to start scouring forums, mailings lists, Twitter, Medium, etc. and really pound the pavement until you get a satisfactory answer? Or are you going to be unsettled in the face of unfamiliar material? See “You lack patience and resolve” above.

You are intending to learn all of Machine Learning/AI next week, next month, this year…

The amount of material that can/could be learned in the process of just becoming a Machine Learning Beginner is simply overwhelming in its volume. To further complicate matters, the body of scholarship and relevant work product is increasing at such a rate that we tend to measure relevance in terms of single months and years as opposed to decades.

There is no “end” to the development of technology now or in the foreseeable future and to try to identify some point at which you will know “everything about machine learning” is to worship at the alter of a false god. Take your time, go over the relevant material a second and third time, write more code, build more things, document your learning process, and enjoy and celebrate the individual, smaller accomplishments along the way.

So, yes, it is quite obviously possible to fail and it is not particularly hard to do. But then again, it is also not particularly hard to avoid most of these issues.

How to succeed at this

Pick a course and stick to it

There are tons of great resources out there that offer a comprehensive introduction to beginning Machine Learning concepts. One well known option is Andrew Ng’s course on Coursera. Another option is offered by fast.ai. There isn’t a right or wrong decision —both have excellent reputations, with each course offering a teaching style that is slightly different than the other.¹

The salient point here is that you should make every effort to stick to a single course of study. Switching courses when you are presented with something you don’t understand is unlikely to produce meaningful progress in the average case over a statistically significant period of time. The solution is not “just one more book from Amazon” or “a different, less technical tutorial.” At some point, you simply have to buckle down, grit your teeth, and fight your way up and to the right of the learning curve.

To be eminently clear, I am not suggesting that you don’t supplement your own study efforts with outside resources or otherwise consult third party materials when you have a problem. I am suggesting that at some point, as you get deeper and deeper, you will run into concepts that are difficult to understand simply because they are, in fact, difficult concepts. Embrace the difficulty as a sign that you are pushing yourself past your comfort zone, which is where (in my experience) all things that are worth discussing in this world happen. Changing courses or books, or video series just isn’t a viable long-term learning strategy and is indicative of a mindset that is looking for a “silver bullet.” As stated above, there are no silver bullets, no shortcuts — there is just hard work.

Pick a framework and stick to it

Now is not the time to compare and contrast TensorFlow with PyTorch with MXNet, etc. As a complete beginner, you don’t have sufficient knowledge or understanding of either the problem space of the implementation of any of these frameworks for any such decision to be something other than a regurgitation of another’s thought process.

Pick a mainstream framework with a large and active development community and stick to that. At this point, focus on learning over-arching concepts and avoid the rabbit hole that is the particulars of a given framework that may or may not be relevant a year from now.

Remember that “it’s just like French”

If you ever were required to study a foreign language, it’s quite possible that you have run into the concept of gendered nouns. To grossly over-simplify, in many languages (of which English is not one), certain nouns are either masculine or feminine. By way of example, take this excerpt from this article (my emphases added).

There are some nouns that express entities with gender for which there is only one form, which is used regardless of the actual gender of the entity, for example, the word for person; personne; is always feminine, even if the person is male, and the word for teacher; professeur; is always masculine even if the teacher is female.

The point is that, just as trying to understand the logic of why a person is always feminine regardless of gender is a fruitless exercise (the answer is inevitably “it just is — accept it as an article of truth and move on”), trying to understand everything about Machine Learning during the first pass is simply not a reasonable expectation. There are certain things that you will simply have to take on faith for the time being.

That is not to say that you don’t note the question for further research at some later point in time, but trying to learn everything about everything that you don’t understand each time you confront something you don’t understand is merely a clever variation of the “You are intending to learn all of Machine Learning” discussed above.

Despite what your better angels may be telling you, your drive to try to delve into the details is not helping you learn. In fact, quite the opposite, as these are rabbit holes that just potentially delay the time until you obtain a solid grasp and foundation on the larger picture that provides the necessary context for more meaningful exploration.

Establish a toolset and a repeatable workflow

I don’t have anything particularly interesting (and certainly nothing original) to add about the specifics of any one tool or workflow, other than to say it is important to have a set of tools that you know, are comfortable with, and that are reasonably reliable.

There are multiple, well-documented solutions on everything from Google Cloud to AWS to Azure to Jupyter Notebooks to plugins for Vim, Emacs, VSCode, et al. Setting up an industry-grade Machine Learning pipeline for a single developer’s use is, as of the latter half of 2019, very affordable and very well-documented.²

Get a decent piece of note-taking software

I don’t believe this gets enough attention. You will be, even in the initial stages of this journey, bombarded with information, some of which you will be able to grasp and some of which you will not. As stated above, given the state of our knowledge, there are certain things that you will have to take on faith for the time being. But you should absolutely noting those concepts that you do not understand so that you can eventually, when you have more context, do the deep dive that is truly required.

I personally use Coggle, but at the risk of boring you with repetition, it is not really about the tool you pick. Text files, Markdown files, Jupyter Notebooks, OneNote, Evernote, etc. will all work. My personal preference stems from the fact that I prefer “mind map”-style diagrams. As opposed to a hierarchical format (as in an outline, for instance), which I often find do not line up with the mental model of the material in my mind. A mind-map allows me to connect nodes to other nodes in fairly arbitrary patterns, which allows me to document things in a way that is more natural to my brain.

Learn to accept failure as a normal, resting state

If you are transitioning to Machine Learning from something other than Software Engineering, this might seem non-intuitive. Put simply (and more than a little tongue-in-cheek), an overwhelming majority of the job could be colloquially described as “getting your code to compile”, which, by definition, means that the majority of time, your code is in a non-working, failing state.

Of course, as human beings, we do everything in our power possible to exacerbate that feeling of inadequacy in others and ourselves by making sure that errors are shown in bold text, in bright red, accompanied by iconography that indicates that you are engaged in wrongdoing, etc.

The reality is that there is an aspect of this work that, like Software Engineering (and I feel comfortable saying this after having observed more than a few Data Scientists at work), represents a somewhat tedious cycle of experimentation, bug-identification and fixing, followed by further experimentation. If you don’t have the tenacity, the drive to build, and a natural curiosity that is capable of powering you through those moments, this might not be the discipline for you.

Closing Thoughts

Becoming even a Machine Learning Beginner is far from a simple undertaking, but then, nothing worth doing ever is.

I will leave you with a simple thought experiment — if (i) Data Science is the fastest growing job in the United States and (ii) only people who have a Data Science background can fill that job, how can (i) be true? How can it possibly be the fastest growing job?

In other words, for the first (I believe, spurious) assertion to be true, wouldn’t there have to be scores of people just sitting around who, for some reason, just happen to have the perfect skill set required by an industry that didn’t meaningfully exist a decade ago. Does that seem likely?

Or is it more likely that the more reasonable explanation — that otherwise capable individuals like yourselves and like me are cross-training and developing new skills in this area precisely because these skills are, in fact, accessible?

I leave that as an exercise to the curious reader."
“Not all data is equal”,"“Not all data is equal”

I’m sharing the first expert anecdote from my book, Human-in-the-Loop Machine Learning. I have been fortunate to chat with many leaders in the machine learning community about their experience. Many shared personal anecdotes with me that deserve a bigger audience.

For each leader that is featured in the book there were two selection criteria:

Their early career was as an engineer or scientist. All the experts in the book were software engineers, physicists, linguists, or similar, at some point early in their career. So, they know what it is like to annotate data and/or build machine learning models. They have founded a company that uses machine learning. So, the experts also know what it means to think about shipping machine learning products that can impact the real world.

All the leaders are great role models for people who want to build a career in data science.

The first anecdote accompanies chapters on Active Learning: the process of selecting the right data for human review. Most deployed machine learning models used supervised learning, with thousands and sometimes millions of human-labeled data items. Active Learning determines which raw data items are the best for human review and annotation.

The first expert is Jennifer Prendki, the founder and CEO of Alectio. She previously led data science teams at Atlassian, Figure Eight, and Walmart, and has a PhD in particle physics. Jennifer’s company, Alectio, specializes in data-efficiency to help machine learning teams build their models with less data.

“Not all data is equal”

If you care about your nutrition, you don’t go to the supermarket and randomly select items from the shelves. You might eventually get the nutrients you need by eating random items from the supermarket shelves, however, you will eat a lot of junk food in the process. I think it is weird that in Machine Learning, people still think it’s better to “sample the supermarket randomly” than figuring out what they need and focusing their efforts there. The first Active Learning system I built was by necessity. I was building Machine Learning systems to help a large retail store make sure that when someone searched on the website, the right combination of products came up. Almost overnight, a company re-org meant that my human labeling budget was cut in half and we had a 10x increase in inventory that we had to label. So, my labeling team had only 5% the budget per item that we previously did. I created my first Active Learning framework to discover which was the most important 5%. The results were better than random sampling with a bigger budget. I have used Active Learning in most of my projects since, because not all data is equal! Jennifer Prendki, in Human-in-the-Loop Machine Learning, Robert Munro, Manning Publications

You don’t go shopping by randomly sampling from every shelf, so why are you doing this with your data?

I am very grateful to Jennifer for sharing her expertise with me and with the readers of my book and this article!

For more on Active Learning, see the cheatsheets that I shared recently for Diversity Sampling and Uncertainty Sampling:

Your data is already biased

Many data scientists insist that random sampling is still their preference because they might bias their data through Active Learning. I think this is the wrong way to look at the data that you already have.

To continue the analogy, a supermarket is not an unbiased sample of food. There are all sorts of factors that go into deciding what ends up on a shelf in the first place, including the store location, time-of-year, specific supplier partnerships, promotions, and expected profit.

The same is probably true of your raw data: there are many factors that led to the creation and storage of the raw data that you have available to you, and chances are that your raw data over-samples and under-samples in many different ways. Like I point out in my book, biases in data often correlate with real-world bias, and so you risk echoing or amplifying real-world biases if you aren’t mindful about how you approach your data annotation strategy.

I recommend that you assume that your raw data is already biased due to the filtering and sampling techniques that generated the data in the first place. Active Learning is then one way to help make your data fairer, while at the same time making your models more accurate.

Robert Munro

November 2019"
"50 Years After the Moon Landing, What Is the Future of Space Exploration? [Infographic]","This month, 50 years ago, the wildest science-fiction fantasies became reality when Neil Armstrong stepped off the Lunar Module ladder and left his iconic footprint on the moon’s surface.

His infamous words, “one small step for man, one giant leap for mankind,” reverberated across the globe, to an audience of over 530 million viewers .

In a few weeks, the world will relive this historic event after half a century of continued space exploration and development.

In honor of this groundbreaking achievement, we take a look back at the history of lunar landings, the future of space exploration and the role of private companies in the burgeoning space economy.

Which Countries Have Landed on the Moon?

Although Apollo 11’s mission to the moon is the most well-known, there were other successful unmanned missions undertaken before 1969.

The Soviet Union’s Luna 2 was the first spacecraft to reach the lunar surface in 1959, while Luna 9 became the first spacecraft to soft-land on the moon in 1966.

Four months later, NASA’s Surveyor 1 successfully soft-landed on the moon, followed by four more successful unmanned missions.

After 14 years of a tension-filled space race between the Soviet Union and the United States, Apollo 11 successfully went to the moon and back-and set a precedent for generations to come.

Following this, NASA sent six more missions to the moon, five of them successful. In 2013, China also successfully soft-landed on the moon in December 2013, 37 years after the Luna 24.

In total, the United States has sent the most missions to the moon, 38 of them successful, closely followed by the former Soviet Union with 56 missions.

Future Missions to the Moon

The current US administration has ambitious plans for getting humans back on the moon by 2024 , four years earlier than what was initially on the timeline. The program, called Artemis, includes eight launches and a mini-station in lunar orbit by 2024.

Meanwhile, Russia plans to send manned flights to the moon starting in 2025, with the end goal of setting up a lunar colony by 2040 .

India hopes to become the fourth country to successfully soft-land on the moon this year, with a spacecraft scheduled to launch this month and land in September.

The space race won’t just be dominated by the nations of the world. Private companies such as SpaceX also have their sights set on the moon, even if travel to Mars is one of their main objectives. While Elon Musk’s SpaceX plans to send a group of artists on a trip around the moon in 2023, Jeff Bezos’ Blue Origin hopes to take astronauts to the lunar surface by 2024.

Is Privatized Space Travel the Future?

With the rising costs of ambitious space exploration plans, such as travel to Mars, NASA is increasingly turning to private companies’ deep pockets to fund such endeavors.

Billionaires such as Elon Musk, Jeff Bezos and Paul Allen are among those who are investing heavily in the commercial space sector.

According to SpaceFund’s launch database , there are currently over 100 space transportation companies from all over the world. As you can see below, the majority are based in the US, followed by the UK and China.

Many believe that private-public partnerships between NASA and companies such as SpaceX, Blue Origin and Virgin Galactic are the future of space travel.

In fact, commercial launch traffic into low-earth orbit has risen dramatically in the last decade:

Your Turn

What do you think of the future of space travel? Will increased space exploration be advantageous in the long-term? Or will it simply distract from more pressing problems here on Earth? Let us know your thoughts below…"
The Support Team — SVM. Topic Overview: Understanding Support…,"The Support Team — SVM

A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model. SVM can be used for classification or regression problem and outlier detection. It is one of the most popular models in Machine Learning that any Data Scientist interested in Machine Learning should have in their toolbox. SVMs are particularly well suited for classifying small or medium-sized complex datasets; small datasets would be just two classes and medium-sized would be more than two classes. You will be introduced to SVM linear and nonlinear classifier and a sprinkle of math to better understand SVMs.

Awesome right!

Linear Support Vector Machine Classification

When we think of the SVM classifier, we can compare it to a street between two sidewalks. The sidewalks of the SVM classifier are two different classes and the wider the street, the larger the margin.

In Machine Learning, we have to keep variance and bias well-balanced to have a good fit model. If you decrease the margin with SVM classifiers, the bias will decrease and the variance will increase (and vice versa, where an increase in the margin will result in an increase in bias and decrease in variance). Notice that adding more instances outside of the margins or “off the street” will not affect the decision boundary at all (the line between the margin). The reason is because it is fully supported by the instances (the red data points on the picture below) located on the edge of the street. These instances are called support vectors. The professional or technical way to explain this would be that the instances are separated by the hyperplane that is best fit due to the adjusted constraints of the margin.

The dotted line margin hold the constraints

Hard and Soft Margin Classification

If we keep all instances off the street and on the right side, this is called hard margin classification. There are two main issues with hard margin classification. Hard Margin Classification only works if the data is linearly separable also Hard Margins are very sensitive to outliers. We can use soft margin classifications to avoid these issues. To avoid issues it is recommended to use a more flexible model with soft margin classifications. Conversely, hard margins will result in overfitting of a model that allows zero errors. Sometimes it can be helpful to allow for errors in the training set, because it may produce a more generalizable model when applied to new datasets. Forcing rigid margins can result in a model that performs perfectly in the training set, but is possibly over-fit / less generalizable when applied to a new dataset. Identifying the best settings for ‘cost’ is probably related to the specific data set you are working with.

Regularization

Using the C hyper-parameter in Scikit-Learn we can adjust the constraints w *x + b = 1 and w*x+b = -1 to create a soft margin. Make sure to scale your data using Standard Scalar and have your dual parameter set to False. Using a high C value the classifier makes fewer margin violations but ends up with a smaller margin. Conversely using a low C value the margin is much larger, but many instances end up on the hyperplane. If the SVM model is overfitting you can use C hyper-parameter to regularize it.

High C

Low C

Top High C, Bottom Low C

The gamma parameter is the inverse of the standard deviation of the RBF kernel (Gaussian function), which is used as a similarity measure between two points. Intuitively, a small gamma value defines a Gaussian function with a large variance. In this case, two points can be considered similar even if they are far from each other. On the other hand, a large gamma value means define a Gaussian function with a small variance and in this case, two points are considered similar just if they are close to each other.

NonLinear SVM Classifier

Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features. Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms. At a low polynomial degree it cannot deal with very complex datasets, while with a high polynomial degrees it creates a huge number of features, making the model too slow.

Luckily we have Kernel Trick, the Kernel Trick is a technique in machine learning to avoid some intensive computation in some algorithms, which makes some computation goes from infeasible too feasible.In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.

Kernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the “kernel trick”]. Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.

In Conclusion

We have fully discussed what and Support Vector Machines are while giving a small portion of the math. SVM is a great tool to have in the Data Science world. SVM can also get overwhelming due to the added dimensions. This can cause the curse of dimensionality, which can be hard to picture and interpret. To reduce that we would use either Principle Component Analysis (PCA) and Linear Discriminant Analysis (LDA). With great power holds great support vector machines. Hope this blog helped you to better understand a bit about SVM."
"Multi-Label Classification using BERT, RoBERTa, XLNet, XLM, and DistilBERT with Simple Transformers","Preface

The Simple Transformers library is built on top of the excellent Transformers library by Hugging Face. You guys are incredible!

Simple Transformers now supports:

There’s plenty more in the pipeline.

Introduction

Transformer models and Transfer Learning methods continue to propel the field of Natural Language Processing forward at a tremendous pace. However, state-of-the-art performance too often comes at the price of tons of (complex) code.

Simple Transformers avoids all the complexity and lets you get down to what matters, training and using Transformer models. Bypass all the complicated setups, boilerplates, and other general unpleasantness to initialize a model in one line, train in the next, and evaluate with the third.

This guide shows how you can use Simple Transformers to perform Multilabel Classification. In Multilabel Classification, each sample can have any combination (none, one, some, or all) of labels from a given set of labels.

All source code is available on the Github Repo. If you have any issues or questions, that’s the place to resolve them. Please do check it out!

Installation"
Interpretation of Kappa Values,"Interpretation of Kappa Values

The kappa statistic is frequently used to test interrater reliability. The importance of rater reliability lies in the fact that it represents the extent to which the data collected in the study are correct representations of the variables measured. Measurement of the extent to which data collectors (raters) assign the same score to the same variable is called interrater reliability. In 1960, Jacob Cohen critiqued the use of percent agreement due to its inability to account for chance agreement. He introduced the Cohen’s kappa, developed to account for the possibility that raters actually guess on at least some variables due to uncertainty. The scale of Kappa value interpretation is the as following:

Kappa value interpretation Landis & Koch (1977):

<0 No agreement

0 — .20 Slight

.21 — .40 Fair

.41 — .60 Moderate

.61 — .80 Substantial

.81–1.0 Perfect

However past researches indicated that multiple factors have influences on Kappa value: observer accuracy, # of code in the set, the prevalence of specific codes, observer bias, observer independence (Bakeman & Quera, 2011). As a result, interpretations of Kappa, including definitions of what constitutes a good kappa should take circumstances into account.

Simulation

To better understand the conditional interpretation of Cohen’s Kappa Coefficient, I followed the computation method of Cohen’s Kappa Coefficient proposed by Bakeman et al. (1997). The computations make the simplifying assumptions that both observers were equally accurate and unbiased, that codes were detected with equal accuracy, that disagreement was equally likely, and that when prevalence varied, it did so with evenly graduated probabilities (Bakeman & Quera, 2011).

Settings

The maximum number of codes: 52

The number of observers: 2

The range of observer accuracies: 0.8, 0.85, 0.9, 0.95

The code prevalence: Equiprobable, Moderately Varied, and Highly Varied

Settings of parameters in the simulation

Findings

In the 612 simulation results, 245 (40%) made a perfect level, 336 (55%) fall into substantial, 27 (4%) in moderate level, 3 (1%) in fair level, and 1 (0%) in slightly. For each observer accuracy (.80, .85, .90, .95), there are 51 simulations for each prevalence level.

Observer Accuracy

The higher the observer accuracy, the better overall agreement level. The ratio of agreement level in each prevalence level at various observer accuracies. The agreement level is primarily depended on the observer accuracy, then, code prevalence. The “perfect” agreement only occurs at observer accuracy .90 and .95, while all categories achieve a majority of substantial agreement and above.

Kappa and Agreement Level of Cohen’s Kappa Coefficient

Observer Accuracy influences the maximum Kappa value. As shown in the simulation results, starting with 12 codes and onward, the values of Kappa appear to reach an asymptote of approximately .60, .70, .80, and .90 percent accurate, respectively.

Cohen’s Kappa Coefficient vs Number of codes

Number of code in the observation

Increasing the number of codes results in a gradually smaller increment in Kappa. When the number of codes is less than five, and especially when K = 2, lower values of Kappa are acceptable, but prevalence variability also needs to be considered. For only two codes, the highest kappa value is .80 from observers with accuracy .95, and the lowest is kappa value is .02 from observers with accuracy .80.

The greater the number of codes, the more resilience Kappa value is toward the observer accuracy difference. There is a decrement of Kappa value when the gap between observer accuracies gets bigger

Prevalence of individual codes

The higher the prevalence level, the lower the overall agreement level. It is a tendency that the agreement level shifts lower when prevalence becomes higher. At observer accuracy level .90, there are 33, 32, and 29 perfect agreement for equiprobable, moderately variable, and extremely variable.

Standard Deviation of Kappa Value vs Number of codes

Code prevalence matters little along with the increase of code number. When the number of codes is 6 or higher, prevalence variability matters little, and the standard deviation of kappa values obtained from observers with accuracies .80, .85, .90 and .85 is less than 0.01.

Recommendation

Recommendation of interpreting Kappa along with the number of codes

Factors that affect values of kappa include observer accuracy and the number of codes, as well as codes’ individual population prevalence and observer bias. Kappa can equal 1 only when observers distribute codes equally. There is no one value of kappa that can be regarded as universally acceptable; it depends on the level of observers accuracy and the number of codes.

With a fewer number of codes (K < 5), epically in binary classification, Kappa value needs to be interpreted with extra cautious. In binary classification, prevalence variability has the strongest impact on Kappa value and leads to the same Kappa value for various observer accuracy vs prevalence variability combination.

On the other hand, when there are more than 12 codes, the increment of expected Kappa value becomes flat. Hence simply calculate the percentage of agreement might have already served the purpose of measuring the level of agreement. Moreover, the increment of values of the performance metrics apartment from sensitivity also reaches the asymptote from more than 12 codes.

If Kappa value is used as a reference for observer training, using a code number between 6 to 12 would help on a more accurate performance evaluation. Since Kappa value and the performance metrics are sensitive enough to performance improvement and less impacted by code prevalence.

Reference

Ayoub, A., & Elgammal, A. (2018). Utilizing Twitter Data for Identifying and Resolving Runtime Business Process Disruptions. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). https://doi.org/10.1007/978-3-030-02610-3_11

Bakeman, R., & Quera, V. (2011). Sequential analysis and observational methods for the behavioral sciences. Sequential Analysis and Observational Methods for the Behavioral Sciences. https://doi.org/10.1017/CBO9781139017343

Mchugh, M. L. (2012). Interrater reliability: the kappa statistic Importance of measuring interrater reliability. Biochemia Medica, 22(3), 276–282.

Nichols, T. R., Wisner, P. M., Cripe, G., & Gulabchand, L. (2011). Putting the Kappa Statistic to Use Thomas. Quality Assurance Journal, 57–61. https://doi.org/10.1002/qaj

W.Zhu, N. Z. and N. W. (2010). Sensitivity, Specificity, Accuracy, Associated Confidence Interval and ROC Analysis with Practical SAS®. Proceeding of NESUG Health Care and Life Sciences, 1–9."
5 reasons to learn a new Programming Language in 2020,"Photo by Clément H on Unsplash

Bruce Lee has once famously quoted “I fear not the man who has practiced 10,000 kicks once, but I fear the man who has practiced one kick 10,000 times”. In year 2020, do you need to learn a new programming language or do you need to work on the languages you already know like Bruce Lee has said? Like many other questions, the answer is: it depends.

If you are just starting to learn your first programming language, then I would advise you to first learn that language properly. But if you already know one language properly or you are a seasoned Software Developer who has already master more than one programming language, then I would suggest you learn a new language next year.

Learning a new programming language has a price. It needs lots of your time, energy, and brain cycle. Still learning a new language can give you significant benefits directly or indirectly.

Here I am shortlisting five main advantages you can have if you learn a new programming language:

Bigger Picture

In modern days, programming language landscapes are vast, varied and complex. Usually, one programming language only covers a small part of the programming language landscape. There is a famous story of the blind men touching only part of an elephant and interpret elephant in their way e.g. someone touches the tail of the elephant and interprets elephant like a rope. The same is true for programming languages.

Here are the mainstream programming language paradigms:"
A Data Analysis of the Third Democratic Debate,"A Data Analysis of the Third Democratic Debate

Photo by Kyle Head on Unsplash

In many ways, politics seems to have replaced baseball as the national pastime. Just as sports blogs and ESPN argue about ballgames, so do news sites and pundits on CNN obsess over the minutiae of political campaigns.

In the spirit of politics as sporting event, I thought it would be fun to steal one more trick from that field— sports statistics. Nate Silver and others have perfected the art of poll punditry, but I hope to take it a step further. Just as Sabermetrics gave new insights into the world of baseball, I hope that political data might give us a better view of the political landscape. To that end, I parsed the transcript of the last Democratic debate and analyzed the data to look for trends. Here’s what I found.

Attention Doesn’t Always Follow Polling

In terms of data, the one piece of information that the news does cover is the speaking time. In theory, the more popular candidates should get more time since viewers are more interested in what they have to say. In practice this isn’t exactly the case.

Below are two plots for comparison. On the left is a graph of the latest Morning Consult Poll. On the right is the number of words spoken by each candidate during the September debate.

Left — Democratic Polling : Right — Words Spoke during Sept Debate

As expected, frontrunner Joe Biden got the largest number of words in during the debate. However below him, there are large divergences between polling and speaking time. Sen. Cory Booker who only polls at 3% spoke almost as much as the former Vice President. Granted, most of this was towards the end of the debate. Still, it illustrates how establishment candidates are given favorable treatment on television.

Interestingly, Bernie Sanders, who got a good amount of time was near the bottom of the list in terms of words spoken. It’s clear that he speaks more slowly and uses fewer words than his opponents. I also observed that he was less willing to exceed his allotted time and didn’t need to be cut off as much as the other candidates."
Web Scraping Basics,"Web Scraping Basics

UNESCO is an organization within the United Nations which fights for the preservation of the world’s natural and cultural heritage. There are many heritage sites around the world including ones that contain natural phenomena such as the Great Barrier Reef. Unfortunately, some of the awarded places are threatened by human intervention. We can deconstruct this human intervention problem with the following questions;

1- Which sites are threatened and where are they located?

2- Are there regions in the world where sites are more endangered than in others?

3- What are the reasons that put a site at risk?

We can look at Wikipedia to find the list of places to further solve these questions. When we go to https://www.wikipedia.org/ and search “List of World Heritage in Danger” we end up with https://en.wikipedia.org/wiki/List_of_World_Heritage_in_Danger Wikipedia page. In this page, we can see a table with columns that provide us with feature information of sites that are in danger.

In this article, we are going to use R as the programming language in terms of scraping the data from the web and loading into a dataframe. Please keep in mind, the article only provides basic web scraping techniques to give us an idea around the process and possible web scraping solutions.

We found the data we want to scrape, the next step is to use R to load required into R.

We have a table that we can load to R using readHTMLTable() function from the XML library we have installed earlier.

We are basically telling R that the imported data come in the form of an HTML document. We did this via parser with the function called htmlParse(). With readHTMLTable() function we are telling R to extract all HTML tables it finds in the parsed heritage_parsed object and store them in heritage_tables.

Now let’s select a table that we can use to get closer to answering the business questions that we have established initially. The table that we want to use, should have the site’s name, location, categorical variable of cultural or natural, year of inscription, and year of endangerment.

We were able to get the data table with the required columns we defined earlier. We can perform simple data cleaning. In order to do that, let's look at the structure of the dataframe.

We have 53 sites and 9 variables. We can see the data type of each variable is not accurate, for example, criteria should be categorical variable between natural or cultural, however in this case we have many factors and the data type is recorded as a character string. We can apply basic cleaning methods to our dataset.

Years variable needs to be numeric, some of the year entries are ambiguous as they have several years attached or combined together. Within those values, we select the last given year by using regular expression, something in the lines of [[:digit:]]4$

Let’s also look at the location variable as it looks a lot messier than the year variable.

This variable actually contains three different values within one row, it has the sites location, country and the geographic coordinates in several varieties. What we can do is to get the coordinates for the map. We can use again regular expressions to extract this information.

We were able to retrieve the coordinates and corresponding world heritage sites. This completes the web scraping and data preparation process for further analysis.

The process of web scraping always includes using tools such as parsing and grabbing tables from the web using R or Python Packages. If we want to summarize the technologies for disseminating, extracting and storing web data we can use the below figure.

The most important phase for web scraping is the initial phase where we define the data requirements. What type of data is most suited to answer our business problem in questions? Is the quality of the data sufficiently high to answer our question? Is the information systematically flawed?

When we grab any data from web, we need to keep in mind the roots of the data. The data might have been collected as part of first party data collection or it can be a secondhand data such as a Twitter post or data that was gathered in an offline environment and posted online manually. However, even if we can't find the source of the data, it makes complete sense for us to use the data that is on the web. The data quality depends on the users purpose and application of the data.

Web Scraping Basics have 5 steps that we can define and follow.

1- Figure out what kind of information you need.

2- Find out if you can find data sources on the web that can provide you with the answer to the business problem.

3- Create a theory of data generation process when trying to figure out the data sources. (Example: The data set is coming from a sampling program or survey etc…)

4- Outline the advantages and disadvantages of the data sources. (Make sure it is legal!)

5- Create a decision and if feasible collect the data from different sources to further combine."
Models as Serverless Functions,"Models as Serverless Functions

I recently published Chapter 3 of my book-in-progress on leanpub. The goal with this chapter is to empower data scientists to leverage managed services to deploy models to production and own more of DevOps.

Serverless technologies enable developers to write and deploy code without needing to worry about provisioning and maintaining servers. One of the most common uses of this technology is serverless functions, which makes it much easier to author code that can scale to match variable workloads. With serverless function environments, you write a function that the runtime supports, specify a list of dependencies, and then deploy the function to production. The cloud platform is responsible for provisioning servers, scaling up more machines to match demand, managing load balancers, and handling versioning. Since we’ve already explored hosting models as web endpoints, serverless functions are an excellent tool to utilize when you want to rapidly move from prototype to production for your predictive models.

Serverless functions were first introduced on AWS in 2015 and GCP in 2016. Both of these systems provide a variety of triggers that can invoke functions and a number of outputs that the functions can trigger in response. While it’s possible to use serverless functions to avoid writing complex code for gluing different components together in a cloud platform, we’ll explore a much narrower use case in this chapter. We’ll write serverless functions that are triggered by an HTTP request, calculate a propensity score for the passed in feature vector, and return the prediction as JSON. For this specific use case, GCP’s Cloud Functions are much easier to get up and running, but we’ll explore both AWS and GCP solutions.

In this chapter, we’ll introduce the concept of managed services, where the cloud platform is responsible for provisioning servers. Next, we’ll cover hosting sklearn and Keras models with Cloud Functions. To conclude, we’ll show how to achieve the same result for sklearn models with Lambda functions in AWS. We’ll also touch on model updates and access control.

3.1 Managed Services

Since 2015, there’s been a movement in cloud computing to transition developers away from manually provisioning servers to using managed services that abstract away the concept of servers. The main benefit of this new paradigm is that developers can write code in a staging environment and then push code to production with minimal concerns about operational overhead, and the infrastructure required to match the required workload can be automatically scaled as needed. This enables both engineers and data scientists to be more active in DevOps, because much of the operational concerns of the infrastructure are managed by the cloud provider.

Manually provisioning servers, where you ssh into the machines to set up libraries and code, is often referred to as hosted deployments, versus managed solutions where the cloud platform is responsible for abstracting away this concern from the user. In this book, we’ll cover examples in both of these categories. Here are some of the different use cases we’ll cover:

Web Endpoints: Single EC2 instance (hosted) vs AWS Lambda (managed)

Single EC2 instance (hosted) vs AWS Lambda (managed) Docker: Single EC2 instance (hosted) vs ECS (managed)

Single EC2 instance (hosted) vs ECS (managed) Messaging: Kafka (hosted) vs AWS Kinesis (managed)

This chapter will walk through the first use case, migrating web endpoints from a single machine to an elastic environment. We’ll also work through examples that thread this distinction, such as deploying Spark environments with specific machine configurations and manual cluster management.

Serverless technologies and managed services are a powerful tool for data scientists because they enable a single developer to build data pipelines that can scale to massive workloads. It’s a powerful tool for data scientists to wield, but there are a few tradeoffs to consider when using managed services. Here are some of the main issues to consider when deciding between hosted and managed solutions:

Iteration: Are you rapidly prototyping on a product or iterating on a system in production?

Are you rapidly prototyping on a product or iterating on a system in production? Latency: Is a multi-second latency acceptable for your SLAs?

Is a multi-second latency acceptable for your SLAs? Scale: Can your system scale to match peak workload demands?

Can your system scale to match peak workload demands? Cost: Are you willing to pay more for serverless cloud costs?

At a startup, serverless technologies are great because you have low-volume traffic and have the ability to quickly iterate and try out new architectures. At a certain scale, the dynamics change and the cost of using serverless technologies may be less appealing when you already have in-house expertise for provisioning cloud services. In my past projects, the top issue that was a concern was latency, because it can impact customer experiences. In chapter 8, we’ll touch on this topic, because managed solutions often do not scale well to large workloads.

Even if your organization does not use managed services in daily operations, it’s a useful skillset to get hands on with as a data scientist, because it means that you can separate model training from model deployment issues. One of the themes in this book is that models do not need to be complex, but it can be complex to deploy models. Serverless functions are a great approach for demonstrating the ability to serve models at scale, and we’ll walk through two cloud platforms that provide this capability.

3.2 Cloud Functions (GCP)

Google Cloud Platform provides an environment for serverless functions called Cloud Functions. The general concept with this tool is that you can write code targeted for Flask, but leverage the managed services in GCP to provide elastic computing for your Python code. GCP is a great environment to get started with serverless functions, because it closely matches standard Python development ecosystems, where you specify a requirements file and application code.

We’ll build scalable endpoints that serve both sklearn and Keras models with Cloud Functions. There are a few issues to be aware of when writing functions in this environment:

Storage: Cloud Functions run in a read-only environment, but you can write to the /tmp directory.

Cloud Functions run in a read-only environment, but you can write to the directory. Tabs: Spaces versus tabs can cause issues in Cloud Functions, and if you are working in the web editor versus familiar tools like Sublime Text, these can be difficult to spot.

Spaces versus tabs can cause issues in Cloud Functions, and if you are working in the web editor versus familiar tools like Sublime Text, these can be difficult to spot. sklearn: When using a requirements file, it’s important to differentiate between sklearn and scikit-learn based on your imports. We’ll use sklearn in this chapter.

Cloud platforms are always changing, so the specific steps outlined in this chapter may change based on the evolution of these platforms, but the general approach for deploying functions should apply throughout these updates. As always, the approach I advocate for is starting with a simple example, and then scaling to more complex solutions as needed. In this section, we’ll first build an echo service and then explore sklearn and Keras models.

3.2.1 Echo Service

GCP provides a web interface for authoring Cloud Functions. This UI provides options for setting up the triggers for a function, specifying the requirements file for a Python function, and authoring the implementation of the Flask function that serves the request. To start, we’ll set up a simple echo service that reads in a parameter from an HTTP request and returns the passed in parameter as the result.

In GCP, you can directly set up a Cloud Function as an HTTP endpoint without needing to configure additional triggers. To get started with setting up an echo service, perform the following actions in the GCP console:

Search for “Cloud Function” Click on “Create Function” Select “HTTP” as the trigger Select “Allow unauthenticated invocations” Select “Inline Editor” for source code Select Python 3.7 as the runtime

An example of this process is shown in Figure 3.1. After performing these steps, the UI will provide tabs for the main.py and requirements.txt files. The requirements file is where we will specify libraries, such as flask >= 1.1.1 , and the main file is where we’ll implement our function behavior.

FIGURE 3.1: Creating a Cloud Function.

We’ll start by creating a simple echo service that parses out the msg parameter from the passed in request and returns this parameter as a JSON response. In order to use the jsonify function we need to include the flask library in the requirements file. The requirements.txt file and main.py files for the simple echo service are shown in the snippet below. The echo function here is similar to the echo service we coded in Section 2.1.1, the main distinction here is that we are no longer using annotations to specify the endpoints and allowed methods. Instead, these settings are now being specified using the Cloud Functions UI.

# requirements.txt

flask #main.py

def echo(request):

from flask import jsonify data = {""success"": False}

params = request.get_json() if ""msg"" in params:

data[""response""] = str(params['msg'])

data[""success""] = True



return jsonify(data)

We can deploy the function to production by performing the following steps:

Update “Function to execute” to “echo” Click “Create” to deploy

Once the function has been deployed, you can click on the “Testing” tab to check if the deployment of the function worked as intended. You can specifying a JSON object to pass to the function, and invoke the function by clicking “Test the function”, as shown in Figure 3.2. The result of running this test case is the JSON object returned in the Output dialog, which shows that invoking the echo function worked correctly.

FIGURE 3.2: Testing a Cloud Function.

Now that the function is deployed and we enabled unauthenticated access to the function, we can call the function over the web using Python. To get the URL of the function, click on the “trigger” tab. We can use the requests library to pass a JSON object to the serverless function, as shown in the snippet below.

import requests result = requests.post(

""https://us-central1-gameanalytics.cloudfunctions.net/echo""

,json = { 'msg': 'Hello from Cloud Function' })

print(result.json())

The result of running this script is that a JSON payload is returned from the serverless function. The output from the call is the JSON shown below.

{

'response': 'Hello from Cloud Function',

'success': True

}

We now have a serverless function that provides an echo service. In order to serve a model using Cloud Functions, we’ll need to persist the model specification somewhere that the serverless function can access. To accomplish this, we’ll use Cloud Storage to store the model in a distributed storage layer.

3.2.2 Cloud Storage (GCS)

GCP provides an elastic storage layer called Google Cloud Storage (GCS) that can be used for distributed file storage and can also scale to other uses such as data lakes. In this section, we’ll explore the first use case of utilizing this service to store and retrieve files for use in a serverless function. GCS is similar to AWS’s offering called S3, which is leveraged extensively in the gaming industry to build data platforms.

While GCP does provide a UI for interacting with GCS, we’ll explore the command line interface in this section, since this approach is useful for building automated workflows. GCP requires authentication for interacting with this service, please revisit section 1.1 if you have not yet set up a JSON credentials file. In order to interact with Cloud Storage using Python, we’ll also need to install the GCS library, using the command shown below:

pip install --user google-cloud-storage

export GOOGLE_APPLICATION_CREDENTIALS=/home/ec2-user/dsdemo.json

Now that we have the prerequisite libraries installed and credentials set up, we can interact with GCS programmatically using Python. Before we can store a file, we need to set up a bucket on GCS. A bucket is a prefix assigned to all files stored on GCS, and each bucket name must be globally unique. We’ll create a bucket name called dsp_model_store where we’ll store model objects. The script below shows how to create a new bucket using the create_bucket function and then iterate through all of the available buckets using the list_buckets function. You’ll need to change the bucket_name variable to something unique before running this script.

from google.cloud import storage

bucket_name = ""dsp_model_store"" storage_client = storage.Client()

storage_client.create_bucket(bucket_name) for bucket in storage_client.list_buckets():

print(bucket.name)

After running this code, the output of the script should be a single bucket, with the name assigned to the bucket_name variable. We now have a path on GCS that we can use for saving files: gs://dsp_model_storage .

We’ll reuse the model we trained in Section 2.2.1 to deploy a logistic regression model with Cloud Functions. To save the file to GCS, we need to assign a path to the destination, shown by the bucket.blob command below and select a local file to upload, which is passed to the upload function.

from google.cloud import storage bucket_name = ""dsp_model_store""

storage_client = storage.Client()

bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(""serverless/logit/v1"")

blob.upload_from_filename(""logit.pkl"")

After running this script, the local file logit.pkl will now be available on GCS at the following location:

gs://dsp_model_storage/serverless/logit/v1/logit.pkl

While it’s possible to use URIs such as this directly to access files, as we’ll explore with Spark in Chapter 6, in this section we’ll retrieve the file using the bucket name and blob path. The code snippet below shows how to download the model file from GCS to local storage. We download the model file to the local path of local_logit.pkl and then load the model by calling pickle.load with this path.

import pickle

from google.cloud import storage bucket_name = ""dsp_model_store""

storage_client = storage.Client()

bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(""serverless/logit/v1"")

blob.download_to_filename(""local_logit.pkl"")

model = pickle.load(open(""local_logit.pkl"", 'rb'))

model

We now can programmatically store model files to GCS using Python and also retrieve them, enabling us to load model files in Cloud Functions. We’ll combine this with the Flask examples from the previous chapter to serve sklearn and Keras models as Cloud Functions.

3.2.3 Model Function

We can now set up a Cloud Function that serves logistic regression model predictions over the web. We’ll build on the Flask example that we explored in Section 2.3.1 and make a few modifications for the service to run on GCP. The first step is to specify the required Python libraries that we’ll need to serve requests in the requirements.txt file, as shown below. We’ll also need pandas to set up a DataFrame for making the prediction, sklearn for applying the model, and cloud storage for retrieving the model object from GCS.

google-cloud-storage

sklearn

pandas

flask

The next step is to implement our model function in the main.py file. A small change from before is that the params object is now fetched using request.get_json() rather than flask.request.args . The main change is that we are now downloading the model file from GCS rather than retrieving the file directly from local storage, because local files are not available when writing Cloud Functions with the UI tool. An additional change from the prior function is that we are now reloading the model for every request, rather than loading the model file once at startup. In a later code snippet, we’ll show how to use global objects to cache the loaded model.

def pred(request):

from google.cloud import storage

import pickle as pk

import sklearn

import pandas as pd

from flask import jsonify data = {""success"": False}

params = request.get_json() if ""G1"" in params:



new_row = { ""G1"": params.get(""G1""),""G2"": params.get(""G2""),

""G3"": params.get(""G3""),""G4"": params.get(""G4""),

""G5"": params.get(""G5""),""G6"": params.get(""G6""),

""G7"": params.get(""G7""),""G8"": params.get(""G8""),

""G9"": params.get(""G9""),""G10"":params.get(""G10"")} new_x = pd.DataFrame.from_dict(new_row,

orient = ""index"").transpose()



# set up access to the GCS bucket

bucket_name = ""dsp_model_store""

storage_client = storage.Client()

bucket = storage_client.get_bucket(bucket_name) # download and load the model

blob = bucket.blob(""serverless/logit/v1"")

blob.download_to_filename(""/tmp/local_logit.pkl"")

model = pk.load(open(""/tmp/local_logit.pkl"", 'rb'))



data[""response""] = str(model.predict_proba(new_x)[0][1])

data[""success""] = True



return jsonify(data)

One note in the code snippet above is that the /tmp directory is used to store the downloaded model file. In Cloud Functions, you are unable to write to the local disk, with the exception of this directory. Generally it’s best to read objects directly into memory rather than pulling objects to local storage, but the Python library for reading objects from GCS currently requires this approach.

For this function, I created a new Cloud Function named pred , set the function to execute to pred , and deployed the function to production. We can now call the function from Python, using the same approach from 2.3.1 with a URL that now points to the Cloud Function, as shown below:

import requests result = requests.post(

""https://us-central1-gameanalytics.cloudfunctions.net/pred""

,json = { 'G1':'1', 'G2':'0', 'G3':'0', 'G4':'0', 'G5':'0'

,'G6':'0', 'G7':'0', 'G8':'0', 'G9':'0', 'G10':'0'})

print(result.json())

The result of the Python web request to the function is a JSON response with a response value and model prediction, shown below:

{

'response': '0.06745113592634559',

'success': True

}

In order to improve the performance of the function, so that it takes milliseconds to respond rather than seconds, we’ll need to cache the model object between runs. It’s best to avoid defining variables outside of the scope of the function, because the server hosting the function may be terminated due to inactivity. Global variables are an execution to this rule, when used for caching objects between function invocations. This code snippet below shows how a global model object can be defined within the scope of the pred function to provide a persistent object across calls. During the first function invocation, the model file will be retrieved from GCS and loaded via pickle. During following runs, the model object will already be loaded into memory, providing a much faster response time.

model = None



def pred(request):

global model



if not model:

# download model from GCS

model = pk.load(open(""/tmp/local_logit.pkl"", 'rb')) if ""G1"" in params:

# apply model return jsonify(data)

Caching objects is important for authoring responsive models that lazily load objects as needed. It’s also useful for more complex models, such as Keras which requires persisting a TensorFlow graph between invocations.

3.2.4 Keras Model

Since Cloud Functions provide a requirements file that can be used to add additional dependencies to a function, it’s also possible to serve Keras models with this approach. We’ll be able to reuse most of the code from the past section, and we’ll also use the Keras and Flask approach introduced in Section 2.3.2. Given the size of the Keras libraries and dependencies, we’ll need to upgrade the memory available for the Function from 256 MB to 1GB. We also need to update the requirements file to include Keras:

google-cloud-storage

tensorflow

keras

pandas

flask

The full implementation for the Keras model as a Cloud Function is shown in the code snippet below. In order to make sure that the TensorFlow graph used to load the model is available for future invocations of the model, we use global variables to cache both the model and graph objects. To load the Keras model, we need to redefine the auc function that was used during model training, which we include within the scope of the predict function. We reuse the same approach from the prior section to download the model file from GCS, but now use load_model from Keras to read the model file into memory from the temporary disk location. The result is a Keras predictive model that lazily fetches the model file and can scale to meet variable workloads as a serverless function.

model = None

graph = None def predict(request):

global model

global graph



from google.cloud import storage

import pandas as pd

import flask

import tensorflow as tf

import keras as k

from keras.models import load_model

from flask import jsonify



def auc(y_true, y_pred):

auc = tf.metrics.auc(y_true, y_pred)[1]

k.backend.get_session().run(

tf.local_variables_initializer())

return auc



data = {""success"": False}

params = request.get_json() # download model if now cached

if not model:

graph = tf.get_default_graph()



bucket_name = ""dsp_model_store_1""

storage_client = storage.Client()

bucket = storage_client.get_bucket(bucket_name) blob = bucket.blob(""serverless/keras/v1"")

blob.download_to_filename(""/tmp/games.h5"")

model = load_model('/tmp/games.h5',

custom_objects={'auc':auc})



# apply the model

if ""G1"" in params:

new_row = { ""G1"": params.get(""G1""),""G2"": params.get(""G2""),

""G3"": params.get(""G3""),""G4"": params.get(""G4""),

""G5"": params.get(""G5""),""G6"": params.get(""G6""),

""G7"": params.get(""G7""),""G8"": params.get(""G8""),

""G9"": params.get(""G9""),""G10"":params.get(""G10"")} new_x = pd.DataFrame.from_dict(new_row,

orient = ""index"").transpose()



with graph.as_default():

data[""response""]= str(model.predict_proba(new_x)[0][0])

data[""success""] = True



return jsonify(data)

To test the deployed model, we can reuse the Python web request script from the prior section and replace pred with predict in the request URL. We have now deployed a deep learning model to production.

3.2.5 Access Control

The Cloud Functions we introduced in this chapter are open to the web, which means that anyone can access them and potentially abuse the endpoints. In general, it’s best not to enable unauthenticated access and instead lock down the function so that only authenticated users and services can access them. This recommendation also applies to the Flask apps that we deployed in the last chapter, where it’s a best practice to restrict access to services that can reach the endpoint using AWS private IPs.

There are a few different approaches for locking down Cloud Functions to ensure that only authenticated users have access to the functions. The easiest approach is to disable “Allow unauthenticated invocations” in the function setup to prevent hosting the function on the open web. To use the function, you’ll need to set up IAM roles and credentials for the function. This process involves a number of steps and may change over time as GCP evolves. Instead of walking through this process, it’s best to refer to the GCP documentation.

Another approach for setting up functions that enforce authentication is by using other services within GCP. We’ll explore this approach in Chapter 8, which introduces GCP’s PubSub system for producing and consuming messages within GCP’s ecosystem.

3.2.6 Model Refreshes

We’ve deployed sklearn and Keras models to production using Cloud Functions, but the current implementations of these functions use static model files that will not change over time. It’s usually necessary to make changes to models over time to ensure that the accuracy of the models do not drift too far from expected performance. There’s a few different approaches that we can take to update the model specification that a Cloud Function is using:

Redeploy: Overwrite the model file on GCS and redeploy the function will result in the function loading the updated file. Timeout: We can add a timeout to the function, where the model is re-downloaded after a certain threshold of time passes, such as 30 minutes. New Function: We can deploy a new function, such as pred_v2 and update the URL used by systems calling the service, or use a load balancer to automate this process. Model Trigger: We can add additional triggers to the function to force the function to manually reload the model.

While the first approach is the easiest to implement and can work well for small-scale deployments, the third approach, where a load balancer is used to direct calls to the newest function available is probably the most robust approach for production systems. A best practice is to add logging to your function, in order to track predictions over time so that you can log the performance of the model and identify potential drift.

3.3 Lambda Functions (AWS)

AWS also provides an ecosystem for serverless functions called Lambda. AWS Lambda is useful for glueing different components within an AWS deployment together, since it supports a rich set of triggers for function inputs and outputs. While Lambda does provide a powerful tool for building data pipelines, the current Python development environment is a bit clunkier than GCP.

In this section we’ll walk through setting up an echo service and an sklearn model endpoint with Lambda. We won’t cover Keras, because the size of the library causes problems when deploying a function with AWS. Unlike the past section where we used a UI to define functions, we’ll use command line tools for providing our function definition to Lambda.

3.3.1 Echo Function

For a simple function, you can use the inline code editor that Lambda provides for authoring functions. You can create a new function by performing the following steps in the AWS console:

Under “Find Services”, select “Lambda” Select “Create Function” Use “Author from scratch” Assign a name (e.g. echo) Select a Python runtime Click “Create Function”

After running these steps, Lambda will generate a file called lambda_function.py . The file defines a function called lambda_handler which we’ll use to implement the echo service. We’ll make a small modification to the file, as shown below, which echoes the msg parameter as the body of the response object.

def lambda_handler(event, context): return {

'statusCode': 200,

'body': event['msg']

}

Click “Save” to deploy the function and then “Test” to test the file. If you use the default test parameters, then an error will be returned when running the function, because no msg key is available in the event object. Click on “Configure test event”, and define use the following configuration:

{

""msg"": ""Hello from Lambda!""

}

After clicking on “Test”, you should see the execution results. The response should be the echoed message with a status code of 200 returned. There’s also details about how long the function took to execute (25.8ms), the billing duration (100ms), and the maximum memory used (56 MB).

We have now a simple function running on AWS Lambda. For this function to be exposed to external systems, we’ll need to set up an API Gateway, which is covered in Section 3.3.3. This function will scale up to meet demand if needed, and requires no server monitoring once deployed. To setup a function that deploys a model, we’ll need to use a different workflow for authoring and publishing the function, because AWS Lambda does not currently support a requirements.txt file for defining dependencies when writing functions with the inline code editor. To store the model file that we want to serve with a Lambda function, we’ll use S3 as a storage layer for model artifacts.

3.3.2 Simple Storage Service (S3)

AWS provides a highly-performant storage layer called S3, which can be used to host individual files for web sites, store large files for data processing, and even host thousands or millions of files for building data lakes. For now, our use case will be storing an individual zip file, which we’ll use to deploy new Lambda functions. However, there are many broader use cases and many companies use S3 as their initial endpoint for data ingestion in data platforms.

In order to use S3 to store our function to deploy, we’ll need to set up a new S3 bucket, define a policy for accessing the bucket, and configure credentials for setting up command line access to S3. Buckets on S3 are analogous to GCS buckets in GCP.

To set up a bucket, browse to the AWS console and select “S3” under find services. Next, select “Create Bucket” to set up a location for storing files on S3. Create a unique name for the S3 bucket, as shown in Figure 3.3, and click “Next” and then “Create Bucket” to finalize setting up the bucket.

FIGURE 3.3: Creating an S3 bucket on AWS.

We now have a location to store objects on S3, but we still need to set up a user before we can use the command line tools to write and read from the bucket. Browse to the AWS console and select “IAM” under “Find Services”. Next, click “Users” and then “Add user” to set up a new user. Create a user name, and select “Programmatic access” as shown in Figure 3.4.

FIGURE 3.4: Setting up a user with S3 access.

The next step is to provide the user with full access to S3. Use the attach existing policies option and search for S3 policies in order to find and select the AmazonS3FullAccess policy, as shown in Figure 3.5. Click “Next” to continue the process until a new user is defined. At the end of this process, a set of credentials will be displayed, including an Access key ID and Secret access key. Store these values in a safe location.

FIGURE 3.5: Selecting a policy for full S3 access.

The last step needed for setting up command line access to S3 is running the aws configure command from your EC2 instance. You’ll be asked to provide the access and secret keys from the user we just set up. In order to test that the credentials are properly configured, you can run the following commands:

aws configure

aws s3 ls

The results should include the name of the S3 bucket we set up at the beginning of this section. Now that we have an S3 bucket set up with command line access, we can begin writing Lambda functions that use additional libraries such as pandas and sklearn.

3.3.3 Model Function

In order to author a Lambda function that uses libraries outside of the base Python distribution, you’ll need to set up a local environment that defines the function and includes all of the dependencies. Once your function is defined, you can upload the function by creating a zip file of the local environment, uploading the resulting file to S3, and configuring a Lambda function from the file uploaded to S3.

The first step in this process is to create a directory with all of the dependencies installed locally. While it’s possible to perform this process on a local machine, I used an EC2 instance to provide a clean Python environment. The next step is to install the libraries needed for the function, which are pandas and sklearn. These libraries are already installed on the EC2 instance, but need to be reinstalled in the current directory in order to be included in the zip file that we’ll upload to S3. To accomplish this, we can append -t . to the end of the pip command in order to install the libraries into the current directory. The last steps to run on the command line are copying our logistic regression model into the current directory, and creating a new file that will implement the Lambda function.

mkdir lambda

cd lambda

pip install pandas -t .

pip install sklearn -t .

cp ../logit.pkl logit.pkl

vi logit.py

The full source code for the Lambda function that serves our logistic regression model is shown in the code snippet below. The structure of the file should look familiar, we first globally define a model object and then implement a function that services model requests. This function first parses the response to extract the inputs to the model, and then calls predict_proba on the resulting DataFrame to get a model prediction. The result is then returned as a dictionary object containing a body key. It’s important to define the function response within the body key, otherwise Lambda will throw an exception when invoking the function over the web.

from sklearn.externals import joblib

import pandas as pd

import json

model = joblib.load('logit.pkl')



def lambda_handler(event, context): # read in the request body as the event dict

if ""body"" in event:

event = event[""body""]



if event is not None:

event = json.loads(event)

else:

event = {}



if ""G1"" in event:

new_row = { ""G1"": event[""G1""],""G2"": event[""G2""],

""G3"": event[""G3""],""G4"": event[""G4""],

""G5"": event[""G5""],""G6"": event[""G6""],

""G7"": event[""G7""],""G8"": event[""G8""],

""G9"": event[""G9""],""G10"":event[""G10""]} new_x = pd.DataFrame.from_dict(new_row,

orient = ""index"").transpose()

prediction = str(model.predict_proba(new_x)[0][1])



return { ""body"": ""Prediction "" + prediction }



return { ""body"": ""No parameters"" }

Unlike Cloud Functions, Lambda functions authored in Python are not built on top of the Flask library. Instead of requiring a single parameter ( request ), a Lambda function requires event and context objects to be passed in as function parameters. The event includes the parameters of the request, and the context provides information about the execution environment of the function. When testing a Lambda function using the “Test” functionality in the Lambda console, the test configuration is passed directly to the function as a dictionary in the event object. However, when the function is called from the web, the event object is a dictionary that describes the web request, and the request parameters are stored in the body key in this dict. The first step in the Lambda function above checks if the function is being called directly from the console, or via the web. If the function is being called from the web, then the function overrides the event dictionary with the content in the body of the request.

One of the main differences from this approach with the GCP Cloud Function is that we did not need to explicitly define global variables that are lazily defined. With Lambda functions, you can define variables outside the scope of the function that are persisted before the function is invoked. It’s important to load model objects outside of the model service function, because reloading the model each time a request is made can become expensive when handling large workloads.

To deploy the model, we need to create a zip file of the current directory, and upload the file to a location on S3. The snippet below shows how to perform these steps and then confirm that the upload succeeded using the s3 ls command. You’ll need to modify the paths to use the S3 bucket name that you defined in the previous section.

zip -r logitFunction.zip .

aws s3 cp logitFunction.zip s3://dsp-ch3-logit/logitFunction.zip

aws s3 ls s3://dsp-ch3-logit/

Once your function is uploaded as a zip file to S3, you can return to the AWS console and set up a new Lambda function. Select “Author from scratch” as before, and under “Code entry type” select the option to upload from S3, specifying the location from the cp command above. You’ll also need to define the Handler , which is a combination of the Python file name and the Lambda function name. An example configuration for the logit function is shown in Figure 3.6.

FIGURE 3.6: Defining the logit function on AWS Lambda.

Make sure to select the Python runtime as the same version of Python that was used to run the pip commands on the EC2 instance. Once the function is deployed by pressing “Save”, we can test the function using the following definition for the test event.

{

""G1"": ""1"", ""G2"": ""1"", ""G3"": ""1"",

""G4"": ""1"", ""G5"": ""1"",

""G6"": ""1"", ""G7"": ""1"", ""G8"": ""1"",

""G9"": ""1"", ""G10"": ""1""

}

Since the model is loaded when the function is deployed, the response time for testing the function should be relatively fast. An example output of testing the function is shown in Figure 3.7. The output of the function is a dictionary that includes a body key and the output of the model as the value. The function took 110 ms to execute and was billed for a duration of 200 ms.

FIGURE 3.7: Testing the logit function on AWS Lambda.

So far, we’ve invoked the function only using the built-in test functionality of Lambda. In order to host the function so that other services can interact with the function, we’ll need to define an API Gateway. Under the “Designer” tab, click “Add Trigger” and select “API Gateway”. Next, select “Create a new API” and choose “Open” as the security setting. After setting up the trigger, an API Gateway should be visible in the Designer layout, as shown in Figure 3.8.

FIGURE 3.8: Setting up an API Gateway for the function.

Before calling the function from Python code, we can use the API Gateway testing functionality to make sure that the function is set up properly. One of the challenges I ran into when testing this Lambda function was that the structure of the request varies when the function is invoked from the web versus the console. This is why the function first checks if the event object is a web request or dictionary with parameters. When you use the API Gateway to test the function, the resulting call will emulate calling the function as a web request. An example test of the logit function is shown in Figure 3.9.

FIGURE 3.9: Testing post commands on the Lambda function.

Now that the gateway is set up, we can call the function from a remote host using Python. The code snippet below shows how to use a POST command to call the function and display the result. Since the function returns a string for the response, we use the text attribute rather than the json function to display the result.

import requests result = requests.post(""https://3z5btf0ucb.execute-api.us-east-1.

amazonaws.com/default/logit"",

json = { 'G1':'1', 'G2':'0', 'G3':'0', 'G4':'0', 'G5':'0',

'G6':'0', 'G7':'0', 'G8':'0', 'G9':'0', 'G10':'0' }) print(result.text)

We now have a predictive model deployed to AWS Lambda that will autoscale as necessary to match workloads, and which requires minimal overhead to maintain.

Similar to Cloud Functions, there are a few different approaches that can be used to update the deployed models. However, for the approach we used in this section, updating the model requires updating the model file in the development environment, rebuilding the zip file and uploading it to S3, and then deploying a new version of the model. This is a manual process and if you expect frequent model updates, then it’s better to rewrite the function so that it fetches the model definition from S3 directly rather than expecting the file to already be available in the local context. The most scalable approach is setting up additional triggers for the function, to notify the function that it’s time to load a new model.

3.4 Conclusion

Serverless functions are a type of managed service that enable developers to deploy production-scale systems without needing to worry about infrastructure. To provide this abstraction, different cloud platforms do place constraints on how functions must be implemented, but the tradeoff is generally worth the improvement in DevOps that these tools enable. While serverless technologies like Cloud Functions and Lambda can be operationally expensive, they provide flexibility that can offset these costs.

In this chapter, we implemented echo services and sklearn model endpoints using both GCP’s Cloud Functions and AWS’s Lambda offerings. With AWS, we created a local Python environment with all dependencies and then uploading the resulting files to S3 to deploy functions, while in GCP we authored functions directly using the online code editor. The best system to use will likely depend on which cloud provider your organization is already using, but when prototyping new systems, it’s useful to have hands on experience using more than one serverless function ecosystem."
“OK Boomer” escalated quickly — a reddit+BigQuery report,"# of unique reddit accounts per day commenting “OK boomer”

“OK Boomer” escalated quickly — a reddit+BigQuery report

Let’s use BigQuery to find the first time that someone commented “OK Boomer” on reddit. Turns out it happened hours and even years before the alleged first tweet. Was this all an attempt by teenagers to replace the word “old” with “boomer”? Let’s dig in. Felipe Hoffa · Follow 5 min read · Nov 9, 2019 -- 1 Listen Share

Important update: I left Google and joined Snowflake in 2020 — so I’m unable to keep my older posts updated. If you want to try Snowflake, join us — I’m having a lot of fun ❄️.

“OK boomer” growth by subreddit:

# of unique accounts per sub that have commented “OK Boomer”

The first “OK boomer”

According to Know Your Meme this tweet marks the first appearance of “ok boomer”:

The first tweet for “Ok Boomer”. Source: Know Your Meme

And the author has been celebrating it:

But was this the real first appearance? 90 minutes earlier, that same day — April 12, 2018— someone else had immortalized “OK BOOMERS” on reddit:

An “OK boomer” on reddit, 90 minutes before that first tweet.

Turns out there are 9 reddit comments even before that, with “Ok boomer” starting in September 2009:

“OK boomer” reddit prehistory: 9 comments between 2009 and April 12, 2018

And things didn’t escalate quickly back then. It took 9 years to get the first 9 “OK boomer” on reddit, and then we have to jump until October 2018. That’s when we start seeing an almost daily dose of the phrase:

Waiting from April 2018 until October 2018 to find an almost daily dose of “OK Boomer” on reddit.

And it had spread through different subreddits! From /r/Tinder, to /r/worldnews, to /r/xboxone, to /r/golang — redditors started embracing it.

And something happened on March 2019. Until then we had a spotty supply of the phrase, but on March 24 six different accounts decide to comment “OK boomer”. Since that day, the trend continues going up until June 2019, with more than 30 accounts commenting “OK boomer” per day.

The embrace of “OK boomer” from Aug 2018 to Jun 2019 on reddit.

And then we have a gap in our data — you can draw your own guess on how the usage continued escalating to ~200 accounts commenting daily by mid October — with a huge peak on September 9, 2019:

What happened on September 9? This post by /r/teenagers:

Oh yeah. This was a petition by teenagers to replace the world “old” with “boomer” — and 30k upvotes agreed with it.

But for the real explosion in usage, you have to wait until October 30, 2019:

October 30, 2019: 2,640 accounts post “OK boomer” on reddit

That’s a day after the NY Times reported on the phrase:

And what happened next? “OK boomer” escalated quickly. With a peak on November 6, when more than 12k accounts posted the phrase:

And you can find these comments all over reddit:

You can now play with the interactive dashboard, to find all sorts of patterns within these comments:

Play with the interactive report, or load it full size.

How-to

I used two different sources of data:

Historical reddit archives stored in BigQuery, for all comments before June 2019.

Live pushshift.io API for all newer comments. Extracted like described in my previous post.

To extract all of the historical reddit comments, I used this query:

CREATE TABLE `reddit_extracts.201906_all_okboomer`

PARTITION BY fake_date

CLUSTER BY subreddit, ts

AS

SELECT TIMESTAMP_SECONDS(created_utc) ts, *, DATE('2000-01-01') fake_date

FROM `reddit_comments.2*`

WHERE REGEXP_CONTAINS(body, r'(?i)\bok boomer')

AND _table_suffix >= '018_03' ;

INSERT INTO `reddit_extracts.201906_okboomer_all` SELECT TIMESTAMP_SECONDS(created_utc) ts, *, DATE('2000-01-01') fake_date

FROM `reddit_comments.2*`

WHERE REGEXP_CONTAINS(body, r'(?i)\bok boomer')

AND _table_suffix BETWEEN '0' AND '018_02'

Find the shared table here: console.cloud.google.com/bigquery?p=fh-bigquery&d=reddit_extracts&t=201906_okboomer_all&page=table

To create the table summarizing live and archived comments for Data Studio:

CREATE OR REPLACE TABLE `reddit_extracts.201911_okboomer_day`

AS WITH data AS (

SELECT ts, author, subreddit, score, body

, permalink

FROM `reddit_extracts.201910_live_okboomer`

UNION ALL

SELECT ts, author, subreddit, score, body

, CONCAT('/r/',subreddit,'/comments/', REGEXP_REPLACE(link_id, 't3_', ''), '//', id, '/') permalink

FROM `reddit_extracts.201906_okboomer_all`

) SELECT day, authors

, (SELECT STRING_AGG(value, ', ') FROM UNNEST(top_reddit)) top_subs

, top_comm

FROM (

SELECT TIMESTAMP_TRUNC(ts, DAY) day, COUNT(DISTINCT author) authors

, APPROX_TOP_COUNT(subreddit, 3) top_reddit

, ARRAY_AGG(STRUCT(score, subreddit, permalink, body) ORDER BY score DESC LIMIT 1)[OFFSET(0)] top_comm

FROM data

WHERE body NOT IN ('[deleted]', '[removed]')

AND REGEXP_CONTAINS(body, r'(?i)\bok boomer[s]?\b')

AND ts < TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(), DAY)

GROUP BY 1

)

ORDER BY 1 DESC

The animated chart

I used a Data Studio custom visualization created by Michael Whitaker.

The query to count uniques uses some HLL magic for efficiency:

CREATE OR REPLACE TABLE `reddit_extracts.201911_okboomer_subgrowth`

AS WITH data AS (

SELECT ts, author, subreddit, score, body

, permalink

FROM `reddit_extracts.201910_live_okboomer`

UNION ALL

SELECT ts, author, subreddit, score, body

, CONCAT('/r/',subreddit,'/comments/', REGEXP_REPLACE(link_id, 't3_', ''), '//', id, '/') permalink

FROM `reddit_extracts.201906_okboomer_all`

)

, sketches AS (

SELECT TIMESTAMP_TRUNC(ts, DAY) day

, subreddit

, HLL_COUNT.INIT(author) sketch

, COUNT(DISTINCT author) authors

FROM data

WHERE body NOT IN ('[deleted]', '[removed]')

AND REGEXP_CONTAINS(body, r'(?i)\bok boomer[s]?\b')

GROUP BY 1,2

), notable_early AS (

SELECT DISTINCT subreddit

FROM data

WHERE ts < '2018-04-15'

), overall_notable AS (

SELECT subreddit, COUNT(DISTINCT author) c

FROM data

GROUP BY 1

ORDER BY c DESC

LIMIT 30

) SELECT *

FROM (

SELECT *, IFNULL(authors - LAG(authors) OVER(PARTITION BY subreddit ORDER BY day), authors) authors_diff

FROM (

SELECT a.day

, CASE

WHEN subreddit IN (SELECT * from notable_early UNION ALL (SELECT subreddit from overall_notable)) THEN subreddit

-- ELSE '[other]'

END AS subreddit

, COUNT(*) c, HLL_COUNT.MERGE(sketch) authors

FROM (

SELECT DISTINCT CASE

WHEN day< '2019-07-01' THEN TIMESTAMP_TRUNC(day, QUARTER)

WHEN day< '2019-09-01' THEN TIMESTAMP_TRUNC(day, MONTH)

WHEN day< '2019-10-30' THEN TIMESTAMP_TRUNC(day, WEEK)

ELSE day

END AS day

FROM sketches) a

JOIN sketches b

ON b.day<=a.day

GROUP BY 1,2

)

)

WHERE subreddit IS NOT null

Want more?

Check my previous post for more details on collecting live data from pushshift.io. Thanks Jason Baumgartner for the constant supply of data!

I’m Felipe Hoffa, a Developer Advocate for Google Cloud. Follow me on @felipehoffa, find my previous posts on medium.com/@hoffa, and all about BigQuery on reddit.com/r/bigquery."
The Competition Mindset: how Kaggle and real-life Data Science diverge,"The Competition Mindset: how Kaggle and real-life Data Science diverge

Major thanks to Rachael and Will from Kaggle for reviewing and providing recommendations on this article.

TLDR: Kaggle competitions are great for Machine Learning education. But it can set wrong expectations about what to expect in real-life business setting. I go through 3 ways Data Science in industry environment is different from competitions, and how it’s is important to align expectations with the reality of a Data Science job.

Intro: The Competition Mindset

Kaggle is a very popular platform that’s great for aspiring Data Scientists to pick up skills on applied Machine Learning: preprocessing, pipelining, algorithm optimization. However, I believe it can also impact those trying to understand and learn real-world Data Science. By real-world, I strictly mean real-life industry setting where Data Science is adopted to meet business aims.

Many career-seekers are currently trying to break into the Data Science field. Some look at Kaggle competitions to jumpstart and guide their new ML journey. Given this trend, I believe that it is critical to distinguish what Kaggle competition is and what it is not. While Kaggle has great educational benefits, I believe that it can also develop what I call the Competition Mindset.

The Competition Mindset: (noun) the mindset that techniques, objectives, and processes prioritized in Kaggle competitions reflect those in a typical business environment.

The biggest challenge with this mindset is that there are core fundamental differences between Data Science used in Kaggle competitions and that practiced in an industry setting (what I call Industry Data Science). I will go into three specific reasons why this is the…"
A Simple CNN: Multi Image Classifier,"A Simple CNN: Multi Image Classifier

Using Tensorflow and transfer learning, easily make a labeled image classifier with convolutional neural network Iftekher Mamun · Follow Published in Towards Data Science · 11 min read · Apr 7, 2019 -- 10 Listen Share

Computer vision and neural networks are the hot new IT of machine learning techniques. With advances of neural networks and an ability to read images as pixel density numbers, numerous companies are relying on this technique for more data. For example, speed camera uses computer vision to take pictures of license plate of cars who are going above the speeding limit and match the license plate number with their known database to send the ticket to. Although this is more related to Object Character Recognition than Image Classification, both uses computer vision and neural networks as a base to work.

A more realistic example of image classification would be Facebook tagging algorithm. When you upload an album with people in them and tag them in Facebook, the tag algorithm breaks down the person’s picture pixel location and store it in the database. Because each picture has its own unique pixel location, it is relatively easy for the algorithm to realize who is who based on previous pictures located in the database. Of course the algorithm can make mistake from time to time, but the more you correct it, the better it will be at identifying your friends and automatically tag them for you when you upload. However, the Facebook tag algorithm is built with artificial intelligence in mind. This means that the tagging algorithm is capable of learning based on our input and make better classifications in the future.

We will not focus on the AI aspect, but rather on the simplest way to make an image classification algorithm. The only difference between our model and Facebook’s will be that ours cannot learn from it’s mistake unless we fix it. However, for a simple neural network project, it is sufficient.

Since it is unethical to use pictures of people, we will be using animals to create our model. My friend Vicente and I have already made a project on this, so I will be using that as the example to follow through. The GitHub is linked at the end.

The first step is to gather the data. This in my opinion, will be the most difficult and annoying aspect of the project. Remember that the data must be labeled. Thankfully, Kaggle has labeled images that we can easily download. The set we worked with can be found here: animal-10 dataset. If your dataset is not labeled, this can be be time consuming as you would have to manually create new labels for each categories of images. Another method is to create new labels and only move 100 pictures into their proper labels, and create a classifier like the one we will and have that machine classify the images. This will lead to errors in classification, so you may want to check manually after each run, and this is where it becomes time consuming.

Now that we have our datasets stored safely in our computer or cloud, let’s make sure we have a training data set, a validation data set, and a testing data set. Training data set would contain 85–90% of the total labeled data. This data would be used to train our machine about the different types of images we have. Validation data set would contain 5–10% of the total labeled data. This will test how well our machine performs against known labeled data. The testing data set would contain the rest of the data in an unlabeled format. This testing data will be used to test how well our machine can classify data it has never seen. The testing data can also just contain images from Google that you have downloaded, as long as it make sense to the topic you are classifying.

Let’s import all the necessary libraries first:

import pandas as pd

import numpy as np

import itertools

import keras

from sklearn import metrics

from sklearn.metrics import confusion_matrix

from keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img

from keras.models import Sequential

from keras import optimizers

from keras.preprocessing import image

from keras.layers import Dropout, Flatten, Dense

from keras import applications

from keras.utils.np_utils import to_categorical

import matplotlib.pyplot as plt

import matplotlib.image as mpimg

%matplotlib inline

import math

import datetime

import time

Defining Dimensions and locating images:

#Default dimensions we found online

img_width, img_height = 224, 224



#Create a bottleneck file

top_model_weights_path = ‘bottleneck_fc_model.h5’ # loading up our datasets

train_data_dir = ‘data/train’

validation_data_dir = ‘data/validation’

test_data_dir = ‘data/test’



# number of epochs to train top model

epochs = 7 #this has been changed after multiple model run

# batch size used by flow_from_directory and predict_generator

batch_size = 50

In this step, we are defining the dimensions of the image. Depending on your image size, you can change it but we found best that 224, 224 works best. Then we created a bottleneck file system. This will be used to convert all image pixels in to their number (numpy array) correspondent and store it in our storage system. Once we run this, it will take from half hours to several hours depending on the numbers of classifications and how many images per classifications. Then we simply tell our program where each images are located in our storage so the machine knows where is what. Finally, we define the epoch and batch sizes for our machine. For neural networks, this is a key step. We found that this set of pairing was optimal for our machine learning models but again, depending on the number of images that needs to be adjusted.

Importing transfer learning model VGG16:

#Loading vgc16 model

vgg16 = applications.VGG16(include_top=False, weights=’imagenet’) datagen = ImageDataGenerator(rescale=1. / 255)

#needed to create the bottleneck .npy files

This is importing the transfer learning aspect of the convolutional neural network. Transfer learning is handy because it comes with pre-made neural networks and other necessary components that we would otherwise have to create. There are many transfer learning model. I particularly like VGG16 as it uses only 11 convolutional layers and pretty easy to work with. However, if you are working with larger image files, it is best to use more layers, so I recommend resnet50, which contains 50 convolutional layers.

For our image classifier, we only worked with 6 classifications so using transfer learning on those images did not take too long, but remember that the more images and classifications, the longer this next step will take. But thankfully since you only need to convert the image pixels to numbers only once, you only have to do the next step for each training, validation and testing only once- unless you have deleted or corrupted the bottleneck file.

Creation of the weights and feature using VGG16:

#__this can take an hour and half to run so only run it once.

#once the npy files have been created, no need to run again. Convert this cell to a code cell to run.__ start = datetime.datetime.now()



generator = datagen.flow_from_directory(

train_data_dir,

target_size=(img_width, img_height),

batch_size=batch_size,

class_mode=None,

shuffle=False)



nb_train_samples = len(generator.filenames)

num_classes = len(generator.class_indices)



predict_size_train = int(math.ceil(nb_train_samples / batch_size))



bottleneck_features_train = vgg16.predict_generator(generator, predict_size_train)



np.save(‘bottleneck_features_train.npy’, bottleneck_features_train)

end= datetime.datetime.now()

elapsed= end-start

print (‘Time: ‘, elapsed)

Since we are making a simple image classifier, there is no need to change the default settings. Just follow the above steps for the training, validation, and testing directory we created above. However, you can add different features such as image rotation, transformation, reflection and distortion.

Once the files have been converted and saved to the bottleneck file, we load them and prepare them for our convolutional neural network. This is also a good way to make sure all your data have been loaded into bottleneck file. Remember to repeat this step for validation and testing set as well.

Creating a bottleneck file for the training data. (Same step for validation and testing):

#training data

generator_top = datagen.flow_from_directory(

train_data_dir,

target_size=(img_width, img_height),

batch_size=batch_size,

class_mode=’categorical’,

shuffle=False)



nb_train_samples = len(generator_top.filenames)

num_classes = len(generator_top.class_indices)



# load the bottleneck features saved earlier

train_data = np.load(‘bottleneck_features_train.npy’)



# get the class labels for the training data, in the original order

train_labels = generator_top.classes



# convert the training labels to categorical vectors

train_labels = to_categorical(train_labels, num_classes=num_classes)

Creating our Convolutional Neural Network code:

#This is the best model we found. For additional models, check out I_notebook.ipynb start = datetime.datetime.now()

model = Sequential()

model.add(Flatten(input_shape=train_data.shape[1:]))

model.add(Dense(100, activation=keras.layers.LeakyReLU(alpha=0.3)))

model.add(Dropout(0.5))

model.add(Dense(50, activation=keras.layers.LeakyReLU(alpha=0.3)))

model.add(Dropout(0.3))

model.add(Dense(num_classes, activation=’softmax’)) model.compile(loss=’categorical_crossentropy’,

optimizer=optimizers.RMSprop(lr=1e-4),

metrics=[‘acc’]) history = model.fit(train_data, train_labels,

epochs=7,

batch_size=batch_size,

validation_data=(validation_data, validation_labels)) model.save_weights(top_model_weights_path) (eval_loss, eval_accuracy) = model.evaluate(

validation_data, validation_labels, batch_size=batch_size, verbose=1) print(“[INFO] accuracy: {:.2f}%”.format(eval_accuracy * 100))

print(“[INFO] Loss: {}”.format(eval_loss))

end= datetime.datetime.now()

elapsed= end-start

print (‘Time: ‘, elapsed)

Now we create our model. First step is to initialize the model with Sequential(). After that we flatten our data and add our additional 3 (or more) hidden layers. This step is fully customizable to what you want. We made several different models with different drop out, hidden layers and activation. But since this is a labeled categorical classification, the final activation must always be softmax. It is also best for loss to be categorical crossenthropy but everything else in model.compile can be changed. Then after we have created and compiled our model, we fit our training and validation data to it with the specifications we mentioned earlier. Finally, we create an evaluation step, to check for the accuracy of our model training set versus validation set."
Review: SqueezeNet (Image Classification),"Review: SqueezeNet (Image Classification)

In this story, SqueezeNet, by DeepScale, UC Berkeley and Stanford University, is reviewed. With equivalent accuracy, smaller CNN architectures offer at least three advantages

Smaller Convolutional Neural Networks (CNNs) require less communication across servers during distributed training. Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car. Smaller CNNs are more feasible to deploy on FPGAs and other hardware with limited memory.

This is a technical report on arXiv in 2016 with over 1100 citations. (Sik-Ho Tsang @ Medium)"
Sentiment Analysis for Hotel Reviews,"Whether you like it or not, guest reviews are becoming a prominent factor affecting people’s bookings/purchases.

Think about your past experience. When you were looking for a place to stay for a vacation on Expedia/Booking/TripAdvisor, what did you do? I am willing to bet you’d be scrolling down the screen to check on the reviews before you knew it.

As a business owner or employee, if you still have doubts about how important guest reviews impacts the business, it may be worth checking out some stats:

In other words, guest reviews clearly influence people’s booking decision, which means, you’d better pay attention to what people are saying about your hotel!

Not only do you want good reviews, but also them in a way that can help you learn the most about your customers. Reviews can tell you if you are keeping up with your customers’ expectations, which is crucial for developing marketing strategies based on the personas of your customers.

Reviews are important and you, as hotel owners, need to start leveraging it.

What is sentiment analysis

Sentiment analysis, also called opinion mining, is a text mining technique that could extract emotions of a given text — whether it is positive, negative or neutral, and return a sentiment score. This technique is usually used on reviews or social media texts.

In this article, I’ll show you how to effectively collect hotel reviews using web scraping tool and conduct sentiment analysis using Python.

Scrape reviews using Octoparse

The web scraping tool I used is called Octoparse. It is a do-it-yourself web scraper built for people without coding backgrounds, like myself. I’ll show you how to use Octoparse to scrape the reviews of the #1 ranked hotel in New York City — Hotel Giraffe by Library Hotel Collection.

Here is the link to the web page:

https://www.tripadvisor.com/Hotel_Review-g60763-d99762-Reviews-Hotel_Giraffe_by_Library_Hotel_Collection-New_York_City_New_York.html#REVIEWS

First, we will import our targeted web URL in Octoparse."
Selenium on Airflow: Automate a daily task on the web!,"This post demonstrates how to build an Airflow plugin, which uses the Selenium WebDriver, to automate a daily online task.

Photo by Jehyun Sung on Unsplash

Automation offers a range of benefits

Increased productivity.

Greater quality.

Removing the possibility of human error and reducing manual labour.

Additional frequency and consistency (Working on the weekend!).

If your daily task involves the web, then using Selenium on Airflow could potentially save hundreds of hours per year and improve the quality and consistency of your work.

Introduction

The goal of this post is to develop a plugin which utilises Selenium to automate a daily task on Airflow.

Setting up the Airflow environment. Developing the Selenium plugin. Using the Selenium plugin within an Airflow DAG.

If you’d like to skip ahead, all the code discussed in this post is available on my GitHub here.

Below is a brief overview of topics and softwares covered:

Selenium: In a nutshell, Selenium automates browsers. Primarily it is used to automate web applications for testing purposes, however it isn’t limited to that at all. A key component of selenium is the WebDriver, the WebDriver API sends commands directly to the browser. Example commands could be navigating to a webpage, filling out a form, or even downloading a file. The WebDriver used in this post will be the Selenium/standalone-chrome driver.

Airflow: Airflow is a platform to programmatically author, schedule and monitor workflows. The key components of Airflow are the web server, scheduler, and workers. The web server refers to the Airflow user interface, while the scheduler executes your tasks on an array of workers as per predefined instructions.

Finally, to use Selenium and Airflow together, the containerisation software docker is also required.

Docker: Docker is a software which makes it easier to deploy and develop software via the use of containers. Containers allow a developer to package up an application with all of its requirements and also isolate the software from its environment to ensure it works in different development settings. A great feature of Docker is the compose tool which is used to define and run multi-container Docker applications.

We will use Docker in the first instance to setup our Airflow environment and then to spin up an additional Selenium Container as part of our plugin.

Setting up the Airflow Environment

The base environment:

As mentioned above, Docker will be used to set up the Airflow environment. To do this go to https://github.com/puckel/docker-airflow and download the docker-compose-CeleryExecutor.yml file. This is a docker-compose file created by a Github user named Puckel which allows you to quickly get up and running with Airflow. The compose file opts for the Celery executor which is necessary if you require tasks to run concurrently as it scales out the number of workers.

There are some basic changes to make to the docker-compose file.

Rename the compose file: docker-compose.yml.

Uncomment the custom plugins volumes.

- ./plugins:/usr/local/airflow/plugin

To ensure that the Airflow containers have the correct permissions on the plugins and dags directory ensure that the directories exist on the host prior to running the compose.

Test that the environment runs locally using the docker-compose up command, the UI should be available at http://localhost:8080.

Modifying the environment to cater for the Selenium plugin

Before completing the environment, it is necessary to briefly explain how the Selenium plugin will work as some of its functionality will directly impact setup. The plugin will be covered in greater detail later in the post.

The plugin will execute the following steps:

Start a Selenium docker container Configure the remote Selenium driver Send commands to the driver: This will result in a file downloaded from the internet. Remove the running container

The steps above can be distilled into two categories:

Using Docker from Airflow.

Interacting with the remote container

Using Docker from Airflow

The Airflow worker needs to be able to create the Selenium container and subsequently send commands to execute the task. As explained very well by Jérôme Petazzoni in this post, it is bad practice to spin up a Docker container within another container, and not necessary so long as a container exists and is accessible. The easiest way to allow the worker to create containers is by exposing the host Docker socket to the worker by mounting it as a volume in the docker-compose file.

worker:

volumes:

- /var/run/docker.sock:/var/run/docker.sock

The Airflow worker still cant access the host Docker socket due to not having the correct permissions so these will have to be changed. This can be achieved by creating a new Dockerfile called ‘Dockerfile-airflow’ which extends the puckel/docker-airflow base image as follows:

FROM puckel/docker-airflow:1.10.4 USER root

RUN groupadd --gid 999 docker \

&& usermod -aG docker airflow

USER airflow

The Dockerfile first calls the puckel/docker-airflow base image

As the root user, creates the docker user group with the id 999 and adds the airflow user to the group.

Sets the airflow user.

It is imperative that the docker group id (999) must be the same on both the worker and the host. To find out the host docker group id use the following command:

grep 'docker' /etc/group

Create the new docker image:

docker build -t docker_airflow -f Dockerfile-selenium .

The next thing to do is change the airflow image name in the Docker-compose file, from puckel/docker-airflow:latest to docker_airflow:latest . This means the compose file will use the newly created image.

The Airflow worker can now create Docker containers on the host, however still requires the Docker python package. Additional installations can also be handled in the Dockerfile. The installations below are necessary for the Selenium plugin and DAG.

RUN pip install docker && \

pip install selenium && \

pip install bs4 && \

pip install lxml && \

pip install boto3

To start a Selenium container with the plugin the image must already exist on the host machine:

docker pull selenium/standalone-chrome

Finally, for the worker container to send commands to the new Selenium container, they will both need to be on the same Docker network. Both containers will be on the external network: ‘container_bridge’ which is created with the following command:

docker network create container_bridge

The container bridge network also needs to be added to the compose file.

worker:

networks:

- default

- container_bridge networks:

default:

container_bridge:

NB It is important to note that the above method for exposing the host docker sock to the worker container and setting permissions will only work in a linux environment, to configure a dev environment for MacOS please refer to these two brilliant articles:

Interacting with the remote container

The Selenium plugin sends commands to the docker container via the remote driver, which it connects to over the container_bridge network. The Selenium commands will be added to the environment as a mounted volume in the home directory: {AIRFLOW_USER_HOME}

volumes:

# Selenium scripts

- ./selenium_scripts:/usr/local/airflow/selenium_scripts

The commands will come from a custom Python module (selenium_scripts) which needs to be in the Python Path. This can be done in the Airflow Dockerfile.

ENV PYTHONPATH=$PYTHONPATH:${AIRFLOW_USER_HOME}

The last change to the Airflow environment is to enable the Selenium container and Airflow workers to share files and content. This is required when downloading content from the internet with Selenium and can be achieved with an external named volume.

docker volume create downloads

The ‘downloads’ volume needs to be added to docker-compose file:

worker:

volumes:

- downloads:/usr/local/airflow/downloads volumes:

downloads:

external: true

When Docker volumes are created on containers, without the corresponding directories already pre-existing, they are created by the root user, which means that the container user won’t have write privileges. The simplest way to circumvent this is to create the directories as the container user during the initial build.

For the Airflow Dockerfile add the line:

RUN mkdir downloads

A new Dockerfile will have to be created for the Selenium container, this will be called Dockerfile-selenium.

FROM selenium/standalone-chrome RUN mkdir /home/seluser/downloads

Build both new images:

docker build -t docker_selenium -f Dockerfile-selenium . docker build -t docker_airflow -f Dockerfile-airflow .

The environment is now complete, the complete environments are below:

Airflow Dockerfile:

Selenium Dockerfile:

Docker-compose:

The Selenium Plugin

A great feature of Airflow is the plugins, plugins are an easy way to extend the existing feature set of Airflow. To integrate a new plugin with the existing airflow environment, simply move the plugin files into the plugins folder.

The Selenium plugin will work as follows:

Start the Selenium Docker container in the host environment. Configure the remote Selenium WebDriver on the docker container. Send commands to the WebDriver to fulfil the task. Stop and remove the container.

This method has been used over using the standalone Docker operator as it provides greater control and facilitates easier debugging.

The Selenium plugin will contain a Hook and Operator, Hooks handle external connections and make up the building blocks of an Operator. The operator will execute our task. The plugin folder structure is as follows:

.

├── README.md

├── __init__.py

├── hooks

│ ├── __init__.py

│ └── Selenium_hook.py

└── operators

├── __init__.py

└── Selenium_operator.py

To create a plugin, you need to derive the AirflowPlugin class and reference the objects you want to plug into Airflow, we do this in the __init__.py file:

The Selenium Hook

The Selenium hook inherits from the BaseHook module, which is the base class for all hooks. The hook consists of several methods to start, stop and send commands to a Selenium container.

Creating the Container: The hook makes use of the Python Docker library to send commands to the host Docker socket and creates the Selenium container on the host. The external named volume (Downloads) is mounted on the local downloads directory which will be configured as the browser default downloads location. To enable interaction with the worker, the container_bridge network is also included.

Configuring the driver: Once the create_container method has been executed, the next step is to configure and connect to the driver so it meets the task requirements.

Since the Selenium container is on the container_bridge network, the WebDriver can be found on the network IP at the following location: <network IP>:4444/wd/hub . The driver can be connected to using the WebDriver remote.

The first step in configuring the driver is to use the Options class to enable the driver to run in headless mode and to set the window size to ensure the page content loads correctly. Headless mode essentially means that the driver doesn’t have a user interface.

options = Options()

options.add_argument(""--headless"")

options.add_argument(""--window-size=1920x1080"")

The second step is to enable the browser to download in headless mode. This is done by sending a post request to the driver which fixes the download behaviour.

driver.command_executor._commands[""send_command""] = (

""POST"", '/session/$sessionId/chromium/send_command')

params = {'cmd': 'Page.setDownloadBehaviour',

'params': {'behavior': 'allow',

'downloadPath': <DOWNLOADS>}}

driver.execute(""send_command"", params)

Executing a task: To keep the plugin as task agnostic as possible the Selenium commands have been abstracted to a separate python module to be imported at run time in the DAG. The one condition on each script is that they are imported as functions and the first argument is the driver. This will be covered in more detail later.

Removing the container: Once the task is complete the container can be removed.

The Selenium Operator

As mentioned above, Airflow hooks are the building blocks for operators. The operator below uses the Selenium hook and Airflow’s execution context to run a Selenium task.

All Airflow operators must inherit the BaseOperator class, this class creates objects that become nodes in the DAG. A great feature of the Airflow operator is the ability to define template fields; these are Jinjafied fields that can accept Airflow macros when executed. The airflow_args variable is a template_field which means they can be set dynamically using macros at runtime.

Using the Selenium Plugin within an Airflow DAG

Since the Airflow environment and Selenium plugin are now complete, the next step is to bring it all together in the form of an Airflow DAG. An Airflow DAG runs a collection of tasks is a predefined way.

The example DAG below is designed to download the daily podcast: Wake up to Money from the BBC and upload the mp3 file to S3 for later consumption. Wake Up to Money is an early morning financial radio programme on BBC Radio 5 Live with new episodes every weekday at 5am.

The Selenium script:

The Wake up to Money script uses the Selenium WebDriver to navigate to the url: https://www.bbc.co.uk/programmes/b0070lr5/episodes/downloads and download the latest episode.

Once the page has been rendered by the browser, Beautiful soup is used to parse the html for the download link. Calling the driver.get method on the download link starts the download, which is polled to completion. Once downloaded, the file is renamed so it is easy to keep track of between tasks.

As mentioned in the plugin section, the Selenium scripts need to be an executable function with the driver set as the first argument.

Once complete, ensure that the Selenium scripts are in the folder which was mounted on the Airflow environment and added to the Python path in the previous steps.

The DAG

As mentioned above a DAG is a collection of tasks, the first step in creating a DAG, is describing how those tasks will run. The arguments used when creating a DAG object do just that.

Since the podcast airs every weekday at 5am, the DAG schedule_interval will be set to pick up each episode at 7am. It’s not that easy to set an Airflow chron expression to run only on weekdays so instead the DAG will run every day and a branch operator will be used to set the task based on the day of the week.

The DAG schedule is now defined, the next step is to add the tasks. The DAG tasks are:

Start: Starts the DAG

Starts the DAG Weekday Branch: Determines which branch of the DAG to follow depending on whether the execution day is a weekday or not.

Determines which branch of the DAG to follow depending on whether the execution day is a weekday or not. Get Podcast: Downloads the podcast.

Downloads the podcast. Upload Podcast to S3: Uploads the podcast to S3.

Uploads the podcast to S3. Remove local Podcast: Removes the local copy of the podcast.

Removes the local copy of the podcast. End: Ends the DAG.

The Start and End tasks make use of the Airflow DummyOperator, they don’t do anything but a useful when grouping tasks.

The Selenium plugin will be used in the download podcast task. The SeleniumOperator will execute the download_podcast function which is imported from at from the Selenium scripts module at runtime. Once the podcast is downloaded it will be saved under the new name: episode_{{ds_nodash}}.mp3 , but since the filename is a templated field, this will be rendered at runtime. e.g. On the 2019–10–13 the file name will be episode_20191013.mp3 .

The Weekday Branch task, splits the DAG into two branches based on whether the execution day is a weekday or weekend. The Branch task uses the Airflow Python Branch Operator to set the next task based on the output of the weekday_branch function.

If the execution day is a weekday, the next task to be run by the DAG is the get_podcast task, if the execution day is a weekend, the next task to run is end.

The last two tasks are: Upload podcast to S3 and Remove local podcast. These both use the PythonOperator which is extended to ‘Jinjafy’ the arguments. This is necessary to keep track of the podcast file name, as per the Selenium operator.

The S3 function will require an S3 connection with the name: S3_conn_id.

Finally the last step is to define the task order, note how the weekday_branch task precedes both the get_podcast and end task.

start >> weekday_branch

weekday_branch >> get_podcast

get_podcast >> upload_podcast_to_s3

upload_podcast_to_s3 >> remove_local_podcast

remove_local_podcast >> end

weekday_branch >> end

I hope you enjoyed this post; if you have any questions or suggestions, or even ideas for future posts, let me know in the comments section and I’ll do my best to get back to you.

Please checkout my other Airflow posts:"
Can Congress help keep AI fair for consumers?,"Can Congress Help Keep AI Fair for Consumers?

How do firms ensure that AI systems are not having a disparate impact on vulnerable communities, and what safeguards should regulators and Congress put in place to protect consumers? To what extent should companies be required to audit these algorithms so that they don’t unfairly discriminate? Who should determine the standards for that? We need to ensure that AI does not create biases in lending toward discrimination.

Two new House Task Forces to regulate fintech and AI

These aren’t questions from an academic discourse or the editorial pages. These were posed to the witnesses of a June 26 hearing before the US House Committee on Financial Services [1] — by both Democrats and Republicans, representatives of Illinois, North Carolina, and Arkansas.

It is a bipartisan sentiment that, left unchecked, AI can pose a risk to fairness in financial services. While the exact extent of this danger might be debated, governments in the US and abroad acknowledge the necessity and assert the right to regulate financial institutions for this purpose.

The June 26 hearing was the first wake-up call for financial services: they need to be prepared to respond and comply with future legislation requiring transparency and fairness.

In this post, we review the notable events of this hearing, and we explore how the US House is beginning to examine the risks and benefits of AI in financial services.

Two new House Task Forces to regulate fintech and AI

On May 9 of this year, the chairwoman of the US House Committee on Financial Services, Congresswoman Maxine Waters (D-CA), announced the creation of two task forces [2]: one on fintech, and one on AI.

Generally, task forces convene to investigate a specific issue that might require a change in policy. These investigations may involve hearings that call forth experts to inform the task force.

These two task forces overlap in jurisdiction, but the committee’s objectives implied some distinctions:

The fintech task force should have a nearer-term focus on applications (e.g. underwriting, payments, immediate regulation).

The AI task force should have a longer-term focus on risks (e.g. fraud, job automation, digital identification).

And explicitly, Chairwoman Waters explained her overall interest in regulation:

Make sure that responsible innovation is encouraged, and that regulators and the law are adapting to the changing landscape to best protect consumers, investors, and small businesses.

The appointed chairman of the Task Force on AI, Congressman Bill Foster (D-IL), extolled AI’s potential in a similar statement, but also cautioned,

It is crucial that the application of AI to financial services contributes to an economy that is fair for all Americans.

This first hearing did find ample AI applications in financial services. But it also concluded that these worried sentiments are neither misrepresentative of their constituents nor misplaced.

From left to right: Maxine Waters (D-CA), Chairwoman of the US House Committee on Financial Services; Bill Foster (D-IL), Chairman of the Task Force on AI; French Hill (R-AR), Ranking Member on the Task Force on AI

Risks of AI

In a humorous exchange later in the hearing, Congresswoman Sylvia Garcia (D-TX) asks a witness, Dr. Bonnie Buchanan of the University of Surrey, to address the average American and explain AI in 25 words or less. It does not go well.

DR. BUCHANAN

I would say it’s a group of technologies and processes that can look at determining general pattern recognition, universal approximation of relationships, and trying to detect patterns from noisy data or sensory perception. REP. GARCIA

I think that probably confused them more. DR. BUCHANAN

Oh, sorry.

Beyond making jokes, Congresswoman Garcia has a point. AI is extraordinarily complex. Not only that, to many Americans it can be threatening. As Garcia later expresses, “I think there’s an idea that all these robots are going to take over all the jobs, and everybody’s going to get into our information.”

In his opening statement, task force ranking member Congressman French Hill (R-AR) tries to preempt at least the first concern. He cites a World Economic Forum study [3] that the 75 million jobs lost because of AI will be more than offset by 130 million new jobs. But Americans are still anxious about AI development.

In a June 2018 survey of 2,000 Americans [4] conducted by Oxford’s Center for the Governance of AI, researchers observed

overwhelming support for careful management of robots and/or AI (82% support)

more trust in tech companies than in the US government to manage AI in the interest of the public

mixed support for developing high-level machine intelligence (defined as “when machines are able to perform almost all tasks that are economically relevant today better than the median human today”)

This public apprehension about AI development is mirrored by concerns from the task force and experts. Personal privacy is mentioned nine times throughout the hearing, notably in Congressman Anthony Gonzalez’s (R-OH) broad question on “balancing innovation with empowering consumers with their data,” which the panel does not quite adequately address.

But more often, the witnesses discuss fairness and how AI models could discriminate unnoticed. Most notably, Dr. Nicol Turner-Lee, a fellow at the the Brookings Institution, suggests implementing guardrails to prevent biased training data from “replicat[ing] and amplify[ing] stereotypes historically prescribed to people of color and other vulnerable populations.”

And she’s not alone. A separate April 2019 Brookings report [5] seconds this concern of an unfairness “whereby algorithms deny credit or increase interest rates using a host of variables that are fundamentally driven by historical discriminatory factors that remain embedded in society.”

So if we’re so worried, why bother introducing the Pandora’s box of AI to financial services at all?

Benefits of AI

AI’s potential benefits, according to Congressman Hill, are to “gather enormous amounts of data, detect abnormalities, and solve complex problems.” In financial services, this means actually fairer and more accurate models for fraud, insurance, and underwriting. This can simultaneously improve bank profitability and extend services to the previously underbanked.

Both Hill and Foster cite a National Bureau of Economic Research working paper [6] finding where in one case, algorithmic lending models discriminate 40% less than face-to-face lenders. Furthermore, Dr. Douglas Merrill, CEO of ZestFinance and expert witness, claims that customers using his company’s AI tools experience higher approval rates for credit cards, auto loans, and personal loans, each with no increase in defaults.

Moreover, Hill frames his statement with an important point about how AI could reshape the industry: this advancement will work “for both disruptive innovators and for our incumbent financial players.” At first this might seem counterintuitive.

“Disruptive innovators,” more agile and hindered less by legacy processes, can have an advantage in implementing new technology. [7] But without the immense budgets and customer bases of “incumbent financial players,” how can these disruptors succeed? And will incumbents, stuck in old ways, ever adopt AI?

Mr. Jesse McWaters, financial innovation lead at the World Economic Forum and the final expert witness, addresses this apparent paradox, discussing what will “redraw the map of what we consider the financial sector.” Third-party AI service providers — from traditional banks to small fintech companies — can “help smaller community banks remain digitally relevant to their customers” and “enable financial institutions to leapfrog forward.”

Enabling competitive markets, especially in concentrated industries like financial services, is an unadulterated benefit according to free market enthusiasts in Congress. However, “redrawing the map” in this manner makes the financial sector larger and more complex. Congress will have to develop policy responding to not only more complex models, but also a more complex financial system.

This system poses risks both to corporations, acting in the interest of shareholders, and to the government, acting in the interest of consumers.

Business and government look at risks

Businesses are already acting to avert potential losses from AI model failure and system complexity. A June 2019 Gartner report [8] predicts that 75% of large organizations will hire AI behavioral forensic experts to reduce brand and reputation risk by 2023.

However, governments recognize that business-led initiatives, if motivated to protect company brand and profits, may only go so far. For a government to protect consumers, investors, and small businesses (the relevant parties according to Chairwoman Waters), a gap may still remain.

As governments explore how to fill this gap, they are establishing principles that will underpin future guidance and regulation. The themes are consistent across governing bodies:

AI systems need to be trustworthy.

They therefore require some government guidance or regulation from government representing the people.

This guidance should encourage fairness, privacy, and transparency.

In the US, President Donald Trump signed an executive order [9] in February 2019 “to Maintain American Leadership in Artificial Intelligence,” directing federal agencies to, among other goals, “ foster public trust in AI systems by establishing guidance for AI development and use. “ The Republican White House and Democratic House of Representatives seem to clash at every turn, but they align here.

The EU is also establishing a regulatory framework for ensuring trustworthy AI. Likewise included among the seven requirements in their latest communication from April 2019 [10]: privacy, transparency, and fairness.

And June’s G20 summit [11] drew upon similar ideas to create their own set of principles, including fairness and transparency, but also adding explainability.

These governing bodies are in a fact-finding stage, establishing principles and learning what they are up against before guiding policy. In the words of Chairman Foster, the task force must understand “how this technology will shape the questions that policymakers will have to grapple with in the coming years.”

An hour before Congresswoman Garcia’s amusing challenge, Dr. Buchanan reflected upon a couple common themes of concern.

Policymakers need to be concerned about the explainability of artificial intelligence models. And we should avoid black-box modeling where humans cannot determine the underlying process or outcomes of the machine learning or deep learning algorithms.

But through this statement, she suggests a solution: make these AI models explainable. If humans can indeed understand the inputs, process, and outputs of a model, we can trust our AI. Then throughout AI applications in financial services, we can promote fairness for all Americans.

Sources

[1] United States House Committee of Financial Services. “Perspectives on Artificial Intelligence: Where We Are and the Next Frontier in Financial Services.” https://financialservices.house.gov/calendar/eventsingle.aspx?EventID=403824. Accessed July 18, 2019.

[2] United States House Committee of Financial Services. “Waters Announces Committee Task Forces on Financial Technology and Artificial Intelligence.” https://financialservices.house.gov/news/documentsingle.aspx?DocumentID=403738. Accessed July 18, 2019.

[3] Leopold, Till Alexander, Vesselina Ratcheva, and Saadia Zahidi. “The Future of Jobs Report 2018.” World Economic Forum. http://www3.weforum.org/docs/WEF_Future_of_Jobs_2018.pdf

[4] Zhang, Baobao and Allan Dafoe. “Artificial Intelligence: American Attitudes and Trends.” Oxford, UK: Center for the Governance of AI, Future of Humanity Institute, University of Oxford, 2019. https://ssrn.com/abstract=3312874

[5] Klein, Aaron. “Credit Denial in the Age of AI.” Brookings Institution. April 11, 2019. https://www.brookings.edu/research/credit-denial-in-the-age-of-ai/

[6] Bartlett, Robert, Adair Morse, Richard Stanton, Nancy Wallace, “Consumer-Lending Discrimination in the FinTech Era.” National Bureau of Economic Research, June 2019. https://www.nber.org/papers/w25943

[7] Snyder, Scott. “How Banks Can Keep Up with Digital Disruptors.” Philadelphia, PA: The Wharton School of the University of Pennsylvania, 2017. https://knowledge.wharton.upenn.edu/article/banking-and-fintech/

[8] “Gartner Predicts 75% of Large Organizations Will Hire AI Behavior Forensic Experts to Reduce Brand and Reputation Risk by 2023.” Gartner. June 6, 2019. https://www.gartner.com/en/newsroom/press-releases/2019-06-06-gartner-predicts-75-of-large-organizations-will-hire

[9] United States, Executive Office of the President [Donald Trump]. Executive order 13859: Executive Order on Maintaining American Leadership in Artificial Intelligence. February 11, 2019. https://www.whitehouse.gov/presidential-actions/executive-order-maintaining-american-leadership-artificial-intelligence/

[10] “Building Trust in Human-Centric Artificial Intelligence.” European Commission. April 8, 2019. https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines#Top

[11] “G20 Ministerial Statement on Trade and Digital Economy.” June 9, 2019. http://trade.ec.europa.eu/doclib/press/index.cfm?id=2027"
Finding similar images using Deep learning and Locality Sensitive Hashing,"Finding similar images using Deep learning and Locality Sensitive Hashing

A simple walkthrough on finding similar images through image embedding by a ResNet 34 using FastAI & Pytorch. Also doing fast semantic similarity search in huge image embeddings collections. Aayush Agrawal · Follow Published in Towards Data Science · 8 min read · Mar 17, 2019 -- 5 Listen Share

Fina output with similar images given an Input image in Caltech 101

In this post, we are trying to achieve the above result, i.e., given an image, we should be able to find similar images from the Caltech-101 database. The post guides with an end to end process on how I went about building this. The entire codebase for replicating the project is in my GitHub repository. The process to achieve the above result can be broken down in these few steps -

Transfer learning from a ResNet-34 model(trained on ImageNet) to detect 101 classes in Caltech-101 dataset using FastAI and Pytorch. Take the output of second last fully connected layer from trained ResNet 34 model to get embedding for all 9,144 Caltech-101 images. Use Locality Sensitive hashing to create LSH hashing for our image embedding which enables fast approximate nearest neighbor search Then given an image, we can convert it into image embedding using our trained model and then search similar images using Approximate nearest neighbor on Caltech-101 dataset.

Part 1 — Data understanding and Transfer learning

As I mentioned above, for this project, my goal is to query any given image and find a semantically similar image in the Caltech-101 database. This database contains 9,144 images divided into 101 categories. Each category has about 50–800 images in them.

Image examples from Caltech-101 database

The first exercise in our project is to obtain a deep learning network which can classify these categories accurately. For this task, we will use a pre-trained ResNet 34 network which is trained on the ImageNet database and transfer learn it to classify 101 categories of Caltech-101 database using Pytorch 1.0 and FastAI library. As I have written about exactly how to do transfer learning with any given dataset in my previous blog, I am just going to outline the process in this blog. You can refer to this notebook to find the code to do the same. Find below the steps to do transfer learning for classifying Caltech-101 images -

Load the data using dataset loaders of Pytorch using FastAI library Take a pre-trained network, in this case, a ResNet 34 and remove it’s last fully connected layers Add new fully connected layers at the end of the network and train only those layers using the Caltech-101 image, while keeping all the other layers frozen Train the entire network by unfreezing all the layers

Part 2 — Extracting image embeddings using Pytorch Hooks

Now that we have a pre-trained network, we need to extract embeddings from this network for all of our Caltech-101 images. Embedding is nothing but a representation of an object in an N-dimensional vector. An image embedding, in this case, is a representation of an image in N-dimension. The basic idea is the closer a given image to another image their embedding will also be similar and close in the spatial dimension.

Image embedding visualization. Credit — Blog

You can see in the above image taken from this blog that image embedding is a spatial representation of an image in the vectorized form where similar images are close in spatial dimension as well.

We can obtain image embeddings from a ResNet-34 by taking the output of its second last Fully-connected layer which has a dimension of 512. To save intermediate calculations in a deep learning model in Pytorch for inspection or in our case to extract embeddings we use Pytorch Hooks. Hooks can be of two types — forward and backward. Forward hooks are used to save information passing forward in a network to make an inference while backward hooks are used to collect information about gradients during backpropagation. In our case, we need output of our second last Fully connected layers in the inference stage which means we need to use a forward hook. Let’s look at the code for creating a hook (also in “Extracting Feature” section of my notebook) —

class SaveFeatures():

features=None

def __init__(self, m):

self.hook = m.register_forward_hook(self.hook_fn)

self.features = None

def hook_fn(self, module, input, output):

out = output.detach().cpu().numpy()

if isinstance(self.features, type(None)):

self.features = out

else:

self.features = np.row_stack((self.features, out))

def remove(self):

self.hook.remove()

The above code is all you need in creating a Pytorch hook. The SaveFeatures class invokes register_forward_hook function from the torch.nn module and given any model layer it will save the intermediate computation in a numpy array which can be retrieved using SaveFeatures.features functions. Let’s see the code to use this class —

## Output before the last FC layer

sf = SaveFeatures(learn.model[1][5]) ## By running this feature vectors would be saved in sf variable initated above

_= learn.get_preds(data.train_ds)

_= learn.get_preds(DatasetType.Valid) ## Converting in a dictionary of {img_path:featurevector}

img_path = [str(x) for x in (list(data.train_ds.items)+list(data.valid_ds.items))]

feature_dict = dict(zip(img_path,sf.features))

Line 1–2: Invokes the class SaveFeatures using model layer reference to the output of second last fully-connected layer as the input.

Line 4–6: Passing the Caltech-101 data to get their predictions. Note that we are not interested in saving predictions and that’s why we used “_.” In this case, the intermediate output of second last layers in saved in the variable named “sf”, which is an instance of SaveFeatures class.

Line 8–10: Creating a python dictionary where image path is the key and image embeddings is the value.

Now we have embedding representation of each image in Caltech-101 in our dictionary.

Part 3 — Locality Sensitive Hashing for fast approximate nearest neighbor search

We can use our newly generated Caltech 101 image embeddings and get a new image, convert it into embedding to calculate distance b/w the new image and all the Caltech 101 database to find similar images. This process is computationally expensive in nature and as a new image embedding have to compare with all the 9K+ image embedding in the Caltech 101 database to find the most similar image(nearest neighbor), which in computational complexity notation is an O(N²) problem and will take exponentially more time to retrieve similar images as the number of images increases.

To solve this problem, we will use locality sensitive hashing(LSH) which is an approximate nearest neighbor algorithm which reduces the computational complexity to O(log N). This blog explains LSH in good details in terms of time complexity and implementation. In short, LSH generates a hash value for image embeddings while keeping spatiality of data in mind; in particular; data items that are similar in high-dimension will have a higher chance of receiving the same hash value.

Below are the steps on how LSH converts an embedding in a hash of size K-

Generate K random hyperplanes in the embedding dimension Check if particular embedding is above or below the hyperplane and assign 1/0 Do step 2 for each K hyperplanes to arrive at the hash value

the hash value of the orange dot is 101 because it: 1) above the purple hyperplane; 2) below the blue hyperplane; 3) above the yellow hyperplane. Image Credit — Link

Let’s now look at how LSH will perform an ANN query. Given a new image embedding, we will use LSH to create a hash for the given image and then compare the distance from image embedding of the pictures of Caltech-101 dataset which shares the same hash value. In this way, instead of doing similarity search over the whole Caltech-101 database we will only do a similarity search with a subset of images which shares the same hash value with the input image. For our project, we are using lshash3 package for an approximate nearest neighbor search. Let’s look at the code to do the same (you can find the code in the “Using Locality Sensitive hashing to find near similar images” section of my notebook)-

from lshash import LSHash



k = 10 # hash size

L = 5 # number of tables

d = 512 # Dimension of Feature vector

lsh = LSHash(hash_size=k, input_dim=d, num_hashtables=L) # LSH on all the images

for img_path, vec in tqdm_notebook(feature_dict.items()):

lsh.index(vec.flatten(), extra_data=img_path)

The above code takes the image embedding dictionary and converts it into LSH table. To query the LSH table, we can use the code below —

# query a vector q_vec

response = lsh.query(q_vec, num_results= 5)

Part 4 — Putting it all together

Now we have our LSH table created let’s write a script which can take an image URL as an input and give us N(user-defined) similar images from CalTech 101 database. The code for this part is on my Github here.

Process flow of the find similar image script.

The script does the following task -

Load the LSH table and our ResNet 34 model (load_model function) Take the image URL from user call and download the image ( download_img_from_url function) Pass the image from ResNet-34 to get 512 dimension image embedding ( image_to_vec function) Query it with LSH table to find N(user-defined) similar images and their path ( get_similar_images function) Return the output at the desired output path, optionally display it using Open CV (get_similar_images function)

We can use a similar concept in various applications like finding similar images in our photo gallery, item-item recommendation of similar looking items, doing a web search on images, finding near-duplicate images, etc.

Summary (TL;DR).

In the blog, we saw an application of deep learning in finding semantically similar images and how to do an approximate nearest neighbor query using Locality-sensitive hashing(LSH) to speed up query time for large datasets. Also, it’s important to note that we used LSH not on the raw features(images) but on the embeddings which help do fast similarity search in huge collections.

I hope you enjoyed reading, and feel free to use my code on Github to try it out for your purposes. Also, if there is any feedback on code or just the blog post, feel free to reach out on LinkedIn or email me at aayushmnit@gmail.com. You can also follow me on Medium and Github for future blog post and exploration project codes I might write."
DeepPiCar — Part 3: Make PiCar See and Think,"DeepPiCar — Part 3: Make PiCar See and Think

Set up computer vision (OpenCV) and deep learning software (TensorFlow). Turn the PiCar into a DeepPiCar. David Tian · Follow Published in Towards Data Science · 7 min read · May 2, 2019 -- 32 Listen Share

Executive Summary

Welcome back! If you have been following my previous two posts (Part 1 and Part 2) on DeepPiCar, you should have a running robotic car that can be controlled via Python. In the article, we will give your car the superpower of Computer Vision and Deep Learning. By the end of the article, it would be transformed into a true DeepPiCar, which is capable of detecting and identifying objects in your room.

OpenCV for Computer Vision

Note that the only Perception Sensor of our PiCar is a USB DashCam. A DashCam gives us a live video, which is essentially a sequence of pictures. We will use OpenCV, a powerful open source computer vision library, to capture and transform these pictures so that we can make sense of what the camera is seeing. Run the following commands (in bold) to install it on your Pi.

Install Open CV and Related Libraries

# install all dependent libraries of OpenCV (yes, this is one long command)

pi@raspberrypi:~ $ sudo apt-get install libhdf5-dev -y && sudo apt-get install libhdf5-serial-dev -y && sudo apt-get install libatlas-base-dev -y && sudo apt-get install libjasper-dev -y && sudo apt-get install libqtgui4 -y && sudo apt-get install libqt4-test -y # install OpenCV and other libraries

pi@raspberrypi:~ $ pip3 install opencv-python

Collecting opencv-python

[Omitted....]

Installing collected packages: numpy, opencv-python

Successfully installed numpy-1.16.2 opencv-python-3.4.4.19 pi@raspberrypi:~ $ pip3 install matplotlib

Collecting matplotlib

Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib)

[Omitted...]

Successfully installed cycler-0.10.0 kiwisolver-1.1.0 matplotlib-3.0.3 numpy-1.16.3 pyparsing-2.4.0 python-dateutil-2.8.0 setuptools-41.0.1 six-1.12.0

Test OpenCV Installation

Here are the most basic tests to see if our python libraries are installed. The Python module name for OpenCV is cv2 . If you don’t see any error when running the following commands, then the modules should be installed correctly. Numpy and Matplotlib are two very useful python modules that we will use in conjunction with OpenCV for image processing and rendering.

pi@raspberrypi:~ $ python3 -c ""import cv2""

pi@raspberrypi:~ $ python3 -c ""import numpy""

pi@raspberrypi:~ $ python3 -c ""import matplotlib""

Ok, let’s try to some live video processing!

cd

pi@raspberrypi:~ $ git clone https://github.com/dctian/DeepPiCar.git

Cloning into 'DeepPiCar'...

remote: Enumerating objects: 482, done.

[Omitted...]

Resolving deltas: 100% (185/185), done. pi@raspberrypi:~ $pi@raspberrypi:~ $Cloning into 'DeepPiCar'...remote: Enumerating objects: 482, done.[Omitted...]Resolving deltas: 100% (185/185), done. pi@raspberrypi:~ $ cd DeepPiCar/driver/code

pi@raspberrypi:~ $ python3 opencv_test.py



If you see two live video screens, one colored and one black/white, then your OpenCV is working! Press q to quit the test. Essentially, the program takes the images captured from the camera and displays it as is (the Original window), and then it converts the image to a black and white image (the B/W window). This is very important as in Part 4: Autonomous Lane Navigation, we will bring up as many as 9-10 screens as the original video images will be processed through many stages, like below.

Install TensorFlow for CPU and EdgeTPU

Google’s TensorFlow is currently the most popular python library for Deep Learning. It can be used for image recognition, face detection, natural language processing, and many other applications. There are two methods to install TensorFlow on Raspberry Pi:

TensorFlow for CPU

TensorFlow for Edge TPU Co-Processor (the $75 Coral branded USB stick)

Install TensorFlow for CPU

The first method installs the CPU version of TensorFlow. We will NOT use Pi to perform any deep learning (i.e. model training), as its CPU is vastly insufficient for backward propagation, a very slow operation required in the learning process. However, we can use the CPU to do inferences based on a pre-trained model. Inference is also known as model prediction, which uses only forward propagation, a much faster computer operation. Even with the CPU just doing inference, it can only do so on a relatively shallow model (say 20–30 layers) in real time. For deeper models (100+ layers), we would need the Edge TPU. As of May 2019, the most recent production version of TensorFlow is version 1.13 (2.0 is still alpha)

pi@raspberrypi:~ $ pip3 install tensorflow

Collecting tensorflow

[omitted...]

pi@raspberrypi:~ $ pip3 install keras

Collecting keras

[omitted...]

Successfully installed h5py-2.9.0 keras-2.2.4 keras-applications-1.0.7 keras-preprocessing-1.0.9 numpy-1.16.3 pyyaml-5.1 scipy-1.2.1 six-1.12.0

Now let’s test and make sure the installation went fine. When you import TensorFlow, it will report some warning messages. But they can be safely ignored. You should not see any errors. (If you do see other errors, please post the commands you typed and the error message in the post down below, and I will try to help.)

pi@raspberrypi:~ $ python3

Python 3.5.3 (default, Sep 27 2018, 17:25:39)

[GCC 6.3.0 20170516] on linux

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import numpy

>>> import cv2

>>> import tensorflow

/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.4 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5

return f(*args, **kwds)

/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: builtins.type size changed, may indicate binary incompatibility. Expected 432, got 412

return f(*args, **kwds)

>>> import keras

Using TensorFlow backend.

>>> quit()

Install TensorFlow for EdgeTPU

When the deep learning models are very deep, 100 layers or more, to achieve real-time performance, it needs to run on the EdgeTPU coprocessor instead of the CPU. However, at the time of writing, Edge TPU is so new (release to the general public around early 2019) that it cannot run all models that can run on the CPU, so we have to choose our model architecture carefully and make sure they will work on EdgeTPU. For more details on what models can run on Edge TPU, please read this article by Google.

Follow the instructions below to install EdgeTPU drivers and APIs. When asked if you want to enable the maximum operating frequency , answer y . The models we run will be relatively lightweight for the TPU, I have never seen it run very hot.

cd

pi@raspberrypi:~ $ wget https://dl.google.com/coral/edgetpu_api/edgetpu_api_latest.tar.gz -O edgetpu_api.tar.gz --trust-server-names

--2019-04-20 11:55:39--

Resolving dl.google.com (dl.google.com)... 172.217.10.78

[omitted]

edgetpu_api.tar.gz 100%[===================>] 7.88M 874KB/s in 9.3s

2019-04-20 11:55:49 (867 KB/s) - ‘edgetpu_api.tar.gz’ saved [8268747/8268747] pi@raspberrypi:~ $pi@raspberrypi:~ $--2019-04-20 11:55:39-- https://dl.google.com/coral/edgetpu_api/edgetpu_api_latest.tar.gz Resolving dl.google.com (dl.google.com)... 172.217.10.78[omitted]edgetpu_api.tar.gz 100%[===================>] 7.88M 874KB/s in 9.3s2019-04-20 11:55:49 (867 KB/s) - ‘edgetpu_api.tar.gz’ saved [8268747/8268747] pi@raspberrypi:~ $ tar xzf edgetpu_api.tar.gz

pi@raspberrypi:~ $ cd edgetpu_api/

pi@raspberrypi:~/edgetpu_api $ bash ./install.sh

Would you like to enable the maximum operating frequency? Y/N

y

Using maximum operating frequency.

Installing library dependencies...

[omitted]

Installing Edge TPU Python API...

Processing ./edgetpu-1.9.2-py3-none-any.whl

Installing collected packages: edgetpu

Successfully installed edgetpu-1.9.2 # restart the pi just to complete the installation

pi@raspberrypi:~/edgetpu_api $ sudo reboot now

After reboot, let’s try to test it by running a live object detection program. We will run a demo object detection app from the DeepPiCar repo.

pi@raspberrypi:~ $ cd ~/DeepPiCar/models/object_detection/ pi@raspberrypi:~/DeepPiCar/models/object_detection $ python3 code/coco_object_detection.py

W0420 12:36:55.728087 7001 package_registry.cc:65] Minimum runtime version required by package (5) is lower than expected (10). couch, 93% [[ 4.81752396 167.15803146]

[381.77787781 475.49484253]] 113.52ms

book, 66% [[456.68899536 145.12086868]

[468.8772583 212.99516678]] 113.52ms

book, 58% [[510.65818787 229.35571671]

[534.6181488 296.00133896]] 113.52ms

book, 58% [[444.65190887 222.51708984]

[467.33409882 290.39138794]] 113.52ms

book, 58% [[523.65917206 142.07738876]

[535.19741058 213.77527237]] 113.52ms

------

2019-04-20 12:36:57.025142: 7.97 FPS, 125.46ms total, 113.52ms in tf

You should see a live video screen coming up, and it will try to identify objects in the screen at around 7–8 Frames/sec. Note that the COCO (Common Object in COntext) object detection model can detect about 100 common objects, like a person, chair, TV, couch, book, laptop, cell phone, etc. Don’t let this simple program fool you, this is Deep Learning at work. The object detection model used in this program is called ssd_mobilenet_coco_v2, and it contains more than 200 layers! (For comparison, I have tried earlier to run the COCO object detection model with Pi’s CPU, it was a much longer set up, can only run at 1 Frame/sec, CPU’s utilization is 100%, and CPU temperature heats up very quickly. So running deep models on CPU is not recommended.) Of course, this is just a demo app that confirms that Edge TPU is set up correctly. We will exploit Edge TPU’s full capabilities in Part 6 of this series, Real-Time Traffic Sign and Pedestrian Detection and Handling.

Update (March 2020): Coral EdgeTPU’s installation has been updated since my blog’s published date of Apr 2019. Please follow the current official instructions from Google here to install Tensorflow Lite API and EdgeTPU runtime, so that you can do the object detection example above.

What’s Next

Congratulations, we have now given your car both an eye (camera and OpenCV) and a brain (TensorFlow), so it is indeed a DeepPiCar. Whenever you are ready, head on over to Part 4, where we will teach DeepPiCar to autonomously navigate within lanes.

Here are the links to the whole guide:

Part 1: Overview

Part 2: Raspberry Pi Setup and PiCar Assembly

Part 3: Make PiCar See and Think (This article)

Part 4: Autonomous Lane Navigation via OpenCV

Part 5: Autonomous Lane Navigation via Deep Learning

Part 6: Traffic Sign and Pedestrian Detection and Handling"
A Deep dive into H2O’s AutoML,"The demand for machine learning systems has soared over the past few years. This is majorly due to the success of Machine Learning techniques in a wide range of applications. AutoML is fundamentally changing the face of ML-based solutions today by enabling people from diverse backgrounds to use machine learning models to address complex scenarios. However, even with a clear indication that machine learning can provide a boost to certain businesses, a lot of companies today struggle to deploy ML models.

This is because there is a shortage of experienced and seasoned data scientists in the industry. In a way, the demand for machine learning experts has outpaced the supply. Secondly, a lot of machine learning steps require more experience than knowledge, especially when deciding which models to train and how to evaluate them. Such gaps are pretty apparent today, and a lot of efforts are being taken to address these issues. Automated Machine learning may be an answer to such impediments, and in this article, we shall understand in-depth how that can be achieved.

Automated Machine Learning: AutoML

Automated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. AutoML tends to automate the maximum number of steps in an ML pipeline — with a minimum amount of human effort — without compromising the model’s performance.

Aspects of Automated Machine Learning

Automated machine learning can be thought of as the standard machine learning process with the automation of some of the steps involved. AutoML very broadly includes:"
XGBoost in Amazon SageMaker,"XGBoost in Amazon SageMaker

What is SageMaker? SageMaker is Amazon Web Services’ (AWS) machine learning platform that works in the cloud. It is fully-managed and allows one to perform an entire data science workflow on the platform. And in this post, I will show you how to call your data from AWS S3, upload your data into S3 and bypassing local storage, train a model, deploy an endpoint, perform predictions, and perform hyperparameter tuning.

The data cleaning and feature engineering code are derived from this blog post, which is written by Andrew Long, who gave full permission to use his code. The dataset can be found here.

Starting Your Notebook Instance

Head over to your AWS dashboard and find SageMaker, and on the left sidebar, click on `Notebook instances`. To create an instance, click the orange button that says `Create notebook instance`. Here you can choose the instance name, the instance type, elastic inference (scales your instance size according to demand and usage), and other security features. Unless you have specific security requirements from your company, I tend to leave those alone and just fill in the instance name. It will take a few minutes to launch, and after it successfully hosts the instance, AWS gives you two options to write your code in; Jupyter Notebook or Jupyter Labs.

Importing Packages

Once you choose either one of the options, you will need to run some code to get your notebook ready."
The Most Underrated Python Packages,"A curated list of awesome libraries

source: delphinmedia, via pixabay (CC0)

In my experience as a Python user, I’ve come across a lot of different packages and curated lists. Some are in my bookmarks like the great awesome-python-data-science curated list, or awesome-python curated list. If you don’t know them, go check them out asap.

In this post, I’d like to show you something else. These are the results of late-night GitHub/Reddit browsing, and cool stuff shared by colleagues.

Some of these packages are really unique, others are just fun to use and real underdogs among the data scientist/statistician I’ve worked with.

Let’s start!

Misc (the weird ones)

Knock Knock : Send notifications from Python to mobile devices or the desktop or email.

Send notifications from Python to mobile devices or the desktop or email. tqdm: Extensible Progress Bar for Python and CLI, with built-in support for pandas.

Extensible Progress Bar for Python and CLI, with built-in support for pandas. Colorama: Simple cross-platform colored terminal text.

Simple cross-platform colored terminal text. Pandas-log: It provides feedback about basic pandas operations. Great for debugging long pipe chains.

It provides feedback about basic pandas operations. Great for debugging long pipe chains. Pandas-flavor: The easy way to extend Pandas DataFrame/Series.

The easy way to extend Pandas DataFrame/Series. More-Itertools : as it sounds, it adds additional functions similar to itertools.

as it sounds, it adds additional functions similar to itertools. streamlit : The easy way to create apps for your machine learning projects.

The easy way to create apps for your machine learning projects. SQLModel: SQLModel, SQL databases in Python, designed for simplicity, compatibility, and robustness.

Data Cleaning and Manipulation

ftfy: Fixes mojibake and other glitches in Unicode text, after the fact.

Fixes mojibake and other glitches in Unicode text, after the fact. janitor: A lot of cool functions to clean data.

A lot of cool functions to clean data. Optimus: Another package for data cleaning.

Another package for data cleaning. Great-expectations: A great package to check if your data obeys your expectations.

Data Exploration and Modelling

P andas-profile : Create an HTML report full of statistics from pandas DataFrame.

Create an HTML report full of statistics from pandas DataFrame. dabl : Allow data exploration using visualisation and preprocessing.

Allow data exploration using visualisation and preprocessing. pydqc: Allow to compare statistics between two datasets.

Allow to compare statistics between two datasets. Pandas-summary: An extension to pandas DataFrames describe function.

An extension to pandas DataFrames describe function. pivottable-js: drag’n’drop functionality for pandas inside jupyter notebook.

Data Structures

Bounter : Efficient Counter that uses a limited (bounded) amount of memory regardless of data size.

Efficient Counter that uses a limited (bounded) amount of memory regardless of data size. python-bloomfilter : Scalable Bloom Filter implemented in Python.

Scalable Bloom Filter implemented in Python. datasketch : Gives you probabilistic data structures like LSH, Weighted MinHash, HyperLogLog and more.

Gives you probabilistic data structures like LSH, Weighted MinHash, HyperLogLog and more. ranges: Continuous Range, RangeSet, and RangeDict data structures for Python

Performance Checking and Optimization

Py-spy: Sampling profiler for Python programs.

Sampling profiler for Python programs. pyperf: Toolkit to run Python benchmarks.

Toolkit to run Python benchmarks. snakeviz : An in-browser Python profile viewer with great support for Jupiter notebook.

An in-browser Python profile viewer with great support for Jupiter notebook. Cachier : Persistent, stale-free, local and cross-machine caching for Python functions.

Persistent, stale-free, local and cross-machine caching for Python functions. Faiss : A library for efficient similarity search and clustering of dense vectors.

A library for efficient similarity search and clustering of dense vectors. mypyc : A library that compile Python code to C extensions using type hints.

A library that compile Python code to C extensions using type hints. Scalene: a high-performance CPU, GPU and memory profiler for Python.

I hope you found something useful or fun for your work. I’m going to expand the post in the future, so stay tuned for new updates!"
From Business Intelligence to Data Science & Machine Learning,"I’m a Manager of Data Science & Machine Learning, specialising in design & building of solutions powered by DS & ML for enterprises for their analytics requirements. I have more than 5 years of extensive experience now in the field and like many, I did not begin my career in Data Science & Machine Learning. I was a Business Intelligence professional with close to 10 years of experience when I decided to take the plunge into Data Science. I’d like to share my experience of transitioning into the field in the hope that it might help a few who’re are looking to transition into the field.

It’s definitely not this hard

I’d imagine it to be something like this

For those of you who are not familiar with Business Intelligence(BI), it is a branch of technology that enabled the organization to gather all the historical enterprise data that is available to them and start using them to make business decisions both tactical & strategic. As the field of data science & machine learning was just beginning to take off in the early part of the decade (Thanks to the article “Data Scientist: The Sexiest Job of the 21st Century” https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century that got published in HBR) I felt a strong urge to ride the wave. But later felt that this is a transition that I had to make sooner or later because it was, in my opinion, a logical extension to the field of BI. While the field of Data Science & Machine Learning was about giving business the edge by providing actionable insights and intelligence about the future (some may argue that it’s also about the present), the field of BI did the same, but using data from the past using lesser sophisticated techniques through dashboarding and visualization techniques.

Or Maybe, little harder

That said, here a few pointers that I’d like to share with all the wannabe Data Science/Machine Learning professionals

Almost every Data Scientist is a Citizen Data Scientist

Given the sudden surge in the demand for data scientists & machine learning professionals when the industry does not have enough talent, most of the demand is being met with people who transitioned to ML & DS from fields as diverse as Bach of Arts, Psychology etc. So don’t ever feel that you are late in the game. In fact, if you are from the field of BI, you can leverage a lot of your existing data skills in DS/ML (Pandas Dataframe operations could very well be done on a relational table using plain SQL).

Apparently even during the age of Darwin, many scientists were ordinary people who took to science out of curiosity & interest and were hence called citizen scientists. Hence, you could be a citizen data scientist too if you have the interest and drive in you.

2. Have a structured learning path

MOOCs on DS & ML are plenty these days. I chose to certify in a couple of courses.

Its okay to be slow, but be steady

Machine Learning Specialisation from University of Washington (Coursera) https://www.coursera.org/specializations/machine-learning

This is 4-course specialization by Carlos Guestrin and Emily Fox who founded Turi that later got acquired by Amazon, covering all important areas of machine learning — Classification, Regression & Clustering using python which is one of the reasons I chose this program

2. Deeplearning.ai(Coursera) https://www.deeplearning.ai/deep-learning-specialization/

This is a 5-part specialization in Deep learning offered by Andew Ng himself explaining the concepts of popular topics — Deep Neural Networks, CNN, Sequence Models et al

3. Udacity Nanodegree in Deep Learning Specialization https://in.udacity.com/course/deep-learning-nanodegree--nd101

This is a 4-month specialization in Deep Learning concepts using Pytorch which is one of the emerging and widely used DL frameworks in the deep learning research community today. I chose this primarily because I wanted to stick to a framework with Python flavour for easier adaptability.

Btw, this is not an exhaustive account of courses that I tried and there are many more courses that I’ve done partially but not naming them here. I’ll cover all my course experience in an exclusive story later.

3. Get ready to be hands-on

Be ready to code, regardless of your title

While these courses help you to get a grounding on the concepts with some hands-on projects, it's important that one practices these concepts in order to build expertise. One of the activities that helped me was to participate in competitions in the popular competition sites such as Kaggle, Analytics Vidhya to keep in touch with your learning. For eg. I took part in the competition “Loan Prediction Challenge III” from Analytics Vidhya and secured 31st rank in the overall leader board. https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/lb

Even though this is a playground competition, it helps you to build confidence in the skills that one has learnt.

4. Open other channels of Learning

Courses are one of the sources of learning, but it cannot be the only source of learning. It's important to tap into other sources of learning as well in order to develop a wide and broad perspective of topics. Here are other learning activities that I undertake.

a) Other online learning content — Subscribed to “Practical Deep Learning with PyTorch” by an NUS researcher on Udemy. https://www.udemy.com/practical-deep-learning-with-pytorch/learn/v4/content”

I liked the way he explains the concept of a convolution filter as a filter that looks for specific shapes/patterns in an image. This gives a different perspective on the concepts learnt in the courses mentioned above.

b) Follow the works of topmost influencers in the field — I follow Jeremy Howard@jeremyphoward, Rachel Thomas@math_rachel ‏(co-founders of fast.ai an advanced pytorch based framework that makes Deeplearning coding a lot easier), Andrej KarpathyAndrej Karpathy,Andrew Ng@AndrewYNg ‏, Jason Brownlee@TeachTheMachine ‏,Soumith Chintala@soumithchintala ‏

You may follow them on twitter or read their blogs to keep a tab on their works from time to time. Since I chose python based frameworks for my learning, almost all of the above use Pytorch as a framework or a python based libraries for ML & DS. So my list could be biased that way, but essentially the point is that you’ll need to follow those people who are the biggest influencers in the field to be in tune with what they are doing.

c) Read blogs — Follow blogs of top technology companies that do path-breaking work in ML & DS. I choose the following companies blogs closely through Feedly.

Facebook AI Research (FAIR), fast.ai, Google AI blog, AWS ML Blog, Open AI blog. This is not an exhaustive list by any account, but I’m just calling the top ones

5. Don’t wait to land the “perfect” ML/DL project

Once the learning is done or even when the learning is on, try to implement the learnings in your current project by proposing ML, DL concepts in your solution design. From my experience working with clients from Fortune 500, most clients don’t specifically ask for an ML/DL project. Their priorities are always meeting the business objective and the design & approach of the solution is entirely left to us. So, I advise you to look for opportunities where you can put some of the things you learnt to practical use. For instance, I took up implementing text classification tasks using ML and simple sentiment classifier using DL in my projects.

6. Take up pet projects

In addition to projects for clients, taking up pet projects is another great learning opportunity for you and also the organization. This is especially true in the case of DL as most of the projects are in the realm of unstructured data in the form of text & images. So working on some pet project could not only help your knowledge grow but also gets your organization to take notice of the skills that you bring to the table and for your team to get hooked onto these concepts.

For instance, I’d taken up the task of building a prototype to automatically detect license plates of cars using object detection technique. More on this later, but below is a snapshot of tool that my team & I built."
"Reducing Uncertainty: The less you know, the more you learn","Reducing Uncertainty: The less you know, the more you learn

In an era where data has become so prevalent, we’ve become too accustomed on solving problems where we feel we have “enough data” and dismiss the ones where we feel there is a lack of. To make matters worse, the buzz of Big Data has altered our expectations to render small data as useless, uninformative, and quite frankly boring. But that’s why we have data scientists, right?

In fact, the title, data scientists, is a bit redundant; what exactly is the other type of scientist? Isn’t every scientist a data scientist? According to sciencecouncil.org, a scientist is someone who:

systematically gathers and uses research and evidence, making a hypothesis and testing it, to gain and share understanding and knowledge.

In essence, science is more about gathering data than about having data. We should instead view ourselves as business or possibly decision scientists: observing and collecting data in order to inform our decisions.

I’ll demonstrate using 3 examples how the very act of gathering, especially where little, or no data is available, can be rewarding.

#1 Estimating Percent of Contaminated Fish

Let’s say there’s a rumor of an outbreak of a certain water-borne disease at a nearby lake that has potentially infected the fish. You have absolutely no idea what fraction of the fish (if any) have been infected. If you wanted to be 100% certain what percent of fish are infected, you’d need to sample every fish (an unreasonable and expensive feat). Instead, you decide to randomly sample a several fish and observe whether they’re contaminated? Of course we know the more we sample, the better our estimate. That said however, does our net increase in the certainty of our estimate grow or diminish as we sample more and more fish? In other words, is the increase in certainty we gain in sampling the 1st fish equal to, less than, or more than the increase in certainty we gain in sampling the 1000th fish? Lets establish a few things before continuing:"
The Geometry of Thought,"The Geometry of Thought

By José Ferraz de Almeida Júnior [Public domain]

We think every day, but it’s hard to pinpoint what exactly we mean by the word. When we try to understand thought, we have to grasp thought by thinking about thought itself, which feels ever so slightly elusive and circular.

And then there are a hundred different ways of thinking about a thing. What do you actually do when thinking about an apple? When thinking of the color red? When thinking about abstract concepts like love, grief, pride, existence?

Wikipedia defines thought as an “aim-oriented flow of ideas and associations that can lead to a reality-oriented conclusion”.

The flow of ideas is easily observable. Think of an apple, and see what happens, which different perceptual dimensions present themselves. A glimpse of red-green peel, a hard-to-define sweet and fruity taste, maybe a hint of sourness, the crunchy sound of a bite, the sensation of juice running down your hand.

The reality-oriented conclusion is naturally given: we shouldn’t forget that our thinking has evolved to serve an evolutionary purpose. Thought exists because it helps the genetic machinery at the heart of life propel itself onward into the future. Thought exists because it allows us to observe useful, structured patterns in the messy world in which we find ourselves. To separate the wheat of useful information from the chaff of irrelevant one.

It serves a purpose to know an apple from a poisonous fruit, to understand if it's ripe and ready to be eaten, to understand what an apple is, what properties it has although every individual apple you will encounter is ever so slightly different.

It serves a purpose to be able to compare apples and oranges. And it makes sense to be able to communicate your thoughts with the people surrounding you, to tell them what it is you feel, what it is you saw.

From the photograph you quickly inferred that these are apples. Photo by Marina Khrapova on Unsplash

There are certain tasks that we constantly need in everyday life, so the brain is optimized to do the job. On the other hand, we are not good at doing “thinking” that computers are really good at (questions…"
Data Democratization,"Data Democratization

In today’s world, every business is bombarded with data from each and every angle possible. There always remains a constant pressure to use various insights we garner from the data to improvise on our business performance. Hence, the incredible amount of the usage of the processed data has surged the desire and demand for data-democratization. So, if you are wondering that the above mentioned term is totally new to you, let’s dive deep into it.

Diagnosis of the term ‘Data Democratization’

Before grabbing some knowledge on data democratization, we need to understand what does democratization mean. We can define it as the introduction of a democratic system or democratic principles; on breaking down the term further we can elucidate it as the action of making something accessible to everyone.

And as aptly named, data democratization is the process of making data accessible to everyone, which implies that there are no gatekeepers that create a bottleneck at the gateway to the data. It provides an ease to the people for a better understanding of the data to expedite the decision-making and uncover brilliant opportunities for an organization. The ultimate mission is to have anybody use the data anytime, as and when required, to make a major impact in decision-making with no barricade or roadblocks.

Of late, data was “retained” by the IT departments. Various business units such as marketing, sales, executives are always in need to data in order to make important business decisions. Needless to say that they always had to go through their respective IT departments to access the required data. This is the way it has been for a long time, few decades may be, and there are people out there who still believe that this is the way it should be.

Pros: Need for Data Democratization

Should we be Liberal regarding data-sharing?

Advocates of data democratization are in the belief that it should be obligatory to disperse information on all the working teams so as to gain a competitive edge. The more people with diverse expertise having the access to data, the more organization will benefit by identifying and taking actions on important and critical business insights. It is believed by many high and esteemed professionals that data democratization can be a game-changer to many organizations. When the data gets spread out all over across the organization, it empowers each and every level of the individual to use the data and participate for the contribution in decision-making.

Cons: Apprehension about Data Democratization

Or should we be Conservative regarding data-sharing?

Yet many organizational individuals are still perplexed as to sharing the data to the non-technical employees could result in misinterpretation of the data which might result in making some bad business decisions. Additionally, the more users having access to data, the bigger is the data-security risk and thus resulting in more challenges to maintain the data integrity.

Talking about the data, some data still exists in silos/warehouse/archive-mode; although several attempts have been made in recent years and the attempts are still in progress, but the reality cannot be denied that people from different departments are still finding it difficult to access the data and view it.

There is another concern which cannot be denied — a duplication of efforts across different teams which could lead to more cost and wastage of time and effort when compared to a centralized analysis group.

Be Conservative or Liberal — What Should be the approach?

For the sake of argument, it might be concluded that it is easy to preach one approach over the other, but the reality states that extremely conservative or liberal views rarely fit into the context of data democratization. The business leaders should carefully weigh the pros and cons of data democratization in determining which kind of approach benefits their organization the most.

The business leaders should decide upon how much liberal or conservative they should be regarding their organization — ultimately, it all comes down to their business or organizational success. But, they can always spread out the metadata (and retaining the raw data) within the organization individuals to gain meaningful and impactful insights and can retain the sensitive data within themselves.

For example, if we speak about a particular health-care center which deals with diabetic patients, it is not permissible to share the personal details of the patients (and thereby acting as intruders to their privacy), but analysis can always be done the generic data and thus finding out the age group or the sex-ratio are being trapped under diabetics.

I agree that sometimes the perception shared by the non-technical people or the people who are not data-literate might be misleading, but attempts can surely be made to add an extra layer of scrutiny on top of that, to avoid any kind of mistakes and ensure the organizational growth — that’s what the business leaders are there for."
How to use NVIDIA GPUs for Machine Learning with the new Data Science PC from Maingear,"Deep Learning enables us to perform many human-like tasks, but if you’re a data scientist and you don’t work in a FAANG company (or if you’re not developing the next AI startup) chances are that you still use good and old (ok, maybe not that old) Machine Learning to perform your daily tasks.

One characteristic of Deep Learning is that it’s very computationally intensive, so all the main DL libraries make use of GPUs to improve the processing speed. But if you ever felt left out of the party because you don't work with Deep Learning, those days are over: with the RAPIDS suite of libraries now we can run our data science and analytics pipelines entirely on GPUs.

In this article we’re going to talk about some of these RAPIDS libraries and get to know a little more about the new Data Science PC from Maingear.

Why do people use GPUs anyway?

Generally speaking, GPUs are fast because they have high-bandwidth memories and hardware that performs floating-point arithmetic at significantly higher rates than conventional CPUs [1]. GPUs' main task is to perform the calculations needed to render 3D computer graphics.

But then in 2007 NVIDIA created CUDA. CUDA is a parallel computing platform that provides an API for developers, allowing them to build tools that can make use of GPUs for general-purpose processing.

GPUs had evolved into highly parallel multi-core systems, allowing very efficient manipulation of large blocks of data. This design is more effective than general-purpose central processing unit (CPUs) for algorithms in situations where processing large blocks of data is done in parallel — CUDA article on Wikipedia [2]

Processing large blocks of data is basically what Machine Learning does, so GPUs come in handy for ML tasks. TensorFlow and Pytorch are examples of libraries that already make use of GPUs. Now with the RAPIDS suite of libraries we can also manipulate dataframes and run machine learning algorithms on GPUs as well.

RAPIDS

RAPIDS is a suite of open source libraries that integrates with popular data science libraries and workflows to speed up machine learning [3].

Some RAPIDS projects include cuDF, a pandas-like dataframe manipulation library; cuML, a collection of machine learning libraries that will provide GPU versions of algorithms available in sciKit-learn; cuGraph, a NetworkX-like accelerated graph analytics library [4].

Pandas and sciKit-learn are two of the main data science libraries, so let’s get to know more about cuDF and cuML.

cuDF: dataframe manipulation

cuDF provides a pandas-like API for dataframe manipulation, so if you know how to use pandas you already know how to use cuDF. There is also the Dask-cuDF library if you want to distribute your workflow across multiple GPUs [5].

We can create series and dataframes just like pandas:

import numpy as np

import cudf s = cudf.Series([1,2,3,None,4]) df = cudf.DataFrame([('a', list(range(20))),

('b', list(reversed(range(20)))),

('c', list(range(20)))])

It’s also possible to convert a pandas dataframe to a cuDF dataframe (but this is not recommended):

import pandas as pd

import cudf df = pd.DataFrame({'a': [0, 1, 2, 3],'b': [0.1, 0.2, None, 0.3]})

gdf = cudf.DataFrame.from_pandas(df)

We can also do the opposite and convert a cuDF dataframe to a pandas dataframe:

import cudf df = cudf.DataFrame([('a', list(range(20))),

('b', list(reversed(range(20)))),

('c', list(range(20)))]) pandas_df = df.head().to_pandas()

Or convert to numpy arrays:

import cudf df = cudf.DataFrame([('a', list(range(20))),

('b', list(reversed(range(20)))),

('c', list(range(20)))])

df.as_matrix() df['a'].to_array()

Everything else we do with dataframes (viewing data, sorting, selecting, dealing with missing values, working with csv files and so on) works the same:

import cudf df = cudf.DataFrame([('a', list(range(20))),

('b', list(reversed(range(20)))),

('c', list(range(20)))]) df.head(2)

df.sort_values(by='b')

df['a']

df.loc[2:5, ['a', 'b']] s = cudf.Series([1,2,3,None,4])

s.fillna(999) df = cudf.read_csv('example_output/foo.csv')

df.to_csv('example_output/foo.csv', index=False)

About performance, just to give an example, loading a 1gb csv file using pandas took 13 seconds and loading it with cuDF took 2.53 seconds."
Regression Discontinuity Design: The Crown Jewel of Causal Inference,"Why Harley? B/C Data Science Rocks!

Introduction

In a series of posts (why experiments, two-way causal direction, pitfalls, correlation & causation, and Natural experiments), we have covered topics including what, why, and how to conduct experimentation. Regardless of how desirable they appear to be, it is impossible to run experiments for all types of business questions, for various reasons.

It could be unethical to do so. Let’s say we are interested in the effect of undergraduate education on students’ future earnings. It would be fundamentally wrong to randomly assign some high students to receive the education and not to others.

Or, it could be expensive, time-consuming, or technically infeasible. Even top companies like Netflix and Airbnb can’t guarantee the internal validity of randomization at the individual user level all the time.

Under these scenarios, we have to rely on other methods, both observational and quasi-experimental designs, to derive causal effects. As an established quasi-experimental technique, Regress Discontinuity Design, RDD, has been through a long period of dormancy and comes back strong until recently.

In this post, we elaborate on RDD’s underlying constructs, such as research ideology, statistical assumptions, potential outcomes framework (POF), merits, limitations, and R illustration.

What is RDD?

First off, it is a quasi-experimental method with a pretest-posttest design, implying that researchers administer the measure of interest before and after the intervention (treatment).

By setting up a “cutoff” point, we select the subjects slightly above the threshold line as the treatment group and the ones slightly below the line as the control group. Since these two groups are geographically close to each other, we can control potential confounding variables and treat it as an as-if random treatment assignment. If there is any difference in the…"
Taking Google Sheets to (a) Class.,"I am currently building a Flask app for teachers. Since Google Drive has been adopted by teachers, Google sheets are used by them also. One of my app’s features is to easily allow teachers to copy and paste the sheet link into the app and submit it through a form. It will then convert it into a pandas data frame and drop the null rows and columns.

Google Sheets Icon

It is a work in progress but I wanted to share the class made so far since I don’t find many Python class examples for data collection or preprocessing. Many of the examples I have seen are useful for teaching the concept of classes. But most of the time they are not practical enough or specific to data science or ML. After all, we could always use more practical examples of Python classes.

See, Yoda knows. (Self-made on imgflip.com)

But why would you need a class? Can’t you just write the code and be ok?

If this were for analysis purposes, then…yes. But production code, as far as I know, follows an object-oriented programming approach. This means that your code is split between scripts. These scripts are primarily composed of classes and functions. Code written in this manner is also easier to reuse for additional purposes. Take the ecdf function I used in this post as an example.

With this function, you can quickly reproduce this plot immediately as opposed to retyping the same matplotlib templating code over and over again.

Great! Show me your example!

Of course! I will share this example in pieces and explain each part as we go.

# Import necessary libraries

import gspread

import pandas as pd

from gspread_dataframe import get_as_dataframe

from oauth2client.service_account import ServiceAccountCredentials"
Stop Using Mean to Fill Missing Data,"Mean imputation was the first ‘advanced’ (sighs) method of dealing with missing data I’ve used. In a way, it is a huge step from filling missing values with 0 or a constant, -999 for example (please don’t do that).

However, it still isn’t an optimal method, and today's post will show you why.

Photo by Pietro Jeng on Unsplash

The Dataset

For this article, I’ve chosen to use Titanic Dataset, mainly because it’s well known, and the Age column contains some missing values. To start out, let’s import everything needed and do some basic data cleaning. Here are the imports:"
"Predicting Lyme Disease, the Fastest Growing Infectious Disease in the U.S.","300,000. That’s the number of people each year who are estimated to be infected with Lyme disease in the United States.

But even more concerning, only 10% of those cases get reported annually to the Center for Disease Control and Prevention (CDC). That means that there’s possibly 270,000 people who get infected with this disease every year and may not even know they have it.

Figure 1: Number of Reported Cases of Lyme Disease to the CDC from 2000–2017 (Data Source)

And the number of people affected is growing too. As seen in Figure 1, the annual cases of Lyme disease have more than doubled in the last 20 years. Mostly concentrated in the Northeast and upper Midwest, it continues to spread to new counties every year.

And it’s all as a result of the Black-legged tick, also known as the Deer tick. If you get bitten by an infected tick, the bacteria, Borrelia burgdorferialone, will enter your blood stream and begin its quiet destruction of your immune system.

Within about a week, 70 to 80 percent of people get the characteristic “bull’s-eye” rash, known as Erythema migrans. And this is generally followed by typical flu-like symptoms, like a fever, headache, chills, and general fatigue.

This doesn’t seem like much, but if left untreated, it will cause widespread inflammation to almost every body system. Some of the symptoms include debilitating arthritis and swelling of the joints, facial paralysis, heart palpitations, shortness of breath, and swelling of the brain and spinal cord leading to severe nerve pain and memory loss.

But it doesn’t have to be this way. If diagnosed in the early stages, Lyme disease can be completely cured with certain antibiotics. It all comes down to increasing public awareness and educating healthcare providers in at-risk areas.

This is where machine learning comes in. My idea was to build a classification model that could predict which U.S. counties would have a high incidence of Lyme disease. In doing so, counties at high risk could be informed by the CDC in advance in order to proactively take measures against the infection.

Acquiring the Data

But here’s the issue. Funding for research of Lyme disease is disproportionately lower than other diseases. Thus, there is very little research and surveillance being done on Lyme disease at the moment and publicly available data is extremely limited. That meant that I would have to build my dataset completely from scratch.

Challenge accepted. Let the data wrangling commence.

If you want to follow along with my code, check out my GitHub repository; I’ve organized it chronologically with this article for your convenience.

Target Variable

First, I needed to engineer my target variable. Luckily, the CDC has data on the number of reported cases per county from 2000 to 2017. Although the disease is severely under-reported as previously mentioned, this still gives a good picture of the geographic distribution of the disease.

For consistency, I decided to restrict the data to only 2010–2017 because, as shown in Figure 1, there is an unusually large spike in 2009. Also, it is likely that data from many years ago is not as representative of today, so removing data before 2010 would likely be beneficial for the predictability of the model.

But still, the data in this form won’t work for a classification problem. One issue is that the number of cases is heavily dependent on how many people live in each county. To solve this, I acquired county population estimate data from the Census Bureau. I then merged the datasets together and divided the number of cases per county by that county’s estimated population at the time.

This resulted in a rate of Lyme disease cases per person. Now we have a metric that allows us to more reasonably compare counties to one another.

There’s still one more step though. For supervised learning algorithms, the target variable must be labeled. I found out that the CDC defines a high incidence county as having 10 or more confirmed cases per 100,000 persons. This allowed me to bin the data such that anything above that cutoff was considered a high incidence county and anything below it was a low incidence county.

Features

Okay, but how do we get features for this model? I decided to dive into the research to find out.

In 1998, Stafford et al. showed that tick abundance had a strong positive correlation with Lyme disease infections. So you’d think that by now we would have a well-established national tick surveillance program.

Figure 2: CDC Map Showing Geographic Distribution of the Black Legged Tick (Source)

Well, no. As shown above, all the CDC offers today is a qualitative map showing where the Black-legged Tick might be. Not very helpful.

In the last few years, some states, like Connecticut and North Dakota, have taken the initiative themselves to measure tick concentrations , but until there is a coordinated national surveillance program, nation-wide tick population data will not exist.

Luckily, there is a good proxy for this. Khatchikian et al. showed that environmental factors such as extreme winter temperatures and summer and winter precipitation directly regulate tick population dynamics. This is likely a result of the climate’s effect on acorns. If the climate is optimal for acorn growth, mice and deer populations that eat them will thrive. The ticks that live off of these host animals will also flourish as a result.

This is great news, as there is a plethora of climate data available from the National Oceanic and Atmospheric Administration (NOAA).

I soon found out though that this would be a little harder that expected because there is a limit of 10,000 API requests per day. Given that I needed data for every weather station in every U.S. county for 7 years, it would have taken me about 56 days to download all the data. Not ideal.

I was able to get around this though. Fortunately, I found a page that allowed access to every Global Summary of the Year file and I downloaded all of it to my local computer. Of course, this meant that I now had data for every station in the world starting from around 1900, which amounted to approximately 78,000 CSV files, each corresponding to a particular station.

Obviously, I only wanted a small percentage of that. So, I had to write code to manually parse through each CSV file.

Figure 3: Example of NOAA Global Summary of the Year CSV Files

Above, you can see the general format of each file (there are many more columns that aren’t shown here). Essentially, I took the latitude and longitude from each file and used that as input for the reverse-geocoder python library, which outputs the country, state and county of that location. If it was a U.S. station, I then took only the rows from 2010 to 2017 and appended them to a global dataframe with their associated state and county as new columns.

After over 24 hours of parsing, I was left with a mess of data set. First of all, there were 209 columns with cryptic names, like “DX90” and “EMSD”. I used the provided documentation to determine the meaning of each feature and then through careful research, meticulously removed the ones that weren’t relevant. I also relabeled the columns to have more understandable names.

Figure 4: The Percentage of Missing Values for Each of the 35 Selected Features

The next problem was the large number of missing values as seen in Figure 4. For whatever reason, many of the stations were not consistent in recording and/or reporting their measurements.

I didn’t want to get rid of columns because then I would have lost some of the information that could have been gleaned from the climate data. So instead, I chose to impute all the missing values.

Now, this is an area where we have to be super careful. We need to put in a placeholder that would be reasonably representative of the actual value.

For example, let’s say that the total precipitation value for Montgomery County, Pennsylvania in 2017 is missing. It is reasonable to believe that the average total precipitation of all the counties in Pennsylvania will come close to Montgomery County’s actual precipitation. And so that’s what I did for all the missing values; I found the average for that county’s state in that particular column and imputed the value.

Hawaii and Washington D.C. had no data in several of the columns, so I had to remove them completely. Hawaii is geographically isolated and has no incidence of Lyme disease so there is no issue removing it. Washington D.C. is significantly smaller than any state so it should have no effect on the modeling.

Merging the Target with the Features

Alright, so now that the data was nice and clean, I was almost ready to merge the features with the target.

Since I was trying to build a predictive model, I would need to use the previous year’s climate data to predict the current year’s Lyme disease incidence.

To do this, I created a new column in the features data that contained the next year. I then merged the target data on that column in order to create a one year offset.

Preparation for Modeling

Class Imbalance

Before any modeling, it’s important to check if the data will work well for the algorithms you’ll be using. In the case of classification models, the classes need to be relatively balanced. That means that there should be an equal number of instances for the positive and negative classes.

Figure 5: Class Imbalance in Data

As seen above, this data has a significant class imbalance problem with 85% of the counties having low Lyme disease incidence and only 15% of the counties having high Lyme disease incidence.

But why is this such an issue? Well, if a majority of the counties have a low Lyme disease incidence, then a model will get the best accuracy by guessing that all the counties have a low incidence. In this case, 85% of the time the model would be right. But this comes at a terrible cost of 0% accuracy for the minority class. In other words, our model would be utterly useless for the counties we care about that have a serious Lyme disease problem.

Resampling Techniques

I experimented with many resampling techniques to counteract this. Random undersampling is where you randomly select a small percentage of the majority class and delete the rest of the data points. This removal results in a balance of the majority class with the minority class. But this comes with the obvious downside of losing information, which could reduce the model’s predictability.

Then there’s random oversampling, which randomly selects a portion of the infrequent class and just duplicates those data points. This also results in balanced classes; the caveat here is that having a bunch of rows that are exactly the same could lead to overfitting, where the model just memorizes the over-represented synthetic data and loses its generalizability to real world unseen data.

That’s where Synthetic Minority Over-sampling Technique, also known as SMOTE, comes in. Instead of just copying parts of the minority class, it seeks to create data that are similar but not exactly the same as the original data. On a high level, it randomly generates new points directly between the infrequent class’s data points, leading to completely unique synthetic data.

And lastly, there’s the Adaptive Synthetic Sampling Approach or ADASYN, which is like SMOTE but creates more synthetic data for minority class samples that are harder to learn and fewer synthetic data for minority samples that are easier to learn. This results in a distribution of synthetic data that will strengthen the model’s ability to distinguish between the classes at the decision boundary.

Splitting the Data

Before we can see how these techniques perform, we must partition the data. I split the data into three parts: 60% for training the different models, 20% for validating and optimizing the best model, and 20% as a test set to demonstrate the generalizability of the final model.

Initial Modeling

Now we get to the fun part. I built a modeling pipeline using the imbalanced-learn python package in combination with scikit-learn’s GridSearchCV. This allowed me to do an exhaustive grid search for every combination of hyperparameters and data transformations (like scaling and resampling techniques), while also doing 5-fold cross validation to more robustly test how each combination performed.

The algorithms I ran through this pipeline were:

I’ve provided links to helpful articles in case you are interested in learning more about each model.

Evaluating Each Model’s Performance

Figure 6: ROC Curves for Each Model

After optimizing each algorithm individually, I tested the resulting five models on the validation set. I then plotted them against each other using the Receiver Operating Characteristic Curve, also known as the ROC curve, in Figure 6. The area under the ROC curve (ROC AUC) was used to compare the models.

Simply put, this metric represents the cost versus benefit of the model at every threshold, quantifying how good the model is at distinguishing between the classes. The best models will have curves that are close to the upper left corner and take up the most area in the plot.

As you can see in Figure 6, Random Forest (in pink) is superior to the other models at basically every threshold, with Support Vector Machines (in orange) coming in second; the models had ROC AUC scores of 0.947 and 0.934 respectively. A score of 1 means it perfectly predicts the data, so these are very respectable results.

Interestingly enough, random oversampling produced the best results for both of these models, which demonstrates that sometimes simplicity can outperform even the most sophisticated methods.

Optimizing the Best Model

Figure 7: Distribution of the Best Random Forest Model’s Outputs

Above is the distribution of the predicted probabilities outputted by the best Random Forest model. These numbers represent how likely it is that a particular county will have a high incidence of Lyme disease.

The outputs are right skewed indicating that the model is predicting that most the counties have a low probability of having a high incidence. This is exactly what we want to see, especially since the classes are imbalanced.

Right now, the default threshold for the model is 0.5, meaning that any county with a probability above 0.5 will be classified as high incidence and any county below 0.5 will be classified as low incidence. But what if this threshold is not the most optimal?

That’s a major reason why I picked ROC AUC as my evaluation metric, as it is independent of threshold. This meant that I could then tune the threshold to optimize the model for the costs associated with making mistakes. I set aside an additional 20% of my data specifically for this purpose.

So now we need to determine these costs. This is arguably the most important step in the process as we are going to place the model in the context of the real world, and not just in the vacuum of mathematics. I had to approximate the costs myself but ideally the stakeholder you are working with would be able to give you this information.

As I mentioned earlier, the idea behind this model is to give the CDC a tool to determine which areas they need to target their efforts. If a county is classified as having high Lyme incidence, the CDC would take two specific actions:

Increase public awareness in order to prevent infections from happening in the first place Provide educational resources for healthcare providers to increase the number of early diagnoses.

False Positive Cost

Okay, but what if the model predicts that a county is going to have a high incidence of Lyme disease but it actually doesn’t? This is called a false positive. One implication of this is that people would needlessly limit their time spent outdoors and possibly even decide not to travel to these areas. This decreased outdoor recreation and tourism could hurt local economies.

To approximate this, I used the estimated total economic contributions that National Parks have on local communities, which is about $20.2 billion annually. Obviously, this doesn’t take into account state parks or any other services that would be affected, but it will work for now.

I then tried to find research that quantified how consumer sales decreased during an established epidemic and there wasn’t alot available. Chou et al. found that the SARS epidemic in Asia cost countries between .2% and 1.56% in consumer related industries. SARS is a very different disease but it’s pretty much all there is at the moment to go off of. I chose to use 1% to roughly approximate the economic cost.

Here is how I arrived at the cost per county of a false positive:

Total Cost of a False Positive:

(20.2 billion x .01) / $64,311 lost per county (20.2 billion x .01) / 3,141 counties

False Negative Cost

Now what about the cost of a false negative? That would be where our model predicts that a county won’t have a problem with Lyme disease when it really does.

This is obviously going to be way more expensive of a mistake. More people would needlessly get infected by Lyme disease due to lack of public awareness. And more people would suffer from chronic Lyme disease, as fewer doctors would be educated on early Lyme disease diagnosis.

Zhang et al. found that the average early Lyme disease patient costs $1,310 annually, while a chronic Lyme disease patient costs $16,199. That’s over 12 times more expensive. This was done in 2000, so I had to account for an inflation factor of 1.41.

Additionally, Aucott et al. approximated that 64% of new Lyme disease cases each year were early while 36% were chronic. As previously mentioned, there are about 300,000 of these new cases every year.

Also, to simplify my calculation, I made the assumption that the CDC’s intervention would be completely effective, resulting in all the new Lyme disease cases being caught early.

To find the cost of a false negative, I would just need to find the cost difference between when the CDC does not intervene and when it does intervene. Here were all my calculations:

Number of Annual Early and Chronic Cases:

.64 x 300,000 = 192,000 cases of early Lyme disease

.36 x 300,000 = 108,000 cases of chronic Lyme disease Inflation-Adjusted Cost of Early vs Chronic:

$1,310 x 1.49 = $1,952 per patient with early Lyme disease

$16,199 x 1.49 = $24,137 per patient with chronic Lyme disease Average Cost for High Incidence County Without CDC Intervention:

192,000 early cases / 3,141 counties = 61 early cases per county

61 early cases * $1,952 = $119,065 108,000 chronic cases / 3,141 counties = 34 chronic cases per county

34 chronic cases * $24,137 = $820,641 $119,065 + $820,641 = $939,706 Average Cost for High Incidence County With CDC Intervention:

300,000 early cases / 3,141 counties = 95 early cases per county

95 early cases * $1,952 = $185,430 ------------------------------ Total Cost of a False Negative:

$939,706 - $185,430 = $754,276 lost per county

Finding the Best Threshold

Finally, I calculated the ratio of the costs between the false negatives and positives:

Ratio of False Negative vs False Positive:

$754,276 / $64,311 = 11.73

According to this, false negatives are almost 12 times more costly than false positives. I then plugged this value into my code and found the optimal threshold was 0.17. This means that any county above 0.17 will be classified as high incidence and any below will be low incidence. Refer back to Figure 7 to visualize where that cutoff would be.

Final Results

To prove the generalizability of my final model, I trained the model on all 80% of the previously used data (the 60% training set and the 20% threshold optimizing set) and then tested with the completely unseen 20% test set from the beginning.

The results were even better than during training with a ROC AUC of 0.961.

Figure 8: Confusion Matrix of Final Model’s Outputs

Additionally, the model had a great recall of about 0.949, meaning that it correctly classified about 95% of the high incidence counties. As seen in Figure 8, it only had 28 false negatives out of the 554 high incidence counties.

This of course came at a cost to the precision which was 0.503. This means that only about 50% of the counties that the model predicted to be high incidence were actually high incidence. As seen in Figure 8, the false positives and true positives are roughly the same.

Because false negatives were so much more costly than false positives, these results make complete sense. It was necessary for the model to be imprecise in order for it to correctly classify as many of the high incidence counties as it did."
What to Avoid: Common Mistakes on Data Science Applications,"What to Avoid: Common Mistakes on Data Science Applications

Or what not to do if you want to get noticed in the competitive field of Data Science. Paul May · Follow Published in Towards Data Science · 9 min read · Sep 17, 2019 -- 1 Share

Photo by Clem Onojeghuo on Unsplash

Data science and machine learning careers are still relatively new and I have previously written an article on the problems data scientists often encounter in their jobs because some companies do not know how to use them to their fullest potential.

Interestingly, having given many technical interviews, I’ve seen that many potential data scientists don’t know how to present themselves well. This is especially true for those with little to no experience in industry (fresh graduate of a data science Msc etc.).

This is a huge shame as I (personally) genuinely want everyone to do their best and get the jobs they want, but I do feel bad when I can’t find strong reasons to get them onto the next rung of the recruitment process. I would imagine this holds true for many other people in my position as well. They want you to succeed but we need to know you can before we invest time for an interview. Oddly they aren’t cost free to the business, if you consider you’ll be meeting 2–3 people for an hour each this can quickly add up in lost productivity.

To put a finger on this costs we can make an order of magnitude estimate. A senior data scientist is £ 60,805 a year (£ 31/hour) with a data scientist generally commanding £ 46,820 a year (£ 24/hour). If you consider the senior data scientist will probably have a total of an hours’ worth of phone conversations with you (or about you to a recruiter) and if you pass that you will then have an interview for an hour with two people (a team data scientist performing technical interviewing and the senior doing some softer skills and fit interviews) followed by a 30 minute private group discussion afterwards about the result of the interview. They have then made an investment of 4 hours or around £ 114 in salary costs in you. However, this is a minimum cost because you would need to factor in additional hours or other factors such as HR personnel etc.

So, an interview is an expensive investment in gauging you, making it important everyone does the best they can."
Step-by-Step Guide to Creating R and Python Libraries (in JupyterLab),"R and Python are the bread and butter of today’s machine learning languages. R provides powerful statistics and quick visualizations, while Python offers an intuitive syntax, abundant support, and is the choice interface to today’s major AI frameworks.

In this article we’ll look at the steps involved in creating libraries in R and Python. This is a skill every machine learning practitioner should have in their toolbox. Libraries help us organize our code and share it with others, offering packaged functionality to the data community.

NOTE: In this article I use the terms “library” and “package” interchangeably. While some people differentiate these words I don’t find this distinction useful, and rarely see it done in practice. We can think of a library (or package) as a directory of scripts containing functions. Those functions are grouped together to help engineers and scientists solve challenges.

THE IMPORTANCE OF CREATING LIBRARIES

Building today’s software doesn’t happen without extensive use of libraries. Libraries dramatically cut down the time and effort required for a team to bring work to production. By leveraging the open source community engineers and scientists can move their unique contribution towards a larger audience, and effectively improve the quality of their code. Companies of all sizes use these libraries to sit their work on top of existing functionality, making product development more productive and focused.

But creating libraries isn’t just for production software. Libraries are critical to rapidly prototyping ideas, helping teams validate hypotheses and craft experimental software quickly. While popular libraries enjoy massive community support and a set of best practices, smaller projects can be converted into libraries overnight.

By learning to create lighter-weight libraries we develop an ongoing habit of maintaining code and sharing our work. Our own development is sped up dramatically, and we anchor our coding efforts around a tangible unit of work we can improve over time.

ARTICLE SCOPE

In this article we will focus on creating libraries in R and Python as well as hosting them on, and installing from, GitHub. This means we won’t look at popular hosting sites like CRAN for R and PyPI for Python. These are extra steps that are beyond the scope of this article.

Focusing only on GitHub helps encourage practitioners to develop and share libraries more frequently. CRAN and PyPI have a number of criteria that must be met (and they change frequently), which can slow down the process of releasing our work. Rest assured, it is just as easy for others to install our libraries from GitHub. Also, the steps for CRAN and PyPI can always be added later should you feel your library would benefit from a hosting site.

We will build both R and Python libraries using the same environment (JupyterLab), with the same high-level steps for both languages. This should help you build a working knowledge of the core steps required to package your code as a library.

Let’s get started.

SETUP

We will be creating a library called datapeek in both R and Python. The datapeek library is a simple package offering a few useful functions for handling raw data. These functions are:

encode_and_bind

remove_features

apply_function_to_column

get_closest_string

We will look at these functions later. For now we need to setup an R and Python environment to create datapeek, along with a few libraries to support packaging our code. We will be using JupyterLab inside a Docker container, along with a “docker stack” that comes with the pieces we need.

Install and Run Docker

The Docker Stack we will use is called the jupyter/datascience-notebook. This image contains both R and Python environments, along with many of the packages typically used in machine learning.

Since these run inside Docker you must have Docker installed on your machine. So install Docker if you don’t already have it, and once installed, run the following in terminal to pull the datascience-notebook:

docker pull jupyter/datascience-notebook

This will pull the most recent image hosted on Docker Hub.

NOTE: Anytime you pull a project from Docker Hub you get the latest build. If some time passes since your last pull, pull again to update your image.

Immediately after running the above command you should see the following:

Once everything has been pulled we can confirm our new image exists by running the following:

docker images

… showing something similar to the following:

Now that we have our Docker stack let’s setup JupyterLab.

JupyterLab

We will create our libraries inside a JupyterLab environment. JupyterLab is a web-based user interface for programming. With JupyterLab we have a lightweight IDE in the browser, making it convenient for building quick applications. JupyterLab provides everything we need to create libraries in R and Python, including:

A terminal environment for running shell commands and downloading/installing libraries;

environment for running shell commands and downloading/installing libraries; An R and Python console for working interactively with these languages;

for working interactively with these languages; A simple text editor for creating files with various extensions;

for creating files with various extensions; Jupyter Notebooks for prototyping ML work.

The datascience-notebook we just pulled contains an installation of JupyterLab so we don’t need to install this separately. Before running our Docker image we need to mount a volume to ensure our work is saved outside the container.

First, create a folder called datapeek on your desktop (or anywhere you wish) and change into that directory. We need to run our Docker container with JupyterLab, so our full command should look as follows:

docker run -it -v `pwd`:/home/jovyan/work -p 8888:8888 jupyter/datascience-notebook start.sh jupyter lab

You can learn more about Docker commands here. Importantly, the above command exposes our environment on port 8888, meaning we can access our container through the browser.

After running the above command you should see the following output at the end:

This tells us to copy and paste the provided URL into our browser. Open your browser and add the link in the address bar and hit enter (your token will be different):

localhost:8888/?token=11e5027e9f7cacebac465d79c9548978b03aaf53131ce5fd

This will automatically open JupyterLab in your browser as a new tab:

We are now ready to start building libraries.

We begin this article with R, then look at Python.

CREATING LIBRARIES IN R

R is one of the “big 2” languages of machine learning. At the time of this writing it has well-over 10,000 libraries. Going to Available CRAN Packages By Date of Publication and running…

document.getElementsByTagName('tr').length

…in the browser console gives me 13858. Minus the header and final row this gives 13856 packages. Needless to say R is not in need of variety. With strong community support and a concise (if not intuitive) language, R sits comfortably at the top of statistical languages worth learning.

The most well-known treatise on creating R packages is Hadley Wickam’s book R Packages. Its contents are available for free online. For a deeper dive on topic I recommend looking there.

We will use Hadley’s devtools package to abstract away the tedious tasks involved in creating packages. devtools is already installed in our Docker Stacks environment. We also require the roxygen2 package, which helps us document our functions. Since this doesn’t come pre-installed with our image let’s install that now.

NOTE: From now on we’ll use the terminal in JupyterLab in order to conveniently keep our work within the browser.

Open terminal inside JupyterLab’s Launcher:

NOTE: If you’d like to change your JupyterLab to dark theme, click on Settings at the top, JupyterLab Theme, then JupyterLab Dark:

Inside the console type R, then….

install.packages(""roxygen2"")

library(roxygen2)

With the necessary packages installed we’re ready to tackle each step.

STEP 1: Create Package Framework

We need to create a directory for our package. We can do this in one line of code, using the devtools create function. In terminal run:

devtools::create(""datapeek"")

This automatically creates the bare bone files and directories needed to define our R package. In JupyterLab you will see a set of new folders and files created on the left side.

NOTE: You will also see your new directory structure created on your desktop (or wherever you chose to create it) since we mounted a volume to our container during setup.

If we inspect our package in JupyterLab we now see:

datapeek

├── R

├── datapeek.Rproj

├── DESCRIPTION

├── NAMESPACE

The R folder will eventually contain our R code. The my_package.Rproj file is specific to the RStudio IDE so we can ignore that. The DESCRIPTION folder holds our package’s metadata (a detailed discussion can be found here). Finally, NAMSPACE is a file that ensures our library plays nicely with others, and is more of a CRAN requirement.

Naming Conventions

We must follow these rules when naming an R package:

must be unique on CRAN (you can check all current R libraries here);

(you can check all current R libraries here); can only consist of letters , numbers and periods ;

, and ; cannot contain an underscore or hyphen ;

or ; must start with a letter ;

; cannot end in a period;

You can read more about naming packages here. Our package name “datapeek” passes the above criteria. Let’s head over to CRAN and do a Command+F search for “datapeek” to ensure it’s not already taken:

Command + F search on CRAN to check for package name uniqueness.

…looks like we’re good.

STEP 2: Fill Out Description Details

The job of the DESCRIPTION file is to store important metadata about our package. These data include others packages required to run our library, our license, and our contact information. Technically, the definition of a package in R is any directory containing a DESCRIPTION file, so always ensure this is present.

Click on the DESCRIPTION file in JupyterLab’s directory listing. You will see the basic details created automatically when we ran devtools::create(“datapeek”) :

Let’s add our specific details so our package contains the necessary metadata. Simply edit this file inside JupyterLab. Here are the details I am adding:

Package : datapeek

: Title : Provides useful functions for working with raw data.

: Version : 0.0.0.1

: Authors@R : person(“Sean”, “McClure”, email=”sean.mcclure@example.com”, role=c('aut','cre'))

: Description : The datapeek package helps users transform raw data for machine learning development.

: The datapeek package helps users transform raw data for machine learning development. Depends : R (≥ 3.5.1)

: License : MIT

: Encoding : UTF-8

: LazyData: true

Of course you should fill out these parts with your own details. You can read more about the definitions of each of these in Hadley’s chapter on metadata. As a brief overview…the package , title , and version parts are self-explanatory, just be sure to keep title to one line. Authors@R must adhere to the format you see above, since it contains executable R code. Note the role argument, which allows us to list the main contributors of our library. The usual ones are:

aut : author

cre : creator or maintainer

ctb : contributors

cph : copyright holder

There are many more options, with the full list found here.

You can add multiple authors by listing them as a vector:

Authors@R: as.person(c(

""Sean McClure <sean.mcclure@example.com> [aut, cre]"",

""Rick Deckard <rick.deckard@example.com> [aut]"",

"" Rachael Tyrell <rachel.tyrell@example.com> [ctb]""

))

NOTE: If you do plan on hosting your library on CRAN be sure your email address is correct, as CRAN will use this to contact you.

The description can be multiple lines, limited to 1 paragraph. We use depends to specify the minimum version of R our package depends on. You should use an R version equal or greater than the one you used to build your library. Most people today set their License to MIT, which permits anyone to “use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software” as long as your copyright is included. You can learn more about the MIT license here. Encoding ensures our library can be opened, read and saved using modern parsers, and LazyData refers to how data in our package are loaded. Since we set ours to true it means our data won’t occupy memory until they are used.

STEP 3: Add Functions

3A: Add Functions to R Folder

Our library wouldn’t do much without functions. Let’s add the 4 functions mentioned in the beginning of this article. The following GIST shows our datapeek functions in R:

We have to add our functions to the R folder, since this is where R looks for any functions inside a library.

datapeek

├── R

├── datapeek.Rproj

├── DESCRIPTION

├── NAMESPACE

Since our library only contains 4 functions we will place all of them into a single file called utilities.R, with this file residing inside the R folder.

Go into the directory in JupyterLab and open the R folder. Click on Text File in the Launcher and paste in our 4 R functions. Right-click the file and rename it to utilities.R.

3B: Export our Functions

It isn’t enough to simply place R functions in our file. Each function must be exported to expose them to users of our library. This is accomplished by adding the @export tag above each function.

The export syntax comes from Roxygen, and ensures our function gets added to the NAMESPACE. Let’s add the @export tag to our first function:

Do this for the remaining functions as well.

NOTE: In larger libraries we would only export functions that need to be usable outside our package. This helps reduce the chances of a conflict with another library.

3C: Document our Functions

It is important to document our functions. Documenting functions provides information for users, such that when they type ?datapeek they get details about our package. Documenting also supports working with vignettes, which are a long-form type of documentation. You can read more about documenting functions here.

There are 2 sub-steps we will take:

add the document annotations

the document annotations run devtools::document()

— Add the Document Annotations

Documentation is added above our function, directly above our #’ @export line. Here’s the example with our first function:

We space out the lines for readability, adding a title, description, and any parameters used by the function. Let’s do this for our remaining functions:

— Run devtools::document()

With documentation added to our functions we then run the following in terminal, just outside the root directory:

devtools::document()

NOTE: Make sure you’re one level outside the datapeek directory.

You may get the error:

Error: ‘roxygen2’ >= 5.0.0 must be installed for this functionality.

In this case open terminal in JupyterLab and install roxygen2. You should also install data.table and mltools, since our first function uses these:

install.packages('roxygen2')

install.packages('data.table')

install.packages('mltools')

Run the devtools::document() again. You should see the following:

This will generate .Rd files inside a new man folder. You’ll notice an .Rd file is created for each function in our package.

If you look at your DESCRIPTION file it will now show a new line at the bottom:

This will also generate a NAMESPACE file:

We can see our 4 functions have been exposed. Let’s now move onto ensuring dependencies are specified inside our library.

STEP 4: List External Dependencies

It is common for our functions to require functions found in other libraries. There are 2 things we must do to ensure external functionality is made available to our library’s functions:

Use double colons inside our functions to specify which library we are relying on; Add imports to our DESCRIPTION file.

You’ll notice in the above GIST we simply listed our libraries at the top. While this works well in stand-alone R scripts it isn’t the way to use dependencies in an R package. When creating R packages we must use the “double-colon approach” to ensure the correct function is read. This is related to how “top-level code” (code that isn’t an object like a function) in an R package is only executed when the package is built, not when it’s loaded.

For example:

library(mltools) do_something_cool_with_mltools <- function() {

auc_roc(preds, actuals)

}

…won’t work because auc_roc will not be available (running library(datapeek) doesn’t re-execute library(mltools)). This will work:

do_something_cool_with_mltools <- function() {

mltools::auc_roc(preds, actuals)

}

The only function in our datapeek package requiring additional packages is our first one:

Using the double-colon approach to specify dependent packages in R.

Notice each time we call an external function we preface it with the external library and double colons.

We must also list external dependencies in our DESCRIPTION file, so they are handled correctly. Let’s add our imports to the DESCRIPTION file:

Be sure to have the imported libraries comma-separated. Notice we didn’t specify any versions for our external dependencies. If we need to specify versions we can use parentheses after the package name:

Imports:

data.table (>= 1.12.0)

Since our encode_and_bind function isn’t taking advantage of any bleeding-edge updates we will leave it without any version specified.

STEP 5: Add Data

Sometimes it makes sense to include data inside our library. Package data can allow our user’s to practice with our library’s functions, and also helps with testing, since machine learning packages will always contain functions that ingest and transform data. The 4 options for adding external data to an R package are:

binary data parsed data raw data serialized data

You can learn more about these different approaches here. For this article we will stick with the most common approach, which is to add external data to an R folder.

Let’s add the Iris dataset to our library in order to provide users a quick way to test our functions. The data must be in the .rda format, created using R’s save() function, and have the same name as the file. We can ensure these criteria are satisfied by using devtools’ use_data function:

Above, I read in the Iris dataset from its URL and pass the data frame to devtools::use_data() .

In JupyterLab we see a new data folder has been created, along with our iris.rda dataset:

datapeek

├── data

└── iris.rda

├── man

├── R

├── datapeek.Rproj

├── DESCRIPTION

├── NAMESPACE

We will use our added dataset to run tests in the following section.

STEP 6: Add Tests

Testing is an important part of software development. Testing helps ensure our code works as expected, and makes debugging our code a much faster and more effective process. Learn more about testing R packages here.

A common challenge in testing is knowing what we should test. Testing every function in a large library is cumbersome and not always needed, while not enough testing can make it harder to find and correct bugs when they arise.

I like the following quote from Martin Fowler regarding when to test:

“Whenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead.” — Martin Fowler

If you prototype applications regularly you’ll find yourself writing to the console frequently to see if a piece of code returns what you expect. In data science, writing interactive code is even more common, since machine learning work is highly experimental. On one hand this provides ample opportunity to think about which tests to write. On the other hand, the non-deterministic nature of machine learning code means testing certain aspect of ML can be less than straightforward. As a general rule, look for obvious deterministic pieces of your code that should return the same output every time.

The interactive testing we do in data science is manual, but what we are looking for in our packages is automated testing. Automated testing means we run a suite of pre-defined tests to ensure our package works end-to-end.

While there are many kinds of tests in software, here we are taking about “unit tests.” Thinking in terms of unit tests forces us to break up our code into more modular components, which is good practice in software design.

NOTE: If you are used to testing in languages like Python, notice that R is more functional in nature (i.e., methods belong to functions not classes), so there will be some differences.

There are 2 sub-steps we will take for testing our R library:

6A: Creating the tests/testthat folder;

6B: Writing tests.

— 6A: Creating the tests/testthat folder

Just as R expects our R scripts and data to be in specific folders it also expects the same for our tests. To create the tests folder, we run the following in JupyterLab’s R console:

devtools::use_testthat()

You may get the following error:

Error: ‘testthat’ >= 1.0.2 must be installed for this functionality.

If so, use the same approach above for installing roxygen2 in Jupyter’s terminal.

install.packages('testthat')

Running devtools::use_testthat() will produce the following output:

* Adding testthat to Suggests

* Creating `tests/testthat`.

* Creating `tests/testthat.R` from template.

There will now be a new tests folder in our main directory:

datapeek

├── data

├── man

├── R

├── tests

└── testthat.R

├── datapeek.Rproj

├── DESCRIPTION

├── NAMESPACE

The above command also created a file called testthat.R inside the tests folder. This runs all your tests when R CMD check runs (we’ll look at that shortly). You’ll also notice testthat has been added under Suggests in our DESCRIPTION file:

— 6B: Writing Tests

testthat is the most popular unit testing package for R, used by at least 2,600 CRAN package, not to mention libraries on Github. You can check out the latest news regarding testthat on the Tidyverse page here. Also check out its documentation.

There are 3 levels to testing we need to consider:

expectation (assertion): the expected result of a computation;

the expected result of a computation; test: groups together multiple expectations from a single function, or related functionality from across multiple functions;

groups together multiple expectations from a single function, or related functionality from across multiple functions; file: groups together multiple related tests. Files are given a human readable name with context().

Assertions

Assertions are the functions included in the testing library we choose. We use assertions to check whether our own functions return the expected output. Assertions come in many flavors, depending on what is being checked. In the following section I will cover the main tests used in R programming, showing each one failing so you can understand how it works.

Equality Assertions

expect_equal()

expect_identical()

expect_equivalent

# test for equality

a <- 10

expect_equal(a, 14) > Error: `a` not equal to 14. # test for identical

expect_identical(42, 2) > Error: 42 not identical to 2. # test for equivalence

expect_equivalent(10, 12) > Error: 10 not equivalent to 12.

There are subtle differences between the examples above. For example, expect_equal is used to check for equality within a numerical tolerance, while expect_identical tests for exact equivalence. Here are examples:

expect_equal(10, 10 + 1e-7) # true

expect_identical(10, 10 + 1e-7) # false

As you write more tests you’ll understand when to use each one. Of course always refer to the documentation referenced above when in doubt.

Testing for String Matches

expect_match()

# test for string matching

expect_match(""Machine Learning is Fun"", ""But also rewarding."") > Error: ""Machine Learning is Fun"" does not match ""But also rewarding."".

Testing for Length

expect_length

# test for length

vec <- 1:10

expect_length(vec, 12) > Error: `vec` has length 10, not length 12.

Testing for Comparison

expect_lt

expect_gt

# test for less than

a <- 11

expect_lt(a, 10) > Error: `a` is not strictly less than 10. Difference: 1 # test for greater than

a <- 11

expect_gt(a, 12) > Error: `a` is not strictly more than 12. Difference: -1

Testing for Logic

expect_true

expect_false

# test for truth

expect_true(5 == 2) > Error: 5 == 2 isn't true. # test for false

expect_false(2 == 2) > Error: 2 == 2 isn't false.

Testing for Outputs

expect_output

expect_message

# testing for outputs

expect_output(str(mtcars), ""31 obs"") > Error: `str\(mtcars\)` does not match ""31 obs"". # test for warning

f <-function(x) {

if(x < 0) {

message(""*x* is already negative"")

}

} expect_message(f(1)) > Error: `f(1)` did not produce any messages.

There are many more included in the testthat library. If you are new to testing, start writing a few simple ones to get used to the process. With time you’ll build an intuition around what to test and when.

Writing Tests

A test is a group of assertions. We write tests in testthat as follows:

test_that(""this functionality does what it should"", {

// group of assertions here

})

We can see we have both a description (the test name) and the code (containing the assertions). The description completes the sentence, “test that ….”

Above, we are saying “test that this functionality does what it should.”

The assertions are the outputs we wish to test. For example:

test_that(""trigonometric functions match identities"", {

expect_equal(sin(pi / 4), 1 / sqrt(2))

expect_equal(cos(pi / 4), 1 / sqrt(10))

expect_equal(tan(pi / 4), 1)

}) > Error: Test failed: 'trigonometric functions match identities'

NOTE: It is worth considering the balance between cohesion and coupling with our test files. As stated in Hadley’s book, “the two extremes are clearly bad (all tests in one file, one file per test). You need to find a happy medium that works for you. A good starting place is to have one file of tests for each complicated function.”

Creating Files

The last thing we do in testing is create files. As stated above, a“file” in testing is a group of tests covering a related set of functionality. Our test file must live inside the tests/testthat/ directory. Here is an example test file for the stringr package on GitHub:

Example Test File from the stringr package on GitHub.

The file is called test-case.R (starts with “test”) and lives inside the tests/testthat/ directory. The context at the top simply allows us to provide a simple description of the file’s contents. This appears in the console when we run our tests.

Let’s create our test file, which will contain tests and assertions related to our 4 functions. As usual, we use JupyterLab’s Text File in Launcher to create and rename a new file:

Creating a Test File in R

Now let’s add our tests:

For the first function I am going to make sure a data frame with the correct number of features is returned:

Notice how we called our encode_and_bind function, then simply checked the equality between the dimensions and the expected output. We run our automated tests at any point to ensure our test file runs and we get the expected output. Running devtools::test() in the console runs our tests:

We get a smiley face too!

Since our second function removes a specified feature I will use the same test as above, checking for the dimensions of the returned frame. Our third function applies a specified function to a chosen column, so I will write a test that checks the result of given specified function. Finally, our fourth function returns the closest matching string, so I will simply check the returned string for the expected result.

Here is our full test file:

NOTE: Notice the relative path to the data in the test file.

Testing our Package

As we did above, we run our tests using the following command:

devtools::test()

This will run all tests in any test files we placed inside the testthat directory. Let’s check the result:

We had 5 assertions across 4 unit tests, placed in one test file. Looks like we’re good. If any of our tests failed we would see this in the above printout, at which point we would look to correct the issue.

STEP 7: Create Documentation

This has traditionally been done using “Vignettes” in R. You can learn about creating R vignettes for your R package here. Personally, I find this a dated approach to documentation. I prefer to use things like Sphinx or Julep. Documentation should be easily shared, searchable and hosted.

Click on the question mark at julepcode.com to learn how to use Julep.

I created and hosted some simple documentation for our R datapeek library, which you can find here.

Of course we will also have the library on GitHub, which I cover below.

STEP 8: Share your R Library

As I mentioned in the introduction we should be creating libraries on a regular basis, so others can benefit from and extend our work. The best way to do this is through GitHub, which is the standard way to distribute and collaborate on open source software projects.

In case you’re new to GitHub here’s a quick tutorial to get you started so we can push our datapeek project to a remote repo.

Sign up/in to GitHub and create a new repository.

…which will provide us with the usual screen:

With our remote repo setup we can initialize our local repo on our machine, and send our first commit.

Open Terminal in JupyterLab and change into the datapeek directory:

Initialize the local repo:

git init

Add the remote origin (your link will be different):

git remote add origin https://github.com/sean-mcclure/datapeek.git

Now run git add . to add all modified and new (untracked) files in the current directory and all subdirectories to the staging area:

git add .

Don’t forget the “dot” in the above command. Now we can commit our changes, which adds any new code to our local repo.

But, since we are working inside a Docker container the username and email associated with our local repo cannot be autodetected. We can set these by running the following in terminal:

git config --global user.email {emailaddress}

git config --global user.name {name}

Use the email address and username you use to sign into GitHub.

Now we can commit:

git commit -m 'initial commit'

With our new code committed we can do our push, which transfers the last commit(s) to our remote repo:

git push origin master

NOTE: Since we are in Docker you’ll likely get asked again for authentication. Simply add your GitHub username and password when prompted. Then run the above command again.

Some readers will notice we didn’t place a .gitignore file in our directory. It is usually fine to push all files inside smaller R libraries. For larger libraries, or libraries containing large datasets, you can use the site gitignore.io to see what common gitignore files look like. Here is a common R .gitignore file for R:

Example .gitignore file for an R package

To recap, git add adds all modified and new (untracked) files in the current directory to the staging area. Commit adds any changes to our local repo, and push transfers the last commit(s) to our remote repo. While git add might seem superfluous, the reason it exists is because sometimes we want to only commit certain files, this we can stage files selectively. Above, we staged all files by using the “dot” after git add .

You may also notice we didn’t include a README file. You should indeed include this, however for the sake of brevity I have left this step out.

Now, anyone can use our library. 👍 Let’s see how.

STEP 9: Install your R Library

As mentioned in the introduction I will not be discussing CRAN in this article. Sticking with GitHub make it easier to share our code frequently, and we can always add CRAN criteria later.

To install a library from GitHub, users can simply run the following command on their local machine:

devtools::install_github(""yourusername/mypackage"")

As such, we can simply instruct others wishing to use datapeek to run the following command on their local machine:

devtools::install_github(""sean-mcclure/datapeek"")

This is something we would include in a README file and/or any other documentation we create. This will install our package like any other package we get from CRAN:

Users then load the library as usual and they’re good to go:

library(datapeek)

I recommend trying the above commands in a new R environment to confirm the installation and loading of your new library works as expected.

CREATING LIBRARIES IN PYTHON

Creating Python libraries follows the same high-level steps we saw previously for R. We require a basic directory structure with proper naming conventions, functions with descriptions, imports, specified dependencies, added datasets, documentation, and the ability to share and allow others to install our library.

We will use JupyterLab to build our Python library, just as we did for R.

Library vs Package vs Module

In the beginning of this article I discussed the difference between a “library” and a “package”, and how I prefer to use these terms interchangeably. The same holds for Python libraries. “Modules” are another term, and in Python simply refer to any file containing Python code. Python libraries obviously contain modules as scripts.

Before we start:

I stated in the introduction that we will host and install our libraries on and from GitHub. This encourages rapid creation and sharing of libraries without getting bogged down by publishing criteria on popular package hosting sites for R and Python.

The most popular hosting site for Python is the Python Package Index (PyPI). This is a place for finding, installing and publishing python libraries. Whenever you run pip install <package_name> (or easy_intall ) you are fetching a package from PyPI.

While we won’t cover hosting our package on PyPI it’s still a good idea to see if our library name is unique. This will minimize confusion with other popular Python libraries and improve the odds our library name is distinctive, should we decide to someday host it on PyPI.

First, we should follow a few naming conventions for Python libraries.

Python Library Naming Conventions

Use all lowercase ;

; Make the name unique on PyPI (search for name on PyPI)

on PyPI (search for name on PyPI) No hyphens (you can use underscore to separate words)

Our library name is datapeek, so the first and third criteria are met; let’s check PyPI for uniqueness:

All good. 👍

We’re now ready to move through each step required to create a Python library.

STEP 1: Create Package Framework

JupyterLab should be up-and-running as per the instructions in the setup section of this article.

Use JupyterLab’s New Folder and Text File options to create the following directory structure and files:

datapeek

├── datapeek

└── __init__.py

└── utilities.py

├── setup.py

NOTE: Bold names are folders and light names are files. We will refer to the inner datapeek folder as the “module directory” and the outer datapeek directory as the “root directory.”

The following video shows me creating our datapeek directory in JupyterLab:

There will be files we do not want to commit to source control. These are files that are created by the Python build system. As such, let’s also add the following .gitignore file to our package framework:

NOTE: At the time of this writing JupyterLab lacks a front-end setting to toggle hidden files in the browser. As such, we will simply name our file gitignore (no preceding dot); we will change it to a hidden file later prior to pushing to GitHub.

Add your gitignore file as a simple text file to the root directory:

datapeek

├── datapeek

└── __init__.py

└── utilities.py

├── setup.py

├── gitignore

STEP 2: Fill Out Description Details

Just as we did for R, we should add metadata about our new library. We do this using Setuptools. Setuptools is a Python library designed to facilitate packaging Python projects.

Open setup.py and add the following details for our library:

Of course you should change the authoring to your own. We will add more details to this file later. The keywords are fairly self-explanatory. url is the URL of our project on GitHub, which we will add later; unless you’ve already created your python repo, in which case add the URL now. We talked about licensing in the R section. zip_safe simply means our package can be run safely as a zip file which will usually be the case. You can learn more about what can be added to the setup.py file here.

STEP 3: Add Functions

Our library obviously requires functions to be useful. For larger libraries we would organize our modules so as to balance cohesion/coupling, but since our library is small we will simply keep all functions inside a single file.

We will add the same functions we did for R, this time written in Python:

Add these functions to the utilities.py module, inside datapeek’s module directory.

STEP 4: List External Dependencies

Our library will often require other packages as dependencies. Our user’s Python environment will need to be aware of these when installing our library (so these other packages can also be installed). Setuptools provides the install_requires keyword to list any packages our library depends on.

Our datapeek library depends on the fuzzywuzzy package for fuzzy string matching, and the pandas package for high-performance manipulation of data structures. To specify our dependencies, add the following to your setup.py file:

install_requires=[

'fuzzywuzzy',

'pandas'

]

Your setup.py file should currently look as follows:

We can confirm all is in order by running the following in a JupyterLab terminal session:

python setup.py develop

NOTE: Run this in datapeek’s root directory.

After running the command you should see something like this:

…with an ending that reads:

Finished processing dependencies for datapeek==0.1

If one or more of our dependencies is not available on PyPI, but is available on GitHub (e.g. a bleeding-edge machine learning package is only available on Github…or it’s another one of our team’s libraries hosted only on GitHub), we can use dependency_links inside our setup call:

setup(

...

dependency_links=['http://github.com/user/repo/tarball/master#egg=package-1.0'],

...

)

If you want to add additional metadata, such as status, licensing, language version, etc. we can use classifiers like this:

setup(

...

classifiers=[

'Development Status :: 3 - Alpha',

'License :: OSI Approved :: MIT License',

'Programming Language :: Python :: 2.7',

'Topic :: Text Processing :: Linguistic',

],

...

)

To learn more about the different classifiers that can be added to our setup.py file see here.

STEP 5: Add Data

Just as we did above in R we can add data to our Python library. In Python these are called Non-Code Files and can include things like images, data, documentation, etc.

We add data to our library’s module directory, so that any code that requires those data can use a relative path from the consuming module’s __file__ variable.

Let’s add the Iris dataset to our library in order to provide users a quick way to test our functions. First, use the New Folder button in JupyterLab to create a new folder called data inside the module directory:

datapeek

├── datapeek

└── __init__.py

└── utilities.py

└── data

├── setup.py

├── gitignore

…then make a new Text File inside the data folder called iris.csv, and paste the data from here into the new file.

If you close and open the new csv file it will render inside JupyterLab as a proper table:

CSV file rendered in JupyterLab as formatted table.

We specify Non-Code Files using a MANIFEST.in file. Create another Text File called MANIFEST.in placing it inside your root folder:

datapeek

├── datapeek

└── __init__.py

└── utilities.py

└── data

├── MANIFEST.in

├── setup.py

├── gitignore

…and add this line to the file:

include datapeek /data/iris.csv

NOTE: The MANIFEST.in is often not needed, but included in this tutorial for completeness. See here for more discussion.

We also need to include the following line in setup.py:

include_package_data=True

Our setup.py file should now look like this:

STEP 6: Add Tests

As with our R library we should add tests so others can extend our library and ensure their own functions do not conflict with existing code. Add a test folder to our library’s module directory:

datapeek

├── datapeek

└── __init__.py

└── utilities.py

└── data

└── tests

├── MANIFEST.in

├── setup.py

├── gitignore

Our test folder should have its own __init__.py file as well as the test file itself. Create those now using JupyterLab’s Text File option:

datapeek

├── datapeek

└── __init__.py

└── utilities.py

└── data

└── tests

└── __init__.py

└── datapeek_tests.py

├── MANIFEST.in

├── setup.py

├── gitignore

Our datapeek directory structure is now set to house test functions, which we will write now.

Writing Tests

Writing tests in Python is similar to doing so in R. Assertions are used to check the expected outputs produced by our library’s functions. We can use these “unit tests” to check a variety of expected outputs depending on what might be expected to fail. For example, we might want to ensure a data frame is returned, or perhaps the correct number of columns after some known transformation.

I will add a simple test for each of our 4 functions. Feel free to add your own tests. Think about what should be checked, and keep in mind Martin Fowler’s quote shown in the R section of this article.

We will use unittest, a popular unit testing framework in Python.

Add unit tests to the datapeek_tests.py file, ensuring the unittest and datapeek libraries are imported:

To run these tests we can use Nose, which extends unittest to make testing easier. Install nose using a terminal session in JupyterLab:

$ pip install nose

We also need to add the following lines to setup.py:

setup(

...

test_suite='nose.collector',

tests_require=['nose'],

)

Our setup.py should now look like this:

Run the following from the root directory to run our tests:

python setup.py test

Setuptools will take care of installing nose if required and running the test suite. After running the above, you should see the following:

All our tests have passed!

If any test should fail, the unittest framework will show which functions did not pass. At this point, check to ensure you are calling the function correctly and that the output is indeed what you expected. It can also be good practice to purposely write tests to fail first, then write your functions until they pass.

STEP 7: Create Documentation

As I mentioned in the R section, I use Julep to rapidly create sharable and searchable documentation. This avoids writing cryptic annotations and provides the ability to immediately host our documentation. Of course this doesn’t come with the IDE hooks that other documentation does, but for rapidly communicating it works.

You can find the documentation I create for this library here.

STEP 8: Share Your Python Library

The standard approach for sharing python libraries is through PyPI. Just as we didn’t cover CRAN with R, we will not cover hosting our library on PyPI. While the requirements are fewer than those associated with CRAN there are still a number of steps that must be taken to successfully host on PyPI. The steps required to host on sites other than GitHub can always be added later.

GitHub

We covered the steps for adding a project to GitHub in the R section. The same steps apply here.

I mentioned above the need to rename our gitignore file to make it a hidden file. You can do that by running the following in terminal:

mv gitignore .gitignore

You’ll notice this file is no longer visible in our JupyterLab directory (it eventually disappears). Since JupyterLab still lacks a front-end setting to toggle hidden files simply run the following in terminal at anytime to see hidden files:

ls -a

We can make it visible again should we need to view/edit the file in JupyterLab, by running:

mv .gitignore gitignore

Here is a quick recap on pushing our library to GitHub (change git URL to your own):

Create a new repo on GitHub called datapeek_py

a new repo on GitHub called datapeek_py Initialize your library’s directory using git init

your library’s directory using Configure your local repo with your GitGub email and username (if using Docker) using:

git config --global user.email {emailaddress}

git config --global user.name {name}

Add your new remote origin using git remote add origin https://github.com/sean-mcclure/datapeek_py.git

your new remote origin using Stage your library using git add .

your library using Commit all files using git commit -m ‘initial commit’

all files using Push your library to the remote repo using git push origin master (authenticate when prompted)

Now, anyone can use our python library. 👍 Let’s see how.

STEP 9: Install your Python Library

While we usually install Python libraries using the following command:

pip install <package_name>

… this requires hosting our library on PyPI, which as explained above is beyond the scope of this article. Instead we will learn how to install our Python libraries from GitHub, as we did for R. This approach still requires the pip install command but uses the GitHub URL instead of the package name.

Installing our Python Library from GitHub

With our library hosted on GitHub, we simply use pip install git+ followed by the URL provided on our GitHub repo (available by clicking the Clone or Download button on the GitHub website):

pip install git+https://github.com/sean-mcclure/datapeek_py

Now, we can import our library into our Python environment. For a single function:

from datapeek.utilities import encode_and_bind

…and for all functions:

from datapeek.utilities import *

Let’s do a quick check in a new Python environment to ensure our functions are available. Spinning up a new Docker container, I run the following:

Fetch a dataset:

Check functions:

encode_and_bind(iris, 'species')

remove_features(iris, ['petal_length', 'petal_width'])

apply_function_to_column(iris, ['sepal_length'], 'times_4', 'x*4')

get_closest_string(['hey there','we we are','howdy doody'], 'doody')

Success!

SUMMARY

In this article we looked at how to create both R and Python libraries using JupyterLab running inside a Docker container. Docker allowed us to leverage Docker Stacks such that our environment was easily controlled and common packages available. This also made it easy to use the same high-level interface to create libraries through the browser for 2 different languages. All files were written to our local machine since we mounted a volume inside Docker.

Creating libraries is a critical skill for any machine learning practitioner, and something I encourage others to do regularly. Libraries help isolate our work inside useful abstractions, improves reproducibility, makes our work shareable, and is the first step towards designing better software. Using a lightweight approach ensures we can prototype and share quickly, with the option to add more detailed practices and publishing criteria later as needed.

As always, please ask questions in the comments section should you run into issues. Happy coding.

If you enjoyed this article you might also enjoy:

FURTHER READING AND RESOURCES"
Ace Deep Learning in a Service-Based Organization,"MVP over POC

Most service-based companies suffer from a phobia of the word “Product”. So service-based companies develop POCs instead.

Let’s first understand what is what:

Product is an article or substance that is manufactured or refined for sale. Proof of Concept (POC) is a miniature representation of the end-product, with a few working features, that aims of verifying that some concept has practical potential. Minimum Viable Product (MVP) is a development technique in which a new product or website is developed with sufficient features to satisfy early adopters.

Not being a product-based company is no excuse to create half-baked solutions in the name of POCs.

Remember the bottom line is always to create what people want. While POCs focus over a few working features, MVP stresses over the fact that a few working features are useless unless they are satisfying the customer.

Often, predominantly in Deep Learning, the working of the solution has this cloud of uncertainty around its end-result. This makes the client nervous about investing in the proposed solution.

Repeatedly, you will encounter clients who already had a few bad experiences before they approached your organization. Showcasing them something they have a hard time wrapping their head around, especially when they are already frustrated, would frankly be stupid."
How to tune hyperparameters of tSNE,"How to tune hyperparameters of tSNE

This is the second post of the column Mathematical Statistics and Machine Learning for Life Sciences. In the first post we discussed whether and where in Life Sciences we have Big Data suitable for Machine / Deep Learning, and emphasized that Single Cell is one of the most promising Big Data resources. t-distributed stochastic neighbor embedding (tSNE) is a Machine Learning non-linear dimensionality reduction technique which is absolutely central for Single Cell data analysis. However, the choice of hyperparameters for the tSNE might be confusing for beginners.

In this post, I will share my recommendations on selecting optimal values of hyperparameters such as perplexity, number of principal components to keep, and number of iterations for running tSNE.

How to Use tSNE Effectively

When teaching single cell RNA sequencing (scRNAseq) course I keep getting questions about sensitivity of tSNE with respect to hyperparameters such as perplexity. The questions are usually inspired by this fantastic post about challenges with interpreting tSNE plots.

A popular tutorial on developing intuition behind tSNE

Despite my great respect for the main message of the post, I think scRNAseq community should not worry too much about perplexity and other tSNE hyperparameters based on what they learn from that post because: a) many examples in the post come from abstract mathematical topologies which do not really resemble scRNAseq data, b) the post concentrates on extreme tSNE hyperparameters which are rarely used in the real world scRNAseq analysis.

If you do scRNAseq analysis you will not avoid the popular Rtsne function and R package which is based on Barnes-Hut C++ implementation of the original tSNE algorithm. The Rtsne function has three main hyperparameters:"
Five Tips for Contributing to Open Source Software,"Five Tips for Contributing to Open Source Software

Photo by Yancy Min on Unsplash

Contributing to Open-Source Software (OSS) can be a rewarding endeavor, especially for new data scientists. It helps improve skills, provides invaluable experience when collaborating on projects, and gives you a chance to showcase your code. However, many data scientists do not consider themselves to be formally trained software developers. This can make contributing to OSS a scary proposition.

One source of fear is that it seems like every step in the software development process, from design to continuous integration, has a set of best practices that are often overlooked in data science training. This leaves data scientists feeling under-equipped to participate in an arena tailored for software developers. It’s been documented that even some skilled software developers have anxiety when deciding to contribute to OSS, well, multiply that anxiety for a data scientist.

Almost all the popular data science libraries we use every day are successful OSS projects sometimes maintained and often improved by the community. Scikit-learn, PyTorch, and TensorFlow come to mind. See, data scientists can write high-quality code! Ultimately, data scientists are writing code with the intent to ship a model to production or deploy a robust data pipeline. Since we’re writing software, we should be held to the same standards as other software projects.

In this post, I’d like to share a few things that I’ve learned over the past couple of years of developing software in data science. My intent is to share the pieces of information that I wish someone shared with me when I was beginning my data science journey. The hope is that inexperienced data scientists who are hesitant to get started will use these tips to feel empowered to start writing better data science code and contribute to OSS sooner rather than later.

Find an experienced software developer…

…and pay attention! This item had the single greatest impact on my personal development. All my current software development habits are a direct result of incorporating experienced software developers in my life in some form or another. It’s worth noting that you don’t have to be best friends with these people. The experienced developer could take the form of a tech lead on your team at work, a classmate, or someone from a local meetup. Additionally, there are some really great developers who regularly tweet, post in forums, write technical blogs, and create podcasts (Talk Python is my favorite). Try your best to identify people who are not data scientists, as they can provide you with opinions that would be more difficult to find in the data science field.

The key to this tip is to listen to and engage with these people. First, ask yourself, “Why are they are implementing such a method”? If you don’t understand, look it up. If you still don’t understand (or if you do) send them an email or reply with a comment asking them to expand on their point. Chances are, these people have good reasons behind their decisions. Seeing or hearing these reasons in the context of your question will give you a better understanding of the topic from a software development perspective. I’m not recommending you continually hound experienced developers, it’s important to respect their time and space.

2. The open-source community is NOT out to get you

An unfortunate misconception that prevents people from contributing to OSS is that if they make their code public it will be laughed at or ridiculed. The majority of the time, this is just not true. It’s no different than any other type of community, a few bad apples can ruin the party for everyone. I have contributed to small OSS projects, popular OSS projects, and even maintain a few of my own. In each of these scenarios, every interaction has been at the very least respectful, and in most cases, people are appreciative. After more than two years of contributions to OSS, if someone were disrespectful toward me, they would clearly be an outlier at this point.

3. Practice git, etc.

It’s never been easier to practice git, all the major hosting platforms allow users to create unlimited private repositories.

Some people state “unfamiliarity with git” or even just “unfamiliarity with GitHub” as their reason for not contributing to OSS. This is why the title for number three contains “etc”. GitHub, Bitbucket, and GitLab have a substantial amount of functionality that supports a git workflow, but is not considered part of “git” per se. So even for an experienced git user who has never used GitHub, there is still a slight learning curve on the road to proficiency.

Instead of coding all of your projects locally, reserve some extra time to push to one of the platforms mentioned above. Make it private, you can still securely share the repo with a friend if you like. Practice making pull requests or visually inspect the git commit history after rebasing a feature branch. Becoming more familiar with the tabs, buttons, tools, and the git workflow in a private repository will give you confidence when working on your public contributions.

You will be hard-pressed to find a successful data science project not using some form of version control for their software development, git just happens to be the most popular. For this reason, try to practice using a git workflow for your everyday projects. Then, when collaborating with people on an OSS project, you won’t be hung up on git workflow mishaps and you spend your time focusing on the code.

4. Use software best practices

Rachel Tatman, a data scientist from Kaggle, recently posted a video and a kernel outlining how to write more professional-looking data science code, so I’ll recommend to follow her advice and keep the sections short where we overlap. Each of these will make your life easier when interacting with collaborators, again allowing you to focus on adding features rather than going back to clean up bad habits.

4a. Make variable names human-readable

Other data scientists may know that i and j are elements of the array A but mathematical notations don’t translate to informative variable names in source code. I like to use variable names that would have meaning when re-reading the source code or for someone else reading for the first time. But I’ll leave it to Will Koehrsen to suggest naming conventions for data scientists.

4b. Comment generously

Whether they are aware of it or not, data scientists often implement a coding style called method chaining. When there is a DataFrame object in memory, we often know the exact next steps to transform a column into the format we want. One downside to method chaining is that the chains can be difficult to debug, especially when there are no comments. Think of others when writing comments and remember, a short comment can go a long way.

4c. Write Unit Tests

You will thank yourself later, or whoever is reviewing your pull request will thank you. Sometimes unit tests just don’t make sense for small one-off data science projects. But unit tests become more important as you think about scaling up your code or collaborating with others.

Code is easier to test when it is modular, which translates to writing functions with the intent that they will perform one or two operations. Below is an example of a function that normalizes a pandas DataFrame in Python. The unit test for this function asserts that the mean and standard deviation are 0.0 and 1.0, respectively. If this test ever fails, the developer knows that something has gone wrong in the `normalize_dataframe` method and will know where to start debugging.

Test that the normalize_dataframe function is working as expected.

4d. Use Continuous Integration (CI)

Congrats, you wrote unit tests and pushed your code to GitHub, but a few days later, someone reports an issue stating they can’t get your code to run on their machine. CI would help identify these problems before they become issues on GitHub. Travis is an example of a CI service that integrates directly with GitHub free of charge for public repos. CI services allow you to build and test your code in containerized environments. I recommend setting up one of the CI services for projects that you intend on collaborating with others.

Travis interface showing a passing build with multiple versions of Python on a Linux kernel.

5. Use a debugger

Using a code editor with a debugger, sometimes referred to as an integrated development environment (IDE), can seem like overkill for data science projects. But once you learn how to use your editor’s debugger, it becomes a powerful tool for software development. A debugger allows users to execute code up until a breakpoint then explore the variable namespace, which makes it much easier to “debug” why a program may be crashing. You should give yourself at least 30 days of trying out a debugger for development before giving up hope and switching back to notebooks. Take the time to read the tutorial and ask a more experienced software developer (perhaps from Tip #1) if they can walk you through debugging your next bug.

Jupyter notebooks are a popular choice for many users when first starting a data science project. Notebooks are great for exploratory data analysis and visualizations, but they have their shortcomings, reviewed here by Joel Grus. Namely, the cells cannot be run out of order, they are difficult for beginners to understand, and they encourage bad habits. The two popular IDEs PyCharm and VS Code have recognized the popularity of notebooks and recently integrated support for them directly in the IDE. Hopefully, data scientists will come to IDEs for the notebook support but stay for the debugger.

Bonus: Be receptive to constructive criticism

Keep in mind that OSS is maintained and improved by both formal and informal code reviews. People will look at your code, critique it, and might ask you to change a few lines. If you’ve made it this far, isn’t this what you were looking for? Someone has taken the time to review your code and spent enough energy to try and improve it.

As your software development skills improve, you will encounter other users asking to you modify your code, but you might want to leave it as-is. Defend your decision with the same respect that you were expecting to receive, it’s really no different than being a sensible human."
All I Want for Christmas Is AI: Write the Next Christmas Hit Using LSTMs,"Language Models

Generating lyrics automatically is a trivial task. The general way of generating a sequence of text is to train a model to predict the next word/character given all previous words/characters.

The engines that power text generation scripts are called Statistical Language Models, or simply Language Models.

Statistical Language Models

A Language Model is a probabilistic model that can predict the next word of a sequence given the sequence of previous words itself, trying to capture the statistical structure (i.e. latent space) of the text it’s trained on. Technically speaking, it is just a probability distributions over a sequence of words P (w1, w2 , … , wₘ) , from which we iteratively draw the most likely next word evaluating P ( wₙₑₓₜ | w1, w2 , … , wₘ ). This is also part of what happens behind the scenes when Google autocompletes our weird queries (providing even weirder suggestions) and our boring Christmas greetings e-mails.

Apparently, Google Search doesn’t have a good opinion on Christmas songs either

Character-Based Neural Language Models

Language models can be developed at characters level too. The main benefits of character-based language models are their small vocabulary and the flexibility in handling any words, punctuation and particular text structures. This comes at the cost of having bigger models and longer training times.

The most common family of ML techniques used to build Language Models nowadays is Recurrent Neural Networks (RNNs), a powerful type of Neural Network capable of remembering and processing past information through their hidden state neurons.

A simple example of RNN (with a single 3-units hidden layer) forward pass using the training sequence “hello”. For each (one-hot encoded) character in the sequence, the RNN predicts the next character assigning a confidence score to every character in the vocabulary ([“h”, “e”, “l”, “o”]). The objective of the network is to learn the set of weights that maximizes the green numbers in the output layer and minimize the red ones.

When enough data are available, RNNs in their Long Short-Term Memory (LSTM) flavor are preferable due to the fact that they can capture more complex text dependencies and deal better with the exploding/vanishing gradient problem.

Christmas Lyrics Generator

In order to generate our Christmas lyrics, we need a proper data source. Luckily for us, the Internet is plenty of lyrics sites that can be easily scraped.

Data Preparation

Once we get our data source, we need to build the corpus importing the raw text and applying some ordinary text preprocessing like (undesired) punctuation removal and lowercasing.

Since we are working on character-based language models, text must be mapped on a character level. Therefore, a unique character vocabulary has to be built.

The inputs of our Neural Network will be sequences of characters. Thus, we split the corpus into maxlen-sized sequences, sampled every step characters.

Model Design and Training

In this article, we’ll try to build the simplest character-based neural language model possible: a 128-sized single-layered LSTM with softmax activation.

Remember that input sequences and outputs must be one-hot encoded.

The network is trained for 1000 epochs, although the loss seems to stop decreasing significantly after 500–600 epochs.

(Depending on your hardware, this might take from a few hours to several days. Feel free to lower down the number of epochs in order to get your model trained within a reasonable time frame)

The loss stops decreasing significantly after 500 epochs approximately

Text Generation

Once the model is trained, we can start predicting. Given a sequence of characters, the model uses its weights to output a character distribution from where we can sample the next character, iterating the process as long as we like. The way we sample the next character from the output distribution is crucial.

If we always pick the most likely word, the language model training objective causes us to get stuck in loops like “Merry Christmas. Merry Christmas. Merry Christmas”. Which can be considered a legit Christmas carol to be fair, but is probably not what we want to achieve here.

Even if we sample from the distribution, we still need to be careful as the most unlikely tokens might represent a big part of the probability mass. Let’s consider, for instance, that the bottom 50% of the characters tokens has an aggregate probability of 25%. This means that we have 1 chance out of 4 to go off-road, causing an unstoppable error propagation across the whole generation process.

Inspired by statistical thermodynamics, Temperature Sampling is one of the most used sampling methods for text generation. High temperature here means that low energy states are more likely encountered. Therefore, lower temperatures make the model increasingly confident in its top choices, while temperatures greater than 1 decrease confidence.

Results

Let’s see what we get using a temperature of 0.2 and the seed:

“Thank God it’s Christmas

for one day.

Thank God it’s Christmas”

After the first epoch, we achieve something like this:

“Thank God it’s Christmas

for one day.

Thank God it’s Christmas

ooas ih e yl a t e a dle sl an ioe ss th h ihet e ei nen ut seihl ia eet oh ehrn s s lt tee a netert tls otl lo ar e rp h d htl th eotr ene rn h eelianees e hh gi oh hli’e tt ahde o etreaolti atha l n loea rii so n etaeraioio m sl en hlnl t reh e e etr r eir sa ee tr eta eee awrmsesur ru uete errea […]””

Clearly, the model didn’t have time to learn anything about the language of the corpus it is trained on. Here are the lyrics after 10 epochs:

“Thank God it’s Christmas

for one day.

Thank God it’s Christmas

I don’t want to you a little bettle mistletoe

and the star of the stars a star of the stars the children and stars the star who tree.

I want a star of the stars

and the stars and Santa Claus

I want to see the Christmas tree. […]”

It looks like 10 epochs are enough to understand how to stick characters together to get words and sentences, but still not enough to learn the higher structures of the language itself. Note that the resulting model gets stuck in a loop on the word “star” and generally fails in generating something that makes sense.

After 100 epochs, we get:

“Thank God it’s Christmas

for one day.

Thank God it’s Christmas

and a happy new year

and I hall like the world every over the mistletoe

I wish I can to know

I wish you windows to your blood

and I can’t bell me home for Christmas and when it’s Christmas eve

I wanna se Christmas morning

about the sky, because the snow is fall […]”

which is already something an illiterate Christina Aguilera could sing on. Structure and recurrent themes of Christmas carols start emerging, together with creepy nonsense like the “I wish you windows to your blood” line, which sounds more like black metal than Christmas.

Fast-forwarding to 1000 epochs:

“Thank God it’s Christmas

for one day.

Thank God it’s Christmas

you’ ll be what you want

I wish you joy. What a bur and bright

and the mother know early

I don’t your way to hear

I have hold you a kid, fate I wish you a man near mer.

Her handling a thing for the snow.

It’s Christmas time

It’s Christmas time

Christmas without give me a Christmas song

a Christmas bell is ringing. […]”

Now we definitely got something! Nonsense is still there, but the overall quality of the lyrics is decent and the whole thing might sound meaningful.

Lastly, we might consider tweaking a bit with the hyperparameters of our model. For instance, we can try to sample longer and/or more distant input sequences increasing the maxlen and step parameters in the data preprocessing phase. Intuitively, increasing the maximum length of the input sequences means feeding the model with more context to learn from, so we expect better results.

This is what we can get when maxlen is increased from 70 to 120:

“[…] and open wide our heavenly home

make safe the way that leads on high,

and close the path to misery. O come the here and shook

the spend of a Christmas songs

I be a little passing all the night

and in a right bar inning, hallelujah? and all the things of son

that I want in the sky wing

I believe in Santa Claus

I believe I pound the street

to see him rousing all the toys

a time of ghembow keeps and loves, and snow gloria! through the snows of the sky

to see the story Christmas tree, oh Christmas day.

the son of God son of births and down

the son of God son of births and down I’m gonna send the stars and snow

and I want a hippo. […]”

Not sure if it can be considered qualitatively better than the previous one, but next Christmas I will ask for a hippo too.

Conclusion

Future Development

Single-layered LSTMs are just the starting point of Neural Language Modelling, as much more complex networks can be designed to better address the problem of text generation.

We have already seen how hyperparameters can play an important role in developing a decent text generator. We might therefore consider to stack more (bigger) layers in our network, to tune the batch size and to experiment more with different sequence lengths and temperatures as well.

Further Readings

In order to keep the reading accessible to everyone, this article deliberately avoids diving deep into the mathematical and probabilistic fundamentals of neural networks and NLP. Here’s a shortlist of sources where you can explore some of the topics we’ve left behind:

Please leave your thoughts and suggestions in the comments section and share if you find this helpful!"
"Why Should You Move to Plano, Texas?","Why Should You Move to Plano, Texas?

In a previous blog post I discussed the potential benefits of being able to predict the value of a house (the primary such benefit being the ability to amass considerable personal wealth).

As someone who’s spent much of their adult life at the mercy of the London rental market, I’m not wont to give an endorsement to the business of property speculation. However, forecasting house prices is an interesting application for the data scientist, and it gives us a chance to explore forecasting techniques, their strengths, and their limitations."
Predicting Hotel Reservation Cancellations with Machine Learning,"Image by Alexas_Fotos from Pixabay

As you can imagine, the cancellation rate for bookings in the online booking industry is quite high. Once the reservation has been cancelled, there is almost nothing to be done. This creates discomfort for many institutions and creates a desire to take precautions. Therefore, predicting reservations that can be cancelled and preventing these cancellations will create a surplus value for the institutions.

In this article, I will try to explain how future cancelled reservations can be predicted in advance by machine learning methods. Let’s start with preprocessing!

Preprocessing

First of all, I should say that you can access the data used in my repository that I will share at the end of my article. I would also like to share that this is the subject of a thesis. [1]

We have two separate data sets, and since we’re going to do preprocessing for both, it makes sense to combine them. But during the modeling phase, we’re going to want to get to these two sets of data separately. So, to distinguish the two, I created the id field.

import pandas as pd h1 = pd.read_csv('data/H1.csv')

h2 = pd.read_csv('data/H2.csv') h1.loc[:, 'id'] = range(1, len(h1) + 1)



start = h1['id'].max() + 1

stop = start + len(h2)

h2.loc[:, 'id'] = range(start, stop) df = pd.concat([h1, h2], ignore_index=True, sort=False)

Here are the preprocessing steps for this project:

Converting string NULL or Undefined values to np.nan

or values to np.nan Deletion of missing observations from columns with a small number of NULL values

values Filling in missing values according to rules

Deletion of incorrect values

Outlier detection

Step 1 — String NULL or Undefined values to np.nan

import numpy as np for col in df.columns:

if df[col].dtype == 'object' and col != 'country':

df.loc[df[col].str.contains('NULL'), col] = np.nan

df.loc[df[col].str.contains('Undefined', na=False), col] = np.nan null_series = df.isnull().sum()

print(null_series[null_series > 0])

With the code above, we convert string NULL and Undefined values to np.nan values. Then, we print the count of NULL values for each column. Here’s what the result looks like,

Null values

Step 2— Delete some missing values

We can delete NULL values in country, children, market_segment, distribution_channel, because there are few NULL values in these fields.

subset = [

'country',

'children',

'market_segment',

'distribution_channel'

]

df = df.dropna(subset=subset)

Step 3— Fill missing values by rule-set

There are a number of rules specified for the data. [2] For example, values that are Undefined/SC mean that they are no meal type. Since we have previously replaced Undefined values with NULL , we can fill the NULL values in the meal field with SC .

The fact that the agent field is NULL means the reservation didn’t come from any agency. Therefore, these reservations can be considered as purchased directly by the customers, without any intermediary organizations such as agencies and etc. That’s why we’re not deleting NULL values, we’re throwing a random value like 999 instead. The same goes for the company field.

More detailed information can be found in the document in the second link in the references.

df.loc[df.agent.isnull(), 'agent'] = 999

df.loc[df.company.isnull(), 'company'] = 999 df.loc[df.meal.isnull(), 'meal'] = 'SC'

Step 4— Delete wrong values

The ADR field refers to the average price per night of the reservation. Therefore, it is not normal for it to take a value smaller than zero. You can use df.describe().T to see such situations. We delete values that are smaller than zero for the ADR field.

df = df[df.adr > 0]

Step 5 —Outlier detection

For integer and float fields, we determine the lower and upper points using the code below. If there is equation between the lower point and the upper point, we do not do any filtering. If not equal, we remove observations larger than the upper point and observations smaller than the lower point from the data set.

Outlier detection with IQR

The lower and upper points of the fields seem to be below,

IQR results

Finally, we’re going to talk about multivariate outlier detection. [3] This is a special inference in which we work a little more and does not apply to every business. Charges such as $5 or $10 for a 1-night stay can be met normally, but that’s not normal for 10-night stay. Therefore, removing these values, which are considered contrary, from the data set will help our model to learn. So I’ve tried the LocalOutlierFactor and EllipticEnvelope , I’m only going over EllipticEnvelope because it yielded better results, but if you want to check out both, you can look at my repository.

from sklearn.covariance import EllipticEnvelope

import matplotlib.pyplot as plt

import numpy as np # create new features: total price and total nights

cleaned.loc[:, 'total_nights'] = \

cleaned['stays_in_week_nights'] + cleaned['stays_in_weekend_nights']

cleaned.loc[:, 'price'] = cleaned['adr'] * cleaned['total_nights'] # create numpy array

X = np.array(cleaned[['total_nights', 'price']]) # create model

ee = EllipticEnvelope(contamination=.01, random_state=0) # predictions

y_pred_ee = ee.fit_predict(X) # predictions (-1: outlier, 1: normal)

anomalies = X[y_pred_ee == -1] # plot data and outliers

plt.figure(figsize=(15, 8))

plt.scatter(X[:, 0], X[:, 1], c='white', s=20, edgecolor='k')

plt.scatter(anomalies[:, 0], anomalies[:, 1], c='red');

The chart is as follows. The red dots show outlier values.

EllipticEnvelope result

As you can see, it would make sense to leave small values outside the data set, especially after 6 nights. By applying this process, we can save the data set.

df_cleaned = cleaned[y_pred_ee != -1].copy() h1_cleaned = df_cleaned[df_cleaned.id.isin(h1.id.tolist())]

h2_cleaned = df_cleaned[df_cleaned.id.isin(h2.id.tolist())] h1_cleaned = h1_cleaned.drop('id', axis=1)

h2_cleaned = h2_cleaned.drop('id', axis=1) h1_cleaned.to_csv('data/H1_cleaned.csv', index=False)

h2_cleaned.to_csv('data/H2_cleaned.csv', index=False)

Feature Engineering

Another important issue that has to be done before building up a model is feature engineering. Adding or removing features may be more efficient for our model.

Step 1— Correlation

First, I’m going to convert categorical data to integer using LabelEncoder , and then I’m going to look at the correlations. [4] The following code does this,

from sklearn.preprocessing import LabelEncoder

import matplotlib.pyplot as plt

import pandas as pd

import seaborn as sns train = pd.read_csv('./data/H1_cleaned.csv')

test = pd.read_csv('./data/H2_cleaned.csv') df_le = train.copy()

le = LabelEncoder()



categoricals = [

'arrival_date_month',

'meal',

'country',

'market_segment',

'distribution_channel',

'reserved_room_type',

'assigned_room_type',

'deposit_type',

'agent',

'company',

'customer_type',

'reservation_status',

]



for col in categoricals:

df_le[col] = le.fit_transform(df_le[col]) plt.figure(figsize=(20, 15))

sns.heatmap(df_le.corr(), annot=True, fmt='.2f');

This code gives us a correlation matrix like the one below,

Correlation matrix

In this matrix, there appears to be a negative high correlation between reservation_status and is_canceled features. There is also a high correlation between total_nights and stays_in_week_nights and stays_in_weekend_nights fields. So, we remove reservation_status and total_nights features from our data set. Since there is a relation between reservation_status_date and reservation_status, we will remove this feature.

columns = [

'reservation_status_date',

'total_nights',

'reservation_status',

]



train = train.drop(columns, axis=1)

test = test.drop(columns, axis=1)

df_le = df_le.drop(columns, axis=1)

Step 2— Dummy Variables vs Label Encoder

Machine learning models requires numerical data to operate. So before we can model, we need to convert categorical variables into numerical variables. There are two methods we can use to do this: Dummy variables and LabelEncoder .With the code you see below, we create features both using LabelEncoder and using dummy variables .

import pandas as pd new_categoricals = [col for col in categoricals if col in train.columns] df_hot = pd.get_dummies(data=train, columns=new_categoricals)

test_hot = pd.get_dummies(data=test, columns=new_categoricals) X_hot = df_hot.drop('is_canceled', axis=1)

X_le = df_le.drop('is_canceled', axis=1)

y = train['is_canceled']

Then we build a logistic regression model with dummy variables and examine the classification report as a first look at the data.

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, classification_report

from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X_hot, y, test_size=.2, random_state=42)



log = LogisticRegression().fit(X_train, y_train)

y_pred = log.predict(X_test) print(accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

The accuracy score appears to be 0.8584, but the accuracy for reservations that have been cancelled is very low when looking at the classification report. Because our data contains 23720 successful cases and 8697 canceled cases. In such cases, it is preferred to dilute the weighted class or to increase the number of samples for the fewer sampled class. We will first select features with feature selection algorithm and then compare the dummy variables and label encoder using diluted data.

First classification report

Step 3— Feature Selection

Feature selection is one of the most important issues for feature engineering. Here we will use SelectKBest , a popular feature selection algorithm for classification problems. Our scoring function will be chi². [5]

Feature selection

With the above function, we select the best features for both LabelEncoder and dummy variables .

selects_hot = select(X_hot)

selects_le = select(X_le)

Then we compare these features in a simple way.

Dummy variables vs label encoder

The comparison results are as follows,

Dummy variables vs label encoder classification reports

We select these fields because the features we create with dummy variables give better results.

from sklearn.model_selection import train_test_split

from sklearn.utils import resample

import pandas as pd last = test_hot[selects_hot + ['is_canceled']]



X_last = last.drop('is_canceled', axis=1)

y_last = last['is_canceled'] # separate majority and minority classes

major = selected[selected['is_canceled'] == 0]

minor = selected[selected['is_canceled'] == 1]



# downsample majority class

downsampled = resample(major, replace=False, n_samples=len(minor), random_state=123)



# combine minority class with downsampled majority class

df_new = pd.concat([downsampled, minor])



# display new class counts

print(df_new['is_canceled'].value_counts()) X = df_new.drop('is_canceled', axis=1)

y = df_new['is_canceled'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

With the code above, we have equalized both the number of success reservations and the number of canceled reservations by 8697 and divided our data set into train and test. We will then measure the performance of our models by creating the following class.

Report class

Let’s go to the last step and compare our models!

Modelling

Many models have been tried here, you can see them in my repository. But here I’m going to share the results of the top 2 models and some code that shows how we do hyperparameter tuning . Here’s how it goes,

from sklearn.model_selection import GridSearchCV

from xgboost import XGBClassifier report = Report(X_test, y_test)

xgb = XGBClassifier().fit(X_train, y_train) xgb_params = {

'n_estimators': [100, 500, 1000],

'max_depth': [3, 5, 10],

'min_samples_split': [2, 5, 10]

} params = {

'estimator': xgb,

'param_grid': xgb_params,

'cv': 5,

'refit': False,

'n_jobs': -1,

'verbose': 2,

'scoring': 'recall',

} xgb_cv = GridSearchCV(**params)

_ = xgb_cv.fit(X_train, y_train) print(xgb_cv.best_params_)

xgb = XGBClassifier(**xgb_cv.best_params_).fit(X_train, y_train) report.metrics(xgb)

report.plot_roc_curve(xgb, save=True)

XGBoost results are as follows:

XGB results

If we replace the XGBoost with the GBM using the codes above, the results are as follows:

GBM results

Conclusion

First, I wanted to emphasize the importance of preprocessing and feature selection steps in model building processes in this article. The way to create a successful model is to get clean data.

The optimization of the model established afterwards and especially the problem of classification should not be overlooked the importance of recall values. The accuracy by class is one of the most critical points of classification problems.

Hopefully, it has been a useful article!"
Building A ‘Serverless’ Chrome Extension,"This is a tutorial on building a Chrome Extension that leverages Serverless architecture. Specifically — we will use Google Cloud Functions in the Back-End of our Chrome Extension to do some fancy Python magic for us.

The Extension we will build is the SummarLight Chrome Extension:

The SummarLight Extension takes the text of he current web page you are on (presumably a cool blog on medium such as mine) and highlights the most important parts of that page/article.

In order to do this, we will setup a UI (a button in this case) which will send the text on our current web page to our Back-End. The ‘Back-End’ in this case will be a Google Cloud Function which will analyze that text and return its Extractive Summary (the most important sentences of that text).

Architecture

A Simple & Flexible Architecture

As we can see, the architecture is very straightforward and flexible. You can setup a simple UI like an App or, in this case, a Chrome Extension, and pass any complex work to your serverless functions. You can easily change your logic in the function and re-deploy it to try alternative methods. And finally, you can scale it for as many API calls as needed.

This is not an article on the benefits of serverless so I will not go into details about the advantages of using it over traditional servers. But usually, a serverless solution can be much cheaper and scalable (but not always…depends on your use case).

The Chrome Extension

Here is a good guide on the setup of a Chrome Extension:

And all the code for the SummarLight Extension can be found here:

The main.py file in the root directory is where we define our Google Cloud Function. The extension_bundle folder has all the files that go into creating the Chrome Extension.

Google Cloud Function

I chose Google instead of AWS Lamba because I had some free credits (thanks Google!) but you can easily do it with AWS as well. It was a huge plus for me that they had just released Google Cloud Functions for Python as I do most of my data crunching in that beautiful language.

You can learn more about deploying Google Cloud Functions here:

I highly recommend using the gcloud sdk and starting with there hello_world example. You can edit the function in the main.py file they provide for your needs. Here is my function:

import sys

from flask import escape

from gensim.summarization import summarize

import requests

import json





def read_article(file_json):

article = ''

filedata = json.dumps(file_json)

if len(filedata) < 100000:

article = filedata

return article



def generate_summary(request):



request_json = request.get_json(silent=True)

sentences = read_article(request_json)



summary = summarize(sentences, ratio=0.3)

summary_list = summary.split('.')

for i, v in enumerate(summary_list):

summary_list[i] = v.strip() + '.'

summary_list.remove('.')



return json.dumps(summary_list)

Pretty straight forward. I receive some text via the read_article() function and then, using the awesome Gensim library, I return a Summary of that text. The Gensim Summary function works by ranking all the sentences in order of importance. In this case, I have chosen to return the top 30% of the most important sentences. This will highlight the top one third of the article/blog.

Alternative Approaches: I tried different methods for summarization including using Glove Word Embeddings but the results were not that much better compared to Gensim (especially considering the increased processing compute/time because of loading in those massive embeddings). There is still a lot of room for improvement here though. This is an active area of research and better text summarization approaches are being developed:

Once we are good with the function we can deploy it and it will be available at an HTTP endpoint which we can call from our App/Extension.

Extension Bundle

Now for the Front-End. To start, we need a popup.html file. This will deal with the UI part. It will create a menu with a button.

<!DOCTYPE html>

<html>

<head>

<link rel=""stylesheet"" href=""styles.css"">

</head>

<body>

<ul>

<li>

<a><button id=""clickme"" class=""dropbtn"">Highlight Summary</button></a>

<script type=""text/javascript"" src=""popup.js"" charset=""utf-8""></script>

</li>

</ul>

</body>

</html>

As we can see, the ‘Highlight Summary’ button has an onClick event that triggers the popup.js file. This in turn will call the summarize function:

function summarize() {

chrome.tabs.executeScript(null, { file: ""jquery-2.2.js"" }, function() {

chrome.tabs.executeScript(null, { file: ""content.js"" });

});

}

document.getElementById('clickme').addEventListener('click', summarize);

The summarize function calls the content.js script (yeah yeah I know we could have avoided this extra step…).

alert(""Generating summary highlights. This may take up to 30 seconds depending on length of article."");



function unicodeToChar(text) {

return text.replace(/\\u[\dA-F]{4}/gi,

function (match) {

return String.fromCharCode(parseInt(match.replace(/\\u/g, ''), 16));

});

}



// capture all text

var textToSend = document.body.innerText;



// summarize and send back

const api_url = 'YOUR_GOOGLE_CLOUD_FUNCTION_URL';



fetch(api_url, {

method: 'POST',

body: JSON.stringify(textToSend),

headers:{

'Content-Type': 'application/json'

} })

.then(data => { return data.json() })

.then(res => {

$.each(res, function( index, value ) {

value = unicodeToChar(value).replace(/\

/g, '');

document.body.innerHTML = document.body.innerHTML.split(value).join('<span style=""background-color: #fff799;"">' + value + '</span>');

});

})

.catch(error => console.error('Error:', error));

Here is where we parse the html of the page we are currently on (document.body.innerText) and, after some pre-processing with the unicodeToChar function, we send it to our Google Cloud Function via the Fetch API. You can add your own HTTP endpoint url in the api_url variable for this.

Again, leveraging Fetch, we return a Promise, which is the summary generated from our serverless function. Once we resolve this, we can parse the loop through the html content of our page and highlight the sentences from our summary.

Since — it can take a little while for all this processing to be done, we add an alert at the top of the page which will indicate this (“Generating summary highlights. This may take up to 30 seconds depending on length of article."").

Finally — we need to create a manifest.json file that is needed to publish the Extension:

{

""manifest_version"": 2,

""name"": ""SummarLight"",

""version"": ""0.7.5"",

""permissions"": [""activeTab"", ""YOUR_GOOGLE_CLOUD_FUNCTION_URL""],

""description"": ""Highlights the most important parts of posts/stories/articles!"",

""icons"": {""16"": ""icon16.png"",

""48"": ""icon48.png"",

""128"": ""icon128.png"" },

""browser_action"": {

""default_icon"": ""tab-icon.png"",

""default_title"": ""Highlight the important stuff"",

""default_popup"": ""popup.html""

}

}

Notice the permissions tab. We have to add our Google Cloud Function URL here to make sure we do not get a CORS error when we call our function via the Fetch API. We also fill out details like the name/description and icons to be displayed for our Chrome Extension on the Google Store.

And that’s it! We have created a Chrome Extension that leverages a serverless backbone aka Google Cloud Function. The end effect is something like this:

A demo of SummarLight

It’s a simple but effective way to build really cool Apps/Extensions. Think of some of the stuff you have done in Python. Now you can just hook up your scripts to a button in an Extension/App and make a nice product out of it. All without worrying about any servers or configurations.

Here is the Github Repo: https://github.com/btahir/summarlight

And you can use the Extension yourself. It is live on the Google Store here:

Please share your ideas for Extensions (or Apps) leveraging Google Cloud Functions in the comments. :)

Cheers."
Dependency Parser or how to find syntactic neighbours of a word,"This article will go through the theory to demystify this insufficiently known part of NLP. Then, in a second article, we will suggest tools to help you understand how to easily implement a Dependency Parser.

When we think about a word’s neighbors, we could think about the neighborhood as their location in a sentence, their relation to other words (subject, determinant, etc.), called syntax, or as their meaning similarity, called semantics. What interests us here is the syntactical neighborhood.

Vocabulary

First, let’s define some vocabulary to make it clearer for everyone.

Semantics is the linguistic and philosophical field that studies meaning and interpretation. It relies a lot on links between words to understand the sentence, and it analyzes the changes in meaning. In programming, semantics is the expected output of a program.

is the linguistic and philosophical field that studies meaning and interpretation. It relies a lot on links between words to understand the sentence, and it analyzes the changes in meaning. In programming, semantics is the expected output of a program. Syntax is the linguistic field of grammar. It is the study of the rules for word patterns in sentences. Known in programming too, errors in syntax often lead to bugs, because rules are often much stricter than in oral language.

What is a Dependency Parser ?

A Dependency Tree is a structure that can be defined as a directed graph, with |V| nodes (vertices), corresponding to the words, and |A| Arcs, corresponding to the syntactic dependencies between them. We may also want to attribute labels to dependencies, called relations. These relations give details about the dependency type (e.g. Subject, Direct Object Complement, Determinant…). You can find all the relations from Universal Dependencies by following this link : https://universaldependencies.org/u/dep/index.html.

Example of Dependency Tree : “What is a parser ?”

In an arc h → d, h is the head and d is the dependent. The head is the most important node in a phrase, while the Root is the most important node in the whole sentence: it is directly or indirectly the head of every other node.

A Dependency Parser simply transforms a sentence into a Dependency Tree.

Metrics : how to recognize a good parser ?

An accurate Dependency Parser recognizes the dependencies and relations between words well. Two Metrics (scores) are useful for this:

- Unlabeled Attachment Score (UAS), which corresponds to the number of correctly predicted dependencies over the number of possibilities;

- Labeled Attachment Score (LAS), which corresponds to the number of correctly predicted dependencies and relations over the number of possibilities.

LAS is always less than or equal to UAS, because an incorrect dependency leads to a suboptimal UAS and LAS, while an incorrect relation (or label) only leads to a LAS decreasing.

Algorithm : How does it work ?

As you might have thought, we could create a Dependency Parser through rules developed by linguists. These parsers are called Rationalists. They are not at all efficient, since languages are very complex, and they change over time. Any small change in the language would lead to tremendous changes in the parser. Machine Learning allows for the development of Empiric parsers, which are data driven. Fed by many sentences, probabilities of dependencies or relations can be drawn. Linguistic knowledge may be used, but does not have the last word, which is a good point if you, like me, have forgotten your primary school lessons…

Several steps are needed to create a Dependency Parser. Our inputs are the words of the sentence with their properties (index, Part of Speech tag, Lemma, Features); then, we must calculate features for all possible arcs in the sentence. Thanks to these features, we compute a score for each possibility, and we finally decode scores with a decoder.

Features and Score

Each word in the sentence has some attributes, like Part of Speech tags or Lemmas. You might know them if you have already read about NLP. You can check it out here, if not:

With these features, we train a Machine Learning regression model that returns the score to be exploited by the decoder.

Feature selection is crucial, and some models allow us to bypass this part via a deep learning part. This is the case with the algorithm we will present in the following section.

Decoders

There are a lot of different decoders already developed. However, we can divide them into two categories: Transition-based decoders and Graph-based ones. Transition-based decoders are faster and need less memory to decode scores, but they are generally less accurate than Graph-based decoders. I will only go through Graph-based model principles in this article.

Other algorithms can apply different transitions, but this one allows us to understand the main principle.

Graph-Based Decoders

It is necessary to deal with graph theory to understand these algorithms.

A graph G=(V, A) is a set of vertices V (called also nodes), that represent the tokens, and arcs (i, j)∈ A where i, j ∈ V. The arcs represent the dependencies between two words.

In a Graph-based dependency parser, graphs are directed, which means links have different directions, and there can be multiple arcs between nodes, this is called a multi-digraph.

Weighted Multi Directed Graph (G)

You can note that some arrows are thicker than others. This represents the weight of arcs. The more an arc weighs, the stronger the link between two nodes. We could interpret this as the strength of the syntactic dependency for our parser. For Example, C and A seem to be very dependent on B, but B does not seem very dependent on C and A.

Graph G is too connected. In order to get a Dependency Tree, we want:

To link each word only with its dependents — not with all the words. The total number of arcs should be equal to the number of nodes minus 1 (|A| = |V|-1).

To keep the same nodes (or tokens or words).

To make it acyclic: we do not want a head to be dependent on one of its dependents (direct or indirect).

Fortunately, all of this already has a name: what we want is a Spanning Tree!

Example of Spanning Tree from the graph G

An other example of Spanning Tree

If I was clear on what a Spanning Tree is, you should know that there are multiple possibilities, since we only have a few conditions to get one. Here comes a trick: we want the best one, certainly, but how could we determine the “best” one?

We have 3 nodes here, and we want to keep them. However, we have 6 arcs and we want to keep only 2. The “best” Dependency Tree is the one that has the highest weights: this is called the Maximum Spanning Tree (MST).

Maximum Spanning Tree of G

Minimum Spanning Tree of G

This Maximum Spanning Tree gives us our Dependency Tree, which we will use to find the closest syntactic neighbors in the sentence.

Conclusion

The insight given here is very light compared to the different existing algorithms. However, this should improve your intuition when developing your Dependency Parser."
Autoencoders vs PCA: when to use ?,"Need for dimensionality reduction

In machine learning projects we often run into curse of dimensionality problem where the number of records of data are not a substantial factor of the number of features. This often leads to a problems since it means training a lot of parameters using a scarce data set, which can easily lead to overfitting and poor generalization. High dimensionality also means very large training times. So, dimensionality reduction techniques are commonly used to address these issues. It is often true that despite residing in high dimensional space, feature space has a low dimensional structure.

Two very common ways of reducing the dimensionality of the feature space are PCA and auto-encoders. I will only provide brief introduction to these, for a more theoretically oriented comparison read this post.

PCA

PCA essentially learns a linear transformation that projects the data into another space, where vectors of projections are defined by variance of the data. By restricting the dimensionality to a certain number of components that account for most of the variance of the data set, we can achieve dimensionality reduction.

Autoencoders

Autoencoders are neural networks that can be used to reduce the data into a low dimensional latent space by stacking multiple non-linear transformations(layers). They have a encoder-decoder architecture. The encoder maps the input to latent space and decoder reconstructs the input. They are trained using back propagation for accurate reconstruction of the input. In the latent space has lower dimensions than the input, autoencoders can be used for dimensionality reduction. By intuition, these low dimensional latent variables should encode most important features of the input since they are capable of reconstructing it.

Comparison

PCA is essentially a linear transformation but Auto-encoders are capable of modelling complex non linear functions. PCA features are totally linearly uncorrelated with each other since features are projections onto the orthogonal basis. But autoencoded features might have correlations since they are just trained for accurate reconstruction. PCA is faster and computationally cheaper than autoencoders. A single layered autoencoder with a linear activation function is very similar to PCA. Autoencoder is prone to overfitting due to high number of parameters. (though regularization and careful design can avoid this)

When to use which?

Apart from the consideration about computational resources, the choice of technique depends on the properties of feature space itself. If the features have non-linear relationship with each other than autoencoder will be able to compress the information better into low dimensional latent space leveraging its capability to model complex non-linear functions. What does it mean for the features to have non-linear relationships? Let us do a couple of simple experiments to answer these questions and shed some light on comparative usefulness of both techniques.

Experiments 2D

Here we construct two dimensional feature spaces (x and y being two features) with linear and non-linear relationship between them (with some added noise). We will compare the capability of autoenocoders and PCA to accurately reconstruct the input after projecting it into latent space. PCA is a linear transformation with a well defined inverse transform and decoder output from autoencoder gives us the reconstructed input. We use 1 dimensional latent space for both PCA and autoencoders.

It is evident if there is a non linear relationship (or curvature) in the feature space, autoencoded latent space can be used for more accurate reconstruction. Where as PCA only retains the projection onto the first principal component and any information perpendicular to it is lost. Let us look at the reconstruction cost as measured by mean squared error (MSE) in the table below.

Experiments 3D

Conducting similar experiments in 3D. We create two three dimensional feature spaces. One is a 2D plane existing in 3D space and the other is a curved surface in 3D space.

We can see that in case of a plane there is a clearly two dimensional structure to the data and PCA with two components can account for 100% of the variance of the data and can thus achieve perfect reconstruction. In case of a curved surface two dimensional PCA is not able to account for all the variance and thus loses information. The projection to the plain that covers the most of variance is retained and other information is lost, thus reconstruction is not that accurate. On the other hand autoencoder is able to reconstruct both plane and surface accurately using two dimensional latent space. So 2D latent space is able to encode more information in case of autoencoder because it is capable of non-linear modelling. Reconstruction cost is provided in the table below.

Random Data Experiment

Here we create a random data without any collinearity. All features are independently sampled from a uniform distribution and have no relationship with each other. We use two dimensional latent space fro both PCA and Autoencoder.

We see that PCA is able to retain the projection onto the plane with maximum variance, and loses a lot of information because the random data did not have a underlying 2 dimensional structure. Autoencoder also does poorly since there was no underlying relationship between features.

Conclusion

For dimensionality reduction to be effective, there needs to be underlying low dimensional structure in the feature space. I.e the features should have some relationship with each other.

If there is non-linearity or curvature in low dim structure than autoencoders can encode more information using less dimensions. So they are a better dimensionality reduction technique in these scenarios.

All code for the experiments can be found here:"
Working with Hive using AWS S3 and Python,"The main objective of this article is to provide a guide to connect Hive through python and execute queries. I’m using “Pyhive” library for that. I’m creating my connection class as “HiveConnection” and Hive queries will be passed into the functions. AWS S3 will be used as the file storage for Hive tables.

import pandas as pd

from pyhive import hive class HiveConnection:

@staticmethod

def select_query(query_str: str, database:str =HIVE_SCHEMA) -> pd.DataFrame:

""""""

Execute a select query which returns a result set

:param query_str: select query to be executed

:param database: Hive Schema

:return:

""""""

conn = hive.Connection(host=HIVE_URL, port=HIVE_PORT, database=database, username=HIVE_USER)



try:

result = pd.read_sql(query_str, conn)

return result

finally:

conn.close()



@staticmethod

def execute_query(query_str: str, database: str=HIVE_SCHEMA):

""""""

execute an query which does not return a result set.

ex: INSERT, CREATE, DROP, ALTER TABLE

:param query_str: Hive query to be executed

:param database: Hive Schema

:return:

""""""

conn = hive.Connection(host=HIVE_URL, port=HIVE_PORT, database=database, username=HIVE_USER)

cur = conn.cursor()

# Make sure to set the staging default to HDFS to avoid some potential S3 related errors

cur.execute(""SET hive.exec.stagingdir=/tmp/hive/"")

cur.execute(""SET hive.exec.scratchdir=/tmp/hive/"")

try:

cur.execute(query_str)

return ""SUCCESS""

finally:

conn.close()

I’m keeping the queries as separated strings. This way you can format the queries with external parameters when necessary. Hive configurations (HIVE_URL, HIVE_PORT, HIVE_USER, HIVE_SCHEMA) as constants. Function “select_query” will be use to retrieve data and function “execute_query” will be used for other queries.

Hive provides a shell interactive tool to initiate databases, tables and manipulate the data in tables. We can go into the Hive command line by typing command “hive”. You can execute all the queries given in this article in the shell also.

Create a new Schema

Schema is a collection of tables which is similar to a database. Both keywords SCHEMA and DATABASE are allowed in Hive. We can pick either. Here we use SCHEMA instead of DATABASE. Schema can be created with “CREATE SCHEMA”. To go inside the schema, the keyword “USE” is available.

CREATE SCHEMA userdb;

USE userdb;

Create tables

There are three types of Hive tables. They are Internal, External and Temporary. Internal tables store metadata of the table inside the database as well as the table data. But external tables store metadata inside the database while table data is stored in a remote location like AWS S3 and hdfs. When dropping an internal table, all the table data will be erased with the metadata. When dropping an external table, only the metadata will be erased; not the table data. In this way, actual data will be protected. If you point a new table to the same location, data will be visible through the new table.

Hive is a data warehouse and uses MapReduce Framework. So the speed of the data retrieving may not fair enough for small queries. Hive tables can be partitioned in order to increase the performance. Partitioning technique can be applied to both external and internal tables. Concepts like bucketing are also there. You can choose any of these techniques to enhance performance.

Temporary tables are useful when copying data from one place to another. It acts as a temporary location to hold the data within a database session. All the temporary tables are cleared after the session timeout. Creating a temporary table is not useful with “Pyhive” library as multiple queries are not supported in a single session. Even though we created a table, the same session will no longer be available to access the table. But this is possible in the Hive command line. You can create a temporary table and then select data from that table in a single session.

Internal Tables

The following query is to create an internal table with a remote data storage, AWS S3. The file format is CSV and field are terminated by a comma. “s3_location” points to the S3 directory where the data files are. This is a user-defined external parameter for the query string. It should be passed in the time of query formatting.

CREATE TABLE `user_info` (

`business_unit` INT,

`employee_id` INT,

`email` VARCHAR(250),

`first_name` VARCHAR(250),

`last_name` VARCHAR(250),

`gender` VARCHAR(250),

`birthday` DATE,

`created_date` DATE,

`updated_date` DATE

)

ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\'

LOCATION '{s3_location}'

TBLPROPERTIES (

""s3select.format"" = ""csv"",

""skip.header.line.count"" = ""1""

);

If the data strings contain commas, it will break the table structure. So I have defined an escape character and all the unnecessary commas needed to be preceded by this escape character before creating the table.

Following is an example record. Note that email contains a comma.

1,1,ann,smith@gamil.com,Ann,Smith,female,'1992–07–01','2019–09–01','2019–12–31'

above record need to be formatted like this :

1,1,ann\\,smith@gamil.com,Ann,Smith,female,'1992–07–01','2019–09–01','2019–12–31'

External Tables

Here, I have partitioned “user_info” table with “business_unit” and “created_date”

CREATE EXTERNAL TABLE `user_info` (

`employee_id` INT,

`email` VARCHAR(250),

`first_name` VARCHAR(250),

`last_name` VARCHAR(250),

`gender` VARCHAR(250),

`birthday` DATE,

`updated_date` DATE

) partitioned by(

`business_unit` INT,

`created_date` DATE,

)

ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' ESCAPED BY '\\'

STORED AS

INPUTFORMAT

'com.amazonaws.emr.s3select.hive.S3SelectableTextInputFormat'

OUTPUTFORMAT

'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'

LOCATION '{s3_location}'

TBLPROPERTIES (

""s3select.format"" = ""csv"",

""s3select.headerInfo"" = ""ignore""

);

Temporary table

Query for creating a temporary table.

CREATE TEMPORARY TABLE `user_info` (

`business_unit` INT,

`employee_id` VARCHAR(250),

`email` VARCHAR(250),

`first_name` VARCHAR(250),

`last_name` VARCHAR(250),

`gender` VARCHAR(250),

`birthday` DATE,

`created_date` DATE,

`updated_date` DATE

) ;

Drop Table

Query to drop a table. If you are dropping an external table data in remote file storage will not be erased.

DROP TABLE IF EXISTS `user_info`;

Insert data

Once the table is created with an external file storage, data in the remote location will be visible through a table with no partition. But this is not true when it comes to a table with partitions. Which means data can not be directly copied into a partitioned table. We need to create a temporary table with no partition and insert data into the partitioned table by providing the partition values. The following query describes how to insert records to such a table.

INSERT INTO TABLE user_static_info PARTITION (business_unit={business_unit}, `created_date`='{execution_date}')

SELECT

Employee_id,

email,

secondary_email,

first_name,

last_name,

orig_gender,

gender,

signup_channel ,

signup_from_fb ,

birthday,

signup_date,

updated_date,

last_activity_date,

subscription_status

FROM

tmp_user_static_info

WHERE business_id={business_unit}

Since “Pyhive” is not supported for multiple queries in a single session; I had to create the internal table “tmp_user_static_info” which points to S3 data directory without partitions. Then it was dropped after inserting data to the external, partitioned table.

Retrieve data

SELECT queries are used to retrieve data in Hive. These are much similar to SQL SELECT queries. It has the following form. You can build the query for your requirements.

SELECT [ALL | DISTINCT] select_expr, select_expr, …

FROM table_reference

[WHERE where_condition]

[GROUP BY col_list]

[HAVING having_condition]

[CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list]]

[LIMIT number];

Update and Delete data

Hive does not support UPDATE and DELETE data directly. If you want to change anything from a table; copy the necessary data to a new table with SELECT queries. Then you can replace the old table with a new table by dropping the old table and renaming the new table.

Alter tables

Table alterations are possible in Hive. But this needs to be done very carefully without affecting the existing data. Because we can’t alter the data. As an example, adding a new field in the middle will not shift data. If we add a new field as the second field, data that belong to the third column will still appear in the second column and fourth field data in the 3rd field and so on. The last field will not contain any data. This is because of the restriction of updating hive table data. If we added a new field as the last field, there will be an empty field and we can insert data into that field.

ALTER TABLE user_static_info ADD COLUMNS (last_sign_in DATE);

If we want to drop external data we can use the following steps.

ALTER TABLE user_static_info SET TBLPROPERTIES('EXTERNAL'='False');

DROP TABLE user_static_info;

Example

Finally, the following code shows how to execute a query using “execute_query” function in “HiveConnection” class."
pix2pix GAN in TensorFlow 2.0,"The interplay of loss function and network architecture is subtle, and as we will see there are often multiple ways to solve a problem, but first we need to define a conditional loss for a generative adversarial network. The key principle here is the same as any GAN, the generative adversarial min max game between the two networks to optimise the loss function:

where G is the generator and D the discriminator, x the conditional input image, y the true image and z a random noise vector.

(A more detailed discussion here, but the basic idea is G outputs an image such that D(G) is maximised, while D is optimised to output 1 for true images. Why this ever converges, and to what extent we’re barking up the wrong tree, is another interesting discussion here.)

This already has one key difference to the vanilla GAN case: it’s conditional. Conditional here means that rather than receiving just a random noise vector, the generator takes in additional information. This could simply be class information as in the cDCGAN, or as in this case where the conditional information is the original image. Where the vannilla GAN depends as G:z -> y, the conditional GAN goes as G:{x, z} -> y.

Interestingly, this isn’t actually the full picture. When the network trains, it generally learns to ignore the random noise vector, so to keep the network non-deterministic dropout was used to reintroduce the stochastic behaviour.

In many GANs, an L2 regularisation loss is added to the optimisation on the basis that by minimising the Euclidean distance between the target and generated image (MSE), the generator would learn structure and colour of the image. However, it was found that this generally leads to blurred images, so to combat this the L1 regularisation loss is added with some pre-factor weight instead, as:

Which gives the total loss function to optimise as:

The next key question is about the structure of the networks.

In this analysis, the generator is based on a U-Net structure, a variation on an auto encoder, while the discriminator is called patch-GAN.

Generator Architecture

The point of using a U-Net structure is that the network forces all information to be passed through a tight bottle neck in the middle of the network. This forces a latent representation of the input image to be found that can be reconstructed to the original, the idea being that a finite amount of information can pass through - the network is forced to learn an optimal reduced mapping, and cannot simply memorise the training set.

This has one obvious limitation; there is a significant proportion of the output and input image that ought to share the same description. The content of the image is supposed to remain unchanged. The variation in U-Net over an auto-encoder is the addition of a skip connection between each symmetric layer in the U-net structure, as can be seen in figure above. These concatenated layers have the effect of passing higher level information directly at the appropriate scale in the network, and reduce the amount of information needed to pass through the latent bottleneck layer. The idea here is that the information passed through focuses on finer details rather than large scale structure.

The discriminator is more atypical and might need more context. In general terms, the L1 and L2 regularisation is a weak constraint on the network that doesn’t produce sharp details as there are many paths to get a small L value. However, this doesn’t write off this part of the loss function, as it encourages generating the high level structure, which is exploited in the choice of discriminator.

Introducing crisp details to the generated image can be done via a number of paths:

Tuning the weight of the lambda pre-factor on the L1/L2 loss — as discussed above, this caps out with relatively blurred images, generally correct but without sharp details. Adding an additional loss that quantifies the performance of the output image — a number of methods using a pre-trained network to assess the quality of the output images have been tried. Specifically in the case of SRGAN, the distance between the latent space of the VGG network on the target and output images is minimised. Update the discriminator to promote the crisp details — patch-GAN — seems almost too obvious, doesn’t it?

Patch-GAN Discriminator [here]

The way patch GAN works is that rather than classifying the entire image, only a portion (NxN) of the image is classified as real or fake. The ‘patch’ scans over the image, makes a prediction for each, and the mean is optimised.

This assumes the image can be treated as a Markov random field, where pixels separated by N or more pixels are independent. This wouldn’t be true for the entire image when considering high level structure, but is a fair assumption when looking at low level detail.

The advantage here is that fine detail is preserved with the patch-GAN; to pass the discriminator test realistic low level details are encouraged, and the general structure is preserved using the regularisation loss. Another vital advantage of this patch method over a whole image classifier is that it’s fast, as small NxN patches are faster to evaluate and train than one single discriminator. It also scales efficiently to arbitrary sized images.

There are two technical points on the training of the network. Firstly, rather than minimising (1 — log D), the training maximises log(D). Secondly, the Discriminator objective function is halved during training to restrict the rate at which the discriminator trains compared to the generator."
Setting Up Swagger Docs for Golang API,"Note: This was originally posted at martinheinz.dev

In the previous post — Building RESTful APIs in Golang — we created RESTful API in Golang. Now, that we have our project ready to be used, we should show our users how to do so, otherwise — if they can’t test it and view its features — they won’t even touch it.

Without meaningful documentation of our APIs as well as an ability to test its endpoints, users won’t even bother trying to use it. Solution to that is writing documentation. However, writing it may take lots of time, which could otherwise used to develop more cool features for our applications… So, what do we do? — we generate Swagger docs!

Libraries

Let’s start with libraries needed to create the Swagger docs. I said libraries, but really just need one — swag — which is Golang library that converts code annotations (comments) to Swagger Documentation 2.0. Aside from swag you will need a middleware/wrapper library for your web framework. In swag docs there are links to libraries for supported frameworks, which include both the simplest option of net/http which a lot of people like to use as well as GIN, which I use and which I will show here. Even though you might be using different web framework, the annotations are gonna be the same, so you can learn something here anyway.

It’s also worth to mention, that there is alternative Golang Swagger library — go-swagger which seems to be more popular and also quite a bit more powerful. I, personally, however prefer to swaggo/swag because of its simplicity. If you need more control over what gets generated you might want switch to go-swagger .

Docstrings

Now, for the annotations/comments/docstring or whatever you want to call it. It’s really just bunch of comments before specific API function, which is used to generate the Swagger docs.

Before we get to describing individual API endpoints, we need to first write general description for our whole project. This part of annotations lives in your main package, right before the main function:

Note: All the examples below come from my repository here, where you can find runnable application with the Swagger UI/Docs included."
"If you like to travel, let Python help you scrape the best cheap flights!","Simply put

The goal of this project is to build a web scraper that will run and perform searches on flight prices with flexible dates (up to 3 days before and after the dates you select first), for a particular destination. It saves an excel with the results and sends an email with the quick stats. Obviously, the objective is to help us find the best deals!

If you get lost in some part, try to have a look at my article about the Instagram bot, as it uses Selenium too.

The real life application for this is up to you. I’ve used it to search both holidays and recently also some short trips to my hometown!

If you’re serious about it, you can run the script on a server (a simple Raspberry Pi will do), and make it start once or twice each day. The results will be mailed to you, and I suggest saving the excel file to a Dropbox folder, so you can access it from anywhere, anytime."
- The Rise and Fall of Adobe Flash -,"Here Today, Gone in a Flash Tali Scheer · Follow 5 min read · Apr 22, 2019 -- Listen Share

- The Rise and Fall of Adobe Flash -

Once a dominant platform for online animation and multimedia content, Adobe Flash has since seen its day in the sun. Officially deprecated in 2017, Flash has a quickly approaching end-of-life scheduled for the end of 2020. The three-year period was designed to provide time for content platforms and creators to migrate existing Flash content to newer platforms, such as HTML5.

History and Rise

Adobe Flash was a platform developed by Adobe Systems with the focus of creating rich internet content. It combined images, graphics, animation, video and sound in order to provide users with a better web experience. It was even able to capture mouse, keyboard, microphone and camera input.

Flash revolutionized the internet. Once a static and dry place, with boring websites and a few gif’s here there, it transformed the web into an interactive and dynamic experience. At one point, it seemed almost impossible to browse the internet without landing on a page that would require you to install or update your Flash Player plug-in.

Adobe Flash’s history can be traced back to the mid 90’s to a product called SmartSketch, a vector drawing application, published by FutureWave Software. SmartSketch was soon transformed into FutureSplash to include frame-by-frame animation. In 1995, FutureWave actually approached Adobe in an attempt to sell them their software, but Adobe turned them down.

In 1996, FutureWave sold their technology to Macromedia which soon rebranded it to FutureSplash Animator as Macromedia Flash 1.0. This matured over time to include video and other graphical animation. In 2005 Macromedia was acquired by Adobe Systems, and from here Adobe transformed Flash into the ubiquitous platform and plugin that we all came to know.

The Adobe Flash platform was comprised of several different technologies, including Flash Professional, Flash Builder Flex, Adobe Integrated Runtime, and Flash Player — the browser plug-in that provided the runtime environment for Flash, and which we are all likely most familiar with.

By the early 2000s, Flash had become a common addition to desktop computers, and was used across the internet by industry leaders such as YouTube — which initially built its entire platform on Flash — Nike, HP, HBO and more.

However, Flash was not adored by all.

A Bumpy Ride

A quick Google search will reveal a plethora of results with titles such as, “You Really Shouldn’t Be Running Adobe Flash Player Anymore,” and “Keep your computer safe from the next Adobe Flash bug.”

As the use of Flash increased across the internet, so too did its vulnerabilities. Flash was installed on nearly every computer and in almost every browser and quickly became an easy target for hackers. Part of the problem stemmed from the fact that Flash Player ran as a browser plugin rather than a native software platform, and plugins tend to be more susceptible to hacking.

Adobe began seeing attack after attack ranging from plugin vulnerabilities to update scams. Hacked websites would direct users to a screen which appeared to prompt the download of a new version of Adobe Flash, but was in fact malware, granting hackers access to millions of users’ computers. Hackers excelled at creating “update” lookalike screens that could fool even an experienced internet user. Some vulnerabilities even enabled spying via web cams.

Other frustrations with Adobe Flash included its frequent use for annoying ads and banners, its control by Adobe as opposed to being open-source, and its leading to slow page load and display times.

The Battleground

Over time, experts began recommending against the installation of Flash or blocking the plugin all-together, but the real impact occurred when third-party platforms began limiting its use.

One of the most notable adversaries to Flash was Steve Jobs. Apple restricted the use of Flash as early as 2007, and ultimately did not adopt the use of Flash on any of their mobile iOs devices, including iPhones, iPads and iPods. This greatly reduced the Flash user base and led to wider use of HTML5, including YouTube’s transition to HTML5 for mobile browsers. HTML5 can be used to accomplish all the same “advanced graphics, typography, animations and transitions without relying on third party plug-ins.”

In 2010, Steve Jobs even wrote a 1,700 word letter, detailing why he chose to abandon Flash, despite Apple’s historical relationship with Adobe.

He listed six comprehensive reasons as to why Apple chose not to use Flash, including stating that newer and better video options exist, the fact that Flash is riddled with security issues, and the general negative effect that Flash had on load time and battery life.

Below is an excerpt from Jobs’ letter:

Symantec recently highlighted Flash for having one of the worst security records in 2009. We also know first hand that Flash is the number one reason Macs crash. We have been working with Adobe to fix these problems, but they have persisted for several years now. We don’t want to reduce the reliability and security of our iPhones, iPods and iPads by adding Flash.

— Steve Jobs

Soon to follow were other large companies such as Facebook, which issued a call to Adobe to discontinue the use of Flash in 2015, followed by Mozilla and Chrome which blacklisted older versions of Flash.

Deprecation

In 2015, Adobe began transitioning from Adobe Flash to their newer Adobe Animate software, though not all were impressed…

But Adobe ultimately succumbed to the pressure and announced its deprecation of Flash in 2017. In their open letter announcing the decision, Adobe seemingly glosses over the security issues in Flash’s history, and instead focused on the progress made by other technologies.

…as open standards like HTML5, WebGL and WebAssembly have matured over the past several years, most now provide many of the capabilities and functionalities that plugins pioneered and have become a viable alternative for content on the web.

Despite its bumpy rise and fall, we cannot deny the role that Adobe Flash played in transforming the internet into the dynamic, exciting place that we know today. Adobe has, and will certainly continue, to be a leader in online innovation.

Sources"
Data Scientist’s toolkit — How to gather data from different sources,"Data Scientist’s toolkit — How to gather data from different sources

Photo by Jakob Owens on Unsplash

Not so long ago!

Do you remember the time when data was sent to you in an external hard drive for your analysis or model building.

Now — as a data scientist, you are not limited to those means. There are several ways of storing data, sharing data as well as different sources to acquire data, augment data.

Below, I’m listing down several ways of gathering data for your analysis

Table of contents:

CSV file Flat File (tab, space, or any other separator) Text File (In a single file — reading data all at once) ZIP file Multiple Text Files (Data is split over multiple text files) Download File from Internet (File hosted on a server) Webpage (scraping) APIs (JSON) Text File (Reading data line by line) RDBMS (SQL Tables)

In Python, a file is characterized as either text or binary, and the difference between the two is important

Text files are structured as sequence of lines. Each line is terminated with a special character called EOL or End of line character. There are several types but most common are

or ,

A Binary file type is basically any type of file that’s not a text file. Because of their nature, binary file can only be processed by an application that know or understand the file’s structure

1. CSV File

Most common format for storing and sharing dataset is comma separated format or a csv file. pandas.read_csv() is THE most useful and powerful method and I strongly recommend you to read its documentation . By using appropriate kind of sep you can load several types of data in dataframe

import pandas df = pd.read_csv('data.csv', sep =',')

2. Flat File

but at times you might receive file that’s tab separated or a fixed width format or…"
Trees in data science,"Trees in data science

One of the most easy to interpret models in machine learning are CART’s(Classification and Regression Trees) known popularly as decision trees. In this post I wish to give an overview of Decision Trees, some primary concepts surrounding them and finally Random Forests. The contents are as follows

Understanding Decision Trees

Purity

Bootstrapping and Bagging

Random Forests

Lets Go!

Decision Trees

Basic structure of a Decision Tree (Source: cway-quan)

In the machine learning universe trees are actually upside down versions of real trees. Suppose we have a dataset consisting of our features ‘X’ and a target ‘Y’. What a decision tree does is that it finds patterns within X and splits the dataset into smaller subsets based on these patterns.

Visualize these splits in the slightly simplified image above. ‘Y’ here is whether or not a job offer is to be accepted. The ‘X’ contains features like “commute_time”, “salary”, “free_coffee”.

Based on patterns in ‘X’ the tree is split into branches until it reaches a point where it arrives at a pure answer to ‘Y’. In our scenario, job offers which are accepted have to provide a salary > 50k, commute time < 1hr and free coffee. In this manner the tree reaches the last leaf which is a pure decision about ‘Y’.

Purity in decision Trees

Decision trees conduct splitting based on the purity of the node. This purity is measured based on the distribution of ‘Y’. If our ‘Y’ is continuous our problem is a regression problem and the nodes are split based on MSE(Mean Squared Error). If ‘Y’ is discrete, our model is dealing with a classification problem and a different measure of purity is required.

A widely used metric to measure in classification cases is Gini Impurity. The formula for Gini impurity is given as follows:

Source: General Assembly DSI curriculum (Authors:David Yerrington, Matt Brems)

While deciding which split to make at a given node, it picks the split that maximizes the drop in Gini impurity from the parent node to the child node.

Bootstrapping and Bagging

To understand bootstrapping and bagging, the first step would be to understand why they are needed in the first place. It is basically trying to emulate the “wisdom of the crowd” principle where in the aggregated result of multiple models is better than the results of a single model. The following image by Lorna Yen gives a great idea about boot strapping.

(Author: Lorna yen, Source)

Bootstrapping as shown above is just the random sampling of data with replacement. Bagging is just a process of building decision trees on each of these samples and getting an aggregate prediction. So to summarize Bagging involves the following steps:

From the original data of size n, bootstrap k samples of size n with replacement Build a decision tree on each bootstrapped sample. Pass test data through all trees and develop one aggregate prediction

Bagging is therefore also called Bootstrapped Aggregating.

Random Forest Model

A closer look at the below image gives a basic intuition on random forests.

A basic hurdle in bagging is that the individual decision trees are highly correlated as the same features are used in all trees. So the predictions of our models suffer from the issue of variance. To know more on variance or bias you can read this link. Decorrelating our models is a solution and is exactly what Random Forests do.

Random forests follow similar steps in bagging except that they use at each split in the learning process, a random subset of the features. This mitigates the variance problem in bagging and generally produces much better results. This efficient and simple methodology has made Random forests a widely implemented Machine learning Model.

Bonus

Code for importing the explained three classification models in sklearn."
Pyspark – data manipulation,"Pyspark – data manipulation

Calcul, aggregate, transform any data Alexandre Warembourg · Follow 5 min read · Oct 14, 2019 -- Share

In this article, we will see how to calculate new variables via joins, window functions, UDFs and vector manipulations.

As a reminder, here is the table we use :

.WithColumn and sql function

to create a new column on Spark just pass the function .withColumn and add sql functions

df = (df

.withColumn('dayofyear', F.dayofyear(F.col(""ID_DAY"")))

.withColumn('Month', F.Month(F.col('ID_DAY')))

.withColumn('ratio_ticket_qty', F.col('F_TOTAL_QTY')/F.col('NB_TICKET'))

)

For more go here : SQL functions

.WithColumn with condition F.when

as with sql it is possible to use the when box for conditional calculations.

df = df.withColumn(""day_to_xmas"", F.when((F.col(""ID_DAY"").between(""2018-12-01"", ""2018-12-31"")) | (F.col('ID_DAY').between(""2019-12-01"", ""2019-12-31"")),

F.lit('xmas_is_coming')).otherwise(F.datediff(F.col(""ID_DAY""), F.lit('2018-12-01').cast(DateType())))

)

.WithColumn and Windows function

windows function are very useful to calculate values on time axes or to save us joins

grouped_windows = Window.partitionBy(F.col('SID_STORE'), F.col('Month'))

rolling_windows = (Window.orderBy(F.col(""dayofyear"").cast(IntegerType())).rangeBetween(-7, 0))

df = (df

.withColumn('rolling_average', F.avg(""F_TOTAL_QTY"").over(rolling_windows))

.withColumn('monthly_qty', F.avg('F_TOTAL_QTY').over(grouped_windows))

)

Just a join

We can do exactly the same calculation for monthly_qty as with the windows function via a join."
“Modern Times Anxiety” in AI: Are we there yet?,"“Modern Times Anxiety” in AI: Are we there yet? Prakhar Ganesh · Follow 4 min read · Sep 17, 2019 -- Listen Share

I recently came across this panel discussion from AAAI conference, 1984 and weirdly enough it felt both ancient yet relevant all at the same time and I was hoping I can throw my thoughts in this pit too.

The most obvious, what you might call the “1984 Big Brother Is Watching Anxiety,” is that somehow the computer will erode our freedom and invade our privacy.

I think the way we have simply accepted the whole ‘Privacy is a myth’ conundrum is a big reflection of what was considered a fear then has just become a part of our life now. While everybody values their privacy and are extremely protective towards it, we also have to acknowledge that most of us don’t really read the ‘Terms and Conditions’ before clicking ‘Accept’!!

.. a closely related anxiety might be called the “Modern Times Anxiety.” People becoming somehow, because of computers, just a cog in the vast, faceless machine; the strong sense of helplessness, that we really have no control over our lives ….. inevitably result in alienation, isolation, enforced conformity, standardization, and all those bad things-leaching away of humanity.

The Sky-High Expectations and the “AI Winter”

There’s a charge often leveled against AI people that they claim too much. To what extent is it due to naivete on the part of the public?

AI has always been a field aimed at trying to understand the working of a human mind, to make machines intelligent. And to be honest, in that regards, we are nowhere close to our original goal. However, there has been steady progress in the field in the last few years with some incredible success stories.

I think there is a pressing need for communication between the researchers and the general public. There are unrealistic expectations from people in AI and frankly, I have heard less ‘Wow’ and more ‘Didn’t the computers do that already?’ when talking about latest innovations with people who claim to follow the latest development in AI but don’t really know the field. And this level of expectation can be dangerous.

I don’t think this scenario is very likely to happen, nor even a milder version of it. But there is nervousness, and I think it is important that, we take steps to make sure the “AI Winter” doesn’t happen-by disciplining ourselves and educating the public.

AI Winter just refers to a stage when funding for AI projects start to dry because people are not getting what they expected. While a lot of you will say that with the recent success in the field, that is definitely not a possibility and I think I might agree with you.

But I think I should warn you, the people in this panel also thought the same and just half a decade later went through what a lot of experts call the second AI Winter!!

How much AI do these AI groups really know?

I got scared when big business started getting into this ….. they were all making investments- they all have AI group ..… those people weren’t trained in AI. They read an AI book, in many of these cases. They started off reading all the best AI research.

There is a tremendous demand for engineers and researchers in the field of AI and the supply is not as efficient as the demand, at least not the quality we should aim for. Most of the people working on the ground level in AI have entered the field through training of a year or even less and are calling themselves AI researchers.

I think that it’s wonderful that those people are being created …… They’re not researchers. The worry I have is that they will begin to think they are researchers. I don’t think we should make that demand of them ……… Ph.D research, at least in my laboratory, used to take three to four years. Now it seems to be taking five to six. It’s a long process. You have learn a lot of stuff and then try to create something on your own. That’s what a Ph.D means. All I can say is that that isn’t the same thing as being trained through tutorials.

Conclusion

I think this debate is as relevant today as it was back then. I think it is important to properly handle the expectations from this field and to train more and more ‘researchers’ that can do the job better. We need to be protective of who we are letting to be the face of AI research. I too want to be a researcher in the field, but I believe that going through the pipeline is important before I count myself among those pushing the field forward."
The Perils of Modernizing SQL Apps on NoSQL,"Over the past 40+ years, traditional relational databases like DB2, Oracle, SQL Server, Postgres, MySQL and many others have proved to be the bedrock on which hundreds of billions of dollars of applications have been built. Applications that interact with back-end databases through SQL are ubiquitous across all industries. If all of the applications that rely on SQL disappeared overnight our world would be thrown into absolute chaos. Amongst an incalculable number of effects: you wouldn’t be able to access or spend money in your bank, your healthcare records would be inaccessible, goods couldn’t be routed or shipped, etc. In many ways, modern life would be unrecognizable without SQL-based applications.

Rise of NoSQL Databases

In the early 2000s, the amount of data being generated and stored, the velocity it was arriving to the database, and the variety of data types began to explode. Initially, this was driven by clickstream data being generated by web applications, but today many other data sources, like IoT, generate similar challenges. Traditional databases, where SQL was the primary interface, struggled in this new environment. The vast majority had no practical way to horizontally scale and their rigid typing made them ill-equipped for storing completely unstructured data.

In response to these challenges new approaches were built, primarily based on mid-2000s research at several internet companies (e.g.,, Google, Amazon, and Yahoo) who were being overwhelmed by the data they were collecting.

One notable category that relies heavily on this research are NoSQL databases such as HBase, Cassandra, Dynamo, and MongoDB. These systems were built to deal with massive amounts of data through horizontally scaling architectures that could accommodate semi-structured or unstructured data.

Was Throwing Away SQL a Mistake ?

Why was SQL initially abandoned by NoSQL projects and vendors? The primary arguments were 1) that an upfront schema makes your business less agile in dealing with semi-structured and unstructured data and 2) SQL was slow and its expressiveness was unnecessary.

First, it’s definitely true that good schema design is difficult, and requires you to understand your data upfront. When data types are changing rapidly, not being locked into a schema seems imminently reasonable and attractive.

The problem is, that all analysis requires that you understand the structure of your data. Structure, in the form of a defined schema, must be applied to do anything useful. Applying structure only when the data is read, a “schema on read” approach, does reduce up-front data engineering work, but it pushes the problem to the actual data users. Typically, this is not a good tradeoff. There are typically many more data users than data engineers, and pushing the responsibility to data users can make a task that would be done once up-front, to a task that needs to be done repeatedly downstream. Additionally, data users often don’t have much context outside of their immediate responsibilities, and might misidentify or misuse data. There’s also the risk that different data users might not interpret data in the same way, creating problems in understanding and reconciling redundant and inconsistent usages.

NoSQL leads to a bunch of spaghetti code to do what SQL does

Second, it turned out that SQL expressiveness was useful. There are a multitude of reasons for this that we’ll touch on in the next section, but the clearest proof point is that many NoSQL projects and vendors implemented “SQL-like” query languages. Cassandra’s CQL is one example. In some ways, these “SQL-like” query languages are helpful. They effectively map NoSQL specific syntax to more widely understood SQL syntax, reduce the “new language” burden on developers, and potentially open some 3rd party integration.

In other ways having a query language that looks like SQL, but is actually not, is counterproductive. It risks lulling developers into a false sense of confidence. They may overestimate the actual level of SQL support present in these languages, and discover vital gaps only after a project is well underway. Potentially worse, “SQL like” functionality that technically works on a proof-of-concept may not work on the full production system because of differences in functional properties that percolate up from the underlying NoSQL database. Sprinkling some “SQL-like” syntactic sugar on top of a NoSQL database won’t make something like table joins “work” in a production environment if the underlying NoSQL database hasn’t been designed or modified with that purpose in mind.

A lack of SQL in NoSQL systems is in and of itself no inherent advantage. All things being equal, there is no virtue in abandoning a well understood, extremely widely deployed data query language for a NoSQL specific language. On the other hand, abandoning SQL completely or using a “SQL like” language has some clear disadvantages, especially in the context of application modernization.

Abandoning SQL is Risky

Over the last 40+ years, companies have made massive investments in mission-critical applications that embody their core competencies. Data is core to virtually all applications, and legacy databases were the primary choice for data storage and retrieval.

A pressing need to modernize these applications often appears when the data that the application stores and operates over has grown beyond what the underlying legacy database can handle, to make the application smarter by leveraging new sources of data that have been made available since the application’s original deployment, and to incorporate modern techniques for utilizing data like artificial intelligence and machine learning.

For example, a demand planning application for a retailer might begin incorporating data about the current weather or local events that might influence local traffic to more efficiently allocate inventory. Or consider an insurance claims system that incorporates vehicle sensor data and camera imagery to more intelligently process claims.

NoSQL databases are often considered for these application modernization projects. In some ways, this makes sense. They do provide the ability to scale-out to support new data sources. However, when replacing a legacy SQL database with NoSQL, you could be significantly lengthening the project and increasing risk.

Abandoning SQL in mission-critical applications impedes application modernization

First, a lack of full SQL support necessitates a large amount of application rewrite. These applications speak SQL. Even when there is signifcant functionality present in a NoSQL database, using the functionality requires the application to “speak” a custom NoSQL or non-compliant “SQL-like” dialect. Additionally, finding developers with the required expertise is significantly more difficult than the huge numbers of developers already fluent in SQL. Tools to assist in converting existing SQL to NoSQL dialects are incomplete or non-existent.

Next, the underlying data model often needs to be completely rethought. As an example, the lack of performant table joins (or in some cases, the ability to join data at all) can cause an explosion of data denormalization that causes data duplication that wastes hardware resources and necessitates extensive new code to ensure that the duplicated data remains consistent across all of its definitions.

Additionally, it is often difficult to achieve similar performance to legacy databases. NoSQL systems typically excel at short-running operational queries that only access a single row or a handful of rows. Performance on analytical queries that scan large amounts of historical data is often poor, and not sufficient to meet the application’s needs. Often, NoSQL databases are paired with another system more appropriate for long-running analytical queries through an implementation of the Lambda architecture. These architectures are complex, difficult to implement, introduce many potential avenues for data inconsistency, and imply a large ongoing maintenance cost.

Also, many database and SQL features that the application relied upon in the legacy database are often not available and need to be built from scratch by your developers or eliminated entirely. Replicating some features present in legacy SQL databases in NoSQL systems, like a consistency model that supports full ACID support, requires such specialized developer skills and such a huge amount of effort that any project that relies on reimplementation is going to spend a huge chunk of time effectively building a one-off database instead of actually modernizing the application.

A much less risky approach for application modernization is to replace your legacy database with a database that scales-out, allows data type flexibility, and provides in-database machine learning and artificial intelligence without abandoning SQL. Does such a database exist?

Can you get the advantages of NoSQL with SQL?

There are a number of scale-out, distributed SQL systems in the marketplace today like Snowflake, Cockroach, and Google Cloud Spanner as well as the company I co-founded Splice Machine. With all of these, I believe it is possible to get the benefits of NoSQL databases without accepting the problems discussed above. To go deeper in one of the solutions, Splice Machine uses a NoSQL system (HBase) as its underlying storage engine but it layers on extensive capabilities specifically designed to modernize OLTP SQL applications It provides:

An exhaustive set of supported ANSI SQL syntax

Row storage with a primary key for quick lookups

Efficient column storage in tables that can be stored as cheap object storage like S3 or ADLS

Full ACID compliance and multi-row transaction support

Support for constraints, referential integrity, primary keys, triggers, and secondary indexes

In-memory, distributed analytical processing

A cost based optimizer that devises efficient query plans and can choose to execute these plans directly against HBase (typically for small, “operational” queries) or via Spark executors (typically for larger, “analytical” queries)

Support for custom stored procedures and functions

The ability to deploy on-premises or on AWS and Microsoft Azure with Kubernetes

The ability to store semi-structured or unstructured data directly in database tables or through integrated Spark functionality

Integrated data science capabilities with in-database analytics, integrated Jupyter notebooks, model workflow management, and cloud deployment capabilities

An example of a scale-out SQL engine that handles OLTP and OLAP

A scale-out SQL RDBMS like this can facilitate legacy modernization.

First, like NoSQL databases, iit can horizontally scale to extremely large data sizes and query volumes, far beyond what’s possible on legacy databases. It also allows extensive flexibility for storing and manipulating data.

Second, it makes migration from the supporting legacy database simple. SQL written against the legacy database needs very little or no rework because of Splice Machine’s exhaustive SQL support. The underlying legacy data model often needs no modification and certainly does not require the wholesale rethinking that a migration to a NoSQL database requires. Splice Machine can handle all of the legacy database workload, whether operational or analytical in nature, without the need to stitch together multiple specialized systems.

Finally, it provides capabilities that allow a migrated application to be “modern”. It can run on-premises or on multiple cloud platforms and abstract away the underlying infrastructure through a modern “containerized” approach. It allows data science to be done directly in-database, so that new data sources and techniques that make applications smarter and more valuable can be applied without the hassle of extracting, transforming, and loading data between multiple systems.

In Summary

Custom applications built on legacy databases often deliver competitive advantages. That is why companies keep them custom versus licensing packaged applications. Modernizing these applications is often desirable, either because the underlying database is under stress and can’t scale, or because there’s a desire to leverage modern deployment or AI techniques. Replacing the legacy database that underlies these applications with a NoSQL database that requires wholesale rework of the existing data model and business logic is risky and significantly increases the scope and schedule of a modernization project. Scale-out SQL databases like Splice Machine provide scalability and flexibility while providing a smooth migration path from a legacy database system.

To learn more about how to make your custom-built applications agile and infused with intelligence download, our white paper on modernizing legacy applications."
Cryptography Crash Course for the Intimidated,"Cryptography Crash Course for the Intimidated

Rotor Cipher Machine via Pixabay

In my last post, I talked about the importance of being mindful when you handle data. Who has access? How can you protect the information? How are some laws changing? What is and isn’t covered in academia vs. industry.

For this post, I decided to give myself a quick crash course in cryptography. The concept of cryptography can sound really intimidating, and the security of data and the secure transfer of data is certainly something to take seriously. But we create data every day. I think it’s important for anyone these days to have a basic grasp on what cryptography is.

Objectives

Try to demystify cryptography to a certain extent.

Explain some of the basic concepts that underlie cryptography.

Explain a couple of examples in which cryptography is used in our everyday lives.

Share resources!

Basic Concepts

To start understanding cryptography, I went for the basics. Here are some basic concepts that come up over and over again when you look up cryptography online. I have linked a YouTube video below, if you are more of a visual person.

First, what is cryptography? Crytography is a field of study focused on communication and data storage that is protected from an unwanted third-party. For example, before cellphones, two kids might try to pass a physical note to each other in the middle of class. They don’t want anyone else (a third-party), such as a teacher to read the note. So they might want to figure out a way to write in a made up language or to scramble the message (encrypt the message) so that even if the teacher can “read” the message, they can’t understand it.

There are different ways to encrypt (scramble) and decrypt (unscramble) information. They generally fall into two buckets:"
Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars 🚗,"Street Lanes Finder - Detecting Street Lanes for Self-Driving Cars 🚗

Lanes Detection with Computer Vision Greg Surma · Follow 5 min read · Jul 25, 2019 -- 1 Share

In today’s article, we are going to use basic Computer Vision techniques to approach the street lanes detection problem which is crucial for self-driving cars. By the end of this article, you will be able to perform real-time lane detection with Python and OpenCV.

Real-Time Lane Detection

Implementation

You can find the full codebase for this project on GitHub and I encourage you to check it and follow along.

Let’s start with defining our problem.

Given an image of the road, we would like to detect street lanes on it.

In order to do it, let’s provide an image path and load it with OpenCV, then let’s invoke find_street_lanes pipeline with it.

test_image = cv2.imread(INPUT_FOLDER + TEST_IMAGE)

street_lanes = find_street_lanes(test_image)

And this is how our find_street_lanes pipeline looks like"
Causal Inference Using Synthetic Control: The Ultimate Guide,"Technical Dilemma

In other posts, I’ve explained what causation is and how to do causal inference using quasi-experimental designs (DID, ITS, RDD). Almost for all research methods, they have to meet two preconditions in order to generate meaningful insights:

1. the treated group looks like the control group (similarity for comparability); 2. a sufficiently large number of observations within each group (a large n).

These two preconditions lay the foundation for causal inference. However, is it possible to do causal inference if we only have one treated case and a few control cases? Even worse, what shall we do if there are no control cases with similar covariates as the treated case?

Under these situations, regression-based solutions (e.g., matching on key variables, or propensity score matching) perform poorly. Besides, other quasi-experimental designs such as the DID method require similar covariates between the treated and control groups and would generate a huge bias under these two scenarios.

In this post, I proudly present a statistical solution, the Synthetic Control Method (SCM), that is proposed by a group of political scientists like me. Honestly, the SCM has tremendous amount of causal potential but remains under-appreciated for now. It begins to capture some tractions in the industry as consumer-facing companies want to understand simulated consumer behaviors.

Basics

The SCM uses a weighted average of multiple cases from the “donor” pool to create an artificial control case.

Here is a simplified process:"
Differentiable Inter Agent Learning to Solve the Prisoners-Switch Riddle,"Backpropagating gradients across agents to learn messaging protocols

Reinforcement Learning is a popular research area. This is mainly because it aims to model systems that otherwise seem intractable. From the famous Atari paper by Deepmind, we have come far. The following post is from what I have learned from the following paper:

https://arxiv.org/pdf/1605.06676.pdf

An interesting avenue of study in reinforcement learning is that of communicating agents: a setup where agents can send messages to each other in order to cooperate. A good case where communication will be essential is that of an environment that is only partially observable to each agent, whereas more information is required for the agents to complete the task cooperatively. These agents will have to pass meaningful messages to each other, where “meaningful” stands for having some correlation with useful information associated with their own environments or the actions they are about to take.

Such an environment where messages are passed will have some important requirements. Consider an agent A sending a message m to another agent B. The protocol that they develop to communicate, as they learn, will have to have certain properties.

Given the architecture of the system, this messaging protocol should be optimal. A system where agents are spitting out random vectors to each other is obviously useless. These vectors that are exchanged (m) have to be rich in information The message that is sent by A has to be understood by B

Considering that we have multiple agents, we can go with the following options (probably even a mid-way-out):

Parameter Sharing: Get a common set of parameters for all agents. That is, use a single network for all the agents, but use these parameters separately during execution, with each agent plugging in its own observation and received messages into the network. This method is generally more stable and easy to learn since if useful features are extracted, these are used for all agents. They do not have to be learned separately. Without Parameter Sharing: Use different weights for each network, and let them learn features independently. This is less stable than the previous option.

One early algorithm that was devised was RIAL. The idea behind RIAL is simple: just provide messages as action choices in the action selector of each agent. This message is passed to the other agents. That was it. Each agent is trained separately. This framework follows decentralized execution as well as independent update of parameters, that is, gradients are passed through each agent independently and updated. They are not passed through multiple agents end-to-end.

This practice of not passing the gradients through multiple agents is limiting in several aspects. It is difficult to land on an optimum policy since messages are similar to action selection.

Then came DIAL (Differential Inter Agent Learning). Let us introduce an output for a dedicated message in each agent. Add an input corresponding to the message sent by the other agent. With this framework, while training, connect all nets together with each step and let the gradients flow through all the agents, end to end while minimizing the loss function (which is the usual Q learning loss function that we get from the Bellman Equation). What we essentially end up with is an optimal protocol to exchange messages. While training, each agent learns to construct meaningful messages and also to understand the messages given by the other agent. This setup follows centralized learning and decentralized execution. Each agent individually acts in the environment while executing, but learn together during the training phase.

One important thing to note here is that each message can be discrete or continuous. There might be restrictions imposed by the problem definition to only allow discrete messages to be passed, such as one-bit messages. How can we discretize messages? Let us introduce a unit called a DRU (Discretize/Regularize Unit), defined as:

Instead of directly plugging in the message from one agent to another, we will pass the message through this unit. This unit can discretize the message because of the noise. When we add noise during training, we force the messages (m) towards the extreme right or left of the decision boundary, so that adding noise does not affect the side the message is on (that is, positive or negative).

Let us try implementing this. Consider the Hundred-Prisoners riddle. I quote the riddle statement directly from the paper:

One hundred prisoners have been newly ushered into prison. The warden tells them that starting tomorrow, each of them will be placed in an isolated cell, unable to communicate amongst each other. Each day, the warden will choose one of the prisoners uniformly at random with replacement, and place him in a central interrogation room containing only a light bulb with a toggle switch. The prisoner will be able to observe the current state of the light bulb. If he wishes, he can toggle the light bulb. He also has the option of announcing that he believes all prisoners have visited the interrogation room at some point in time. If this announcement is true, then all prisoners are set free, but if it is false, all prisoners are executed. The warden leaves and the prisoners huddle together to discuss their fate. Can they agree on a protocol that will guarantee their freedom

The solution has to obviously encode information in patterns of the switch across time, and it will most probably be one which might take infinitely many steps to execute.

Let us now solve the riddle. We will be using TensorFlow, specifically 2.0. We will have to solve this for a small number of prisoners, such as 3 or 4 since the policy space increases exponentially with the number of agents. This problem can be modeled as prisoners being chosen at random from their cell each day, and allowed to pass a 1-bit message to the next one. This one-bit prisoner is the state of the light bulb. Hence, a prisoner in the room receives 1 bit from the previous prisoner (random bit if it is the first prisoner) and is allowed to send a bit to the next prisoner. Keeping a count of days and using that one bit to send to the next prisoner, they will have to solve the problem.

For this article, refer to my implementation.

The Environment

Refer to the class Jail, which implements the environment we will be using. Random prisoners are chosen for each step. Actions can be taken by calling the step() method. When a “Declare” action is taken, a reward of 0.5 or -0.5 is given to each prisoner, depending on whether the declaration is correct or not.

Defining the DRU

The following code snippet shows the definition of the DRU which will be used only during the training phase (I have directly discretized for the testing):

The Agent Network

Playing an Episode

Now the core part. First some important variables and initializations:

Using these, let us run a loop until we are not done:

This first part runs the messages through the agents. If an agent is in the room, the current bulb state is given to the agent, otherwise, a DUMMY_MESSAGE, indicating that the agent is in its room, and a day passed. Without knowing that a day has passed, agents will not be able to keep track of time, which gets encoded in the GRU’s hidden state when we push this DUMMY_MESSAGE. We log all useful variables into lists so that we can later compute the loss simply iterating through them. The target is computed using the same, familiar, famous Bellman Equation, giving us one term in the loss function:

Note that the protocols used by the main Q networks and the target Q networks will be slightly different, hence we have to pass a different set of messages through the target networks as well, in order to use this update rule. The Q values given by the target networks will make sense only if we let them follow the protocol they have devised until then.

The Main Training Loop

Run episodes as long as you want. Under a tf.GradientTape() object, call the play_episode method. Then we simply compute gradients and update weights, as is the usual practice.

Note that I have had to remove some details from these snippets to show the bare idea. You can get all those details in the implementation. One important detail is that in the above loop, we have to run test loops where we use the discrete message DRU and no exploration (no random action selection). They reflect how well the system is doing. We also have to update the target network, for which a recommended interval is that of 100 episodes, as followed in the paper."
Data analytics with MODIS data,"Have a look how you can choose from a variety of collections in the left pane. I simply chose aerosol and it landed me with two choices- the L2 Swath 3km and the L2 Swath 10 km data. It simply represents the amount of area covered in one single snapshot of the earth (it such projects, it always helps to think about the data as images of the earth, because that’s what they really are!). So depending on whether you wish to work with a 3 km x 3 km area or a 10 km x 10 km area, you may choose the required collection.

Rest of the options are pretty straightforward. The menu asks you to put in the time duration, the location, and displays the files (in HDF) format that you can download. The filenames carry essential information about the file:

What your file name says!

Since I wanted to analyse AOD over North India for a year, I downloaded HDF files over the required region for every single day of the year. The files can be found in my Github repo.

Loading data into Python

After a lot of failures and frustrations, I came across gdal (geospatial data abstraction library) that proved to be pretty useful in handling the above data. However, gdal proved harder for me to install using Python’s default package manager pip . So I went ahead and straightaway installed it into my Anaconda environment; just do conda install gdal and I was good to go.

Opening a single file

The following code opens up the file pointed to by the path.

Each file downloaded has loads of sub-datasets attached to it. Just for having a look at what we got, GetSubDatasets() can list different datasets separately, returning a path to open the dataset and a desc of the dataset. Printing the descriptions tell you a lot about the kind of data you are dealing with:

Just a small number in this screen. There are several others to deal with as well

Basically the names are mostly self-explanatory. Attached to the description are the size of the matrix (to the left) the dataset is formatted into and the datatype used to store the data (to the right). So let’s write some general functions to load any particular sub-dataset that we wish to.

The above function takes the FILEPATH and loads the SUBDATASET_NAME from that very file. For instance, to open the first sub-dataset in the file, simply point FILEPATH to the directory where the HDF file has been stored, and pass Scan_Start_Time mod04(64-bit floating-point) to SUBDATSET_NAME . This code will open the sub-dataset, read it as an array ( subdataset.ReadAsArray() ), and return a pandas dataframe that can be worked upon later.

Getting data at the location you wish

If you need the averaged product over the entire region, simply average the entire matrix and that shall be the AOD over that region for that day (though you need to handle missing data. We’ll come to that in a minute). But if you, need to extract only a small portion of that 10 km x 10 km image, you need to find the pixel on the image that is closest to the location you are looking for.

So how do you do this? Although I believe there would be several methods to go about this, I did that following process:

Open the Latitude sub-dataset and find the row number of the cell whose value is closest the latitude I am looking for. This idea comes from the understanding that every cell in the latitude sub-dataset represents the latitude of that pixel in the image. Now using the row number from step 1, I’ll find the column number of pixel in the Longitude sub-dataset that is closest to the place I am looking for. So now I have the row number and the column number of the pixel that is closest to the location (geographical coordinates) I am aiming at. I can now open other sub-datasets, say Deep_Blue_Aerosol_Optical_Depth_550_Land mod04 (16-bit integer) and extract the value at that pixel. This value gives me the AOD measured over that place using Deep Blue retrieval algorithm for that day. We might want to have an average of say 9 to 25 pixels, centred about the pixel we just found, just in case, the pixel has NULL value for that day (data over that pixel for that day couldn’t be recorded).

Finds the latitude pixel

The above code finds the row number of the pixel whose value (essentially the latitude) is closest to the CITY_LATIDUDE we are looking for.

Finds the longitude pixel

A similar story with the longitudes. Do notice that instead of searching in the entire matrix, I search in the row whose row number I just found in the previous function (denoted by LATITUDE_ROW_NUMBER ).

The above one is a pretty straightforward function that, given a particular sub-dataset, the row number found by the latitude function, and the column number found by the longitude function, returns the value of the product at that pixel in that sub-dataset.

The last thing we need is to somehow handle which sub-dataset to open, find the pixel coordinates, and get a required average (basically put everything above together):

Quite evidently, the above function finds lat_row and the corresponding lon_column, decides which sub-datasets to consider, and averages over a grid of 9 cells (with our pixel at the centre of it). Something like below:

Also note in the datasets, a value of -9999 represents failure to record data for that location. So I simply put a 0 there.

Handling loads of data…

The above driver code simply iterates over all HDF files in a month, extracts the Scattering Angle, AOD (Deep Blue), Cloud Fraction, Combined (Deep Blue and Dark Target retrieval algorithms combined), and the Angstrom exponent, adds everything to a list, converts all into a series by pd.Series() , and adds a new column to the pd.DataFrame() . Finally, we can export that month’s .csv files for further analysis.

Analysis

The following two helper routines will help us in plotting each month’s data as well as average the parameter so that we may look at its average variation over a period of time. A couple of things that I included in the plot routine was to superpose the line plots one after the other, retrieved whatever was returned by the function, got the figure out of it (using get_figure() ), and saved the figure to file, for further use.

Everything’s pretty obvious out here. I used a dictionary to track which month is being processed in what order (for instance, according to the filename, April was the first month to be processed). It simply required creating a map as given, and when appending the averaged Deep Blue AOD data, append the integer corresponding to the month in the calendar. This shall help in sorting the list, and returning a variation in time, from January to December.

I’ll share a few of the plots and some insights I could make out of them. Find the others here.

General trend first

The above figure plots the annual variation of average Aerosol Product. y-axis reports values retrieved from the Deep Blue algorithm. To get the actual value, the retrieved values need to be scaled by 0.001 (the scaling factor used by MODIS).

The above figure is the averaged monthly AOD variation over the entire year. It can be seen that the AOD rises in the summers, falls during the monsoons, and again rises in the winters

January 2018 trends (Note MODIS data uses a scale factor of 0.001. So the actual values are obtained when the data values are multiplied with the scale factor)

Variation of Deep Blue AOD and Cloud Fraction over January 2018

The above figure plots values of AOD obtained from Deep Blue retrieval algorithm and the values of cloud fraction data obtained. It is clear that in general, whenever the cloud fraction is high, AOD tends to be low. In reality, it isn’t low, just that the satellite-imaging sensor is not able to record data for that day. And do observe the spike around Jan 2. And if you think a little, you will realise that spike is all of the New Year celebrations!

Variation of Combined AOD and Cloud Fraction over January 2018

It can be reasonably seen here that the Combined AOD using Dark Target and Deep Blue gives a somewhat incorrect estimation of the AOD. Most of the days have been reported as having ~ 0 AOD (or clear skies). It is a known fact though, that this is not a case, especially around New Year.

Variation of Deep Blue AOD and Scattering Angle over Januray 2018

Both Deep Blue AOD and Scattering Angle are scaled up by a factor of 0.001. It can be seen here how a spike in AOD causes an abrupt spike in the scattering angle, suggesting more blockage of sunlight.

Variation of Deep Blue AOD and Angstrom Exponent over January 2018

Angstrom Exponent is a measure of how the AOD changes relative to the various wavelength of light (known as ‘spectral dependence’). This is related to the aerosol particle size. Higher values suggest coarser particles, while smaller values suggest finer ones. Here, typically higher values ~ 0.5 (500 * 0.001[scale factor]) suggest typically finer particles. A PM2.5 study can complement this finding.

Strange results were found in August data. MODIS failed to retrieve AOD data due to consistently high cloud fraction during this month.

Variation of Deep Blue AOD and Cloud Fraction over August 2018

As expected, other variables also suffered from similar consequence.

Variation of Combined AOD and Cloud Fraction over August 2018

Variation of Deep Blue AOD and Angstrom Exponent over August 2018

Variation of Deep Blue AOD and Scattering Angle over August 2018

Summary

Finally, let us draw some inferences out here:

Fall in AOD levels during the monsoons is due to heavy insolation , facilitating columnar mixing of the aerosols.

in AOD levels during the is due to , facilitating columnar mixing of the aerosols. Rise in AOD levels during the winters is due to less insolation and cold temperatures ; opposing columnar mixing of aerosols due to low atmospheric boundary layers and trapping the aerosols near the land.

in AOD levels during the is due to and ; opposing columnar mixing of aerosols due to low atmospheric boundary layers and trapping the aerosols near the land. Stronger convention in summer (mostly from western disturbances and trade winds) accompanied by a deeper atmospheric boundary layer causes aerosol collection over Kanpur in summer.

in (mostly from western disturbances and trade winds) accompanied by a causes over Kanpur in summer. Cloud fraction is an important factor in AOD measurements. Higher cloud fractions usually result in no data for the day . In this report, such days are replaced with the mean of the month under consideration .

. In this report, such days are . NASA’s Deep Blue retrieval algorithm gives more accurate results than Dark Target retrieval over land imagery.

over land imagery. People in Kanpur did burst a lot of crackers on New Year and on Diwali eve in 2018! Kanpus on Jan 1, 2018 and on Diwali was too hazy to even record an image to get data, and Jan 2, 2018 saw ~ 1.2 AOD.

Due to consistently high cloud fraction, August 2018 saw a straight failure to record AOD data.

There is so much more you can do with MODIS data. You can find forest covers data, study oceans, and much more. I hope this article helps you to deal with any MODIS product, and you can complete your study without any hassle.

Have a good day!"
Football: Why Winners Win and Losers Lose,"Football: Why Winners Win and Losers Lose

Intro

In this notebook we will explore modern metrics in football (xG, xGA and xPTS) and its’ influence in sport analytics.

Expected Goals (xG) — measures the quality of a shot based on several variables such as assist type, shot angle and distance from goal, whether it was a headed shot and whether it was defined as a big chance.

— measures the quality of a shot based on several variables such as assist type, shot angle and distance from goal, whether it was a headed shot and whether it was defined as a big chance. Expected Assits (xGA) — measures the likelihood that a given pass will become a goal assist. It considers several factors including the type of pass, pass end-point and length of the pass.

— measures the likelihood that a given pass will become a goal assist. It considers several factors including the type of pass, pass end-point and length of the pass. Expected Points (xPTS) — measures the likelihood of a certain game to bring points to the team.

These metrics let us look much deeper into football statistics and understand the performance of players and teams in general and realize the role of luck and skill in it. Disclaimer: they are both important.

The process of data collection for this notebook is described in this Kaggle kernel: Web Scraping Football Statistics

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

import collections

import warnings



from IPython.core.display import display, HTML



# import plotly

import plotly

import plotly.figure_factory as ff

import plotly.graph_objs as go

import plotly.offline as py

from plotly.offline import iplot, init_notebook_mode

import plotly.tools as tls



# configure things

warnings.filterwarnings('ignore')



pd.options.display.float_format = '{:,.2f}'.format

pd.options.display.max_columns = 999



py.init_notebook_mode(connected=True)



%load_ext autoreload

%autoreload 2



%matplotlib inline

sns.set()



# !pip install plotly --upgrade

Import Data and Visual EDA

df = pd.read_csv('../input/understat.com.csv')

df = df.rename(index=int, columns={'Unnamed: 0': 'league', 'Unnamed: 1': 'year'})

df.head()

In the next visualization, we will check how many teams from each league were in top 4 during last 5 years. It can give us some info about stability of top teams from different countries.

f = plt.figure(figsize=(25,12))

ax = f.add_subplot(2,3,1)

plt.xticks(rotation=45)

sns.barplot(x='team', y='pts', hue='year', data=df[(df['league'] == 'Bundesliga') & (df['position'] <= 4)], ax=ax)

ax = f.add_subplot(2,3,2)

plt.xticks(rotation=45)

sns.barplot(x='team', y='pts', hue='year', data=df[(df['league'] == 'EPL') & (df['position'] <= 4)], ax=ax)

ax = f.add_subplot(2,3,3)

plt.xticks(rotation=45)

sns.barplot(x='team', y='pts', hue='year', data=df[(df['league'] == 'La_liga') & (df['position'] <= 4)], ax=ax)

ax = f.add_subplot(2,3,4)

plt.xticks(rotation=45)

sns.barplot(x='team', y='pts', hue='year', data=df[(df['league'] == 'Serie_A') & (df['position'] <= 4)], ax=ax)

ax = f.add_subplot(2,3,5)

plt.xticks(rotation=45)

sns.barplot(x='team', y='pts', hue='year', data=df[(df['league'] == 'Ligue_1') & (df['position'] <= 4)], ax=ax)

ax = f.add_subplot(2,3,6)

plt.xticks(rotation=45)

sns.barplot(x='team', y='pts', hue='year', data=df[(df['league'] == 'RFPL') & (df['position'] <= 4)], ax=ax)

As we can see from these bar charts, there are teams that in last 5 years were in top 4 only once, which means it is not something common, which means if we dig deeper, we can find that there is a factor of luck that might have played in favour to these teams. It’s just a theory, so let’s look closer to those outliers.

The teams that were in top 4 only once during the last 5 seasons are:

Wolfsburg (2014) and Schalke 04 (2017) from Bundesliga

Leicester (2015) from EPL

Villareal (2015) and Sevilla (2016) from La Liga

Lazio (2014) and Fiorentina (2014) from Serie A

Lille (2018) and Saint-Etienne (2018) from Ligue 1

FC Rostov (2015) and Dinamo Moscow (2014) from RFPL

Let’s save these teams.

# Removing unnecessary for our analysis columns

df_xg = df[['league', 'year', 'position', 'team', 'scored', 'xG', 'xG_diff', 'missed', 'xGA', 'xGA_diff', 'pts', 'xpts', 'xpts_diff']]



outlier_teams = ['Wolfsburg', 'Schalke 04', 'Leicester', 'Villareal', 'Sevilla', 'Lazio', 'Fiorentina', 'Lille', 'Saint-Etienne', 'FC Rostov', 'Dinamo Moscow'] # Checking if getting the first place requires fenomenal execution

first_place = df_xg[df_xg['position'] == 1]



# Get list of leagues

leagues = df['league'].drop_duplicates()

leagues = leagues.tolist()



# Get list of years

years = df['year'].drop_duplicates()

years = years.tolist()

Understanding How Winners Win

In this section, we will try to find some patterns that can help us understand what are some of the ingredients of the victory soup :D. Starting with Bundesliga.

Bundesliga

first_place[first_place['league'] == 'Bundesliga']

pts = go.Bar(x = years, y = first_place['pts'][first_place['league'] == 'Bundesliga'], name = 'PTS')

xpts = go.Bar(x = years, y = first_place['xpts'][first_place['league'] == 'Bundesliga'], name = 'Expected PTS')



data = [pts, xpts]



layout = go.Layout(

barmode='group',

title=""Comparing Actual and Expected Points for Winner Team in Bundesliga"",

xaxis={'title': 'Year'},

yaxis={'title': ""Points"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

By looking at the table and barchart we see that Bayern every year got more points than they should have, they scored more than expected and missed less than expected (except for 2018, which didn’t break their plan of winning the season, but it gives some hints that Bayern played worse this year, although the competitors didn’t take advantage of it).

# and from this table we see that Bayern dominates here totally, even when they do not play well

df_xg[(df_xg['position'] <= 2) & (df_xg['league'] == 'Bundesliga')].sort_values(by=['year','xpts'], ascending=False)

La Liga

first_place[first_place['league'] == 'La_liga']

pts = go.Bar(x = years, y = first_place['pts'][first_place['league'] == 'La_liga'], name = 'PTS')

xpts = go.Bar(x = years, y = first_place['xpts'][first_place['league'] == 'La_liga'], name = 'Expected PTS')



data = [pts, xpts]



layout = go.Layout(

barmode='group',

title=""Comparing Actual and Expected Points for Winner Team in La Liga"",

xaxis={'title': 'Year'},

yaxis={'title': ""Points"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

As we can see from the chart above that in 2014 and 2015 Barcelona was creating enough moments to win the title and do not rely on personal skills or luck, from these numbers we can actually say that THE Team was playing there.

In 2016 there were lots of competition between Madrid and Barcelona and in the end, Madrid got luckier / had more guts in one particular game (or Barcelona got unlucky / didn’t have balls) and it was the cost of the title. I am sure that if we dig deeper that season we can find that particular match.

In 2017 and 2018 Barcelona’s success was mostly tributed to actions of Lionel Messi who was scoring or making assists in situations where normal players wouldn’t do that. What led to such a jump in xPTS difference. What makes me think (having the context that Real Madrid is very active on transfer market this season) can end up bad. Just subjective opinion based on numbers and watching Barcelona games. Really hope I am wrong.

# comparing with runner-up

df_xg[(df_xg['position'] <= 2) & (df_xg['league'] == 'La_liga')].sort_values(by=['year','xpts'], ascending=False)

EPL

first_place[first_place['league'] == 'EPL']

pts = go.Bar(x = years, y = first_place['pts'][first_place['league'] == 'EPL'], name = 'PTS')

xpts = go.Bar(x = years, y = first_place['xpts'][first_place['league'] == 'EPL'], name = 'Expected PTS')



data = [pts, xpts]



layout = go.Layout(

barmode='group',

title=""Comparing Actual and Expected Points for Winner Team in EPL"",

xaxis={'title': 'Year'},

yaxis={'title': ""Points"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

In EPL we see the clear trend that tells you: “To win you have to be better than statistics”. Interesting case here is Leicester story of victory in 2015: they got 12 points more than they should’ve and at the same time Arsenal got 6 points less than expected! This is why we love football, because such inexplicable things happen. I am not telling is total luck, but it played its role here.

Another interesting thing is Manchester City of 2018 — they are super stable! They scored just one goal more than expected, missed 2 less and got 7 additional points, while Liverpool fought really well, had little bit more luck on their side, but couldn’t win despite being 13 points ahead of their expected.

Pep is finishing building the machine of destruction. Man City creates and converts their moments based on skill and do not rely on luck — it makes them very dangerous in the next season.

# comparing with runner-ups df_xg[(df_xg['position'] <= 2) & (df_xg['league'] == 'EPL')].sort_values(by=['year','xpts'], ascending=False)

Ligue 1

first_place[first_place['league'] == 'Ligue_1']

pts = go.Bar(x = years, y = first_place['pts'][first_place['league'] == 'Ligue_1'], name = 'PTS')

xpts = go.Bar(x = years, y = first_place['xpts'][first_place['league'] == 'Ligue_1'], name = 'Expected PTS')



data = [pts, xpts]



layout = go.Layout(

barmode='group',

title=""Comparing Actual and Expected Points for Winner Team in Ligue 1"",

xaxis={'title': 'Year'},

yaxis={'title': ""Points"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

In French Ligue 1 we continue to see the trend “to win you have to execute 110% because 100% is not enough”. Here Paris Saint Germain dominates totally. Only in 2016 we get an outlier in the face of Monaco that scored 30 goals more than expected!!! and got almost 17 points more than expected! Luck? Quite a good piece of it. PSG was good that year, but Monaco was extraordinary. Again, we cannot claim it’s pure luck or pure skill, but a perfect combination of both in the right place and time.

# comparing with runner-ups df_xg[(df_xg['position'] <= 2) & (df_xg['league'] == 'Ligue_1')].sort_values(by=['year','xpts'], ascending=False)

Serie A

first_place[first_place['league'] == 'Serie_A']

pts = go.Bar(x = years, y = first_place['pts'][first_place['league'] == 'Serie_A'], name = 'PTS')

xpts = go.Bar(x = years, y = first_place['xpts'][first_place['league'] == 'Serie_A'], name = 'Expecetd PTS')



data = [pts, xpts]



layout = go.Layout(

barmode='group',

title=""Comparing Actual and Expected Points for Winner Team in Serie A"",

xaxis={'title': 'Year'},

yaxis={'title': ""Points"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

In Italian Serie A Juventus is dominating 8 years in a row although cannot show any major success in Champions League. I think by checking this chart and numbers we can understand that Juve doesn’t have strong enough competition inside the country and gets lots of “lucky” points, which again derives from multiple factors and we can see that Napoli outperformed Juventus by xPTS twice, but it is a real life and in, for example, 2017, Juve was crazy and scored additional 26 goals (or created goals from nowhere), while Napoli missed 3 more than expected (due to error of goalkeeper or maybe excellence of some team sin 1 or 2 particular matches). As with the situation in La Liga when Real Madrid became a champion I am sure we can find 1 or 2 games that were key that year.

Details matter in football. You see, one error here, one woodwork there and you’ve lost the title.

# comparing to runner-ups df_xg[(df_xg['position'] <= 2) & (df_xg['league'] == 'Serie_A')].sort_values(by=['year','xpts'], ascending=False)

RFPL

first_place[first_place['league'] == 'RFPL']

pts = go.Bar(x = years, y = first_place['pts'][first_place['league'] == 'RFPL'], name = 'PTS')

xpts = go.Bar(x = years, y = first_place['xpts'][first_place['league'] == 'RFPL'], name = 'Expected PTS')



data = [pts, xpts]



layout = go.Layout(

barmode='group',

title=""Comparing Actual and Expected Points for Winner Team in RFPL"",

xaxis={'title': 'Year'},

yaxis={'title': ""Points"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

I do not follow Russian Premier League, so just by coldly looking at data, we see the same pattern as scoring more than you deserve and also interesting situation with CSKA Moscow from 2015 to 2017. During these years these guys were good but converted their advantages only once, the others two — if you do not convert, you get punished or your main competitor just converts better.

There is no justice in football :D. Although, I believe with VAR the numbers will become more stable in the next seasons. Because one of the reasons of those additional goals and points are errors of arbiters.

# comparing to runner-ups df_xg[(df_xg['position'] <= 2) & (df_xg['league'] == 'RFPL')].sort_values(by=['year','xpts'], ascending=False)

Statistical Overview

As there are 6 leagues with different teams and stats, I decided to focus on one, in the beginning, to test different approaches and then replicate the final analysis model on other 5. And as I watch mostly La Liga I will start with this competition as I know the most about it.

# Creating separate DataFrames per each league

laliga = df_xg[df_xg['league'] == 'La_liga']

laliga.reset_index(inplace=True)

epl = df_xg[df_xg['league'] == 'EPL']

epl.reset_index(inplace=True)

bundesliga = df_xg[df_xg['league'] == 'Bundesliga']

bundesliga.reset_index(inplace=True)

seriea = df_xg[df_xg['league'] == 'Serie_A']

seriea.reset_index(inplace=True)

ligue1 = df_xg[df_xg['league'] == 'Ligue_1']

ligue1.reset_index(inplace=True)

rfpl = df_xg[df_xg['league'] == 'RFPL']

rfpl.reset_index(inplace=True) laliga.describe()

def print_records_antirecords(df):

print('Presenting some records and antirecords: n')

for col in df.describe().columns:

if col not in ['index', 'year', 'position']:

team_min = df['team'].loc[df[col] == df.describe().loc['min',col]].values[0]

year_min = df['year'].loc[df[col] == df.describe().loc['min',col]].values[0]

team_max = df['team'].loc[df[col] == df.describe().loc['max',col]].values[0]

year_max = df['year'].loc[df[col] == df.describe().loc['max',col]].values[0]

val_min = df.describe().loc['min',col]

val_max = df.describe().loc['max',col]

print('The lowest value of {0} had {1} in {2} and it is equal to {3:.2f}'.format(col.upper(), team_min, year_min, val_min))

print('The highest value of {0} had {1} in {2} and it is equal to {3:.2f}'.format(col.upper(), team_max, year_max, val_max))

print('='*100)



# replace laliga with any league you want

print_records_antirecords(laliga) Presenting some records and antirecords:



The lowest value of SCORED had Cordoba in 2014 and it is equal to 22.00

The highest value of SCORED had Real Madrid in 2014 and it is equal to 118.00

================================================================

The lowest value of XG had Eibar in 2014 and it is equal to 29.56

The highest value of XG had Barcelona in 2015 and it is equal to 113.60

================================================================

The lowest value of XG_DIFF had Barcelona in 2016 and it is equal to -22.45

The highest value of XG_DIFF had Las Palmas in 2017 and it is equal to 13.88

================================================================

The lowest value of MISSED had Atletico Madrid in 2015 and it is equal to 18.00

The highest value of MISSED had Osasuna in 2016 and it is equal to 94.00

================================================================

The lowest value of XGA had Atletico Madrid in 2015 and it is equal to 27.80

The highest value of XGA had Levante in 2018 and it is equal to 78.86

================================================================

The lowest value of XGA_DIFF had Osasuna in 2016 and it is equal to -29.18

The highest value of XGA_DIFF had Valencia in 2015 and it is equal to 13.69

================================================================

The lowest value of PTS had Cordoba in 2014 and it is equal to 20.00

The highest value of PTS had Barcelona in 2014 and it is equal to 94.00

================================================================

The lowest value of XPTS had Granada in 2016 and it is equal to 26.50

The highest value of XPTS had Barcelona in 2015 and it is equal to 94.38

================================================================

The lowest value of XPTS_DIFF had Atletico Madrid in 2017 and it is equal to -17.40

The highest value of XPTS_DIFF had Deportivo La Coruna in 2017 and it is equal to 20.16

Now let’s see those number on a chart.

trace0 = go.Scatter(

x = laliga['position'][laliga['year'] == 2014],

y = laliga['xG_diff'][laliga['year'] == 2014],

name = '2014',

mode = 'lines+markers'

)



trace1 = go.Scatter(

x = laliga['position'][laliga['year'] == 2015],

y = laliga['xG_diff'][laliga['year'] == 2015],

name='2015',

mode = 'lines+markers'

)



trace2 = go.Scatter(

x = laliga['position'][laliga['year'] == 2016],

y = laliga['xG_diff'][laliga['year'] == 2016],

name='2016',

mode = 'lines+markers'

)



trace3 = go.Scatter(

x = laliga['position'][laliga['year'] == 2017],

y = laliga['xG_diff'][laliga['year'] == 2017],

name='2017',

mode = 'lines+markers'

)



trace4 = go.Scatter(

x = laliga['position'][laliga['year'] == 2018],

y = laliga['xG_diff'][laliga['year'] == 2018],

name='2018',

mode = 'lines+markers'

)



data = [trace0, trace1, trace2, trace3, trace4]



layout = go.Layout(

title=""Comparing xG gap between positions"",

xaxis={'title': 'Year'},

yaxis={'title': ""xG difference"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

trace0 = go.Scatter(

x = laliga['position'][laliga['year'] == 2014],

y = laliga['xGA_diff'][laliga['year'] == 2014],

name = '2014',

mode = 'lines+markers'

)



trace1 = go.Scatter(

x = laliga['position'][laliga['year'] == 2015],

y = laliga['xGA_diff'][laliga['year'] == 2015],

name='2015',

mode = 'lines+markers'

)



trace2 = go.Scatter(

x = laliga['position'][laliga['year'] == 2016],

y = laliga['xGA_diff'][laliga['year'] == 2016],

name='2016',

mode = 'lines+markers'

)



trace3 = go.Scatter(

x = laliga['position'][laliga['year'] == 2017],

y = laliga['xGA_diff'][laliga['year'] == 2017],

name='2017',

mode = 'lines+markers'

)



trace4 = go.Scatter(

x = laliga['position'][laliga['year'] == 2018],

y = laliga['xGA_diff'][laliga['year'] == 2018],

name='2018',

mode = 'lines+markers'

)



data = [trace0, trace1, trace2, trace3, trace4]



layout = go.Layout(

title=""Comparing xGA gap between positions"",

xaxis={'title': 'Year'},

yaxis={'title': ""xGA difference"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

trace0 = go.Scatter(

x = laliga['position'][laliga['year'] == 2014],

y = laliga['xpts_diff'][laliga['year'] == 2014],

name = '2014',

mode = 'lines+markers'

)



trace1 = go.Scatter(

x = laliga['position'][laliga['year'] == 2015],

y = laliga['xpts_diff'][laliga['year'] == 2015],

name='2015',

mode = 'lines+markers'

)



trace2 = go.Scatter(

x = laliga['position'][laliga['year'] == 2016],

y = laliga['xpts_diff'][laliga['year'] == 2016],

name='2016',

mode = 'lines+markers'

)



trace3 = go.Scatter(

x = laliga['position'][laliga['year'] == 2017],

y = laliga['xpts_diff'][laliga['year'] == 2017],

name='2017',

mode = 'lines+markers'

)



trace4 = go.Scatter(

x = laliga['position'][laliga['year'] == 2018],

y = laliga['xpts_diff'][laliga['year'] == 2018],

name='2018',

mode = 'lines+markers'

)



data = [trace0, trace1, trace2, trace3, trace4]



layout = go.Layout(

title=""Comparing xPTS gap between positions"",

xaxis={'title': 'Position'},

yaxis={'title': ""xPTS difference"",

}

)



fig = go.Figure(data=data, layout=layout)

py.iplot(fig)

From the charts above, we can clearly see that top teams score more, concede less and get more points than expected. That’s why these teams are top teams. And totally opposite situation with outsiders. The teams from the middleplay average. Totally logical, no huge insights here.

# Check mean differences

def get_diff_means(df):

dm = df.groupby('year')[['xG_diff', 'xGA_diff', 'xpts_diff']].mean()



return dm



means = get_diff_means(laliga)

means

# Check median differences

def get_diff_medians(df):

dm = df.groupby('year')[['xG_diff', 'xGA_diff', 'xpts_diff']].median()



return dm



medians = get_diff_medians(laliga)

medians

Outliers Detection

Z-Score

Z-Score is the number of standard deviations from the mean a data point is. We can use it to find outliers in our dataset by assuming that |z-score| > 3 is an outlier.

# Getting outliers for xG using zscore

from scipy.stats import zscore

# laliga[(np.abs(zscore(laliga[['xG_diff']])) > 2.0).all(axis=1)]

df_xg[(np.abs(zscore(df_xg[['xG_diff']])) > 3.0).all(axis=1)]

# outliers for xGA

# laliga[(np.abs(zscore(laliga[['xGA_diff']])) > 2.0).all(axis=1)]

df_xg[(np.abs(zscore(df_xg[['xGA_diff']])) > 3.0).all(axis=1)]

# Outliers for xPTS

# laliga[(np.abs(zscore(laliga[['xpts_diff']])) > 2.0).all(axis=1)]

df_xg[(np.abs(zscore(df_xg[['xpts_diff']])) > 3.0).all(axis=1)]

12 outliers in total detected with zscore. Poor Osasuna in 2016 — almost 30 not deserved goals.

As we can see from this data, being in outlier space top does not yet make you win the season. But if you miss your opportunities or receive goals where you shouldn’t and do that toooooo much — you deserve relegation. Losing and being average is much easier than winning.

Interquartile Range (IQR)

IQR — is the difference between the first quartile and third quartile of a set of data. This is one way to describe the spread of a set of data.

A commonly used rule says that a data point is an outlier if it is more than 1.5 ⋅ IQR above the third quartile or below the first quartile. Said differently, low outliers are below Q1 − 1.5 ⋅ IQR and high outliers are above Q3 + 1.5 ⋅ IQR.

Let’s check it out.

# Trying different method of outliers detection

df_xg.describe()

# using Interquartile Range Method to identify outliers

# xG_diff

iqr_xG = (df_xg.describe().loc['75%','xG_diff'] - df_xg.describe().loc['25%','xG_diff']) * 1.5

upper_xG = df_xg.describe().loc['75%','xG_diff'] + iqr_xG

lower_xG = df_xg.describe().loc['25%','xG_diff'] - iqr_xG



print('IQR for xG_diff: {:.2f}'.format(iqr_xG))

print('Upper border for xG_diff: {:.2f}'.format(upper_xG))

print('Lower border for xG_diff: {:.2f}'.format(lower_xG))



outliers_xG = df_xg[(df_xg['xG_diff'] > upper_xG) | (df_xg['xG_diff'] < lower_xG)]

print('='*50)



# xGA_diff

iqr_xGA = (df_xg.describe().loc['75%','xGA_diff'] - df_xg.describe().loc['25%','xGA_diff']) * 1.5

upper_xGA = df_xg.describe().loc['75%','xGA_diff'] + iqr_xGA

lower_xGA = df_xg.describe().loc['25%','xGA_diff'] - iqr_xGA



print('IQR for xGA_diff: {:.2f}'.format(iqr_xGA))

print('Upper border for xGA_diff: {:.2f}'.format(upper_xGA))

print('Lower border for xGA_diff: {:.2f}'.format(lower_xGA))



outliers_xGA = df_xg[(df_xg['xGA_diff'] > upper_xGA) | (df_xg['xGA_diff'] < lower_xGA)]

print('='*50)



# xpts_diff

iqr_xpts = (df_xg.describe().loc['75%','xpts_diff'] - df_xg.describe().loc['25%','xpts_diff']) * 1.5

upper_xpts = df_xg.describe().loc['75%','xpts_diff'] + iqr_xpts

lower_xpts = df_xg.describe().loc['25%','xpts_diff'] - iqr_xpts



print('IQR for xPTS_diff: {:.2f}'.format(iqr_xpts))

print('Upper border for xPTS_diff: {:.2f}'.format(upper_xpts))

print('Lower border for xPTS_diff: {:.2f}'.format(lower_xpts))



outliers_xpts = df_xg[(df_xg['xpts_diff'] > upper_xpts) | (df_xg['xpts_diff'] < lower_xpts)]

print('='*50)



outliers_full = pd.concat([outliers_xG, outliers_xGA, outliers_xpts])

outliers_full = outliers_full.drop_duplicates() IQR for xG_diff: 13.16

Upper border for xG_diff: 16.65

Lower border for xG_diff: -18.43

==================================================

IQR for xGA_diff: 13.95

Upper border for xGA_diff: 17.15

Lower border for xGA_diff: -20.05

==================================================

IQR for xPTS_diff: 13.93

Upper border for xPTS_diff: 18.73

Lower border for xPTS_diff: -18.41

================================================== # Adding ratings bottom to up to find looser in each league (different amount of teams in every league so I can't do just n-20)

max_position = df_xg.groupby('league')['position'].max()

df_xg['position_reverse'] = np.nan

outliers_full['position_reverse'] = np.nan



for i, row in df_xg.iterrows():

df_xg.at[i, 'position_reverse'] = np.abs(row['position'] - max_position[row['league']])+1



for i, row in outliers_full.iterrows():

outliers_full.at[i, 'position_reverse'] = np.abs(row['position'] - max_position[row['league']])+1 total_count = df_xg[(df_xg['position'] <= 4) | (df_xg['position_reverse'] <= 3)].count()[0]

outlier_count = outliers_full[(outliers_full['position'] <= 4) | (outliers_full['position_reverse'] <= 3)].count()[0]

outlier_prob = outlier_count / total_count

print('Probability of outlier in top or bottom of the final table: {:.2%}'.format(outlier_prob)) Probability of outlier in top or bottom of the final table: 8.10%

So we can say that it is very probable that every year in one of 6 leagues there will be a team that gets a ticket to Champions League or Europa Legue with the help of luck on top of their great skills or there is a looser that gets to the second division, because they cannot convert their moments.

# 1-3 outliers among all leagues in a year

data = pd.DataFrame(outliers_full.groupby('league')['year'].count()).reset_index()

data = data.rename(index=int, columns={'year': 'outliers'})

sns.barplot(x='league', y='outliers', data=data)

# no outliers in Bundesliga

Our winners and losers with brilliant performance and brilliant underperformance.

top_bottom = outliers_full[(outliers_full['position'] <= 4) | (outliers_full['position_reverse'] <= 3)].sort_values(by='league')

top_bottom

# Let's get back to our list of teams that suddenly got into top. Was that because of unbeliavable mix of luck and skill?

ot = [x for x in outlier_teams if x in top_bottom['team'].drop_duplicates().tolist()]

ot

# The answer is absolutely no. They just played well during 1 season. Sometimes that happen. []

Conclusions

Football is a low-scoring game and one goal can change the entire picture of the game and even end results. That’s why long term analysis gives you a better picture of the situation.

With the introduction of xG metric (and others that derive from this) now we can really evaluate the performance of the team on a long run and understand the difference between top teams, middle class teams and absolute outsiders.

xG brings new arguments into discussions around football what makes it even more interesting. And at the same time, the game doesn’t lose this factor of uncertainty and the possibility of crazy things happening. Actually now, these crazy things have a chance to be explained.

In the end, we have found that it is almost 100% chance that something weird will happen in one of the leagues. It is just a question of time how epic that will be.

Original work with interactive charts can be found here."
Python Vs R: What’s Best for Machine Learning,"Are you thinking to build a machine learning project and stuck between choosing the right programming language for your project? Well, then this article is going to help you clear the doubts related to the characteristics of Python and R. Let’s get started with the basics.

R and Python both share similar features and are the most popular tools used by data scientists. Both are open-source and henceforth free yet Python is structured as a broadly useful programming language while R is created for statistical analysis.

In this article, we will be looking at some pros and cons of both languages so you can decide which option suits you the best.

Python

The Python programming language was developed in the late 80s and plays a crucial role in powering the internal infrastructure of Google. Python comprises of enthusiastic developers and now it’s been used in the widely used applications of YouTube, Instagram, Quora, and Dropbox. Python is broadly utilized over the IT business and permits simple effort of collaboration inside development groups. In this way, in the event that you need an adaptable and multi-reason programming language with a supporting enormous network of engineers alongside the extendable AI bundles then Python is a top pick.

Advantages of Python

● General-purpose language — Python is regarded as a better choice if your project demands more than just statistics. For instance — designing a functional website

● Smooth Learning Curve — Python is easy to learn and easily accessible which enables you to find the skilled developers on a faster basis.

● The bulk of Important libraries — Python basts countless libraries for munging, gathering, and controlling the information. Take an occasion of Scikit-realize which comprises tools for information mining and investigation to support the incredible AI convenience utilizing Python. Another bundle called Pandas gives engineers superior structures and data examination devices that help to diminish the improvement time. If your development team demands one of the major functionalities of R then RPy2 is the one to go for.

● Better Integration — Generally, in any engineering environment, the Python integrates better than R. Thus, regardless of whether the designers attempt to exploit a lower-level language like C, C++ or Java, it generally gives better joining different components with Python wrapper. Additionally, a python-based stack is anything but difficult to incorporate the remaining task at hand of data researchers by bringing it easily into creation.

● Boosts Productivity — The syntax of Python is exceptionally decipherable and like other programming languages, however unique in relation to R. In this way, it guarantees high profitability of the development groups.

Disadvantages of Python

● Includes a very few statistical model packages.

● Due to the presence of the Global Interpreter Lock (GIL), threading in Python becomes tricky and quite problematic. Subsequently, multi-threaded CPU-bound applications act slower than single-thread ones. An AI undertaking is more valuable for executing multiprocessing instead of utilizing multithreaded programming.

R

R was developed by statisticians and basically for the statisticians which any developer can predict the same by looking at its syntax. As the language contains mathematical computations involved in machine learning which is derived from statistics, R becomes the right choice who wants to gain a better understanding of the underlying details and build innovative. If your project is heavily based on statistics then R can be considered as an excellent choice for narrowing down your projects which requires one-time dive into the dataset. For instance — if you like to analyze a corpus of text by deconstructing paragraphs into words or phrases to identify their patterns then R is the best choice.

Advantages of R

● Suitable for Analysis — if the data analysis or visualization is at the core of your project then R can be considered as the best choice as it allows rapid prototyping and works with the datasets to design machine learning models.

● The bulk of useful libraries and tools — Similar to Python, R comprises of multiple packages which help to improve the performance of the machine learning projects. For instance — Caret boosts the machine learning capabilities of the R with its special set of functions which helps to create predictive models efficiently. R developers gain advantage from the advanced data analysis packages which cover the pre- and post-modeling stages which are directed at specific tasks like model validation or data visualization.

● Suitable for exploratory work — If you require any exploratory work in statistical models at the beginning stages of your project then R makes it easier to write them as the developers just need to add a few lines of code.

Disadvantages of R

● Steep learning curve — It is tough to deny that R is a challenging language and therefore you can find very rare experts for building your project team.

● Inconsistent — As the algorithms of R come from third parties, it happens that you might end up with inconsistencies. Every time your development team makes use of a new algorithm, all the connected resources need to learn different ways to model data and make predictions. Similar to this, every new package requires learning and there is no detailed documentation of R as it leads to a negative impact on the development speed.

R vs. Python: Which One to Go for?

When it comes to machine learning projects, both R and Python have their own advantages. Still, Python seems to perform better in data manipulation and repetitive tasks. Hence, it is the right choice if you plan to build a digital product based on machine learning. Moreover, if you need to develop a tool for ad-hoc analysis at an early stage of your project then go for R. The ultimate choice depends on which programming language you want to go. Till then — keep learning!

Author Bio:

Vikash Kumar working in a software development company https://www.tatvasoft.com. He likes to share new ideas on machine learning, AI and many more. Apart from his daily routine of his professional work he also likes to cook and roam around. You can visit here to more about his company and follow him on Twitter and LinkedIN."
Handbook of Anomaly Detection with Python Outlier Detection — (4) Isolation Forest,"Figure (A)

(Revised on October 13, 2022)

If you were asked to separate the above trees one by one, which tree will be the first one to start with? You may pick the one to the left because it stands alone by itself. After removing that tree, what is the next tree that is easy to separate? Probably the one in the bottom left in the big cluster. After removing that tree, which one is the next? Probably the one in the upper left, and so forth. Here I present a very important intuition: an outlier should be the easiest to be isolated. Just like peeling an onion, an outlier is on the outside layer. That’s the intuition of Isolate Forest to find outliers.

Isolation Forest is fast and does not consume much memory because it does not use any distance measures to detect anomalies. This advantage makes it suitable for large data sizes and high-dimensional problems.

(A) What Is the Isolate Forest?

Many outlier detection methods profile the norm data points first, then identify those observations that do not conform to the patterns of the normal data. The Isolation Forest or IForest, proposed by Liu, Ting, and Zhou (2008), departs from those methods. Rather than profiling normal data points in order to find outliers, IForest identifies anomalies directly. It applies a tree structure to isolate every observation. Anomalies will be the data points first to be singled out; whereas normal points tend to hide deep in the tree. They call each tree the Isolation Tree or iTree. Their algorithm builds an ensemble of iTrees. Anomalies are those observations that have short average path lengths on the iTrees.

Figure (A) uses a partition map and a tree to explain how iTree isolates the data points. The red dot is the farthest from the other dots, then the green dot, then the blue dot. In the partition map, it takes only one “cut” to separate the red dot from the others. The second cut is for the green dot, and the third cut is for the blue dot, and so on. The more cuts it takes to separate a dot, the deeper it is in the tree. The inverse of the number of cuts is the anomaly score. The tree structure on the right of Figure (A) tells the same story. It takes one split to single out the red dot, then the second split to…"
A Step-by-Step Implementation of Gradient Descent and Backpropagation,"A Step-by-Step Implementation of Gradient Descent and Backpropagation

The original intention behind this post was merely me brushing upon mathematics in neural network, as I like to be well versed in the inner workings of algorithms and get to the essence of things. I then think I might as well put together a story rather than just revisiting the formulas on my notepad over and over. Though you might find a number of tutorials for building a simple neural network from scratch. Different people have varied angles of seeing things as well as the emphasis of study. Another way of thinking might in some sense enhance understanding. So let’s dive in.

Photo from Unsplash

Neural network in a nutshell

The core of neural network is a big function that maps some input to the desired target value, in the intermediate step does the operation to produce the network, which is by multiplying weights and add bias in a pipeline scenario that does this over and over again. The process of training a neural network is to determine a set of parameters that minimize the difference between expected value and model output. This is done using gradient descent (aka backpropagation), which by definition comprises two steps: calculating gradients of the loss/error function, then updating existing parameters in response to the gradients, which is how the descent is done. This cycle is repeated until reaching the minima of the loss function. This learning process can be described by the simple equation: W(t+1) = W(t) — dJ(W)/dW(t).

The mathematical intuition

Photo from https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75

For my own practice purpose, I like to use a small network with a single hidden layer as in the diagram. In this layout, X represents input, subscripts i, j, k denote the number of units in the input, hidden and output layers respectively; w_ij represents the weights connecting input to hidden layer, and w_jk is the weights connecting hidden to output layer.

The model output calculation, in this case, would be:

Often the choice of the loss function is the sum of squared error. Here I use sigmoid activation function and assume bias b is 0 for simplicity, meaning weights are the only variables that affect model output. Let’s derive the formula for calculating gradients of hidden to output weights w_jk.

The complexity of determining input to hidden weights is that it affects output error indirectly. Each hidden unit output affects model output, thus input to hidden weights w_ij depend on the errors at all of the units it is connected to. The derivation starts the same, just to expand the chain rule at z_k to the subfunction.

More thoughts:

Notice that the gradients of the two weights have a similar form. The error is backpropagated via the derivative of activation function, then weighted by the input (the activation output) from the previous layer. In the second formula, the backpropagated error from the output layer is further projected onto w_jk, then repeat the same way of backpropagation and weighted by the input. This backpropagating process is iterated all the way back to the very first layer in an arbitrary-layer neural network. “The gradients with respect to each parameter are thus considered to be the contribution of the parameter to the error and should be negated during learning.”

Putting the above process into code:

Below is the complete example:

import numpy as np class NeuralNetwork:

def __init__(self):

np.random.seed(10) # for generating the same results

self.wij = np.random.rand(3,4) # input to hidden layer weights

self.wjk = np.random.rand(4,1) # hidden layer to output weights



def sigmoid(self, x, w):

z = np.dot(x, w)

return 1/(1 + np.exp(-z))



def sigmoid_derivative(self, x, w):

return self.sigmoid(x, w) * (1 - self.sigmoid(x, w))



def gradient_descent(self, x, y, iterations):

for i in range(iterations):

Xi = x

Xj = self.sigmoid(Xi, self.wij)

yhat = self.sigmoid(Xj, self.wjk)

# gradients for hidden to output weights

g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))

# gradients for input to hidden weights

g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))

# update weights

self.wij += g_wij

self.wjk += g_wjk

print('The final prediction from neural network are: ')

print(yhat) if __name__ == '__main__':

neural_network = NeuralNetwork()

print('Random starting input to hidden weights: ')

print(neural_network.wij)

print('Random starting hidden to output weights: ')

print(neural_network.wjk)

X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])

y = np.array([[0, 1, 1, 0]]).T

neural_network.gradient_descent(X, y, 10000)

References:"
Applying Data Science to Cybersecurity Network Attacks & Events,"The cyber world is such a vast concept to comprehend. At the time, I decided I wanted to get into cybersecurity during my undergrad in college. What intrigued me was understanding the concepts of malware, network security, penetration testing & the encryption aspect that really plays a role in what cybersecurity really is.

Being able to protect the infrastructure is important but interesting nonetheless. Sure there is coding but I never really learned how we can implement code into cybersecurity principles. This is what I really want to know next that could stretch my knowledge in information technology & computer science. I learned more about coding especially in Python. I dabbled a little in Scala & I already had a good foundation of Sequel & Java applications during my undergrad that learning it during my boot camp allowed me to feel a little more comfortable with it.

The data science immersive program taught me how to gather data through Sequel, JSON, HTML or web-scrapping applications where I went into cleaning the data & then applying Python related code for statistical analysis. I then as able to model the data to either find trends, make predictions, or provide suggestions/recommendations. I wanted to apply this to my background in Cisco NetaCad, cybersecurity principles & software development

I then wanted to relate this to my cybersecurity background. I decided to gather data from Data.org regarding a ton of cyber attacks regarding the Federal Communications Commission (FCC). In this blog post, I decided to write about how I was able to connect my data science knowledge to my cybersecurity background in real world industries. I will provide some background of the project, a code along & some insight into what the FCC could do to better understand the data as I did. This could be useful for future situations or other government related cybersecurity projects.

So the FCC, to the best of my knowledge, follows the CSRIC Best Practices Search Tool which allows you to search CSRIC’s collection of Best Practices using a variety of criteria including Network Type, Industry Role, Keywords, Priority Levels & BP Number.

The Communications Security, Reliability & Interoperability Council’s (CSRIC) mission is to provide recommendations to the FCC to ensure, among other things, optimal security & reliability of communications systems, including telecommunications, media & public safety.

CSRIC’s members focus on a range of public safety & homeland security-related communications matters, including: (1) the reliability & security of communications systems & infrastructure, particularly mobile systems; (2) 911, Enhanced 911 (E911), & Next Generation 911 (NG911); & (3) emergency alerting.

The CSRIC’s recommendations will address the prevention & remediation of detrimental cyber events, the development of best practices to improve overall communications reliability, the availability & performance of communications services & emergency alerting during natural disasters, terrorist attacks, cyber security attacks or other events that result in exceptional strain on the communications infrastructure, the rapid restoration of communications services in the event of widespread or major disruptions & the steps communications providers can take to help secure end-users & servers.

The first step for me in tackling this project was understanding what I needed to accomplish & what kind of direction this project had me taking. I had remembered that I needed to provide recommendations to the FCC to ensure optimal security & reliability of communication systems in telecommunications, media & public safety.

There are different approaches I had taken into account when attempting this project. The 1st is diving straight in in order to better understand the data itself where I focused on just the priority of the event & applying a ton of machine learning classification models to it. The 2nd approach was being able to apply natural language processing techniques on the description of the event & see how that correlates to the priority of the event.

With this we can make predictions & following that, recommendations to better prevent, understand or control the event. My idea is if we can focus on the more critical events by fixing the less complex ones, we can save enough assets to further improve the systems to combat the more complex ones.

What is needed:

A Python IDE

Machine Learning & Statistical Packages

Strong knowledge in Data Science Concepts

Some knowledge in cybersecurity & network related concepts

Let’s begin! Let’s first start by importing any packages we intend on using, I usually copy & paste a list of useful modules that has helped me with previous data science projects.

import pandas as pd

import numpy as np

import scipy as sp

import seaborn as sns

sns.set_style('darkgrid')

import pickle

import regex as re

import gensim from nltk.stem import WordNetLemmatizer

from nltk.tokenize import RegexpTokenizer

from nltk.stem.porter import PorterStemmer

from nltk.stem.snowball import SnowballStemmer

from nltk.corpus import stopwords

from sklearn.feature_extraction import stop_words

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV

from sklearn.preprocessing import StandardScaler

from sklearn.naive_bayes import MultinomialNB

from sklearn.linear_model import LinearRegression,LogisticRegression

from sklearn import metrics

from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier

from sklearn.neighbors import KNeighborsClassifier

from sklearn.tree import DecisionTreeClassifier

from sklearn.pipeline import Pipeline

from keras import regularizers

from keras.models import Sequential

from keras.layers import Dense, Dropout import warnings

warnings.filterwarnings('ignore') %matplotlib inline

We then need to load in the data & view it by:

# Loads in the data into a Pandas data frame

fcc_csv = pd.read_csv('./data/CSRIC_Best_Practices.csv')

fcc_csv.head()

Now that we have our data we can explore, clean & understand the data. Below, I have provided a function for basic data exploratory analysis. We want to do this to fully understand the data we’re dealing with & what the overall goal needs to be met.

The following function will allow us to view any null values, replace any blank spaces with an underscore, reformat the data frame index, see the data types for each column, display any duplicated data, describes the statistical analysis of the data & checks the shape.

# Here is a function for basic exploratory data analysis: def eda(dataframe):

# Replace any blank spaces w/ a underscore.

dataframe.columns = dataframe.columns.str.replace("" "", ""_"")

# Checks for the null values.

print(""missing values{}"".format(dataframe.isnull().sum().sum()))

# Checks the data frame range size.

print(""dataframe index: {}"".format(dataframe.index))

# Checks for data types of the columns within the data frame.

print(""dataframe types: {}"".format(dataframe.dtypes))

# Checks the shape of the data frame.

print(""dataframe shape: {}"".format(dataframe.shape))

# Gives us any statistical information of the data frame.

print(""dataframe describe: {}"".format(dataframe.describe()))

# Gives us the duplicated data of the data frame. print(""duplicates{}"".format(dataframe[dataframe.duplicated()].sum()))

# A for loop that does this for every single column & their

# values within our data frame giving us all unique values.

for item in dataframe:

print(item)

print(dataframe[item].nunique()) # Let's apply this function to our entire data frame.

eda(fcc_csv)

Based off of the data we say that this was a unbalanced classification problem! What we need to do next is eliminate any “NaN” or null values & somewhat balance our class. Based off of our exploratory data analysis, we can see that most of the data has “NaN” values in the object type columns. We can fix this with the following function below!

# Here's a function to convert NaN's in the data set to 'None' for

# string objects.

# Just pass in the entire data frame.

def convert_str_nan(data):

return data.astype(object).replace(np.nan, 'None', inplace = True) convert_str_nan(fcc_csv)

Checking the amount of values in the Priorities column

Looking at the priority column(s), we have a object related column that ranks the severity of the event to important, highly important & critical. Another column that corresponds to that priority column ranks them 1–3, with 1 being important, 2 as highly important & 3 being critical. Based off of the variety of priorities, we see that the data is unbalanced. We fix this by renaming our column for better understand & then balancing it where we focus on the highly important & critical events.

# Let's rename the 'Priority_(1,2,3)' column so we can utilize it.

fcc_csv.rename(columns = {

'Priority_(1,2,3)': 'Priorities'

},

inplace = True) # Let's view the values & how the correspond to the 'Priority'

# column.

fcc_csv['Priorities'].value_counts() # We notice that we have an unbalanced classification problem.

# Let's group the ""Highly Important"" (2) & ""Critical"" (3) aspects

# because that's where we can make recommendations.

# Let's double check that it worked.

fcc_csv['Priorities'] = [0 if i == 1 else 1 for i in fcc_csv['Priorities']]

fcc_csv['Priorities'].value_counts()

Result of the above code after we’ve balanced the class

First Approach (Understanding the Data)

In this next section I will discuss my initial approach in understanding the priorities column. I learned fast that this approach was not the best but was very informative in where I should look elsewhere when looking at the priorities of the attack in making recommendations.

My next step was understanding which columns correlated best for patterns & trends with my priorities column. It seemed that all columns that worked well were binary! The description column were text related & machines don’t like handling text objects. With the below code, we can see which columns are the most positively correlated & most negatively correlated with our predictor column.

# Let's view the largest negative correlated columns to our

# ""Priorities"" column.

largest_neg_corr_list = fcc_csv.corr()[['Priorities']].sort_values('Priorities').head(5).T.columns

largest_neg_corr_list # Let's view the largest positive correlated columns to our

# ""Priorities"" column.

largest_pos_corr_list = fcc_csv.corr()[['Priorities']].sort_values('Priorities').tail(5).T.columns.drop('Priorities')

largest_pos_corr_list

Most Negatively Correlated

Most Positively Correlated

Now we can start making our models to see what we’ve done will give us accurate predictions or enough information for recommendations. Let’s first start with a train-test split with these correlated columns serving as our features & our priorities column serving as our predictor variable.

# Let's pass in every column that is categorical into our X.

# These are the strongest & weakest correlated columns to our

# ""Priorities"" variable.

X = fcc_csv[['Network_Operator', 'Equipment_Supplier', 'Property_Manager', 'Service_Provider', 'wireline', 'wireless', 'satellite', 'Public_Safety']]

y = fcc_csv['Priorities'] # Our y is what we want to predict. # We have to train/test split the data so we can model the data on

# our training set & test it.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42) # We need to transpose the trains so they contain the same amount of # rows.

X = X.transpose()

Now that we’ve train-test split our features, let’s apply grid search to find the best parameters or features for full accuracy on a Naive Bayes Classifier, Random Forest Classifier, Adaboost/Gradient Boost Classifier & a Keras Neural Network! But what do these classifier models even mean?

In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Let’s see it on our data!

# Instantiates the Naive Bayes classifier.

mnb = MultinomialNB()

params = {'min_samples_split':[12, 25, 40]} # Grid searches our Naive Bayes.

mnb_grid = {}

gs_mnb = GridSearchCV(mnb, param_grid = mnb_grid, cv = 3)

gs_mnb.fit(X_train, y_train)

gs_mnb.score(X_train, y_train) # Scores the Naive Bayes.

gs_mnb.score(X_test, y_test)

Random Forest Classifier creates a set of decision trees from a randomly selected subset of the training set, which then aggregates the votes from different decision trees to decide the final class of the test object. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction. Let’s model it!

# Instantiates the random forest classifier.

rf = RandomForestClassifier(n_estimators = 10) # Grid searches our random forest classifier.

gs_rf = GridSearchCV(rf, param_grid = params, return_train_score = True, cv = 5)

gs_rf.fit(X_train, y_train)

gs_rf.score(X_train, y_train) # Our random forest test score.

gs_rf.score(X_test, y_test)

Graph of our Adaboost Model, it’s overfit!

AdaBoost is short for Adaptive Boosting. It is basically a machine learning algorithm that is used as a classifier. Whenever you have a large amount of data and you want divide it into different categories, we need a good classification algorithm to do it. Hence the word ‘boosting’, as in it boosts other algorithms!

scores_test = []

scores_train = []

n_estimators = [] for n_est in range(30):

ada = AdaBoostClassifier(n_estimators = n_est + 1, random_state = 42)

ada.fit(X_train, y_train)

n_estimators.append(n_est + 1)

scores_test.append(ada.score(X_test, y_test))

scores_train.append(ada.score(X_train, y_train)) # Our Ada Boost score on our train set.

ada.score(X_train, y_train) # Our Ada Boost score on our test set.

ada.score(X_test, y_test)

Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. We can even apply regularization to combat the over fitting issue!

Our regularized Neural Network, it’s overfit!

model_dropout = Sequential() n_input = X_train.shape[1]

n_hidden = n_input model_dropout.add(Dense(n_hidden, input_dim = n_input, activation = 'relu'))

model_dropout.add(Dropout(0.5)) # refers to nodes in the first hidden layer

model_dropout.add(Dense(1, activation = 'sigmoid')) model_dropout.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc']) history_dropout = model_dropout.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 100, batch_size = None)

The Accuracy of Our Models

What is this telling us?! Based off of the training score & test score we can see that our model is over fit & is not great at making predictions or analyzing trends. What does it mean that our model is over fit? We have high variance & low bias! High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs. We still can’t make recommendations off of this since it really doesn’t give us much information!

Second Approach (Natural Language Processing) — The Best Route

My 2nd approach was focusing on the description column. After my 1st approach, I wanted to see how the priority of the attack correlated with what was given in the description. The description column gave us a short explanation of what happened & a suggested FCC compliant resolution in what they might do in stopping that similar event.

In order to better understand the description column, I needed to apply natural language processing (NLP) since computers & statistical models dislike handling text & words. But we can get around this! My approach is similar when cleaning the data & balancing the priorities column, however I have applied some NLP concepts to better understand the description, analyze it, make recommendations & even predict what the next event would be based off of the occurrence of words specific to the events.

Some of the concepts include:

Pre-processing is the technique of converting raw data into a clean data set.

is the technique of converting raw data into a clean data set. Regex, regular expression is a string of text that allows you to create patterns that help match, locate & manage text. Another way to clean the text.

regular expression is a string of text that allows you to create patterns that help match, locate & manage text. Another way to clean the text. Lemmatizing is the process of grouping together the inflected forms of a word so they can be analyzed as a single term.

is the process of grouping together the inflected forms of a word so they can be analyzed as a single term. Stemming is the process of reducing inflected words to their stem, base or root form.

is the process of reducing inflected words to their stem, base or root form. Countvectorize counts the word frequencies.

counts the word frequencies. TFIDFVectorizer is the value of a word increases proportionally to count, but is offset by the frequency of the word in the corpus.

Let’s start off by applying regex concepts to our already cleaned data. We also want to strip or clean out common words that are useful but are sporadic in every single description.

# Let's clean the data using Regex.

# Let's use regex to remove the words: service providers, equipment # suppliers, network operators, property managers, public safety

# Let's also remove any mention of any URLs. fcc_csv['Description'] = fcc_csv.Description.map(lambda x: re.sub('\s[\/]?r\/[^s]+', ' ', x))

fcc_csv['Description'] = fcc_csv.Description.map(lambda x: re.sub('http[s]?:\/\/[^\s]*', ' ', x))

fcc_csv['Description'] = fcc_csv.Description.map(lambda x: re.sub('(service providers|equipment suppliers|network operators|property managers|public safety)[s]?', ' ', x, flags = re.I))

Now that we’ve cleaned our data, we should apply some pre-processing techniques to better understand the words given to use in every description of the event.

# This is a text preprocessing function that gets our data ready for # modeling & creates new columns for the

# description text in their tokenized, lemmatized & stemmed forms.

# This allows for easy selection of

# different forms of the text for use in vectorization & modeling. def preprocessed_columns(dataframe = fcc_csv,

column = 'Description',

new_lemma_column = 'lemmatized',

new_stem_column = 'stemmed',

new_token_column = 'tokenized',

regular_expression = r'\w+'):



tokenizer = RegexpTokenizer(regular_expression)

lemmatizer = WordNetLemmatizer()

stemmer = PorterStemmer()



lemmatized = []

stemmed = []

tokenized = []





for i in dataframe[column]:

tokens = tokenizer.tokenize(i.lower())

tokenized.append(tokens) lemma = [lemmatizer.lemmatize(token) for token in tokens]

lemmatized.append(lemma) stems = [stemmer.stem(token) for token in tokens]

stemmed.append(stems)



dataframe[new_token_column] = [' '.join(i) for i in tokenized]

dataframe[new_lemma_column] = [' '.join(i) for i in lemmatized]

dataframe[new_stem_column] = [' '.join(i) for i in stemmed]



return dataframe

We then want to apply countvectorize on our stemmed, lemmatized & tokenized description words in order to control common stop words in the English language, we can do this with the following code. We then can see what the most common words are in the entire data set.

# Instantiate a CountVectorizer removing english stopwords, ngram

# range of unigrams & bigrams. cv = CountVectorizer(stop_words = 'english', ngram_range = (1,2), min_df = 25, max_df = .95) # Create a dataframe of our CV transformed tokenized words

cv_df_token = pd.SparseDataFrame(cv.fit_transform(processed['tokenized']), columns = cv.get_feature_names())

cv_df_token.fillna(0, inplace = True) cv_df_token.head()

Top Word Count in the Entire Data Set

We can see that most of the words that popped up are network or security related. We can use this information to better understand the scope of what these events are! Are the network attacks? Are they network warehouse related? Etc.

But what if we wanted more information? We can group the descriptions based on the urgency or severity of the importance of the event. Maybe it’s nothing serious so it’s ranked a 0 (Not Important) or really bad ranked as 1 (Really Important). We can do this with the below code based on our pre-processed columns. We can then visualize what the most common really important words are.

# Split our data frame into really ""important"" & ""not important""

# columns.

# We will use the ""really_important"" descriptions to determine

# severity & to give recommendations/analysis.

fcc_really_important = processed[processed['Priorities'] == 1]

fcc_not_important = processed[processed['Priorities'] == 0] print(fcc_really_important.shape)

print(fcc_not_important.shape)

Top Word Count of the Really IMPORTANT words

Finally, we can start modeling regression & classification metrics on the tokenized data. Let’s start by applying a logistic regression model through a pipeline where can apply a grid search tool in order to tune our BEST features or BEST parameters. Let’s establish our X variable, our features! We will use the words or features within the tokenized column within the processed data frame we created above. The processed data frame is an entirely NEW data frame that contains our tokenized, stemmed & lemmatized columns.

Here I decided to focus on the tokenized columns because this specific column functioned the best for parameter tuning & accuracy. To cut on time length for this blog post I decided to focus on the tokenized, what works best, as well! Let’s train-test split it as well.

X_1 = processed['tokenized'] # We're train test splitting 3 different columns.

# These columns are the tokenized, lemmatized & stemmed from the

# processed dataframe.

X_1_train, X_1_test, y_train, y_test = train_test_split(X_1, y, test_size = 0.3, stratify = y, random_state = 42)

Now let’s created our pipeline that uses grid search concepts for the best hyper parameters. Once the grid search has fit (this can take awhile!) we can pull out a variety of information and useful objects from the grid search object. Often, we’ll want to apply several transformers to a data set & then finally build a model. If you do all of these steps independently, your code when predicting on test data can be messy. It’ll also be prone to errors. Luckily, we’ll have pipelines!

Here we will be applying a logistic model which can take into consideration LASSO & Ridge penalties.

You should:

Fit and validate the accuracy of a default logistic regression on the data. Perform a gridsearch over different regularization strengths, Lasso & Ridge penalties. Compare the accuracy on the test set of your optimized logistic regression to the baseline accuracy & the default model. Look at the best parameters found. What was chosen? What does this suggest about our data? Look at the (non-zero, if Lasso was selected as best) coefficients & associated predictors for your optimized model. What appears to be the most important predictors?

pipe_cv = Pipeline([

('cv', CountVectorizer()),

('lr', LogisticRegression())

]) params = {

'lr__C':[0.6, 1, 1.2],

'lr__penalty':[""l1"", ""l2""],

'cv__max_features':[None, 750, 1000, 1250],

'cv__stop_words':['english', None],

'cv__ngram_range':[(1,1), (1,4)]

}

Now we can apply the pipeline on our logistic regression model within a grid search object. Notice the countvectorize model being instantiated. We did this because we want see how that factors into our accuracy & importance of the words we associated with our network attacks.

# Our Logistic Regression Model.

gs_lr_tokenized_cv = GridSearchCV(pipe_cv, param_grid = params, cv = 5)

gs_lr_tokenized_cv.fit(X_1_train, y_train)

gs_lr_tokenized_cv.score(X_1_train, y_train) gs_lr_tokenized_cv.score(X_1_test, y_test)

Our Improved Accuracy of our Logistic Model

So what can infer from this? Well it looks like we’ve seen a massive increase in model accuracy for a training data as well as 10% increase in accuracy for our testing data. However, the model is still over fit! But still great job! What were our best parameters? We could use this information if we want to tune future logistic models! The below code will show us that!

gs_lr_tokenized_cv.best_params_

Our BEST Hyper Parameters from our Logistic Gridsearch within the Pipeline

A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression. From our best hyper parameters, our models favors a Ridge Regression technique. Now we want to make predictions off of our logistic regression & be able to make suggestions. How do we go upon doing that? Let’s look at the coefficients associated with our features that will predict the outcomes of the best y variable.

coefs = gs_lr_tokenized_cv.best_estimator_.steps[1][1].coef_

words = pd.DataFrame(zip(cv.get_feature_names(), np.exp(coefs[0])))

words = words.sort_values(1)

Predictions for Suggestions Based off Occurrence & Importance of Words

Recommendations & Conclusions

Now that I’ve finished modeling & analyzing the data, I can now make suggestions to the FCC as well as any other data scientist or data analyst that plans on doing a project similar to mine.

For future data scientists doing a similar project. Get more data, a better sample of data, increase/decrease complexity of model & regularize. This can help combat the issue of over fitting your data as I experienced through this project. Understand unbalanced classification problems. This can lead to your main direction in solving the problem.

For the FCC’s CSRIC’s best practices, my best recommendation would be to fix the simple issues first so they don’t happen to often & soak up your resouces. This can allow them to focus on the more important & complex events or attacks. Based off what I could predict & analyzing the given data.

Simple problems:

Different Cables

Color Code Cables

Better ventilation of warehouse

Increase power capacity

Better hardware

Spacing of antennas

Moderate problems:

Utilize Network Surveillance

Provide secure electrical software where feasible

Find thresholds for new hardware & software

Virus protection

Complex:"
Building a Topic Modeling Pipeline with spaCy and Gensim,"Python, like most many programming languages, has a huge amount of exceptional libraries and modules to choose from. Generally of course, this is absolutely brilliant, but it also means that sometimes the modules don’t always play nicely with each other. In this short tutorial, I’m going to show you how to link up spaCy with Gensim to create a coherent topic modeling pipeline.

Yeah, great. What’s a topic model?

Good question. I thought about re-writing the Wikipedia definition, then thought that I probably should just give you the Wikipedia definition:

In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract “topics” that occur in a collection of documents.

Basically, we’re looking for what collections of words, or topics, are most relevant to discussing the content of the corpus. For this tutorial we’ll be using Latent Dirichlet Allocation (LDA).

A short note about libraries

For those of you folks who aren’t aware, Gensim is one of the pre-eminent libraries for topic modeling. Meanwhile, spaCy is a powerful natural language processing library that has won a lot of admirers in the last few years.

Building the pipeline

First things first, let’s import our libraries:"
How 90% of Drivers Can be “Above Average” or Why You Need to be Careful When Using Statistics,"How 90% of Drivers Can be “Above Average” or Why You Need to be Careful When Using Statistics

Most people see the headline “90% of Drivers Consider Themselves Above Average” and think “wow, other people are terrible at evaluating themselves objectively.” What you should think is “that doesn’t sound so implausible if we’re using the mean for average in a heavily negative-skewed distribution.”

Although a headline like this is often used to illustrate the illusion of superiority, (where people overestimate their competence) it also provides a useful lesson in clarifying your assertions when you talk statistics about data. In this particular case, we need to differentiate between the mean and median of a set of values. Depending on the question we ask, it is possible for 9/10 drivers to be above average. Here’s the data to prove it:"
Algorithmic trading based on mean-variance optimization in Python,"Member-only story Algorithmic trading based on mean-variance optimization in Python

This is the fifth part of a series of articles on backtesting trading strategies in Python. The previous ones described the following topics:

introducing the zipline framework and presenting how to test basic strategies (link)

importing custom data to use with zipline (link)

(link) evaluating the performance of trading strategies (link)

implementing trading strategies based on Technical Analysis (link)

This time, the goal of the article is to show how to create trading strategies using Markowitz’s portfolio optimization and the Modern Portfolio Theory.

In this article, I first give a brief introduction/reminder on the mean-variance optimization and then show how to implement it into trading strategies. Just as before, I will backtest them using the zipline framework.

The Setup

For this article I use the following libraries:

zipline 1.3.0

matplotlib 3.0.0

json 2.0.9

empyrical 0.5.0

numpy 1.14.6

pandas 0.22.0

pyfolio 0.9.2

Primer on mean-variance optimization

In 1952 Harry Markowitz published the ‘Portfolio Selection’, which described an investment theory now known as the Modern Portfolio Theory (MPT in short). Some of the key takeaways are:

the portfolio return is the weighted average of the individual portfolio constituents, however, the volatility is also impacted by the correlation between the assets

the investors should not evaluate the performance of the assets separately, but see how they would influence the performance of a portfolio

diversification (spreading the allocation over multiple assets instead of one or very few) can greatly decrease the portfolio’s volatility

We will not go deeply into the assumptions of MPT, but the main ones are that all investors share the goal of maximizing the returns on investment while avoiding as much risk as…"
Ants and the Problems with Neural Networks,"Ants and the Problems with Neural Networks

Photo by Mikhail Vasilyev on Unsplash

Ants are pretty dumb. They live for a week and don’t do much besides walking around, looking for food and carrying twigs to their anthill (now that I think about it, we humans also don’t do much else).

But they are also dumb apart from living uninspired ant lives. They are dumb in a technical sense: an individual ant has just at the order of 250.000 (2,5*10⁵) neurons. As a comparison: an average homo sapiens has on average 80 billion neurons (8*10¹⁰), so if we assume that intelligence scales in at least some ways with the size of the brain (ignoring the fact that some animals have larger brains than we do), then we are approximately 320.000 times as smart as ants.

But despite the tininess of their brains, ants have their moments. They are constantly doing things that are so sophisticated that they could be taken straight out of a university-level math exam. One of these things is called dead reckoning.

Dead reckoning

When ants like the Cataglyphis bicolor go out foraging for food, they leave their home and move into the direction of an expected food source. As they don’t know precisely where the food source is to be found, they walk around in wobbly lines and circles (see this paper by Maroudas and Harkness for more details and an illustration). If you have observed ants in the wild, you might have thought that it all looks completely random.

But once the ant has found the food source and taken its share, it goes straight back to its home. And when I say straight, I mean literally along a straight line, without the help of any landmarks to guide them.

Dead reckoning is the art of knowing where you are by remembering where you came from. No one really knows why it’s called dead reckoning. You need to be very much alive to do it. And it’s no trivial task.

For ants to keep an understanding of where they are, they need to take into account the time with which they travel in a certain direction and the velocity with which they do it. They need to sum up all the small displacements of their path over long stretches of time, and they need to be able to store this information reliably and…"
ReFocus: Making Out-of-Focus Microscopy Images In-Focus Again,"Microscopy images are widely used for the diagnosis of various diseases such as infections and cancers. Furthermore, they facilitate basic biomedical research that has been continuously generating new insights into the causes of human diseases. Therefore, microscopy images are of great importance in improving our health. However, obtaining high-quality in-focus microscopy images poses one of the biggest challenges in the field of microscopy. For example, certain tissues, such as lung and intestines, are uneven and can result in out-of-focus images. In this post, we will tackle this problem by using deep learning to refocus out-of-focus microscopy images. In other words, we will turn out-of-focus microscopy images into in-focus ones (see the figure below) using deep learning.

Left: out of focus. Right: in focus (Image source: https://data.broadinstitute.org/bbbc/BBBC006/)

Data

We will be using the Broad Bioimage Benchmark Collection 006 (BBBC006) image set, which was acquired from one 384-well microplate containing human cells whose nuclei were labeled by Hoechst stains. A z-stack of 32 images (z = 16 at the optimal focal plane, 15 images above the focal plane, and 16 below) was taken for each of the 768 fields of view (384 wells, 2 fields of view per well).

Approach

The overall strategy is to build a convolutional neural network that takes out-of-focus images as input and generates in-focus images as output. We will base our neural network on the U-net architecture. In addition, we will use feature loss (originally called perceptual loss by Johnson et al. ) as the loss function to quantify the differences between the output of the neural network and its corresponding optimal focal plane image at z = 16, or the target.

U-net

U-net was originally developed by Ronneberge et al. for biomedical image segmentation problems. U-net essentially consists of three components: a downsampling path that reduces the image size, a subsequent upsampling path that increases the image size, and cross connections that transfer activations from selected parts of the downsampling path to their corresponding parts in the upsampling…"
Neural Fictitious Self-Play,"Update: The best way of learning and practicing Reinforcement Learning is by going to http://rl-lab.com

Introduction

This article is based on a scientific paper by Heinrich & Silver that introduces the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge.

Important Reminders

Fictitious Play: is an iterative method that finds Nash Equilibrium in Two Player Zero Sum game. Its problem is that it applies to Normal Form Games which are tabular and do not capture time or sequence. Details can be found in the article “Introduction to Fictitious Play”.

Fictitious Self Play: is a method that fixes the problem of the Fictitious Play by integrating time/sequence by using Extensive Form Game. It also uses Reinforcement Learning to find an approximation of the best response and Supervised Learning to update the average strategy. It is proven that it can converge to a Nash Equilibrium. More details in the article “Fictitious Self Play”

Imperfect Information Games

In imperfect information, players are simply unaware of the actions chosen by other players. However they know who the other players are, what their possible strategies/actions are, and the preferences/payoffs of these other players. Hence, information about the other players in imperfect information is complete.

On the other hand incomplete information games, players may or may not know some information about the other players, e.g. their “type”, their strategies, payoffs and their preferences.

Chess is an example of a game with perfect information as each player can see all the pieces on the board at all times. Other examples of games with perfect information include tic-tac-toe, checkers, infinite chess, and Go.

Card games where each player’s cards are hidden from other players such as poker and bridge are examples of games with imperfect information.

Reservoir Sampling"
A Recipe for using Open Source Machine Learning models,"A Recipe for using Open Source Machine Learning models

Photo by Luca Bravo on Unsplash

Machine learning continues to produce state of the art (SOTA) results for an increasing variety of tasks and more companies are looking to ML to solve their problems. With the incredibly rapid pace of machine learning research, many of these SOTA models come from academic and research institutions which open source these models. Often, using one of these open source models to bootstrap your machine learning efforts within your company can be much more effective than building a model from scratch.

However, these models are often released by researchers whose focus isn’t necessarily to enable easy use and modification of their models (though there are many exceptions). Using these open source models for your tasks can be quite difficult.

In this post, my goal is to provide a recipe you can follow to evaluate and use open source ML models to solve your own tasks. These are the steps I’ve used over and over again in my own work (as of this writing I have anaconda environments set up over 15 open source models). As my work is mostly using deep learning for vision and NLP, my focus here is specifically on using neural network-based models.

Whether you’re trying to use machine learning to solve real problems within your company or experiment with some fun SOTA results at home, my hope is that after this post, you’ll have a path to take an open source model and modify and use it to address your own task with your own dataset.

Step 1: Naming your task

The first step is figuring out what your particular task is called in the research literature so you can successfully search for it. This can initially be quite frustrating. For example, finding all the instances of a dog in a picture would be an “object detection” task. But if you want to know exactly which pixels in the picture correspond to dogs that’s called “image segmentation.”

There are a few ways you can try to figure this out. First, if you happen to know any ML researchers or practitioners, definitely start there. Another option is to ask in r/machinelearning or r/learnmachinelearning. If none of these pan out, the next step is to google to the best of your ability. As you land on research papers you’ll often see the name commonly associated with the task in the literature.

Step 2: Finding papers and code

Once you know what to search for, the next step is to find those open source models that best suit your task. There are a few resources that are helpful here:

paperswithcode: A repository of papers and associated code, organized by task. This is a really good starting point, especially if it’s a well-known task.

arxiv-sanity: Many open source models are associated with research papers. Most papers in machine learning are (fortunately!) openly published on arxiv. Searching arxiv for recent papers that solve for your task is another good place to start. Not all published papers here have code associated with them. If you find a paper you like, try searching for “<paper name> github” to see if the code has been released.

Kaggle: If there happens to be a Kaggle competition with a task similar to yours, this can be a great way to get high quality, state of the art models. Pay particularly close attention to winner blogs for past competitions, these often have great explanations and code. The little tricks that were used to win the competition can often be really valuable for your task as well.

Dataset benchmarks: If there’s a benchmark dataset that’s similar to the task you’re working on, the leaderboard for that benchmark is a quick way to find papers with demonstrably SOTA results.

Google: For standard/common tasks like image segmentation, searching for “image segmentation github”, “image segmentation pytorch” or “image segmentation tensorflow” will give you a lot of results.

Step 3: Read the papers

This can be intimidating because academic papers can be pretty inaccessible, even to experienced software engineers. But if you focus on the abstract, introduction, related work, results, and delay a lot of the deep details/math for later readings, you’ll find you can get a lot out of the paper and a deeper understanding of the problem.

Pay particularly close attention to the dataset(s) they use and the constraints of those datasets or their model. Often you’ll find that the constraints may not be applicable to you and are fundamental to the model design. For example, classification models for imagenet expect there to be one and only one salient object in an image. If your images have zero, one or more objects to identify, those models are probably not directly applicable. This isn’t something you want to find out after investing the time needed to bring up the model.

Also, follow some of the references, especially those you see in multiple papers! You’ll regularly find that at least one of the references provides a very clear description of the problem and dramatically increases your understanding. Referenced papers may also end up being more useful and may have nicer code associated with them, so it’s worth doing a little digging here.

Step 4: Make sure the code is usable

Once you’ve found a paper with open source code, make sure it’s usable. Specifically:

Check the license: While a lot of code is released under liberal open source licenses (MIT, BSD, Apache etc), some of it isn’t. You may find the model has a non-commercial use only license, or no license at all. Depending on your use case and company, the code may or may not be usable for you.

Check the framework: If you’re working with a particular framework (eg. Tensorflow, Pytorch) check the framework the model is built in. Most of the time you’re stuck with what you get, but sometimes there’s a reimplementation of the model in your preferred framework. A quick Google to check for this (eg. “<paper name> pytorch”) can save you a lot of trouble.

Check the language: Similarly, if the model is in Lua and you’re not a Lua developer, this can be really painful. See if there’s a reimplementation in the language of your choice (often Python, since in deep learning Python should be part of your repertoire), and if not you might be better off finding another model.

Check the coding style: Researchers aren’t all software engineers so you can’t have as high a bar as for other open source projects, but if the code is a total mess you may want to look for another model.

Step 5: Get the model running

Results from NVIDIA’s StyleGAN trained on a custom furniture dataset

Once you’ve found a model you think is a good fit, try to get the model running. The goal here is to run the training and inference loop for the model as-is, not to get it running on your specific dataset or to make any significant modifications. All you want to do is make sure that you have the right dependencies and that the model trains and runs as advertised. To that end:

Create a conda environment for the model: You may be trying out multiple models, so create a conda environment (assuming Python) for each model (nvidia-docker is another option here, but personally I find it to be overkill).

I’ll often set up my environments like so: conda create -n <name of the github repo> python=<same version of python used by the repo>

A quick way to figure out which version of python the repo is using is to look at the print statements. If there are no parens, it’s python 2.7, otherwise 3.6 should work.

Install the libraries: I highly recommend starting off by installing the exact same version of the framework that the original code used to start. If the model says it works with pytorch>0.4.0 , don’t assume it’ll work with pytorch 1.0. At this stage, you don’t want to be fixing those kinds of bugs, so start with pytorch=0.4.0 . You can install a particular version of a framework (eg. pytorch) with the command conda install pytorch=0.4.0 -c pytorch . A lot of code won’t have a requirements.txt file, so it may take some sleuthing and iterating to figure out all the libraries you need to install.

Get the original dataset and run the scripts: At this point, you should be able to download the original dataset and run the testing and training script. You’ll probably have to fix some paths here and there and use the README and source to figure out the correct parameters. If a pre-trained model is available, start with the testing script and see if you’re getting similar results to the paper.

Once you have the testing script running, try to get the training script up. You’ll probably have to work through various exceptions and make slight modifications to get it to work. Ultimately your goal with the training script is to see the loss decreasing with each epoch.

If it’s straightforward (ie. only requires changing some command-line flags), at this point you might try running training script on your own dataset. Otherwise, we’ll do this in step 7.

Step 6: Create your own testing notebook

At this point, you’ve confirmed that the model works and you have the right environment set up to be able to use it. Now you can dig in and start really playing with it. At this point, I recommend creating a Jupyter notebook, copy-pasting in the testing script, and then modifying till you can use it with a single item of data. For example, if you’re using an object detection model that finds dogs in an image, you want a notebook where you can pass it a picture and have it output the bounding boxes of the dogs.

The goal here is to get a feel for the inputs and outputs, how they must be formatted and how exactly the model works, without having to deal with the additional complexity of training or munging your own data into the right formats. I recommend doing this in a Jupyter notebook because I find that being able to see the outputs along each step is really helpful in figuring it out.

Step 7: Create your own training notebook with your dataset

Now that you have some familiarity with the model and data, it’s time to try to create a training notebook. Similar to step 6, I start by copying and pasting in the training script, separating it into multiple cells, and then modifying it to fit my needs.

If you’re already feeling comfortable with the model, you may want to go directly to modifying the training notebook so it works with your dataset. This may involve writing dataloaders that output the same format as the existing dataloaders in the model (or simply modifying those dataloaders). If you’re not yet comfortable enough to do that, start by just getting the training script to work as-is in the notebook and removing code that you don’t think is useful. Then work on getting it to work with your dataset.

Keep in mind the goal here isn’t to modify the model, even if it’s not quite solving the exact task you want yet. It’s just to get the model working with your dataset.

Step 8: Start modifying the model to suit your task!

By this point, you should have a notebook that can train the model (including outputting appropriate metrics/visualizations) and a notebook where you can test new models you create. Now is a good time to start to dig in and make modifications to the model (adding features, additional outputs, variations etc) that make it work for your task and/or dataset. Hopefully having the starting point of an existing state of the art model saved you a lot of time and provides better results than what you might get starting from scratch.

There’s obviously a lot happening in this step and you’d use all your existing model building strategies. However, below are some pointers that may be helpful specifically when building off of an existing model.

Modify the dataset before modifying the model: It’s often easier to munge your data into the format the model expects rather than modifying the model. It’s easier to isolate problems and you’re likely to introduce fewer bugs. It’s surprising how far you can sometimes push a model just by changing the data.

Reuse the pre-trained model as much as possible: If your model changes aren’t drastic, try to reuse the pre-trained model parameters. You may get faster results and the benefits of transfer learning. Even if you expand the model, you can often load the pre-trained parameters into the rest of the model (eg. use strict=False when loading the model in pytorch).

Make incremental changes and regularly check the performance: One benefit of using an existing model is you have an idea of the performance you started with. By making incremental changes and checking the performance after each one, you’ll immediately identify when you’ve made a mistake or are going down a bad path.

Ask for help: If you’re totally stuck, try reaching out to the author and asking for some pointers. I’ve found they’re often willing to help, but remember they’re doing you a favor and please act accordingly.

Automatically texturing a 3D model using neural renderer

Step 9: Attribute and Contribute

Depending on the license and how you’re distributing your model, you may be required to provide attribution to the developer of the original code. Even if it’s not required, it’s nice to do it anyway.

And please contribute back if you can! If you come across bugs and fix them during your own development, submit a pull request. I’m sure well-written bug reports are welcome. Finally, if nothing else, sending a quick thank you to the author for their hard work is always appreciated."
Version Control ML Model,"dvc workflow from https://dvc.org

Machine Learning operations (let’s call it mlOps under the current buzzword pattern xxOps) are quite different from traditional software development operations (devOps). One of the reasons is that ML experiments demand large dataset and model artifact besides code (small plain file).

This post presents a solution to version control machine learning models with git and dvc (Data Version Control).

And following features will come along with this solution:

ML models are persisted with scalability, security, availability, virtually unlimited storage space ML model are under version control i.e. any specific version of it can be conveniently tagged and accessed Auditability, transparency, reproducibility of ML model can be ensured by version controlling dataset, analytical code alongside it New experiemts can be initialized quickly and collaboratively based on the existing model setting

Git and DVC

The solution includes two layers of version control:

git: handles code and metadata (of dataset and model artifact) dvc: handles large dataset and model artifact

First we have the project folder prepared and the tools installed.



git clone

cd example-versioning # Download codegit clone https://github.com/iterative/example-versioning.git cd example-versioning

# install and initialize dvc ( # install and initialize git# install and initialize dvc ( https://dvc.org/doc/get-started/install

then connect dvc to backend storage, where dataset and model artifact will be actually stored (in this example AWS S3).

dvc remote add -d s3remote s3:// my-test-bucket-dvc /myproject

Now in the example-versioning folder, where our ML experiments will happen, should contain two subfolders for metadata.

.dvc/

.git/

Workflow

Next step, we train a model using data and script from dvc.org

cd example-versioning # Install dependencies

pip install -r requirements.txt

cd example-versioning

wget

unzip data.zip

rm -f data.zip # Download datacd example-versioningwget https://dvc.org/s3/examples/versioning/data.zip unzip data.ziprm -f data.zip # Build the ML model

python train.py

After getting the model (model.h5), put it under version control using dvc + git workflow.

step 1: add model metadata to dvc

dvc add model.h5

output:

It can be observed that:

the “real” model is stored under .dvc/cache/40

model metadata model.h5.dvc records where it is

step 2: persist model by pushing it to backend storage

dvc push model.h5.dvc

in s3, we can check the model is stored exactly under the instruction of model metadata

step 3: persist model metadata with git

It is the model metadata that can lead us to the real model object which is stored in backend storage. To prevent from losing the metadata, it should be added to version control using git.

git add .gitignore model.h5.dvc data.dvc metrics.json

git commit -m ""model first version, 1000 images""

git tag -a ""v1.0"" -m ""model v1.0, 1000 images""

“git tag” can be used here to record version of the model.

step 4: access the model anytime

It is easy to fetch a specific version of the model by searching for the tag on git branch. From git we can check out the model metadata.

git checkout tags/<tag_name> -b <branch_name>

Following the metadata we can find the model object and download it into current workspace with command

dvc pull model.h5.dvc

Conclusion

In a similar way, the problem of version controlling large dataset for machine learning experiment can also be solved. Other comparable tools to solve ML pipeline challenges are for example mlflow, datanami and sacred."
Gender Inference with Deep Learning,"Gender Inference with Deep Learning

Photo by Alex Holyoake on Unsplash

Summary

I wanted to build a model to infer gender from images. By fine-tuning the pretrained convolutional neural network VGG16, and training it on images of celebrities, I was able to obtain over 98% accuracy on the test set. The exercise demonstrates the utility of engineering the architecture of pretrained models to complement the characteristics of the dataset.

Task

Typically, a human can distinguish a man and a woman in the photo above with ease, but it’s hard to describe exactly why we can make that decision. Without defined features, this distinction becomes very difficult for traditional machine learning approaches. Additionally, features that are relevant to the task are not expressed in the exact same way every time, every person looks a little different. Deep learning algorithms offer a way to process information without predefined features, and make accurate predictions despite variation in how features are expressed. In this article, we’ll apply a convolutional neural network to images of celebrities with the purpose of predicting gender. (Disclaimer: the author understands appearance does not have a causative relationship with gender)

Tool

Convolution neural networks (ConvNets) offer a means to make predictions from raw images. A hallmark of the algorithm is the ability to reduce the dimensionality of images by using sequences of filters that identify distinguishing features. Additional layers in the model help us emphasize the strength of often nonlinear relationships between the features identified by the filters and the label assigned to the image. We can adjust weights associated with the filters and additional layers to minimize the error between the predicted and observed classifications. Sumit Saha offers a great explanation that is more in-depth: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53

There are a number of pretrained ConvNets that have been trained to classify a range of images of anything from planes to corgis. We can save computation time and overcome some sampling inadequacy by employing the weights of pretrained models and fine-tuning them for our purpose.

Dataset

The CelebA dataset contains over 200K images of celebrities labeled with 20 attributes including gender. The images are from the shoulders up, so most of the information is in the facial features and hair style.

Example image available from CelebA

Modeling

Feature Extraction

We’re going to use the VGG16 pretrained model and fine tune it to best identify gender from the celebrity images.

vgg=VGG16(include_top=False, pooling=’avg’, weights=’imagenet’,

input_shape=(178, 218, 3))

We use “include_top=False” to remove the fully connected layer designed for identifying a range of objects the VGG16 was trained to identify (e.g. apples, corgis, scissors), and we download the weights associated with the ImageNet competition.

Table 1 below shows the convolutional architecture for VGG16; there are millions of weights for all the convolutions that we can choose to either train or keep frozen at the pretrained values. By freezing all the weights of the model, we risk underfitting it because the pretrained weights were not specifically estimated for our particular task. In contrast, by training all the weights we risk overfitting because the model will begin “memorizing” the training images given the flexibility from high parameterization. We’ll attempt a compromise by training the last convolutional block:

# Freeze the layers except the last 5

for layer in vgg.layers[:-5]:

layer.trainable = False # Check the trainable status of the individual layers

for layer in vgg.layers:

print(layer, layer.trainable)

Table 1: Architecture of VGG16 model after turning final layers on

The first convolutional blocks in the VGG16 models are identifying more general features like lines or blobs, so we want to keep the associated weights. The final blocks identify more fine scale features (e.g. angles associated with the wing tip of an airplane), so we’ll train those weights given our images of celebrities.

Model Compilation

Following feature extraction by the convolutions, we’ll add two dense layers to the model that enable us to make predictions about the image given the features identified. You could use a single dense layer, but an additional hidden layer allows predictions to be made given a more sophisticated interpretation of the features. Too many dense layers may cause overfitting.

# Create the model

model = models.Sequential() # Add the VGG16 convolutional base model

model.add(vgg)



# Add new layers

model.add(layers.Dense(128, activation=’relu’))

model.add(layers.BatchNormalization())

model.add(layers.Dense(2, activation=’sigmoid’))

We added a batch normalization layer that will scale our hidden layer activation values in a way to reduce overfitting and computation time. The last dense layer makes predictions about gender (Table 2).

Table 2: Custom Model Architecture

Because we are allowing the model to train convolutional layers and dense layers, we’ll be estimating millions of weights (Table 3). Given the depth of the network we built, picking the best constant learning rate for an optimizer like stochastic gradient decent would be tricky; instead we’ll use the ADAM optimizer, that adjusts the learning rate to make smaller steps further into training.

model.compile(optimizer=’adam’, loss=’binary_crossentropy’, metrics=[‘accuracy’])

Using Keras, we’ll set up our data generators to feed our model, and fit the network to our training set.

data_generator = ImageDataGenerator(preprocessing_function=preprocess_input) train_generator = data_generator.flow_from_directory(

‘C:/Users/w10007346/Pictures/Celeb_sets/train’,

target_size=(178, 218),

batch_size=12,

class_mode=’categorical’) validation_generator = data_generator.flow_from_directory(

‘C:/Users/w10007346/Pictures/Celeb_sets/valid’,

target_size=(178, 218),

batch_size=12,

class_mode=’categorical’) model.fit_generator(

train_generator,

epochs=20,

steps_per_epoch=2667,

validation_data=validation_generator,

validation_steps=667, callbacks=cb_list)

After 6 epochs, the model achieved a maximum validation accuracy of 98%. Now to apply to the test set.

Testing

We have a test set of 500 images per gender. The model will give us predicted probabilities for each image fed through the network and we can simply take the maximum value of those probabilities as the predicted gender.

# obtain predicted activation values for the last dense layer

pred = saved_model.predict_generator(test_generator, verbose=1, steps=1000) # determine the maximum activation value for each sample

predicted_class_indices=np.argmax(pred,axis=1)

Our model predicted the gender of celebrities with 98.2% accuracy! That’s pretty comparable to human capabilities.

Does the model generalize to non-celebrities? Lets try on the author. The model did well with a recent picture of the author.

The predicted probability for the above image was 99.8% male.

The model also did well with the author’s younger, mop-head past; it predicted 98.6% male.

Conclusion

This exercise demonstrates the power of fine-tuning pretrained ConvNets. Each application will require a different approach to optimize the modeling process. Specifically, the architecture of the model needs to be engineered in a way that complements the characteristics of the dataset. Pedro Marcelino offers a great explanation of general rules for adapting the fine-tuning process to any dataset: https://towardsdatascience.com/transfer-learning-from-pre-trained-models-f2393f124751

I appreciate any feedback and constructive criticism on this exercise. The code associated with the analysis can be found on github.com/njermain"
"Domo Arigato, Misses Roboto","Hey Alexa, Tell Me a Joke!

I don’t remember what her response was, but I do remember being so excited with the new device that I posted an AMA (which is an acronym for Ask Me Anything) on Reddit. The headline was: I just got Alexa, type in what you want me to ask her!

For the next day, Redditors would send me questions to ask, I’d ask Alexa, and I’d reply to those Redditors with her response. Some of them might’ve been Alexa engineers themselves; they asked questions that gave Easter Egg answers, sort of like ordering from a secret menu (if you have Alexa, try asking her: Alexa, who is the fairest of them all?). That day, I also earned the highest amount of Karma I’d ever earned on Reddit (Karma is a points system on Reddit that is equivalent to likes on Facebook or Twitter). Thanks, Alexa.

Before long, I was purchasing smart bulbs and smart outlets, all switched on and off by Alexa and thus, via the transitive property (if A=B and B=C, then A=C), my voice. It was just too convenient compared to finding your phone, unlocking it, opening up the smart bulb app, and then manually adjusting lighting with your fingers. And while Alexa was already convenient throughout my home, she was even better in bed— I no longer needed to get up to turn off the lights before going to sleep.

A Human Connection"
Bootstrap sampling,"Whenever you are manipulating data, the very first thing you should do is investigating relevant statistical properties. In particular, you might be interested in knowing whether your data follow a known distribution.

Why is this important? Think about the goal of your data analysis: once you are provided with a sample of observations, you want to compute some statistics (i.e. mean, standard deviation…) as well as build confidence intervals and conduct hypotheses tests. To do so, you need to assume your data to be following a known distribution, such as Normal, X-square or T-student.

Unfortunately, most of the time your data are presented to you without having a known distribution, hence you don’t know the shape of their density function. Here Bootstrap sampling comes to aid: the aim of this technique is assessing stats and properties of a potential distribution without actually knowing its shape.

How does it work? Imagine you are provided with a set of data (your population) and you get a sample of size n of them.

Now, if you proceed with a re-sampling of your initial sample for B times (you can set B as large as you want. In general, it is set equal to or greater than 10000), you will generate B further samples, each with length n (with the possibility of one or more values to be repeated).

Now, for each sample, you can compute the estimation of the parameter you are interested in. It will be a generic function of each sample T(x^1*) and we will refer to it as θ̂1*.

Now, the idea is that, if we collect all the statistics we computed, we can generate an approximation of the probability function of our initial population. One standard choice for an approximating distribution is the empirical distribution function of the observed data.

In statistics, an empirical distribution function is the distribution function associated with the empirical measure of a sample. It is a cumulative distribution which jumps…"
The Little Robot that Lived at the Library,"The Little Robot that Lived at the Library

How we built an emotive social robot to guide library customers to books Minja Axelsson · Follow Published in Towards Data Science · 9 min read · Jul 25, 2019 -- 1 Listen Share

The Oodi library

Our team at Futurice designed and built a social robot to guide people to books at Helsinki’s new central library, Oodi. Opened in 2018, Oodi is the biggest of Helsinki’s 37 public libraries. It has 10,000 visitors a day, and an estimated 2 million visitors a year (compared to Finland’s 5.5 million population, that is a significant portion).

Automatic returns system

The MiR200 wagon moving books and their boxes

Oodi is big on automation and robotics. It has an automatic returns system: customers set their books on a conveyor belt, which brings the books to the basement, where they get sorted into boxes, which are picked up by a mobile MiR200 robot, which brings the books to the 3rd floor. At the 3rd floor, librarians place the books back on the shelves.

At the start of our project, we brainstormed how Oodi could use social robots: helping kids learn to read, instructing people on using equipment such as 3D printers, giving information about the library in several languages, and helping people find their way at the library.

We eventually settled on a robot that would help customers find the books and book categories they want. Since Oodi is so big, customers have a hard time getting around, and library employees spend a significant amount of time advising people how to find things. But this is not the work librarians are meant to be doing, or want to be doing. Librarians are very knowledgeable about literature. Their expertise is better used in in-depth service, helping visitors find specific books that fit their needs best. This type of work can take 30–40 minutes. In comparison, “Where is the psychology section?” takes 1–3 minutes to answer. Stacked together, a whole day of 1–3 minute tasks becomes tedious, and a waste of skills.

This is where the robot steps (or rather, rolls) in. A whole day of menial tasks would not bother a robot. We realized we could re-purpose the MiR200 mobile robots that the library already had, and was using to move books between the basement and the 3rd floor.

The robot design team: Oodi librarians, Oodi’s customers, and Futurice’s roboticists

The robot would have the advantage of being able to access Oodi’s database directly, and provide real-time information on which books are currently on the shelf. The robot could be more approachable to people who have social anxiety, and are afraid to approach library employees. Additionally, it could save both the customers’ time (no need to queue for a librarian), and the librarians’ time (who can help customers with more meaningful tasks).

First draft

A Mobile Robot with (the Illusion of) a Personality

The design team, consisting of Oodi’s librarians, Oodi’s customers, and Futurice’s roboticists, defined design guidelines for the robot that would be built on top of the MiR200 robot, using these social robot co-design canvases (available as open source):

The robot is sincerely a machine — it beeps expressively, and doesn’t talk

The robot has a touch-screen UI, and users don’t talk to the robot

The robot uses lights, sounds, and movement to communicate

The use of the robot should not depend on how familiar the user is with technology

The design needs to account for accessibility, the level of background noise, the library’s changing layout and furniture, and dodging customers

The design team decided that the robot should not be too humanoid. We wanted a more abstract form for the robot, with expressive, non-speaking forms of communication. We wanted a design with a bit of imagination and whimsy.

The team also wanted to make sure that the robot aligned with Oodi’s strategy and policies. The following ethical considerations were underlined:

GDPR (the EU’s data regulation) needs to be followed. Data about the person who looks for the book should not be combined with data about which book they were looking for.

Accessibility is important. The library’s principle is that everyone is served equally. Physical limitations, different languages, and impaired vision need to be taken into account.

The customer should be able to choose to be served by a human librarian.

If the robot is not working, it may cause frustration and rude behaviour by customers. This should be prepared for, so that librarians are not negatively affected.

We started testing the robot at the library, mapping out appropriate routes, and building the user journey. Luckily, we had some very excited testers."
Overloading Operators in Python,"…and a bit on overloading in general (but I’ll try not to overload you)

Most of us learning to program in Python run into concepts behind operator overloading relatively early during the course of our learning path. But, like most aspects of Python (and other languages; and, for that matter, pretty much anything), learning about overloaded operators necessarily ties into other concepts, both broadening the scope of topic and somewhat obfuscating the route through our individual learning curve. With that in mind, I will try to keep on-topic without pulling in too many other areas of learning Python- some Object-Oriented Programming naturally ties in, however; and, while I’d like to focus on overloading operators, the broader, more complex topic of overloading functions also warrants at least some mention.

Overloading, in the context of programming, refers to the ability of a function or an operator to behave in different ways depending on the parameters that are passed to the function, or the operands that the operator acts on [1]. In Python, operator overloading (also known as “operator ad-hoc polymorphism”) in particular is a form of ‘syntactic sugar’ that enables powerful and convenient ways to prescribe an operator’s, er, operations, as an infix expression of specified types. Said another way, operator overloading gives extended meaning to operators beyond their pre-defined operational meaning.

Operator Overload examples for + and *

The classic operator-overloading example in Python is the plus sign, a binary (i.e., two operands) operator that not only adds a pair of numbers, but also concatenates a pair of lists or strings. The asterisk is similarly overloaded as not only a multiplier for numbers, but also as a repetition operator for lists or strings. Comparison operators (such as >, ==, or !=) exhibit similar behavior; however, for all of these overloaded operators, we as Python users ought be somewhat careful when considering type checking. As MIT’s Professor John Guttag reminds us, “type checking in Python is not as strong as in some other programming languages (e.g., Java), but it is better in Python 3 than in Python 2. For example, it is pretty clear what < should mean when it is used to compare two strings or two numbers. But what should the value of ‘4’ < 3 be? Rather arbitrarily, the designers of Python 2 decided that it should be False because all numeric values should be less than all values of type str. The designers of Python 3 and most other modern languages decided that since such expressions don’t have an obvious meaning, they should generate an error message.”[2]

This is all well and good, but what if an operator is being used on one or more user-defined data types (i.e., from a created class) as an operand? In such a case- say, attempting to add a pair of (x, y) coordinates, as shown here- the compiler will throw an error since it doesn’t know how to add the two objects. And, while overloading can only be done on existing operators in Python, there are a handful of them, along with the corresponding magic method each of these operators invoke; using these corresponding methods, we can create/access/edit their inner workings (see end of article).

Like other nomenclature in this rapidly-evolving field, there doesn’t seem to be a consensus on what to call them- they’re somewhat commonly referred to as “magic methods”- called “magic” since they’re not invoked directly- and that seems to be closest to standard, perhaps since the alternative “special method” sounds, well, not so special. Some herald a more colorful moniker- “dunder methods” as a shorthand for “double-underscore methods” (i.e., “dunder-init-dunder”). Anyhow, they’re a special type of method, and are not only limited to the methods associated with operators (__init__() or __call__(), as examples); in fact, there are quite a few of them.

Just as a quick aside- printing has its own associated magic method, __str__(). If we were to print the plain Point class with just the lone __init__(), we would get the not-so-user-friendly output shown above.

Adding the __str__() method into the Point class will remedy that. Interestingly, format() also invokes the same __str__() method that print() does.

A simple * overloading example using our cohort’s nickname (3x like the Thundercats, Schnarf Schnarf)

It turns out that using (x, y) coordinates to walk through examples of overloading, methods, and other Python concepts is a somewhat common practice when learning Python, probably since we get to create our own class with something as mathematically familiar as coordinate points. From there, one can create a number of useful magic methods for one’s user-defined class and use them to overload operators.

A noteworthy aspect of operator overloading is the position of each operand in relation to its operator. Take the less than operator < as an example- it calls the __lt__() method for the first (or left/preceding) operand. In other words, the expression x < y is shorthand for x.__lt__(y); if the first operand is a user-defined class, it needs to have its own corresponding __lt__() method in order to be able to use “<”. That may seem like a nuisance, but it actually adds some handy flexibility for designing one’s classes, since we could customize what any operator’s function does for a class. “In addition to providing the syntactic convenience of writing infix expressions that use <,” Professor Guttag points out, “this overloading provides automatic access to any polymorphic method defined using __lt__(). The built-in method sort is one such method.”[2]

In light of this distinction between first and second operands, Python also provides us with a set of reverse methods, such as __radd__(), __rsub__(), __rmul__(), and so on. Keep in mind that these reverse methods are only called if the left operand does not support the corresponding operation and the operands are of different types. A Pythonista redditor named Rhomboid explains it way better than I ever could, so I humbly defer to his take:"
Machine Learning (kmeans clustering) in SparkML vs AWS SageMaker — My Two Cents,"Machine Learning, the ability to learn from data, has been one of the most successful and disruptive use-cases of Big Data. In the landscape of data and analytics, one has access to myriad of tool-set to undertake machine learning tasks of varying nature and complexity.

However when one is operating on data at scale, generally the traditional Machine Learning libraries in the go-to languages like R and Python (e.g. pandas, scikit-learn) fall short as these natively operate in a single machine where the data to be analyzed should fit within memory (though there are approaches like out-of-core learning that are available to circumvent that but it has its own set of caveats). Thus for true machine learning at scale, distributed training and inference pipelines become crucial. This by itself isn’t a trivial task but the availability of frameworks and libraries facilitate the process significantly. Libraries like Mahout (that operated on good old MapReduce) and SparkMLLIB (Spark’s RDD based Machine Learning library) were among the first players in this category and since then we have seen the advent of many other like. This trend has went in a few tangents as follows:

1. Maturity of Spark’s Machine Learning Library (SparkML) which now supports the versatile Dataframe/Dataset API and provides coverage to a number of algorithms and feature engineering transformations

2. Availability of libraries that work seamlessly with Spark e.g. Sparkling Water, DeepLearning4J, XgBoost4J, Stanford Core NLP Wrapper for Spark given the unprecedented growth of Spark being the leading distributed computation framework

3. Integration and compatibility of deep learning libraries like TensorFlow with other distributed computing frameworks like Apache Beam (e.g. TensorFlow Transform and is well supported in in Google Cloud Platform)

4. Availability of Cloud native machine learning frameworks and libraries e.g. SageMaker from AWS

Among all of these trends, the core driver had been to enable and democratize scalable machine learning so that organizations can focus more on the actual work instead of bogging down with the underlying complexity of how this all operates under the hood."
Build your first Machine Learning Model using TensorFlow,"Member-only story Build your first Machine Learning Model using TensorFlow Shadab Hussain · Follow 10 min read · Apr 22, 2019 -- Share

Welcome to this article where you will learn how to train your first Machine Learning model using TensorFlow and use it for Predictions! As the title suggests, this tutorial is only for someone who has no prior understanding of how to use a machine learning model. The only pre-requisite for this course is to have a basic understanding of Python programming language. I have tried to keep things simple here, and only introduced basic concepts of machine learning and neural network.

What is TensorFlow: TensorFlow is an end-to-end open-source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML-powered applications. Learn more about TensorFlow from here.

We will be using Google Colab for this demo. Google Colab is a free cloud service which can be used to develop deep learning applications using popular libraries such as Keras, TensorFlow, PyTorch, and OpenCV. To know more about Google Colab, click here.

Let’s jump straight to a sample problem we are going to solve using TensorFlow and Machine-Learning concepts.

Let’s assume that we are given the marketing budget spent (in thousands of dollars) by a media-services provider in the last 8 months along with the number of new subscribers (also in thousands) for the same time in the table given below:

Data Set

As you can see there is a trend or relationship between the amount spent and new subscribers gained. As the amount is increasing, the number of new subscribers is also increasing.

If you work out the Maths using theory of linear equation you will find out:

Subscribers gained = 2 * Amount Spent + 40

Our goal is to find this relationship between the amount spent on marketing and the number of subscribers gained using Machine-Learning technique.

Using the relationship found above we can also predict how many new subscribers can be expected…"
Is Julia the best language for quantitative finance?,"I have been working on quantitative intraday strategies during the last months. As a side result, I have tested workflows for similar tasks in Python, C, Fortran and Julia. Here are my findings.

The context

To give a background on the nature of the projects that have been tested I will begin clarifying that:

The projects are related to instruments trading (i.e. I design and simulate derivatives market algorithmic/quantitative strategies). I have not used Machine Learning or AI techniques in these strategies, just plain/vanilla statistics and simulation. Handled data sets are large but not huge, normally my simulations cover 30 million records data sets per asset/instrument, every data is used several times, and I do parametric and Monte Carlo analysis. This implies a large number of iterations. I am not an expert programmer and I am not interested in becoming one, I just want to focus on the market logic and the strategies that exploit profitable edges.

My quest is to find the right tool combination that performs well enough and simplify my workflow. Hence the review is based on the perspective of an end-user of these technologies.

This context has some implications:

I need a language that can deal easily and without efforts with large data sets. I need speed. I do not need that much speed to require multi-core or parallel processing. I do not need —at this time— Machine Learning or AI libraries.

This post is the outcome from the journey I have done to find an optimal workflow. It is a subjective but still informed view of each language strengths and weaknesses for this particular endeavour. I hope you find it useful and enjoyable.

The beginnings: Python and R

Approaching the field means that you will probably begin with Python or R, so did I.

R language has been the natural choice for doing statistics in the scientific/academic community well before Data Science term was coined. R is the open-source implementation of the S…"
"Implementation of RNN, LSTM, and GRU","Recurrent Neural Network

The Recurrent neural networks are a class of artificial neural networks where the connection between nodes form a directed graph along a temporal sequence. Unlike the feed-forward neural networks, the recurrent neural networks use their internal state memory for processing sequences. This dynamic behavior of the Recurrent neural networks allows them to be very useful and applicable to audio analysis, handwritten recognition, and several such applications.

Simple RNN implementation in Keras.

Mathematically the simple RNN can be formulated as follows:

Where x(t) and y(t) are the input and output vectors, Wᵢₕ, Wₕₕ, and Wₕₒ are the weight matrices and fₕ and fₒ are the hidden and output unit activation functions.

The implementation of RNN with 2 Simple RNN layers each with 32 RNN cells followed by time distribute dense layers for 10 class classification can be illustrated as follows:"
12 Things I Learned During My First Year as a Machine Learning Engineer,"Machine learning and data science are both broad terms. What one data scientist does can be very different to another. The same goes for a machine learning engineer. What’s common is using the past (data) to understand or predict (build models) the future.

To put the points below in context, I’ll explain what my role was.

We had a small machine learning consulting team. And we did it all, from data collection to manipulation to model building to service deployment in every industry you can think of. So everyone wore many hats.

The past tense is because I’ve since left my role as a machine learning engineer to work on my own business. I made a video about it.

What my day looked like

9am, I’d walk in, say the good mornings, put my food in the fridge, pour a cup of joe and walk over to my desk. Then I’d sit down, look at my notes from the previous day and open up Slack. I’d read the messages and open up any links to papers or blog posts the team had shared, there’d be a few, this field moves fast.

Once the messages were cleared, I’d skim through the papers and blog posts and read the ones which stuck. Usually there was something which may have helped with what I was working on. Reading took up to an hour, sometimes more, depending on what it was.

Why so long?

Reading is the ultimate meta-skill, if there was a better way of doing what I was doing, I could save time and effort by learning it and implementing it.

It’s now 10am.

If there was a deadline approaching, reading would be cut short to push forward on the project(s). That’s where the biggest chunk of the day went. I’d review my work from the previous day and check my notepad for next steps I put down.

My notepad was a flowing journal of the day."
Multiple Linear Regression — with math and code,"Photo by 贝莉儿 DANIST on Unsplash

Linear regression is a form of predictive model which is widely used in many real world applications. Quite a good number of articles published on linear regression are based on single explanatory variable with detail explanation of minimizing mean square error (MSE) to optimize best fit parameters. In this article, multiple explanatory variables (independent variables) are used to derive MSE function and finally gradient descent technique is used to estimate best fit regression parameters. An example data set having three independent variables and single dependent variable is used to build a multivariate regression model and in the later section of the article, R-code is provided to model the example data set.

Multivariate Regression Model

The equation for linear regression model is known to everyone which is expressed as:

y = mx + c

where y is the output of the model which is called the response variable and x is the independent variable which is also called explanatory variable. m is the slope of the regression line and c denotes the intercept. Usually we get measured values of x and y and try to build a model by estimating optimal values of m and c so that we can use the model for future prediction for y by giving x as input.

Practically, we deal with more than just one independent variable and in that case building a linear model using multiple input variables is important to accurately model the system for better prediction. Therefore, in this article multiple regression analysis is described in detail. Matrix representation of linear regression model is required to express multivariate regression model to make it more compact and at the same time it becomes easy to compute model parameters. I believe readers do have fundamental understanding about matrix operations and linear algebra. However, in the last section, matrix rules used in this regression analysis are provided to refresh the knowledge of readers.

Algebraic form of Linear Regression

Let’s say we have following data showing scores obtained by different students in a class. The scores are given for four exams in a year with last column being the scores obtained in the final exam. From data, it is understood that scores in the final exam bear some sort of relationship with the performances in previous three exams.

Here considering that scores from previous three exams are linearly related to the scores in the final exam, our linear regression model for first observation (first row in the table) should look like below.

152 = a×73 + b×80 + c×75 + d

Where a, b, c and d are model parameters.

The right hand side of the equation is the regression model which upon using appropriate parameters should produce the output equals to 152. But practically no model can be perfectly built to mimic 100% of the reality. Always, there exists an error between model output and true observation. Therefore, the correct regression equation can be defined as below:

152 = a×73 + b×80 + c×75 + d ×1+ e1

Where e1 is the error of prediction for first observation. Similarly for other rows in the data table, the equations can be written

185 = a×93 + b×88 + c×93 + d×1 + e2

180 = a×89+ b×91+ c×90 + d×1 + e3

196 = a×96+ b×98+ c×100 + d×1 + e4

………………………………………………..

………………………………………………..

192 = a×96+ b×93+ c×95+ d×1 + e25

Above equations can be written with help of four different matrices as mentioned below.

Using above four matrices, the equation for linear regression in algebraic form can be written as:

Y = Xβ + e

To obtain right hand side of the equation, matrix X is multiplied with β vector and the product is added with error vector e. As we know that two matrices can be multiplied if the number of columns of 1st matrix is equal to the number of rows of 2nd matrix. In this case, X has 4 columns and β has four rows.

Rearranging the terms, error vector is expressed as:

e = Y - Xβ

Now, it is obvious that error, e is a function of parameters, β. In the next section, MSE in matrix form is derived and used as objective function to optimize model parameters.

MSE in Matrix Form

MSE is calculated by summing the squares of e from all observations and dividing the sum by number of observations in the data table. Mathematically:

Replacing e with Y — Xβ in the equation, MSE is re-written as:

Expanding above equation as follows:

Above equation is used as cost function (objective function in optimization problem) which needs to be minimized to estimate best fit parameters in our regression model. Gradient needs to be estimated by taking derivative of MSE function with respect to parameter vector β and to be used in gradient descent optimization.

Gradient of MSE

As mentioned above, gradient is expressed as:

Where,∇ is the differential operator used for gradient. Using matrix. differentiation rules, we get following equations.

The above matrix is called Jacobian which is used in gradient descent optimization along with learning rate (lr) to update model parameters.

Gradient Descent Method

The formula for gradient descent method to update model parameter is shown below.

βold is the initialized parameter vector which gets updated in each iteration and at the end of each iteration βold is equated with βnew. lr is the learning rate which represents step size and helps preventing overshooting the lowest point in the error surface. The iteration process continues till MSE value gets reduced and becomes flat.

Illustration of gradient descent method. Source: http://www.claudiobellei.com/2018/01/06/backprop-word2vec/

Example Data

In this section, a multivariate regression model is developed using example data set. Gradient descent method is applied to estimate model parameters a, b, c and d. The values of the matrices X and Y are known from the data whereas β vector is unknown which needs to be estimated. Initially, MSE and gradient of MSE are computed followed by applying gradient descent method to minimize MSE.

R-code

Read data and initialize β:

dataLR <- read.csv(""C:\\Users\\Niranjan\\Downloads\\mlr03.csv"", header = T)

beta <- c(0,0,0,0) ## beta initialized

beta_T <- t(beta) X = matrix(NA,nrow(dataLR),ncol = 4) X[,1] <- dataLR$EXAM1

X[,2] <- dataLR$EXAM2

X[,3] <- dataLR$EXAM3

X[,4] <- 1 XT <- t(X)

y <- as.vector(dataLR$FINAL)

yT <- t(y)

Compute MSE and update β

mse <- (1/nrow(dataLR))* (yT%*%y - 2 * beta_T%*%XT%*%y + beta_T%*%XT%*%X%*%beta)

betanew <- beta - (lr *(2/nrow(dataLR)) * (XT%*%X%*%beta - XT%*%y))

Complete code for parameter estimation

##multivariate linear regression

dataLR <- read.csv(""C:\\Users\\Niranjan\\Downloads\\mlr03.csv"", header = T)

beta <- c(0,0,0,0)

beta_T <- t(beta) X = matrix(NA,nrow(dataLR),ncol = 4) X[,1] <- dataLR$EXAM1

X[,2] <- dataLR$EXAM2

X[,3] <- dataLR$EXAM3

X[,4] <- 1 XT <- t(X)

y <- as.vector(dataLR$FINAL)

yT <- t(y) iteration <- 1

lr = 0.00001 msef = NULL

while (iteration < 10) {

mse <- (1/nrow(dataLR))* (yT%*%y - 2 * beta_T%*%XT%*%y + beta_T%*%XT%*%X%*%beta)

betanew <- beta - (lr *(2/nrow(dataLR)) * (XT%*%X%*%beta - XT%*%y))

msef <- rbind(msef,mse)

beta <- betanew

beta_T <- t(betanew)

iteration <- iteration + 1

} plot(1:length(msef), msef, type = ""l"", lwd = 2, col = 'red', xlab = 'Iterations', ylab = 'MSE')

grid(nx = 10, ny = 10) print(list(a = beta[1],b = beta[2], c = beta[3], d = beta[4]))

Code for plotting output

library(plot3D)

ymod <- X%*%beta

scatter3D(dataLR$EXAM1,dataLR$EXAM2,dataLR$EXAM3, colvar = ymod,

pch = 17, cex = 2,bty = ""g"",ticktype = ""detailed"",phi = 0,lwd=2.5, xlab = ""Exam1"", ylab = 'Exam2',zlab = 'Exam3')

scatter3D(dataLR$EXAM1,dataLR$EXAM2,dataLR$EXAM3, colvar = dataLR$FINAL,

pch = 16, cex = 2,bty = ""g"",ticktype = ""detailed"",phi = 0,lwd=2.5, xlab = ""Exam1"", ylab = 'Exam2',zlab = 'Exam3',add = T) plot(dataLR$FINAL, ymod, pch = 16, cex = 2, xlab = 'Data', ylab = 'Model')

lines(ymod,ymod, lwd = 4, col = ""green"", lty = 6)

grid(nx = 10, ny = 10)

legend(""topleft"",c('Model-Data Points','Best fit line'), lty = c(NA,6), lwd = c(NA,4), col = c(""black"",""green""), pch = c(16,NA))

Output

The value of MSE gets reduced drastically and after six iterations it becomes almost flat as shown in the plot below. The corresponding model parameters are the best fit values.

Minimizing MSE:

MSE change with iterations

Optimized β:

Optimized model parameters

The computed final scores are compared with the final scores from data. Model efficiency is visualized by comparing modeled output with the target output in the data. Coefficient of determination is estimated to be 0.978 to numerically assess the performance of the model. The plot below shows the comparison between model and data where three axes are used to express explanatory variables like Exam1, Exam2, Exam3 and the color scheme is used to show the output variable i.e. the final score.

Comparison between model output and target in the data:

Visualization of model out and target in the data

Comparison between model output and target in the data

Basic Matrix Rules"
The One with all the FRIENDS Data Analysis,"The One with all the FRIENDS Data Analysis

The crew (source)

FRIENDS is one of my favourite shows (probably the favourite) and I’m sure I’m not alone in having rewatched the entire series more than once. I’ve always wondered if there was anything left to know about this oh-so familiar group. After seeing this post using R to look at the show, I thought I would give it a go myself. This post dives into the show’s scripts to find out more, including the most popular characters and their journey through the seasons. But first it will introduce methods to format and export text files into a SQLite database using Python. The text files used in this projects contain scripts from the T.V. show F.R.I.E.N.D.S. and was downloaded from this repository. Then provide some interesting findings about the characters we know so well, some expected and some surprising! It has been a really enjoyable hobby project and one I have been wanting to do for a while. Feel free to skip the coding bits and jump to the visualisations, hope you enjoy it!

Iterating Through Scripts

Each script is a text file containing some information about the episode, the title, writers and transcribers before the script actually starts. We need to find a way to turn a script into rows in a database and then work out how to do this for multiple scripts.

Example of a Script in the .txt Files

We will start trying to iterate through the scripts. They are stored in multiple text files and helpfully titled using the format season.episode . We can utilise the OS library in python to navigate through our text files.

Currently where all Scripts are Stored

As it stands, the code below will iterate through all the files in our scripts folder to obtain the filename . This filename is then split using the . separator and those numbers are stored in variables to be appended to the master list. The master_list is created as eventually we will want to store the results in a DataFrame.

Regular Expressions

Now we know how to move through our FRIENDS files, we need to see how to isolate the lines from each file. To do so I will be using regular expressions, the scripts are quite messy and all formatted differently depending on the transcriber. The pertinent pattern is character_name: speech however this can sometimes span multiple lines. Regular Expressions is like a really powerful ctrl-F , they are used to search for patterns in strings, a nice intro on can be found here. The aim of our regular expression is to match the space before our intended line as indicated by the pink dots. We aim to find this space as we can then split the whole file using these positions, giving us groups of character-speech pairs.

Purple dots indicate where the expression will match

The regular expression used is shown below. First we mathc the string before a colon \w+(?=:) , so now we have ""found"" the names of each character. However if we want to match the space before we must use \s . You can test it out for yourself, as you can see in the example, the regular expression also matches the space before the writers and transcribers, this will need to be removed after. Now we implement the regular expression in python. In the below code we are also able to split the character name and the speech.

This is combined with our loop in the previous section and the mater_array is converted to a pandas data frame:

Cleaning

Despite our best efforts, the results are still not 100% ready for analysis. Our first issue is that there are multiple names for each character, this can be seen by executing sorted(df['char'].unique()) , this will return a list of all unique values in the column. To rectify this takes some manual work which involves looking at the multiple spellings of a certain name, case sensitive! To change the names we use the pandas replace method:

Now we need to address the issues caused by our regular expression, as it caught the authors and transcribers. The format of these lines all end in by. Therefore the regular expression takes the last word before the colon as the character name. This means we can drop all of these rows by removing the character by. Bye by.

Written by

Transcribed by

Clean data is key! via gfycat

Sentiment

Sentiment analysis is on the table when dealing with strings, a more in-depth discussion can be found in this blog post. Similar methods are used, for each line in the database a sentiment score is calculated and stored in the line_sent column:

Export to SQL

Now this may not be a necessary step as most of the SQL commands we would be using could be done using pandas. However, I think sometimes altering different data frame scan sometimes get messy and SQL language may provide a ore readable way to access this data. Therefore we are now going to move the pandas dataframe into a SQL database. I am using DB Browser for SQLite.

Finally our scripts are formatted and placed in a SQL database. Data wrangling in this way can transform raw data into a more useful data set. Even though we are not adding too much to the data set, the different organisational structure can enable a wider breadth of analysis. Now we have the scripts formatted in this way, we can utilise SQL to gain further insights into the show as carried out in this article.

via tenor

The Most Popular Friend

This section looks at each character’s role in the show. The previous post walked through the process of putting the data into a SQL database. This was in order to make a query like “who had the most number of lines during the whole series” fairly simple:

Rachel just edges the top spot with 9294 lines over the entire series Ross coming in a very close second (9070), both averaging around 39-ish lines per episode. This isn’t entirely a shock, as they were both the main plot throughout 10 seasons. Almost inseparable are Monica and Chandler, 8403 and 8398 respectively.

A look at the number of lines breakdown throughout the series confirms this pattern, we can see Ross and Rachel dominating the lines until around Season 4. This is when the London episodes happen and Chandler and Monica have a bigger joint story, translating in more lines. I think it is a shame Phoebe never got more lines, staying rooted at around 800 lines per season. Rachel did say it:

Ugh, it was just a matter of time before someone had to leave the group. I just always assumed Phoebe would be the one to go. — Rachel 5.05

Most Spoken About

via giphy

Being the one doing the most talking does not necessarily mean you’re the most popular, so now we will take a look at who’s talked about the most. This is a pretty difficult task to accurately capture all mentions of each character. A possible solution is a list of nicknames for each character (let me know if I have missed any out!). It’s pertinent to note, this is the method we will use to find any reference to each character throughout this post, using the nicknames detailed below.

In order to get the count, we first iterate through the characters, keeping a count of the mentions. Using a nested for-loop to get each characters nickname, we use the pandas count() method to keep a tally of the number of mentions.

When using only full names, Ross is the most mentioned. “Chan”, “Joe”, “Mon” and “Rach” are all mentioned more than their full names. This supports the decision to include the nicknames but does also highlight how sensitive the results are to picking the right names.

Words

Catchphrases

There are a few running catchphrases, for example “Smelly Cat “ was mentioned 37 times throughout the whole show. The infamous “We were on a break” line was referred to 17 times. And Joey’s pick up line “How you doin’” was said 37 times.

Largest Vocabulary

via tenor

Another interesting aspect to look at is the lexicon of words each character uses. This is done by first selecting all the lines said by the main characters as shown above. After which all non alphabetical characters are removed. Every line by each characters is then split into words (using the space in between to split) and added to a set. A set allows no repeated values which is perfect for our use in this case.

Unsurprisingly Ross tops the list his passion for dinosaurs is a running joke throughout the series. Despite his career, starting off at the New York Museum of Prehistoric History and then professor at New York University, some real-life paleontologists aren’t convinced. I’m sure I’m not the only one surprised to see Joey in not-last-place. Given the role’s stereotypical caricature it appears Joey does have a couple of words up his sleeve, even if they are made up!

How you Doin’?

As we have calculated a sentiment score for each line, we are able to monitor this score throughout the course of a season.

The chart above shows tracks the sentiment for Rachel and Ross throughout the first 2 seasons. Total sentiment score per episode is calculated, as the scores range in-between -1 to 1 the total will give an indication of the majority of sentiment throughout a particular episode.

Episode 104 is where Rachel gets her first paycheck, may be the cause of such positive sentiment as is episode 117 with a guest appearance from George Clooney. Ross really experiences the highs and lows throughout the first episodes, finding out he was having a boy in episode 112 before saying bye to marcel in episode 121. Before finally, both characters show a spike on episode 207, The One where Ross Finds Out and a conflicted Ross finds out Rachel has feelings for him. This may be why Ross’ overall sentiment for that episode was “muted but positive”.

Networks

So far we have mostly looked at out FRIENDS isolation, here we will see how they interact. Looking at how many times a character mentions another characters name the show so we can draw networks relating each character to another. The table below shows the results; read from left to right tells us that Rachel mentioned herself 187 times and mentioned Joey the most: 739 times. Read from top to bottom can be understood as Rachel mentioned Chandler 321 times, Ross mentioned him 332 times and his wife (Monica) mentioned him the most: 622.

The table throws up some interesting findings, Rachel was mentioned the most by Ross (622, and one cost him his marrige ) and Ross was mentioned by Rachel the most: 550. Interestingly, although Monica says chandler the most, Chandler says Joey the most.

via gfycat

The table does provide some insight but it isn’t the most ascetically pleasing way to look at the findings. So we can create a chord diagram using this fucntion provided on Github. The size of the chords for each characters section represents how many times they said the connecting characters name. In other words, if you were to read the values from left to right in the table, that is what each characters portion shows. This makes it clearer just how much both Joey and Monica occupy Chandler’s mentions by looking at the pink slice.

Graph and Centrality

So we have now built a network of FRIENDS we can calculate a centrality score for each of them. Centrality aims to answer the question: Who is the most important or central person in this network?. Obviously this is a subjective question depending on the definition of importance. Before we define our measure of importance, we must first convert our table into a graph. We will use network x to create a directed, weighted graph using the values in the table above (stored in network_data ). Nodes are the characters and the weights are the number of mentions. We can also check the graph has been created correctly by checking the edge weights between nodes.

out: {'weight': 426} # yay! it matches our table

Now we have created our graph, we calculate the Eigenvector Centrality as a measure of importance (used in Google’s page rank). This algorithm aims quantify influence of people in a social network, based on connections with important people. In this case we are defining “importance” as connections with important people. With an emphasis on links with other people, it is easy to see how this may be applied to other larger networks such as Twitter. Using “interactions” (retweets and likes) as weights, this algorithm may be able to give you the most connected accounts in a network, potentially gaining more insight than a count of the highest number of followers. Valuable information for anyone looking to gauge (or alter) public opinion.

Networkx makes life easy, apply the eigenvector_centrality_numpy method and define the weights to calculate the scores for each node. The result in order of importance is shown below. I was surprised upon initially looking at the results, however when I thought about the measure it started to make sense. I think Joey could be seen as the glue of the group, always interacting with the other characters. To see Ross and Rachel at the lower end isn't entirely surprising given that they occupy most of each others time. This post hasn't been great for Phoebe 🙁 ​ .These results are subjective, as as is the interpretation and I would love to hear what you think about the centrality scores.

I hope you enjoyed this alternative view on the popular show. Whilst I understand FRIENDS may not be everyone’s cup of tea I do think this kind of analysis can be applied to almost any long running series. Maybe you could try out something similar for your favourite show and let me know what you find!

Thanks for reading 🙂"
Why Nobody Cares About Your Data Science Project,"The Hedgehog and the Fox

Recently, while reading ‘On Grand Strategy’ by John Lewis Gaddis, I came across the philosopher Isaiah Berlin’s work. Gaddis’ use of the classification framework of foxes and hedgehogs based on Isaiah Berlin’s 1953 book The Hedgehog and the Fox: An Essay on Tolstoy’s View of History struck me as particularly interesting. In his book, Berlin builds on a fragment attributed to Greek philosopher Archilochus, who supposedly stated that “a fox knows many things, but a hedgehog one important thing.” Expanding upon this quote, Berlin suggests that writers and thinkers could potentially be classified as either foxes or hedgehogs. Foxes, according to Berlin, have a wide variety of interests and rely on a great number of experiences and sources for their decision-making. While expounding on the traits of foxes in greater detail, Berlin names Aristotle and Goethe as potential examples of thinkers with fox-like characteristics. Hedgehogs, on the other hand, tend to base their interpretation of the world on a single, defining idea. To illustrate his point, Berlin lists thinkers such as Plato and Nietzsche as thinkers with hedgehog-like traits.

Data Science Projects in Big Organizations

While at first glance, Berlin’s framework may offer relatively little value for data science projects, examining the core idea behind it reveals an entirely different picture. Due to the recent boost in the popularity of data science, many organizations that are not primarily invested in the technology industry have started hiring data scientists. However, the circumstances that these data scientists find themselves in are profoundly different from those in tech companies. For one, the supervisor in charge of the data science team might very well not have a data science or analytics background at all. As a result, the data science team will have a much harder time communicating its proposed projects to management. Besides the people in charge of the budget not fully understanding the data science teams’ needs, the data scientists might also find themselves confronted with a significant amount of skepticism towards their projects from the general workforce of large organizations. Whilst there could be many drivers behind this skepticism, the most prevalent ones generally stem from a lack of understanding of what the data science team actually does. Organizations whose business has little to do with tech generally have a workforce that perhaps knows how to utilize Microsoft Office (if you are lucky). Thus, the knowledge gap between data scientists and the remaining workforce is quite significant from the get-go. Adding to this confusion, in my opinion, is labeling everything as ‘AI’. Most people, when confronted with the term ‘Artificial Intelligence’ immediately think of robots that have the same capabilities as human beings or other science fiction figments. In doing so, people build up a natural hesitance towards everything that the data science team does, assuming it to be too complicated to understand and thus not worthy of investing their time into it in the first place.

Should a Data Scientist Be a Hedgehog or a Fox?

The classification framework proposed by Berlin relates to the challenges data science teams find themselves confronted with in several ways. In the following paragraphs, I will attempt to extend Berlin’s framework to data scientists in large organizations by applying the general concepts to specific challenges in the daily life of data scientists.

Let us first examine what characteristics might lead to a data scientist being classified as a hedgehog. In my opinion, a data scientist could qualify as a hedgehog if he or she exhibits an unrelenting focus on their own motivations. When proposing a project, he or she is not concerned with the opinions and worries of non-technical stakeholders (who might actually be the ones in charge of the budget) but only with her own fascination with this particular project. While there is absolutely nothing wrong with being captivated by the technical aspects of a project, quite the contrary, trying to pursue data science projects using this mindset will cause tremendous problems in non-tech organizations. By only being able to view the project from the point of view of a data scientist, you will very quickly lose sight of the thoughts and concerns of people in charge of the budget. In communicating with management, especially at non-tech organizations, project approval is highly correlated with management being able to see how your project creates value for the organization. Value, in most cases, is either a cost reduction or an increase in revenue generated. Therefore, much of the difficulties that a data scientist hedgehog encounters directly relate to the ability to approach issues from various angles and being capable of framing your project proposals accordingly.

In contrast to hedgehogs, foxes rely on a plentitude of inputs to help guide them. Intuitively, this seems like the more beneficial of the two relating to data science after having established that a narrow focus on technical aspects can bring about various issues. Nevertheless, only being a fox in data science does not necessarily have to be better than only being a hedgehog. While strictly focusing on your point of view can be harmful to getting support for your project, trying to incorporate everybody else’s opinions into your project can doom the project’s success just as much. Assuring everyone around you that you will adapt your project according to their needs might simplify securing funding for the project, however, it might contort your project to the point where it is an entirely different project. In my opinion, the issue of having to pitch a project to someone not familiar with data science at all is no best solved by almost completely giving up on your input and instead purely relying on the opinions of people that do not have as deep of an understanding of the matter at hand as you do. In that case, I think being able to stand your ground when it comes to the core concept of the project is of great importance.

Conclusion

All in all, it seems to me that having the ability of a hedgehog to maintain focus on your final goal is crucial in order to get your data science project done. At the same time, not getting caught up in your personal perceptions and way of thinking but instead trying to understand where other stakeholders are coming from and how to best approach them is equally important to me. I am not sure whether this conclusion fits the framework proposed by Berlin, however, Berlin himself is reported to have never wanted people to take this classification too seriously. Nonetheless, the hedgehog and fox framework can provide a stimulus for an important discussion.

If you have any thoughts on the article, feel free to reach out to me. I would be happy to hear and discuss your opinions."
Exploratory Data Analysis of the FIFA 19 Dataset in Python,"In this post we will perform simple exploratory data analysis of the FIFA 19 data set. The data set can be found on Kaggle. FIFA is the Fédération Internationale de Football Association and FIFA 19 is part of the FIFA series of association football video games. It is one of the best selling video games of all time selling over 260 million copies to date.

For this analysis we will be using the python pandas library, numpy, seaborn and matplotlib. The dataset contains 89 columns but we will limit our analysis to the following ten columns:

Name — Name of Player Age — Age of Player Nationality — Nationality of Player Value — Current Market Value Wage — Wage of Player Preferred Foot — Preferred foot of player Height — Height of Player Weight — Weight of player Position — Position on the pitch Overall — Player’s Overall Rating

Let’s get started!

First let’s import the pandas library and read the csv file into a pandas dataframe and print the first five rows for the first sixteen columns:

import pandas as pd df = pd.read_csv(""data.csv"") print(df.head())

We can filter the dataframe so it only includes the ten columns we want:

df = df[['Name', 'Age', 'Nationality', Value', 'Wage', 'Preferred Foot', 'Height', 'Weight', Position', 'Overall']]

print(df.head())

First thing we can do is generate some statistics on the height column. The values are given as strings so we will want to convert them to a number we can use in calculations. Here we convert the height strings into centimeter…"
Understanding PCA (Principal Components Analysis),"Member-only story Understanding PCA (Principal Components Analysis)

We Discover How Principal Components Analysis Helps Us Uncover the Underlying Trends in Our Data Tony Yiu · Follow Published in Towards Data Science · 10 min read · Jul 6, 2019 -- 11 Share

In data science and finance (and pretty much any quantitative discipline), we are always sifting through a lot of noise in search of signal. Now if only, there were an algorithm that could do that for us…

There is! Today we will explore how PCA (Principal Components Analysis) helps us uncover the underlying drivers hidden in our data — a super useful feature as it allows us to summarize huge feature sets using just a few principal components.

If you are interested in the code that I used to generate the charts below, you can find it on my GitHub here.

Variance is Both Our Enemy and Our Friend

If you have spent some time reading statistics or data science textbooks, you will notice that the main reason we go through all the trouble of building models is to explain variance.

But what does that really mean? Let’s unpack this step by step. First, what do we actually mean by variance? Imagine that our data looks like this:

A Flat Line Has Zero Variance

You are thinking, “Tony why are you showing me a flat line? This is so boring.” And that is exactly the point. If our data was just a flat line, it would be very easy to predict (just predict five all the time) but also completely uninteresting. A flat line is an example of data with zero variance — there is absolutely no vertical variation in the data.

What is an example of zero variance data in real life? It sounds ridiculous but let’s pretend your boss told you to predict the number of floors in a five story building. So every day for 100 days you measure the number of floors of the building in question and at the end you get the chart above, a straight line. When your boss comes back and asks for your prediction, you say with confidence “I predict that tomorrow the building will still be five floors tall!” Rocket science right?"
A Beginners Introduction into MapReduce,"Many times, as Data Scientists, we have to deal with huge amount of data. In those cases, many approaches won’t work or won’t be feasible. A massive amount of data is good, it’s very good, and we want to utilize as much as possible.

Here I want to introduce the MapReduce technique, which is a broad technique that is used to handle a huge amount of data. There are many implementations of MapReduce, including the famous Apache Hadoop. Here, I won’t talk about implementations. I’ll try to introduce the concept in the most intuitive way and present examples for both toy and real-life examples.

Let’s start with some straightforward task. You’re given a list of strings, and you need to return the longest string. It’s pretty easy to do in python:

def find_longest_string(list_of_strings):

longest_string = None

longest_string_len = 0 for s in list_of_strings:

if len(s) > longest_string_len:

longest_string_len = len(s)

longest_string = s return longest_string

We go over the strings one by one, compute the length and keep the longest string until we finished.

For small lists it works pretty fast:

list_of_strings = ['abc', 'python', 'dima'] %time max_length = print(find_longest_string(list_of_strings)) OUTPUT:

python

CPU times: user 0 ns, sys: 0 ns, total: 0 ns

Wall time: 75.8 µs

Even for lists with much more than 3 elements it works pretty well, here we try with 3000 elements:

large_list_of_strings = list_of_strings*1000 %time print(find_longest_string(large_list_of_strings)) OUTPUT:

python

CPU times: user 0 ns, sys: 0 ns, total: 0 ns

Wall time: 307 µs

But what if we try for 300 million elements?

large_list_of_strings = list_of_strings*100000000

%time max_length = max(large_list_of_strings, key=len) OUTPUT:

python

CPU times: user 21.8 s, sys: 0 ns, total: 21.8 s

Wall time: 21.8 s

This is a problem, in most applications, 20 seconds response time is not acceptable. One way to improve the computation time is by buying a much better and faster CPU. Scaling your system by introducing better and faster hardware is called “Vertical Scaling”. This, of course, won’t work forever. Not only it’s not so trivial to find a CPU that work 10 times faster, but also, our data will probably get bigger, and we don’t want to upgrade our CPU every time the code gets slower. Our solution is not scalable. Instead, we can do “Horizontal Scaling”, we’ll design our code so it could run in parallel, and it will get much faster when we’ll add more processors and/or CPUs.

To do that, we need to break our code into smaller components and see how we can execute computations in parallel. The intuition is as follows: 1) break our data into many chunks, 2) execute the find_longest_string function for every chunk in parallel and 3) find the longest string among the outputs of all chunks.

Our code is very specific and it hard to break and modify, so instead of using the find_longest_string function, we’ll develop a more generic framework that will help us perform different computations in parallel on large data.

The two main things we do in our code is computing the len of the string and comparing it to the longest string until now. We’ll break our code into two steps: 1) compute the len of all strings and 2) select the max value.

%%time # step 1:

list_of_string_lens = [len(s) for s in list_of_strings]

list_of_string_lens = zip(list_of_strings, list_of_string_lens) #step 2:

max_len = max(list_of_string_lens, key=lambda t: t[1])

print(max_len) OUTPUT:

('python', 6)

CPU times: user 51.6 s, sys: 804 ms, total: 52.4 s

Wall time: 52.4 s

(I’m calculating the length of the strings and then zip them together because this is much faster than doing it in one line and duplicating the list of strings)

In this state, the code runs actually slower than before because instead of performing a single pass on all of our strings, we do it 2 times, first to compute the len and then to find the max value. Why it is good for us? because now our “step 2” gets as input not the original list of strings, but some preprocessed data. This allows us to execute step two using the output of another “step two”s! We’ll understand that better in a bit, but first, let’s give those steps a name. We’ll call “step one” a “mapper” because it maps some value into some other value, and we’ll call “step two” a reducer because it gets a list of values and produces a single (in most cases) value. Here’re two helper functions for mapper and reducer:

mapper = len def reducer(p, c):

if p[1] > c[1]:

return p

return c

The mapper is just the len function. It gets a string and returns its length. The reducer gets two tuples as input and returns the one with the biggest length.

Let’s rewrite our code using map and reduce , there are even built-in functions for this in python (In python 3, we have to import it from functools ).

%%time #step 1

mapped = map(mapper, list_of_strings)

mapped = zip(list_of_strings, mapped) #step 2:

reduced = reduce(reducer, mapped) print(reduced) OUTPUT:

('python', 6)

CPU times: user 57.9 s, sys: 0 ns, total: 57.9 s

Wall time: 57.9 s

The code does exactly the same thing, it looks bit fancier, but also it is more generic and will help us parallelize it. Let’s look more closely at it:

Step 1 maps our list of strings into a list of tuples using the mapper function (here I use the zip again to avoid duplicating the strings).

Step 2 uses the reducer function, goes over the tuples from step one and applies it one by one. The result is a tuple with the maximum length.

Now let's break our input into chunks and understand how it works before we do any parallelization (we’ll use the chunkify that breaks a large list into chunks of equal size):

data_chunks = chunkify(list_of_strings, number_of_chunks=30) #step 1:

reduced_all = []

for chunk in data_chunks:

mapped_chunk = map(mapper, chunk)

mapped_chunk = zip(chunk, mapped_chunk)



reduced_chunk = reduce(reducer, mapped_chunk)

reduced_all.append(reduced_chunk)



#step 2:

reduced = reduce(reducer, reduced_all) print(reduced) OUTPUT:

('python', 6)

In step one, we go over our chunks and find the longest string in that chunk using a map and reduce. In step two, we take the output of step one, which is a list of reduced values, and perform a final reduce to get the longest string. We use number_of_chunks=36 because this is the number of CPUs I have on my machine.

We are almost ready to run our code in parallel. The only thing that we can do better is to add the first reduce step into a single the mapper. We do that because we want to break our code into two simple steps and as the first reduce works on a single chunk and we want to parallelize it as well. This is how it looks like:

def chunks_mapper(chunk):

mapped_chunk = map(mapper, chunk)

mapped_chunk = zip(chunk, mapped_chunk)

return reduce(reducer, mapped_chunk) %%time data_chunks = chunkify(list_of_strings, number_of_chunks=30) #step 1:

mapped = map(chunks_mapper, data_chunks) #step 2:

reduced = reduce(reducer, mapped) print(reduced) OUTPUT:

('python', 6)

CPU times: user 58.5 s, sys: 968 ms, total: 59.5 s

Wall time: 59.5 s

Now we have a nice looking two steps code. If we’ll execute it as is, we’ll get the same computation time, but, now we can parallelize step 1 using the multiprocessing module simply by using the pool.map function instead of the regular map function:

from multiprocessing import Pool pool = Pool(8) data_chunks = chunkify(large_list_of_strings, number_of_chunks=8) #step 1:

mapped = pool.map(mapper, data_chunks) #step 2:

reduced = reduce(reducer, mapped) print(reduced) OUTPUT:

('python', 6)

CPU times: user 7.74 s, sys: 1.46 s, total: 9.2 s

Wall time: 10.8 s

We can see it runs 2 times faster! It’s not a huge improvement, but the good news is that we can improve it by increasing the number of processes! We can even do it on more than one machine, if our data is very big, we can use tens or even thousands of machines to make our computation time as short as we want (almost).

Our architecture is built using two functions: map and reduce . Each computation unit maps the input data and executes the initial reduce. Finally, some centralized unit executes the final reduce and returns the output. It looks like this:

This architecture has two important advantages:

It is scalable: if we have more data, the only thing we need to do is to add more processing units. No code change needed! It is generic: this architecture supports a vast variety of tasks, we can replace our map and reduce function with almost anything and this way computer many different things in a scalable way.

It is important to note that in most cases, our data will be very big and static. It means the breaking into chunks every time is inefficient and actually redundant. So in most applications in real life, we’ll store our data in chunks (or shards) from the very beginning. Then, we’ll be able to do different computations using the MapReduce technique.

Now let's see a more interesting example: Word Count!

Say we have a very big set of news articles and we want to find the top 10 used words not including stop words, how would we do that? First, let's get the data:

from sklearn.datasets import fetch_20newsgroups

data = news.data*10

For this post, I made the data x10 larger so we could see the difference.

For each text in the dataset, we want to tokenize it, clean it, remove stop words and finally count the words:

def clean_word(word):

return re.sub(r'[^\w\s]','',word).lower() def word_not_in_stopwords(word):

return word not in ENGLISH_STOP_WORDS and word and word.isalpha()





def find_top_words(data):

cnt = Counter()

for text in data:

tokens_in_text = text.split()

tokens_in_text = map(clean_word, tokens_in_text)

tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)

cnt.update(tokens_in_text)



return cnt.most_common(10)

Let’s see how much time does it take without MapReduce:

%time find_top_words(data) OUTPUT: [('subject', 122520),

('lines', 118240),

('organization', 111850),

('writes', 78360),

('article', 67540),

('people', 58320),

('dont', 58130),

('like', 57570),

('just', 55790),

('university', 55440)] CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s

Wall time: 51.7 s

Now, let’s write our mapper , reducer and chunk_mapper :

def mapper(text):

tokens_in_text = text.split()

tokens_in_text = map(clean_word, tokens_in_text)

tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)

return Counter(tokens_in_text) def reducer(cnt1, cnt2):

cnt1.update(cnt2)

return cnt1 def chunk_mapper(chunk):

mapped = map(mapper, chunk)

reduced = reduce(reducer, mapped)

return reduced

The mapper gets a text, splits it into tokens, cleans them and filters stop words and non-words, finally, it counts the words within this single text document. The reducer function gets 2 counters and merges them. The chunk_mapper gets a chunk and does a MapReduce on it. Now let’s run using the framework we built it and see:

%%time data_chunks = chunkify(data, number_of_chunks=36) #step 1:

mapped = pool.map(chunk_mapper, data_chunks) #step 2:

reduced = reduce(reducer, mapped) print(reduced.most_common(10)) OUTPUT:

[('subject', 122520),

('lines', 118240),

('organization', 111850),

('writes', 78360),

('article', 67540),

('people', 58320),

('dont', 58130),

('like', 57570),

('just', 55790),

('university', 55440)] CPU times: user 1.52 s, sys: 256 ms, total: 1.77 s

Wall time: 4.67 s

This is 10 times faster! Here, we were able to really utilize our computational power because the task is much more complex and requires more.

To sum up, MapReduce is an exciting and essential technique for large data processing. It can handle a tremendous number of tasks including Counts, Search, Supervised and Unsupervised learning and more. Today there’s a lot of implementations and tools that can make our lives much more comfortable, but I think it is very important to understand the basics."
Identifying the Sources of Winter Air Pollution in Bangkok Part II,"Mae Fah Luang University Campus on March 2019. (Photo by MFU Photoclub with permission)

In the previous blog, I looked at the winter air pollution in Bangkok. The main source of pollution comes from particles smaller than 2.5 micrometer (PM 2.5 particles). These particles are smaller than the width of a human hair and can easily enter our bodies, even making their way into our blood. Last week (March 17, 2019), many provinces in the northern part of Thailand had the worst Air Quality Index (AQI) in the world due to particle pollution. So far, no long term solution has been proposed because the source of the PM 2.5 particle pollution has not been clearly pinpointed. In this notebook, I identify the sources of high PM 2.5 particles in Bangkok through a machine learning model. The code can be found in my GitHub page.

High PM2.5, Who Are the Culprits ?

There are three major theories regarding the source of air pollution in Bangkok: (1) The temperature inversion effect where cold air along with pollution is trapped close to the surface of the Earth. This theory was proposed by the government at the beginning of the 2019 winter season. The government blamed emission from old diesel engines for the pollution. (2) Agricultural burning, either locally or from surrounding provinces. During winter, a lot of open agricultural burning occurs throughout the country. Some officials have tried to tackle the air pollution problem by reducing open agricultural burning. (3) Pollution from other provinces or countries. Some NGOs blamed the pollution on near by power plants.

My analysis procedure is as follows: Build a machine learning model(ML) to predict the air pollution level in Bangkok using environmental factor such as weather, traffic index, and fire maps. Include date-time features such as local hour, and weekday versus weekend in the model to capture other effects from human activities. Identify dominant sources of pollution using the feature of importance provided by the ML model.

If the source of the pollution is local, then the AQI will depend on factors such as weather patterns (wind speed, humidity, average temperature), local traffic, and hour of day. If the pollution is from agricultural burning, the AQI will depend on active fires with some time lag to account for geographical separation. Fire activities are included based on the distance from Bangkok. On the other hand, if the pollution not correlated with the fire map, then the model should put more weight on weather patterns, such as wind direction and wind speed.

Here are a list of features I considered and their data sources:

Active fire information from NASA’s FIRMS project

Weather pattern: temperature, wind speed, humidity, and rain, scraped from the Weather Underground website

Traffic index from Longdo Traffic

Date time features: hour of day, time of day, and holiday patterns (explored in the Part I blog post)

Let me first walk through all the features included in the model.

Agricultural Burning is a Major Problem !

Farmers in Southeast Asia pick January — March as their burning season. For the north and northeastern provinces in Thailand, these burning activities are large enough to make these provinces among the most polluted places in the world during this time. For Bangkok, one might argue that because the region is heavily industrial rather than agricultural, it may not be affected as much by agricultural burning. But this is not the case.

Because of the tiny size of PM 2.5 particles, they remain suspended in the atmosphere for prolonged periods and can travel over very long distances. From the weather data, the average wind speed is 10 km/hour. The reported PM 2.5 level is a rolling average over 24 hours. A rough estimate is that the current PM 2.5 reading may be from sources as far as 240 km away. The picture below shows the fire map measured by NASA’s satellites, indicative of agricultural burning, on Jan 8, 2018 and on Feb 8, 2018. The yellow circle indicates the area within 240 km of Bangkok. The number of fires on Jan 8, which has an acceptable level of pollution, is much lower than the number of fires on Feb 8, which has an unhealthy level of pollution.

Fire spots from NASA’s satellites

In fact, the fire pattern closely aligns with the PM 2.5 pattern.

The number of fires aligns with spikes in PM 2.5 levels

Weather Patterns

The temperature inversion effect often occurs during winter because the temperature is cooler near the ground. The hotter air on top traps the cool air from flowing. This stagnant atmospheric condition allows the PM 2.5 particles to remain suspended in the air for longer. On the other hand, higher humidity or rain will help remove particles from the atmosphere. This is one reason why in the past when the air pollution was high, the government has sprayed water in the air. Unfortunately, this mitigation does not appear to be effective, since the volume of water is minuscule compared to actual rain. How much influence does weather pattern have on air pollution? Let’s compare the weather in winter versus other seasons.

compare the weather pattern in winter and other seasons

Temperature, wind speed and humidity are all lower in winter, but not by a large amount. Now, let’s look at the relationship of each of these with the PM 2.5 level.

Effect of temperature, wind speed, and humidity on PM 2.5 level in winter

Higher temperature (which disrupts the temperature inversion effect), wind speed and humidity have a negative correlation with the pollution level.

Effect of wind on PM 2.5 level in winter

On windy days, the pollution is clearly better. The median of the distribution for PM 2.5 levels is lower on windy days compared to on days without wind.

In fact, the pollution level also depends on the wind direction, as seen in this plot. I selected only four major wind directions for simplicity.

PM2.5 relationship with the wind direction in winter

On the days where the wind comes from the south, the pollution level is lower likely because the Thai gulf is to the south of Bangkok. The clean ocean wind improves the air quality. Wind from the other three directions pass overland. However, having any wind is better than the stagnant atmospheric conditions on calm days.

The shift in the median PM 2.5 level is smaller between rainy days and days with no rain. There are fewer rainy days during the winter season, so the data is somewhat noisy, but a difference can be observed in the cumulative density function."
The Secrets of Successful AI Startups. Who’s Making Money in AI Part II?,"Cross the AI Commercial Divide to the Enterprise

Those startups that are doing well generally understand the nature of AI technology and the opportunity in the enterprise. But more than that AI startups that are starting to scale have all crossed the commercial divide from a technical world to the enterprise. They have learnt:

1. Don’t move too fast and break things; embrace Responsible AI

This culture popularised by Silicon Valley works well in a business to consumer (B2C) world where the consequences of a bug in an application are relatively limited. Software developers globally have embraced the lean startup and agile methodology. But this means live applications can often have bugs and break. This doesn’t really work well in the enterprise.

This is especially true in highly regulated enterprises such as financial services, agriculture or pharmaceuticals. Technology is driving greater more and more regulation. Europe introduced GDPR regulation in 2018 that provides rights to individuals as to the use of their personal data by companies. There will be significant fines — up to 2% of a company’s revenue — for the misuse of consumers’ personal data. And in the financial industry the response of “whoops, I guess the app didn’t work” doesn’t work when you are dealing with real financial data and financial transactions. Revolut a new UK challenger bank that relies heavily on AI has found itself in hot waters with the regulators over alleged compliance lapses. And in a world where the consequences of automated decision can be life-changing, such as autonomous vehicle crashing or critical health care treatment diagnoses, you better make sure you are confident in the reliability and accuracy of your automated decision making.

Corporates are increasingly putting in place board governance and oversight to manage reputational risks to their firm from the use of AI. For example if the datasets used to train AI algorithms have sample biases then a company’s brand can suffer if they are seen as being discriminatory. We all saw the recent headlines where algorithms were shown to be much better at recognising the gender of white males compared to other ethnic groups. Or the recent case from Amazon where they abandoned their hiring recommendations systems as the machine learning simply mirrored the fact that hiring historically was heavily male, white and young.

Increasingly startups will need to offer enterprises some level of assurance around the risks of their AI offering. Does your AI startup technology provide explainability for fully automated decisions that have legal effect? For example if your technology can be used to automate hiring decisions then you will need to explain how the algorithms work under GDPR. You need to demonstrate that the algorithm is not biased against people based on protected classes such as gender, age, socio-demographics or health challenged (feel free to take a look at our introductory readings on the topic of explainable AI). The Information Commissioners Office in the UK recently released a discussion paper that identifies eight AI specific risk areas that enterprises are likely need to manage including (a) fairness and transparency in profiling which is especially concerned with bias and discrimination, (b) accuracy of the AI models, (c) the level of automated decision making be it fully automated or human in the loop, (d) security and cyber risks and (e) tradeoffs in accuracy versus privacy versus explainability.

Startups and corporates are going to need to be really well versed in this topic which is increasingly being referred to as Responsible AI practices. As a startup don’t move too fast and break too many things. It could get you in a lot trouble. And ensure you are embracing and demonstrating Responsible AI practices (a topic I will write more on later).

2. Solve really high value use cases, not nice to haves

In the past few years we have seen a tidal wave of consumer mobile applications that addresses any imaginable consumer need. Apps can be built in weeks, launched and consumer traffic bought by placing ads on Google and Facebook. But this approach doesn’t work in the enterprise. There is something of a zero sum game where there is little appetite from the chief information officer (CIO) and other executive leaders to embrace, yet, another technology solution. We all recognise work frustrations trying to use the latest application from HR or finance or sales or marketing. We struggle to remember our passwords. None of the new applications work the same. And then we often can’t remember where we stored that file be it in the project or personal cloud folder. Technology leaders are frustrated trying to integrate more technologies into their existing and often fraying legacy databases and technology platforms.

To get the attention of a CIO and a head of department, such as a chief marketing officer (CMO), your solution better be addressing a really important problem. The type of problem it should be addressing should be one where the manager is going to bed worrying about it and their bonus plan is clearly tied to it. “Nice to haves” do not work in the enterprise. I was chatting recently with a CTO of an AI startup who has spent two years building an AI product suite to drive enterprise intelligence but the sales aren’t coming. Why? Not high enough value yet.

An example of a really high value use is from HireVue. They worked with Unilever to save over 50,000 hours in candidate interview time and delivered over £1M annual savings and improved candidate diversity with machine analysis of video-based interviewing. That’s a lot of money. Reinfer, a British startup that uses advanced NLP algorithms to sift through billions of emails and messaging to determine what people are communicating about, recently completed a pilot with a major international bank. It identified major issues in post-trade operations by analysing mailboxes with the use of machine learning with the potential for millions in operational savings. These are really high value use cases.

3. Master B2B enterprise sales and learn calculated patience

Startups need to ensure they master enterprise sales. These skills are at a premium for AI suppliers as it takes time and hands-on experience to master consultative sales . The most valuable training course I took early in my career was the SPIN sales methodology — S(ituation), P(roblem), I(mplication), N(eed). Selling requires time to identify stakeholders, get a meeting with those stakeholders, assess the current business situation, ask questions to identify critical problems, assess the implications of those problems across multiple departments, build a consensus as to the implications and the need. And even then there is no guarantee that your solution to this need will get budgetary prioritisation.

A typical corporate manager has a to-do list that lasts from here to kingdom come. For a startup solving a high value use case need is critical so don’t just base your company on a “product hunch.”

And patience is at a premium when a sales cycle can last 12–18 months. Many AI startups run out of money or assign precious resources to sales opportunities that are not qualified. Make sure you qualify out opportunities quickly if you think they will ultimately not lead anywhere. Re:infer found the sales cycle was really really long but they stuck with it identifying a really high value use case, finding an internal champion, and completing a successful sales pilot. They also identified that enterprises’ readiness to rollout AI across an organisation can really slow the sales process.

4. Translate AI for the real world

Startups need to be able to explain the value of their unique algorithm, technology, product or solution in a language that is readily understandable by a business audience is critical. They need to be bi-lingual. With many AI startups being founded by young, wicked smart and highly technical minds there is often a communications gap with the “suits.” We see executives’ eyes glaze over all the time as the minutiae of this cutting-edge technology is explained. And if little context is provided as to how this technology can help drive a business forward — by impacting revenues, efficiency and customer service — the executive will lose patience rapidly. It is critical that startups speak the language of technology and business. They need to translate between technical and commercial languages.

5. Lower the barriers to trial (a pilot)

It is important that startups make it really simple to implement a pilot. AI solutions often require much data and much time to train the machine learning models. And AI often requires significant engineering integration into back-end technologies to work well. It can take months to acquire, clean and wrangle data then months to use it to train models. In enterprises that are very busy there is little appetite to do a lot of heavy lifting to setup a pilot. Companies want quick proof of concepts. DigitalGenius has done a good job on this front. Their customer service technology works on top of existing enterprise platforms, such as Salesforce, and they have minimised the amount of time it takes to train the technology to answer customer queries. They have also made it very simple to integrate into existing work-flows providing human-over-the-loop decision making. KLM, the Dutch airline, claims to respond to over 50% of customer enquiries on social media by implementing a machine learning chatbot with DigitalGenius.

6. Technical founders need to hire business people.

Startups need to ensure they have the skills to cross the commercial divide from the world of techies to the enterprise. The Harvard Business Review recently reported that the startups most likely to succeed have technical founders who have quickly hired business people. “One theory for why technical skills seem to matter more for a founder is simply that the average technical founder has better business skills than the average business-trained founder has technical skills.” But blending the DNA of founders with commercial people often requires alchemy. I have lost count of the number of times that I have seen startups struggling to get this cultural fit right. The most common is technical founders will hire sales “farmers” instead of sales “hunters.” Farmers don’t know how to go out and hunt for business which is what is required for a startup . The are used to big, fat marketing departments bringing them a torrent of leads to harvest."
Building an Article Recommender using LDA,"Due to keen interest in learning new topics, I decided to work on a project where a Latent Dirichlet Allocation (LDA) model can recommend Wikipedia articles based on a search phrase.

This article explains my approach towards building the project in Python. Check out the project on GitHub below.

Structure

Photo by Ricardo Cruz on Unsplash

I developed the complete project in Python using classes and did not use Jupyter notebooks like I usually do to understand about classes and how to develop general Python projects. The modules, WikipediaCrawler, Cleaner and Content are defined as classes inside the Modules folder. config file includes the configurations. collectData, generateLDA and evaluator are used to develop and run the model.

Modules

|- __init__.py

|- WikipediaCrawler.py

|- Cleaner.py

|- Content.py config.yml

collectData.py

generateDLA.py

evaluator.py sample_images

|- recommendations.png .gitignore

LICENSE

Pipfile.lock

README.md

requirements.txt

When you try to run the project, you can use either Pipfile.lock or requirements.txt to install all dependencies.

Configuration

Photo by Tim Mossholder on Unsplash

It’s always a good practice to include any configurations for your project in a common file. While there isn’t much information in this project, I did define the paths for storing the database, LDA Model, dictionary and corpus inside config.yml file. I decided to keep all of these inside the data folder.

The configuration file is based on YAML which is a commonly used data serialisation method in the industry to store human readable configurations. The pyyaml package is required to read YAML files in Python.

Modules

Photo by Louis Reed on Unsplash

I developed and designed three modules (as classes) to be used for scraping data from Wikipedia, and working with the data.

Wikipedia Crawler

The class WikipediaCrawler let’s us crawl Wikipedia articles based on a certain category. On initialisation of this class, it creates a sqlite3 connect and then adds a table wikiData that stores the page id , category , url and content . The collect_data method uses the wptools package to extract the pages and store them in the table. wptools is a Python package that allows us to scrape Wikipedia articles based on a given category.

I’ve added two additional methods, get_ids to fetch all page ids and get_urls to fetch all urls, if needed.

Cleaner

This module takes in document text and pre-processes that text. I just need to use the function clean_text as it calls all the other functions on our behalf and returns the final result. It does the following:

Removes unnecessary new line characters

Removes punctuation Removes numbers Removes stopwords (words that are too common and do not qualify for being good keywords for search) Applies lemmatization (converts each word to its lemma word like ran, running are converted to run)

Content

This module connects to sqlite3 database and helps us iterate over the pages and clean their content using Cleaner module. I added other methods to get the page and url by id.

Application

Photo by Jason Leung on Unsplash

Once I had the modules set up, I began scraping for data, training the LDA model and recommending articles.

Collect Data

First, I run the file collectData.py which expects two arguments to begin extracting data from Wikipedia and storing it in the database.

category: The category for which we want to develop the article recommender system depth: To what depth do we want to extract the webpages for a given category. For example, when browsing through an article when beginning with depth 2, it’ll go one step deeper (i.e. its related articles) with depth 1 but will end at the next depth as it will be 0.

It creates the directory data if it does not exist. Using WikipediaCrawler , it extracts the pages and stores them to wikiData.db to be used by other files. On completion, it outputs the message: The database has been generated

Generate LDA

The next step is to use the database we created, build a LDA model from it and store it in the data folder.

First, I read the database and create a dictionary. I remove all words that appear in less than 5 documents and that appear in more than 80% documents. I tried multiple values and concluded on these numbers by hit and trial. Then, using doc2bow , I create the bag of words which act as the list of keywords. Finally, I generated the LDA Model and saved the model, dictionary and corpus.

Evaluator

Finally, everything is ready. We invoke the evaluator.py and pass in a query string based on which we identify the keywords and list the top 10 articles that match the search criteria.

I read the query and identify the keywords from it. Then, by invoking the get_similarity method, I calculated the similarity matrix and sort them in the decreasing order so the maximum similarity documents are at the top.

Next, I iterate over these results and present the top 10 urls which represent the recommended articles.

Real Example

Use Case

I created the database with depth 2 and category Machine Learning. It generated the file, wikiData.db . Next, using the generateLDA.py , I created the LDA model.

Usage

I used the search query as Machine learning applications and was recommended the articles as can be seen in the image below:

Recommended articles for ‘Machine learning applications’

Conclusion

In this article, I went about how I developed a LDA model to recommend articles to users based on a search query. I worked with Python classes and devised a complete application."
Can Deep Learning Perform Better Than Pigeons?,"Can Deep Learning Perform Better Than Pigeons?

I’m working through Lesson 2 of the marvelous “Practical Deep Learning for Coders” course by Jeremy Howard and Rachel Thomas of fast.ai, and last week, I trained a deep learning model to classify images of pregnancy tests whose results were not distinctly positive or negative, but were either faintly positive or showing evaporation lines. The model’s accuracy wasn’t what I wanted it to be, however, so I decided to start again with a classification problem for which I could easily acquire better sets of images than I could for the tricky pregnancy test classification problem. I then tried to train a model to classify redwood vs. sequoia trees, but again achieved a very low accuracy rate, with most of the errors coming from sequoias classified as redwoods. Sequoias’ trunks are much larger than redwoods’, but in images, it can be challenging to get a sense of scale, and I think that issue is most likely the reason why the model mistook sequoias for redwoods.

So once again, I need a new problem. I consider classifying paintings as having been painted by either Monet or Manet, but I find my own ability to distinguish between the two insufficient to check the model’s accuracy, so I decide to try Picasso vs. Monet instead. I want to be able to compare my model’s accuracy to another’s, and don’t find a deep learning model for this problem, but do find a study that trained several sets of pigeons to identify paintings by Picasso vs. Monet with over 90% accuracy! I figure beating the pigeons is an acceptable first benchmark for my model.

Following the instructions in the Lesson 2 lecture, I search for “picasso paintings” and “monet paintings” on Google Images and download both sets of images (see my previous post for the slight tweaks I needed to make to the code for this in the notebook).

On its first time out of the gate, my model beats the pigeons!

My training set loss is 0.396 while my validation set loss is 0.086 and my error rate is 0.278. It’ll be hard to improve on that! My spidey sense is pinging, though, because my training set loss is higher than that of my validation set, and I think Jeremy said in the lecture that that’s a sign of overfitting. I search and find this post on the fast.ai forum in which Jeremy says that as long as the error rate is low, a little overfitting is acceptable. Whew!

Now for some hyperparameter tuning, though the accuracy is so high that I’m not at all sure I can increase it significantly. I run the learning rate finder and try to plot the graph, but it shows as blank. There’s a tip in the notebook that says if the learning rate finder plot is blank, to try running learn.lr_find(start_lr=1e-5, end_lr=1e-1 .

That works! Here’s the graph.

Based on the graph, I set the learning rate to stay between 1e-6 to 1e-4:

I’ve brought down both my training set loss and my validation set loss considerably, but the error rate is essentially the same (which I expected). The model only made one mistake, classifying a Picasso as a Monet.

I run the ImageCleaner, but I don’t find any poor quality or noisy images, so I keep them all. Since my loss and my error rate are both low, I think my model’s all set!

Next, I’m meant to put my model into production in a web app. I’ve played with HTML and CSS before, and like the card-carrying Millennial that I am, I absolutely did have a Geocities site long ago, but I’ve never built a web app. Jeremy says “Build a web app,” like I might say “Make some coffee,” so perhaps it’ll be relatively easy?

I follow the fast.ai instructions for deploying on Render and fork the repo provided. I run into a small roadblock: the instructions tell us to edit the server.py file in the “app” directory, but I can’t find one anywhere in the Jupyter directory tree. Eventually, I realize the “app” directory is in the forked repo on GitHub, not in Jupyter! Maybe that would have been obvious to some, but it wasn’t to me. The next tiny snag is that a variable we need to edit in the server.py file is called ‘export_file_url,’ not ‘model_file_url,’ as indicated in the instructions. No big deal.

I continue following the deployment instructions, but my web app fails to deploy, and I get this error in my logs on Render: TypeError: __call__() missing 2 required positional arguments: 'receive' and 'send' . I have no idea what that means, so I consult the fast.ai forums and find a thread addressing this issue. Apparently there’s a file called requirements.txt in the forked repo which needs the version numbers of the various libraries my model is running. I find my requirements.txt file, create a new cell in the notebook, and in it run ! pip list . I find most of the values sought by the requirements.txt file, but there’s no entry for starlette, aiofiles, or aiohttp. I copy and paste the values given in the most recent post on the thread, crossing my fingers, but that doesn’t work.

So I post to the incredibly friendly and helpful forums myself and get some new values to try in my requirements.txt file.

Progress! My web app deploys, but it seems to have defaulted to the teddy bear classifier example that’s given as the default in the repo. I did update the URL link in the server.py file, but since my export.pkl was under 100 MB, I didn’t generate an API key the first time. Perhaps I need one, though, so this time I follow these instructions to do that. I edit the server.py file again with the new link, and re-deploy. Nope, still bears.

There’s an example link still in server.py, but it appears to be commented out. I’ll try deleting it? But I also edited the classes to be “picasso” and “monet” instead of types of bears, so those should have changed, too. Hmm… what if they did? Maybe it looks like the bear classifier, but it’s actually my painting classifier?

I take the advice of the Helpful Worm, and try ̶ w̶a̶l̶k̶i̶n̶g̶ ̶t̶h̶r̶o̶u̶g̶h̶ ̶i̶t̶ running it anyway.

Yes! I run a Picasso image through the classifier, and the model classifies it correctly. So it is displaying as if it’s the bear classifier, while actually running my classifier.

I see that I need to edit some code for the user-facing text somewhere, but can’t figure out where to do that. The wonderfully helpful mrfabulous1 on the fast.ai forums comes through for me again and tells me that the code I need to edit is the index.html file in the “view” subdirectory of the “app” directory. It works!

So now I have an image classification model with 97% accuracy, and a web app deployed to play with it. On to Lesson 3!

Check out the code on GitHub!

Other posts on this topic:

Lesson 1: Getting Started with fast.ai

Lesson 2 (attempt 1): Classifying Pregnancy Test Results!

Lesson 3: 10,000 Ways that Won’t Work

Lesson 4: Predicting a Waiter’s Tips

Lesson 5: But Where Does the Pickle Go?

Lesson 6: Everybody Wants to be a Cat"
Which 2020 Candidate is the Best at Twitter?,"Which 2020 Candidate is the Best at Twitter?

The contest for the 2020 Democratic party nomination will be fought in many arenas. Before the first debates in a month, before the campaign rallies in key states, and even before prime time TV interviews, the fight for the nomination has begun on Twitter. Each of the major Democratic candidates has a signifiant social media following. With these accounts, the candidates have the means to directly communicate to voters, the media, and the world. After all, we’ve seen that carefully crafted tweets can change narratives in the real world.

Knowing this, I decided to collect all of the tweets from 11 of the top Democratic candidates for president. Three of these contenders have separate work accounts, so in total 14 profiles were analyzed. With this data, it’s possible to see which candidates make the best use of this new and powerful platform.

Twitter Statistics

Followers

The candidate with the most Twitter followers is definely Bernie Sanders. Between his senate (@SenSanders) and personal (@BernieSanders) accounts, Sanders has over 17 million followers. No doubt some of these overlap, but it goes to show that his 2016 campaign created a massive social media following. Elizabeth Warren’s senate account is a distant third, while Cory Booker, Joe Biden, and Kamala Harris are also followed by multiple millions of people.

The follower count can best be seen as measure of the potential influence of a candidate online. The actual effectiveness of a large following depends on how good the candidate is at communicating.

Number of Tweets

If a follower count is like potential energy, then the number of tweets issued is analogous to kinetic energy. In this respect, Andrew Yang is the most energetic and also the most prolific of the 2020 candidates. With almost 3000 tweets in 2019, Yang uses social media far more than his peers. He is the one contender who probably leverages this…"
Generate Modern Stylish Wordcloud with stylecloud,"Almost every Text Analytics Project with a need to find insights from Text Corpus would contain a word cloud. But as many of you remember, Wordclouds have got a very boring image and perception in the minds of a data scientist that we always try to beautify them — ultimately giving up in the process — except a few would choose some masking image and then try to get the wordcloud in that shape. That’s the maximum level where most of us would take our wordclouds.

But deep down, all of us have always wished for modern-stylish-beautiful wordclouds. That wish has become true with this new python package — stylecloud by Max Woolf (who’s famously known as minimaxir )

About styelcloud

stylecloud is a Python package that leverages the popular word_cloud package, adding useful features to create truly unique word clouds that are stylistic including gradients and icon shapes.

stylecloud installation

stylecloud is just one pip away

pip3 install stylecloud

stylecloud — Basics

stylecloud offers two ways to generate a style-wordcloud:

stylecloud as a CLI command that could be invoked from your Terminal/Shell/Cmd Prompt to generate wordcloud (fast and stylish)

as a CLI command that could be invoked from your Terminal/Shell/Cmd Prompt to generate wordcloud (fast and stylish) Typical pythonic way of importing the stylecloud package and creating the wordcloud using stylecloud() in the code

Sample Text File

For this article, We’ll consider the iconic inaugural address of former US President Barack Obama in 2008 when he was elected as the POTUS.

Download the file — 2009–01–20-inaugural-address.txt — from here

stylecloud — CLI

Simply, open your Terminal or Command Prompt and try this below command stylecloud pointing to the file that we downloaded above

stylecloud --file_path 2009-01-20-inaugural-address.txt

This simple command from stylecloud results in this beautiful plot (automatically saved in the same current directory as stylecloud.png )

That’s simple, fast and beautiful isn’t it? ⚑

stylecloud — in Python Script

stylecloud in CLI is for normal humans, but we are coders who like to code in Python . So let’s build the same in Python with the following 2 lines of code.

import stylecloud stylecloud.gen_stylecloud(file_path = ""2009-01-20-inaugural-address.txt"")

stylecloud — Customization

Now, let’s say we don’t want it in the shape of a flag but in the form of twitter logo. After all, this is the age of internet, isn’t it? A little change in the code — just a new argument to give the specific fontawesome icon name would get us the twitter-shaped stylecloud of Mr. Obama’s speech.

stylecloud.gen_stylecloud(file_path = ""2009-01-20-inaugural-address.txt"", icon_name= ""fab fa-twitter"")

Now, Let’s change the color palette a bit and also a dark theme (which everyone’s fond of these days)

stylecloud.gen_stylecloud(file_path = ""2009-01-20-inaugural-address.txt"", icon_name= ""fab fa-twitter"", palette=""cartocolors.diverging.TealRose_7"", background_color=""black"")

That’s real Dark! And if you’re a fan of Linkedin (instead of Twitter), not to leave you behind — here’s your Linkedin Icon-shaped style-wordcloud

Summary

Thanks to Max Woolf , We’re gifted with this amazing library stylecloud. We just quickly learnt how to play with this modern-stylish wordcloud generator both as a CLI tool and in Python script. The PNG files and notebook can be found here."
Is Your Company Truly Data Driven?,"Member-only story Is Your Company Truly Data Driven?

Every company claims to be data driven these days, but what does that actually entail?

This is a critical question to answer, both for investors choosing where to place their bets and for data scientists looking for the right company to work for. From an investor’s perspective, I want to invest in companies that know what to measure, how to measure it, and actually use data to drive sound decision making. As a data scientist, I want to work for a company that has interesting data, invests time and resources into machine learning models, and has a management team that listens to data driven recommendations.

In this post, I attempt to lay out a checklist for deciding whether a company is actually data driven or just pretending to be.

Photo by rawpixel.com from Pexels

The Checklist"
The problem with data science job postings,"Every once in a while, you notice something that you realize you probably should have noticed a long time ago. You start to see it everywhere. You wonder why more people aren’t talking about it.

For me, “every once in a while” was yesterday when I was scrolling through the #jobs channel in the SharpestMinds Slack workspace, and the “something” is a big problem in the data science industry that I really don’t think we’re taking seriously enough: the vast majority of data science job descriptions do not convey the actual requirements of the position they’re advertising.

How do I know this? For one, quite a few of the jobs posted to our internal board included notes from the users (usually SharpestMinds mentors) who posted them, saying things like, “I know the posting says they’re looking for X and Y, but they’re actually fine with Z.” As often as not I’d also get direct messages from them saying the same thing.

In other words, when senior data scientists are called upon to recruit “for real”, their first move is often to throw away the job posting altogether.

This is not good, for several reasons. First, a misleading job description means that recruiters get a *ton* of irrelevant applications, and that candidates waste a *ton* of time applying to irrelevant positions. But there’s another problem: job descriptions are the training labels that any good aspiring data scientist will use to prioritize their personal and technical skills development.

Despite the obvious downsides of these mangled job postings, companies keep putting them out there, so a very natural question to ask is: why? Why are job postings so confusing (in that they fail to clearly specify the skills they expect from a candidate), or so outrageously over-reaching (“looking for a machine learning engineer with 10 years’ experience in deep learning…”)?

There are many reasons. For one, companies make hiring decisions based on a candidate’s (perceived) ability to solve a real problem that they actually have. Because there are many ways to solve any given data science problem, it can be hard to narrow down the job description to a specific set of technical skills or libraries. That’s why it usually makes sense to put in an application for a company if you think you can solve the problems they have, even if you don’t know the specific tools they ask for.

Another possible reason is that many companies don’t actually know what they want — especially companies with relatively new data science teams — either because the early stage of their data science effort forces everyone to be a jack of all trades, or because they lack the expertise they need to even know what problems they have, and who can help solve them. If you come across an oddly non-specific posting, it’s worth taking the time to figure out which bucket it belongs to, since the former can be a great experience builder, whereas the latter can be a recipe for disaster.

But perhaps the most important reason is that job postings are often written by recruiters, who are not remotely technical. This has the unfortunate side-effect of resulting in occasionally incoherent asks (“Must have 10+ years’ experience with deep learning…”, “…including natural language toolkits, such as OpenCV…”) or asks that no human being could possibly satisfy.

The net result of this job qualifications circus is that I regularly get questions from our mentees about whether they’re qualified for an opening, despite their having read all the information available on the internet about that position. Those questions are actually surprisingly consistent — so much so that I think it’s worth listing the answers to the most common ones here, in the form of simple rules you can follow to make sure you’re applying to the right roles (and not being scared away by fake requirements):

If a company asks for more than 6 years of deep learning experience, then their posting was written by someone who has zero technical knowledge (AlexNet came out in 2012, so this basically narrows the field down to Geoff Hinton’s entourage). Unless you want to build a data science team from the ground up (which you shouldn’t if you’re new to the field), this should be a big red flag.

If you have no prior experience, don’t bother applying to jobs that ask for more than 2 years of it.

When they say “or equivalent experience”, they mean, “or about 1.5X that much experience working in a MSc or a PhD where you worked on something at least related to this”.

If you meet 50% of the requirements, that might be enough. If you meet 70%, you’re good to go. If you meet 100%, there’s a good chance you’re overqualified.

Companies *usually* care less about the languages you know than the problems you can solve. If they say Pytorch and you only know TensorFlow, you’re probably going to be ok (unless they stress the Pytorch part explicitly).

Don’t ignore lines like, “you should be detail-oriented and goal-driven, and thrive under pressure”. They sound like generic, cookie-cutter statements — and sometimes they are — but they’re usually written in a genuine attempt to tell you what kind of environment you’ll be getting yourself into. At the very least, you should use these as hints about what aspects of your personality you should emphasize to establish rapport with your interviewers.

None of these rules are universally applicable, of course: the odd company will insist on hiring only candidates who meet all their stated requirements, and others will be particularly interested in people who know framework X, and will disregard people who can solve similar problems, but with different tools. But because there’s no way to know that from job descriptions alone (unless they’re explicit about it), your best bet is almost always to bet on yourself, and throw your hat in the ring.

If you want to connect, you can find me on Twitter at @jeremiecharris!"
Visualising Singapore’s changing weather patterns: 1983–2019,"The lack of seasonal variations lull many into thinking that Singapore’s weather is predictable and unchanging. Nothing is further from the truth, with climate change making the city state’s weather even more unpredictable.

We are beginning to feel the impact of warmer days and more intense storms. At the ground-level, it feels as if Singapore is getting much warmer at an earlier hour.

On many days, the 8am sun that greets me upon stepping out feels more like something I would expect in the late morning. The mid-afternoon blaze has never felt more intense.

In this project, I’ll attempt to illustrate Singapore’s changing weather patterns using classic as well as new visualisation libraries like Plotly Express.

The repo for the project is here. The charts in this Medium post can be found in this notebook, while the data can be found here and here (raw version)."
Reinventing Personalization For Customer Experience,"Reinventing Personalization For Customer Experience

“Remember that a person’s name is, to that person, the sweetest and most important sound in any language.” — Dale Carnegie, How to Win Friends and Influence People

When it comes to building good relationships with customers, learning their names is an essential step for businesses at any level. Consumers expect to be treated as individuals when it comes to the brands they do business with. Remembering a person’s name and using it whenever appropriate is key to winning that person over to your way of thinking. This fact is backed by science which says that hearing one’s own name has a powerful impact on the listener’s brain. Hence, it is only logical to remember and use not only the customers’ names, but also their likes and dislikes to make them feel valued — in other words, providing consumers with a personalized customer experience can change them into brand loyalists in no time.

Personalizing the customer experience across all touch points

The age of the customer marked the end of the one-size-fits-all messaging era. Today, a single message can’t get the job done unless it’s perfectly tailored to relate with every customer. This is the age where consumers are more empowered than ever before and in control of their relationship with a brand. These consumers continually demand personalization throughout the buying journey.

Living in a noisy world of instant gratification, how can a business make an impression in an already overcrowded field? It’s simple actually — by making use of something known as the “cocktail party” effect. Here’s how cocktail party effect works: when you’re at a cocktail party with dozens of people chattering around you, you’ll find that you can easily blur out those conversations. To you, they’re just background noise. But, as soon as someone says your name or something that is of particular interest to you, your ears perk up and tune into that specific conversation. This information will rise above the noise because it is important to you.

Similarly, adding a personal touch to the customer experience, for instance, using dynamic recipient name tags in emails can enable businesses to get their voice heard, allowing them to stand out in an overcrowded market.

Personalization — A winning strategy"
NLP 102: Negative Sampling and GloVe,"NLP 102: Negative Sampling and GloVe

Photo by Joshua Sortino on Unsplash

One way to generate a good quality word embedding from a corpus is using Word2Vec — CBOW or Skip-gram model. Both models have a few things in common:

The training samples consisted of a pair of words selected based on proximity of occurrence.

The last layer in the network was a softmax function.

Problems With CBoW/Skip-gram

Firstly, for each training sample, only the weights corresponding to the target word might get a significant update. While training a neural network model, in each back-propagation pass we try to update all the weights in the hidden layer. The weight corresponding to non-target words would receive a marginal or no change at all, i.e. in each pass we only make very sparse updates.

Secondly, for every training sample, the calculation of the final probabilities using the softmax is quite an expensive operation as it involves a summation of scores over all the words in our vocabulary for normalizing.

The softmax function.

So for each training sample, we are performing an expensive operation to calculate the probability for words whose weight might not even be updated or be updated so marginally that it is not worth the extra overhead.

To overcome these two problems, instead of brute forcing our way to create our training samples, we try to reduce the number of weights updated for each training sample.

Negative Sampling

Negative sampling allows us to only modify a small percentage of the weights, rather than all of them for each training sample. We do this by slightly modifying our problem. Instead of trying to predict the probability of being a nearby word for all the words in the vocabulary, we try to predict the probability that our training sample words are neighbors or not. Referring to our previous example of (orange, juice), we don’t try to predict the probability for juice to be a nearby word i.e P(juice|orange), we try to predict whether (orange, juice) are nearby words or not by calculating P(1|<orange, juice>).

So instead of having one giant softmax — classifying among 10,000 classes, we have now turned it into 10,000 binary classification problem.

We further simplify the problem by randomly selecting a small number of “negative” words k(a hyper-parameter, let’s say 5) to update the weights for. (In this context, a “negative” word is one for which we want the network to output a 0).

For our training sample (orange, juice), we will take five words, say apple, dinner, dog, chair, house and use them as negative samples. For this particular iteration we will only calculate the probabilities for juice, apple, dinner, dog, chair, house. Hence, the loss will only be propagated back for them and therefore only the weights corresponding to them will be updated.

The Objective Function

Overall Objective function in Skip-gram and Negative Sampling. Here sigmoid = 1/(1+exp(x)), t is the time step and theta are the various variables at that time step, all the U and V vectors.

The first term tries to maximize the probability of occurrence for actual words that lie in the context window, i.e. they co-occur. While the second term, tries to iterate over some random words j that don’t lie in the window and minimize their probability of co-occurrence.

We sample the random words based on their frequency of occurrence. P(w) = U(w) raised to the 3/4 power, where U(w) is a unigram distribution. The 3/4 power makes less frequent words be sampled more often, without it probability of sampling frequent words such as “the”, “is” etc would be much higher than words like “zebra”, “elephant” etc.

In our above mentioned example, we try to maximize the probability P(1|<orange, juice>) and maximize (because we have a negative sign in front of it in our objective function, so when we will choose the max value, we will encourage them from NOT happening) the probability of our negative samples P(1|<orange, apple>), P(1|<orange, dinner>), P(1|<orange,dog>), P(1|<orange, chair>), P(1|<orange, house>).

A small value of k is chosen for large data sets, roughly around 2 to 5. While for smaller data sets, a relatively larger value is preferred, around 5 to 20.

Sub Sampling

The distribution of words in a corpus is not uniform. Some words occur more frequently than the other. Words such as “the” “is” “are” etc, occur so frequently that omitting a few instances while training the model won’t affect its final embedding. Moreover, most of the occurrences of them don’t tell us much about its contextual meaning.

In sub-sampling, we limit the number of samples for a word by capping their frequency of occurrence. For frequently occurring words, we remove a few of their instances both as a neighboring word and as the input word.

Performance Considerations

The training time of word2Vec can be significantly reduced by using parallel training on multiple-CPU machine. The hyper-parameter choice is crucial for performance (both speed and accuracy), however, varies for different applications. The main choices to make are:

Architecture : skip-gram (slower, better for infrequent words) vs CBOW (fast).

: skip-gram (slower, better for infrequent words) vs CBOW (fast). The Training Algorithm : hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors).

: hierarchical softmax (better for infrequent words) vs negative sampling (better for frequent words, better with low dimensional vectors). Sub-sampling of Frequent Words : can improve both accuracy and speed for large data sets (useful values are in range 1e-3 to 1e-5).

: can improve both accuracy and speed for large data sets (useful values are in range 1e-3 to 1e-5). Dimensionality of the word vectors : usually more is better, but not always.

: usually more is better, but not always. Context (window) Size: for skip-gram usually around 10, for CBOW around 5.

Almost all the objective functions used are convex, so initialization matter. In practice, using small random numbers to initialize the word embedding yields good results."
SQL Subqueries,"SQL Subqueries

Photo by Henri L. on Unsplash

The code I used for this blog can be found on my GitHub.

Every time I learn something new about SQL I start to find that there are so many applications that I am unaware of. After learning the basic syntax of queries and a few basic tools, I moved onto subqueries in SQL.

Subqueries (aka inner queries or nested queries) are useful tools when you’re performing multiple steps. It’s feels Inception-like, since you’re querying into queries.

Subqueries can be used in several areas within the query, so today we’ll cover using them in the most common areas: SELECT , FROM , and WHERE clauses.

Setting Up

For these examples, we’ll use the Chinook database which is also used in the SQL Basics blog.

Let’s set up the database file, libraries, and functions:"
Machine Learning Model Comparison for Breast Cancer Classification and Bio-Marker Identification,"Age, BMI, MCP-1 appear to be normally distributed. Glucose, Insulin, HOMA, Leptin, Adiponectin, and Resistin have a right skew distribution. If you look at HOMA and Insulin you will see a strong positive linear relationship in the scatter plot. Meaning, the relationship between those two variables can be accurately modeled using a straight line regression model. The same goes for Insulin and HOMA. BMI and Glucose seem to have a solid cluster separation where we can see the two classes existing in their own areas. These two variables could, later, be strong features if used in a KNN type model.

Let’s look to see if any of these bio-markers are highly correlated with one another. If we try to train a machine learning model using variables with no or little correlation our model will return inaccurate results. To make a correlation matrix heat-map I bring in the Python package ‘seaborn.’ It visualizes the correlation coefficient that calculates the positive or negative linear strength of the relationship between two variables.

Values close to 1.0, like 0.93 and 0.7, show a strong positive linear relationship between the two variables while values close to -1.0 show the inverse, strong but negatively correlated.

BMI and Leptin have a positive correlation value of 0.57

Glucose and HOMA have a moderately strong positive correlation value of 0.7

Glucose and Insulin have a positive correlation value of 0.5

Insulin and HOMA have a very strong positive correlation value of 0.93

Glucose, Insulin, HOMA, and Resistin are all positively correlated with the classification variable at a level above 0.2

All variables are, in some way, correlated with one another to some degree

Next I test out various classification models provided by the sklearn package and, upon determining the best one, I will investigate its architecture. I use the train_test_split function from the sklearn package to create training and test data sets, as well as apply scaling. It’s important to note that the numerical values do not all have the same scale, some are measured in mg/dL, pg/dL, and kg/m², so therefore applying a scaling function is a critical pre-processing step before feeding the data to an ML model.

The first model I trained on the data was a logistic regression, a simple classification method. Initially, it returned an accuracy of 62% on the test data with 8 false negatives (missed 8 cancer samples) and 3 false positives (3 healthy samples classified as cancer). The model returned a 57% precision value and 33% recall value for predicting healthy classes. As a result of the small sample size, the results are preliminarily low. A good remedy for low sample sizes in classification problems is to change the loss function. Classification problems use cross-entropy loss to train and optimize the model, with a low sample size the model becomes biased towards one class as it tries to optimize its loss value. To remedy, I add weight values to the losses that correspond with different classes to even out the bias. This is easily done by adding the argument class_weight = ‘balanced’ which finds the class weights and balances the data ratio to remove bias. I run another logistic regression model using the balanced class weights argument.

This returns an accuracy of 69% on the test set, a 7% increase, and we also eliminated 3 of the false negatives.

Next, I trained a linear-discriminant analysis model, a single decision tree model, and a k-nearest-neighbors model next using both balanced and non-balanced class weight arguments. The best model was the KNN with an accuracy of 79% and only 3 false negatives and 3 false positives.

I attempted tuning the k-nn by finding the optimal value of k. The max accuracy I could achieve was 79%, though, so there was no improvements to be made there.

This visualizes the attempt to find the best value for k based on accuracy. We see that 1, 5, 7 are the best values for k when optimizing for accuracy.

An important part of this project to me is the identification of bio-markers. I chose to train a random forest classifier so we could receive a variable importance output and look at what bio-markers are considered the ‘most important,’ statistically speaking, in determining if a sample has cancer or not.

A random forest is an aggregation of many single decision trees. It takes the majority result of the decision trees as the final value. The initial random forest model performed horribly with an accuracy of 58.6%. I implemented a randomized search using cross validation to identify the best hyper-parameters for the random forest model in hopes of achieving better accuracy and a more transparent machine model.

This code builds a random search to identify the best parameters for the random forest to make its classifications. The following code uses the above defined random grid search, runs 100 different combinations of models, and identifies the best one.

These are the best parameters:

{‘n_estimators’: 266, ‘min_samples_split’: 5, ‘min_samples_leaf’: 1, ‘max_features’: ‘sqrt’, ‘max_depth’: 30, ‘bootstrap’: True}

Using the best possible random forest model we achieve an accuracy of 68.97%. This score is on par with the logistic regression’s results and performs worse than the KNN. The most useful output of the random forest, in my opinion, is the feature importance. It shows what variables were, statistically speaking, the most important in making the classification. This output makes the random forest model less of a ‘black box’ of abstraction and allows us to take a look at how its making certain decisions.

Based on the variable importance results above, I take the two most important features and visualize them together and apply the best performing model thus far, the KNN.

In the code chunk above I set the value of K to one of the values I identified earlier that attained the best accuracy, in this case that is K = 5. I then fit a k-nearest-neighbors model to just the two features: Glucose and BMI.

Red = Healthy Controls. Green = Breast Cancer Patients

This visualization is useful for making inferences about our predictor model. At around 90 (mg/dL) Glucose level there is a vertical cut off where some samples from either class intermingle. After 120 (mg/dL) all samples are breast cancer patients.

Conclusion

Glucose and BMI, together, could be two strong bio-markers for detecting breast cancer. Modelling these two bio-markers, and adding HOMA and Resistin levels (the next two features of importance), I achieved almost 80% accuracy in classifying breast cancer samples with the k-nearest-neighbors method. So, this project shows that combining these four important bio-markers breast cancer could be classified with some ease. If the data-set could be expanded upon, other methods could prove to be even more accurate, like the random forest. Adding another hundred or so samples has the potential to return even better results than the 80% accuracy of the KNN Classifier. These models all run extremely quick, too, and therefore are practical for medical applications if we could negate the false positive/false negative predictions that are most likely a result of bias due to the low sample size. This article aimed to show the influence small sample sizes can have on powerful machine learning methods and how easily applied methods can be used for medical diagnosis."
"musicnn: an open source, deep learning-based music tagger","The musicnn library (pronounced as “musician”) employs deep convolutional neural networks to automatically tag songs, and the models that are included achieve the best scores in public evaluation benchmarks. These state-of-the-art models have been released as an open-source library that can be easily installed and used. For example, you can use musicnn to tag this emblematic song from Muddy Waters — and it will predominantly tag it as blues!

Interestingly, although musicnn is quite confident about its blues prediction, it also considers (with less determination!) that some parts of the song can be tagged as jazz, soul, rock or even country — that are music genres that are closely related. See the taggram (which is the evolution of the tags probabilities across time) of the song above:

Taggram representation: Muddy Waters — see the evolution of the tags across time.

Vertical axis: tags. Horizontal axis: time. Model employed: MTT_musicnn

This project has been developed by a music and audio research laboratory in Barcelona, and is the result of several years of research. This research institution, the Music Technology Group of the Universitat Pompeu Fabra, is well known for its research on music and audio technologies. For example: they contributed to pioneering singing voice synthesis engines like Vocaloid, or they maintain open-source projects like Freesound.

These researchers have open-sourced musicnn, and you can use it by simply installing it:

pip install musicnn

After installing, you just need to run this one-liner in a terminal to tag your song with musicnn — but in their documentation you will find more options, like how to use it from a jupyter notebook:

python -m musicnn.tagger your_song.mp3 –print

Now that you know how to use it, you can try to automatically tag the Bohemian Rhapsody hit from Queen — and see if you agree with musicnn‘s predictions (attached below):

Taggram representation: Queen (Bohemian Rhapsody)— see the evolution of the tags across time.

Vertical axis: tags. Horizontal axis: time. Model employed: MTT_musicnn

Note that the choral prelude is well detected, as well as the piano and rock sections. A particularly interesting confusion is that Freddie Mercury‘s voice was tagged as female voice! In addition, the model is very consistent with its predictions (being “reasonable” confusions or not). Up to the point where one can clearly identify the different sections of the song in the taggram. For example, “repeated patterns” can be found for sections that have the same structure, or the model is quite successful at detecting when a singing voice is present or not.

To know more about musicnn, you can consult the Late Breaking / Demo article the authors will be presenting at ISMIR 2019 or their ISMIR 2018 article (that won the best student paper award of this international scientific venue)."
But what is a Random Variable ?,"Randomness and Variability

Now we are finally ready to see where the Random Variable is in all this and more importantly parts that correspond to Randomness & Variability.

Randomness

You see, the events corresponding to your experiment have inherent uncertainty (randomness) associated with it i.e. your two coin toss in above experiment could be HH or HT or TT or TH. You then use probability theory to quantify the uncertainty corresponding to these events.

I appreciate that at the end of the day it is simply semantics but I really liked the word uncertainty as it helps me not bring in my understanding of randomness from other disciplines. This also means that Random Variables in statistics could have been called Uncertain Variables. But they are not called so :( ….. the literature consistently calls them Random Variable so if it helps, you could (as I often) do the translation in your mind to Uncertain Variables.

I explain more on quantifying uncertainty of Random Variables in the latter part of the article.

Variability

In the coin toss experiment, we used the words HEAD & TAIL in our sample space. Instead, we could use numbers to represent them, say I would use 1 for HEAD and 0 for TAIL. In other words, I could have said I am mapping HEAD to 1 and TAIL to 0. Mapping implies a function that does this transformation. Recall from the earlier definition you found on wikipedia or in your google search — “A Random Variable is a function that maps outcomes to real values”.

You may wonder why I chose to assign 1 to HEAD and 0 to TAIL …. could it have been the other way i.e. 0 for HEAD and 1 for TAIL ?. In many ways, you are free with your assignment of (or rather I should say mapping to) numerical value but as you would see with more complex examples there is certain meaning & consistency to these mappings and it mostly depends on your definition of experiment. For e.g. even for a simple single coin toss example, I could justify the assignment of 1 to HEAD by posing my experiment as —” I am interested in observing a HEAD” . I would use 1 (=TRUE) as it indicates a boolean logic for this very experiment definition.

To be not limited to the boolean experiment let’s go through another example. Here I have two coin toss example i.e Ω = {HH,HT,TH,TT} and my definition of experiment is about number of heads observed. So, I would define my (Random) Variable to generate (remember it is a mapper/function) 0 for {TT}, 1 for both{TH} & {HT} and 2 for {HH}.

The Random Variables are generally represented using an uppercase letter. For e.g. for above experiment I would write it as H = {0,1,2}.

This being said, the mapping of outcomes of sample space (Ω={H,T}) to (real) numbers seems pretty deterministic i.e. there is no randomness here in the mapping (function) aspect. This means that it is not the variable part of ‘Random Variable’ that is random rather it represents that we are working with sample space that has uncertainty (randomness) associated with the outcomes.

If you are wondering - Is it simply about assigning a number to an outcome in sample space or there is more to the story then you are on the right path. There is indeed more to it!

You see, the outcomes of a given sample space could be used to define many different experiments. For e.g. this time around you would roll two dice such that if they are indistinguishable (i.e. you do not care about which dice produces which number) then the possible sample space is Ω = {(1,1),(1,2),(1,3),(1,4),(1,5),(1,6), (2,2),(2,3),(2,4),(2,5),(2,6),(3,3),(3,4),(3,5),(3,6),(4,4),(4,5),(4,6),(5,5),(5,6), (6,6)}. There are 21 possible outcomes here. From here on I could pose a few different experiments that would lead to their respective event spaces. One experiment could be — Sum of the two dice greater than 8, other could be — observe when the product of two dice is even, yet another one could be — observe when after subtracting 3 from the product the outcome is even and many more.

This creation of experiments using a sample space is where the Random Variable starts to use its ‘functional’ powers and map the outcomes to real numbers depending on how you have posed your experiment definition.

But why call it a Variable? I wondered for a long time about this aspect and the best (rather I should say only) explanation I could come up with is that often your task is not about a single sample space. The final result of your task may depend on many phenomenons which have their respective sample spaces. You would be applying algebra on these sample spaces and therefore you would end up adding, subtracting, multiplying the outcomes. Since you typically add/subtract/multiply variables and not functions they ended up calling them variables."
Caged Brain - Can Computer Predict Your Next Move? 🧠 vs 🤖,"Caged Brain - Can Computer Predict Your Next Move? 🧠 vs 🤖

AI Predictions with Simple Statistical Methods Greg Surma · Follow 6 min read · Apr 22, 2019 -- Share

In today’s article, we are going to build a prediction system with simple statistical methods. We are going to show that Machine Learning applications aren’t limited to neural networks only and we can achieve a decent predictive behavior with Computational Statistics. By the end of this article, you will be able to implement a very basic, yet impressive prediction system.

Caged Brain - simple prediction game

Inspiration

Caged Brain project was heavily inspired by Scott Aaronson’s Oracle:

In a class I taught at Berkeley, I did an experiment where I wrote a simple little program that would let people type either “f” or “d” and would predict which key they were going to push next. It’s actually very easy to write a program that will make the right prediction about 70% of the time. Most people don’t really know how to type randomly. They’ll have too many alternations and so on. There will be all sorts of patterns, so you just have to build some sort of probabilistic model. Even a very crude one will do well. I couldn’t even beat my own program, knowing exactly how it worked. I challenged people to try this and the program was getting between 70% and 80% prediction rates. Then, we found one student that the program predicted exactly 50% of the time. We asked him what his secret was and he responded that he “just used his free will.”

Scott Aaronson, Quantum Computing Since Democritus.

Caged Brain

We are going to build a predictive system encapsulated in a very simple game. While the user, is going to pick either ‘LEFT’ or ‘RIGHT’, the computer is going to analyze user’s moves and predict the next one. User’s goal is to be unpredictable and make computer fail in its predictions (green color). The lower the computer’s prediction % the better!"
What we can learn from 400 years of US executions,"Heavy content warning: almost everything I talk about is disturbing.

—

George Kendall stood in front of a group of men holding long, German-style handguns pointed at his chest. He probably wore a long white linen shirt tucked into his breeches and embellished his neck with a fine lace cravat. At the signal, his executioners would shove glowing hot wires into their 16-inch iron barrels, thus discharging the gunpowder inside of their heavy weapons. Kendall, a man of wealth and influence in the new colony of Virginia, was guilty of mutiny and spying for the Spanish royalty. He would be the first person to be executed in the area that is now the United States using Western execution methods.

Despite numerous studies across various states that demonstrate how carrying out the death penalty is more expensive than a “life without parole” sentence, Hammurabi-style vengeance still reigns in the North American superpower. I wanted to know how we got to where we are, where we started, and what capital punishment looks like today.

Please note my limitations and blind spots: I am not a capital punishment expert, nor am I at all qualified to write about the racial overtones of this subject in the country I was born in. As a data scientist and masters-level economist by training, I have experience in research. I am qualified to look at data and derive insights. I started my search using publicly-available data sets (see references). After mining these for insights, I sought out other historical and journalistic sources to understand the patterns I saw. I learned a thing or two.

Execution fashions, capital punishment fads

Since the nascent English colonial government of Virginia shot George Kendall with a firing squad in 1608, the methods by which the state “carried out” death sentences has not so much evolved, but been subject to fashion trends that lasted decades or centuries at a time.

Data sources: [1] and [2]

Along with the punishment from England came methods. Some of these methods are actually medieval. One of the earliest colonial execution methods imported from England was “Gibbetting,” or to “hang in chains,” where a person was put into a sort of custom metal suit, sometimes still alive, and then displayed for everyone to see and smell. It was so gruesome, the English posited, that it must deter crime (it didn’t!).

The Gibbet. You get the idea.

The worst sentences were not evenly dealt. Even George Kendall, the first executed Virginian, was spared the noose around his cravat because of his social standing (hanging was for more “lowly” folk). Death by hanging was notoriously gruesome — if it was botched (which it was fairly often), a person could be hanging and dying for tens of minutes. What kind of treatment, then, could diaspora from Africa expect in a land where they were seen as less than human?

The area that is presently the United States had colonial settlements from European nations other than England. France had a settlement on the Mississippi river in present-day Louisiana. The French brought over an execution method called “breaking on the wheel,” and it was an absolutely awful way to die that I won’t even describe (read at your own risk). The French Louisiana government did it 11 times, which is 92% of the share of wheel-breaking death penalties in the present-day US (New York executed a white man on the wheel in 1712).

Source [5]

What’s worse, 100% of the horrible wheel-breaking executions in Louisiana were exacted on enslaved African diaspora from 1730 to 1748 even though the enslaved people were probably only 65% of the population at that time and there were exactly zero white executions during the same period in the same area. It’s clear that execution and, more specifically, the wheel-breaking method, was a special way for a cruel society to “deter” crime regarding its “property.”

The wheel-breaking method had a brief and final resurgence in 1754 at the start of the French and Indian War, when three unnamed three white people were passed the sentence. They must have done something very bad to warrant that method in Louisiana.

Data source [2]

Unfortunately, this wasn’t the only method that seemed made special for African-Americans. Twenty-nine individuals, some unnamed in the records or with one name (like Jack, 1825, executed on a certainly hot August day in 1825 or Monk, who was killed by the state on February 9, 1791) were put to death by “burning,” which needs no further detail because the word speaks for itself. This seems to have set the tone for execution in the now United States.

These methods that seem a little gruesome even by modern standards, and they went mostly out of vogue by the founding of the new United States. Other methods that are more familiar to us today had more staying power. Hanging was consistently the most popular method for hundreds of years until another more seemingly “painless” method came along.

It’s said that Dr. Southwick came up with the idea of execution by electrocution when he saw a person experiencing homelessness touch an electric generator. Southwick watched the man drop to the ground. The doctor deemed it a “painless” death by his superb powers of third party pain observation, and thus the newest form of American execution was born from a strolling dentist in Buffalo, New York (yes, Dr. Southwick was a dentist kind of doctor).

The electric chair was not the humane execution miracle that Thomas Edison touted. He promoted it as he toured the US by shocking stray animals to death in front of crowds that came to see him (yes, he did this). This punishment that sounds like it’s from a bad horror movie in 1975 — criminals unlucky enough to have this sentence were known to catch fire, give off charred flesh smells as they died, and often took more than the “easy two jolts” that were supposed to kill them quickly.

The electric chair that Ted Bundy was executed in in the state of Florida.

The last and most common execution method to come into fashion is the lethal injection, first used in 1977 Oklahoma. A series of chemicals are injected into the condemned until they die, which was, in theory, supposed to kill them painlessly in five minutes. Unfortunately, some botched executions have taken up to two hours, mixing up the drug order has caused some condemned criminals to feel excruciating pain but be paralyzed so they are unable to express that something went wrong (which sounds like a 2018 horror movie). What’s worse, the people administering these injections are not medically trained, thus making them more likely to make such mistakes. Doctors cannot carry out the death sentence — it would go against a doctor’s Hippocratic oath to harm a person in such a way.

The seemingly asinine nature of the lethal injection from the lens of the American spectator juxtaposed with the horrific reality of its pain and cruelty probably gave Sonia Sotomayor existential pause. As she wrote in Arthur v. Dunn, of the lethal injection:

“States have designed lethal-injection protocols with a view toward protecting their own dignity, but they should not be permitted to shield the true horror of executions from official and public view.”

Though the lethal injection seems to feel like the only type of capital punishment that will persist so long as capital punishment continues, the drugs necessary for the sentence can be hard to get. This has led states such as Virginia, Alabama, and Tennessee to pass laws allowing executions by the electric chair due to issues with lethal injection drugs.

Then there was the tried and true firing squad, which was the method of the first execution in the colonial US and remains the only method that is still in use today (Ronnie Lee Gardner is the most recent one, in 2010). The firing squad, being the same method as the first Virginia execution, can have an archaic feel. But as we have learned from horror of botching and general administration of the lethal injection, we must ask ourselves: is it the cruelty of the punishment to the condemned or to the spectator that we care about in the 14th Amendment? As Justice Sonia Sotomayor also argued in Arthur v. Dunn,

“ In addition to being near instant, death by shooting may also be comparatively painless.… And historically, the firing squad has yielded significantly fewer botched executions.”

Five thirty eight seems to agree.

The peaks and troughs of death

The story of the death penalty in the United States is one of ebb, flow, dissolution, and re-institution. Though the death penalty was an import from England, its application and end did not follow suit. England executed 89% fewer people than the United States in the first half of the 20th century (1900–1949) when they both had capital punishment. What’s more, England abolished the death penalty in the 1960s and it stuck. The abolition of the death penalty in the modern United States only lasted ten years.

In the US, executions peaked in 1935 with 197 state-sanctioned deaths. If not for World War II and the horrific discovery of the Holocaust, 1935 might have just been another smaller peak on an ever-increasing trend of state-sanctioned death over a growing population. In 1948, the United Nations came together to forge a new morality — a “right to life.” Though most nations in the world did not fulfill high hope of this decree (eliminating the death penalty worldwide), it did serve as a turning point in the US. Executions sharply fell following WWII, marking a moral shift in the view toward killing individuals as a form of acceptable punishment.

Data: [1] and [2]

This fall in opinion regarding capital punishment aligned with an ideological shift in the 1967 Supreme Court with the 5–4 ruling of Furman v. Georgia. After this ruling, the US no longer passed the death sentence.

In Furman, William Henry Furman, a Black man, shot and killed his victim through a door when attempting to break and enter a home. The justices’ opinions in the case highlight how the execution of racial and economically oppressed individuals violated the Constitution — the 14th Amendment protects citizens from “cruel and unusual punishments.” Justice Douglas explained:

“…we know that the discretion of judges and juries in imposing the death penalty enables the penalty to be selectively applied, feeding prejudices against the accused if he is poor and despised, and lacking political clout, or if he is a member of a suspect or unpopular minority, saving those who by social position may be in a more protected position.”

It is appropriate that Furman and all of the lesser-known men that were included on his suit were Black; it is the awareness of the racially uneven application of the death penalty that has put execution under scrutiny for fairness. In a broad sense, the question of the death penalty in modern times is a question of race. In 1986, a man named McCleskey’s prosecution demonstrated to the Supreme Court how the death penalty was passed disproportionately to African Americans in the state of Georgia in an empirical study called the Baldus Study. After an analysis of over 2,000 homicide cases which controlled for over 200 variables in the state of Georgia, the study reported that 11% of Black people who killed white people were passed the death penalty, whereas only 1% of white people killing Black people were passed the same sentence. The majority opinion of the highest court didn’t exactly disagree with the study, they just found that

“…the Baldus study “fail[ed] to contribute anything of value” to McCleskey’s claim.”

The 5–4 decision was not made among appointed party lines. Two of the three democratically-appointed justices (the white ones) concurred with the majority opinion. Three of the four dissenting opinions were appointed by Republican presidents.

Sources for data: [1] (top) and [4] (bottom)

After the death penalty became active again in 1977, its climb began again in earnest, as did the climb in approval ratings for the death penalty, which hit an all-time high of 80% in 1994. But at the turn of the century, something happened that would send it decreasing yet again. In the late 1980’s and early 1990’s, DNA evidence became a revolutionary tool of objectivity in courts across the country. With the emergence of CODIS (Combined DNA Index System), the country’s first national database with DNA information, convicted felons who were wrongfully convicted had a real chance at release and being absolved of their crimes for the first time. This combined with a changing tide of public opinion and shifting law allowed for innocent people to be freed from death row. Starting in 1973, a slew of exonerations began that would grow to 166 into the modern day, many aided by DNA technology.

Amid growing concern over executing innocent people, the National Conference on Wrongful Convictions was held near Chicago for the first time in 1998. One year later, executions hit a peak for the last time in modern history.

The South is the place for the death penalty and Texas leads the way

Ever since the year 1608, the South has been the home of execution. When English settlers began to occupy and expand into land on what we now call North America, they began by displacing native peoples first in the area that we call Virginia (named aptly for England’s “Virgin Queen,” Elizabeth I). Virginia is a Southern state, so it follows that colonial-era executions took place largely in the South until the 18th century. It also follows that the South would lead what we now call the “West” and the “Midwest,” which were not yet occupied, stolen, colonized, bought, or a mix of all of these until the 19th century.

Data: [2]

This pattern continues into the modern day, however. In the past 20 years, 7 out of the 10 states with the most executions were located in the South. There are likely many reasons for this. From a strictly statistical perspective, it the South remains the most common place to be executed because every single Southern state still has the execution in place as of 2019, whereas many Northern and Western states have either declared it unconstitutional or have a governor-imposed moratorium on the sentence. As for why the South hasn’t outlawed the death penalty, my guess is values and beliefs around capital punishment that are regionally-grounded.

Data: [1]

If we had to isolate one state, Texas is clearly the driver of the entire country’s execution rate since the death penalty was reinstated in 1977 until about 2004. Not only do they execute far more individuals than any other state, they tend to execute their criminals quickly.

There are a few reasons why Texas leads the way in capital punishment. First, They do not have a public defender system. This means that people who cannot afford lawyers are given court-appointed ones which may have no experience in these types of cases. Secondly, Texas has a law passed in 1995 that speeds up the process between conviction and execution. Third, Texas’ appellate judges are elected officials that more closely follow the politics of the region, meaning that they might run “tough on crime” campaigns more often and have to live up to those promises in a way that appointed judges do not.

Data: [1] and [2]

There is also speculation that the Texas capital punishment system is efficient in a way that other states can achieve in the future. It is not, this opinion posits, that Texas passes out more death sentences, it is just that they are better at carrying them out.

In sum

We have come a long way in changing the ways that we execute people with humane methods in theory, but the common method is not the most humane. The actual most “humane” method (if that is possible) currently in practice might just be the very first: the firing squad. I am far from the first person to say this and it has been discussed openly in the Supreme Court.

The number of people executed in the United States has largely declined over the past century, but we still live in a system where capital punishment has been unevenly dealt since colonial times. Capital punishment remains in place despite costing the states at hand more money to kill criminals than to keep them alive, and the number of people executed in a given place (largely the South, even more largely Texas) seems to be culturally driven. The question of the death penalty largely appears to be a question of race.

Given the immense knowledge that we have regarding how painful even modern execution methods can be, the large taxpayer expense of carrying out the death sentence, and the absence of capital punishment in progressive nations, I find it surprising that the United States executes criminals.

References:"
Principal Components of PCA,"Photo by Katie Smith on Unsplash

Principal Component Analysis (PCA) is used in machine learning applications to reduce the dimensionality of the data. It has been especially useful for image compression among other applications. In this post we will go through Lindsay Smith’s A Tutorial on Principal Component Analysis with an implementation in python.

Our objective is to develop an intuition for PCA by laying out the mathematical formulations, and go beyond fit and fit_transform methods. Before moving on it is helpful to be familiar with measures of spread and linear algebra. Feel free to skip ahead to the implementation if the explanations seem trivial.

Measures of Spread

Variance

Variance is a measure of spread, and indicates how far a set of number are spread out from their average value. for a one dimensional array X , the variance s2 is:

Xi = The value of the ith entry of array X

X bar = the average of X

n = the number of entries

Covariance Between 2 Dimensions

What if we wanted to measure the joint variability of two random variables? Covariance measures how much the entries vary from the mean with respect to each other. It is given by:

Xi = The value of the ith entry of array X

Yi = The value of the ith entry of array Y

X bar = the average of array X

Y bar = the average of array Y

n = the number of entries, same for X and Y

When looking at the output of the covariance it is important to look at the sign. If it is positive then there is a positive correlation, meaning that X and Y increase together; if it is negative then both increase. If we care about the value then it’s better to use Pearsons’s Correlation Coefficient.

Covariance Between n Dimensions

What if we an n-dimensional data, how to we measure covariance? Recall that covariance is only between 2 dimensions, therefore the result is a covariance matrix. It is useful to know that the number of covariance values we can calculate is:

n = number of dimensions

For example if we had a dataset with dimensions x, y, and z then our covariance matrix C will be:

Linear Algebra

Photo by Alex Block on Unsplash

Eigenvectors

When we multiply two compatible matrices we are effectively doing a linear transformation. Consider a square matrix A and a vector v and the following property:

A = square matrix

v = vector

λ = scalar value

What the above tells us is that v is in fact an eigenvector. Geometrically speaking, the expression means that applying a linear transformation on the vector v returns a scaled version of it.

If A is a diagonalizable n x n matrix then it has n eigenvectors. An important property of eigenvectors is that they are all orthogonal. Later on we will see how we can express the data in terms of these vectors instead of its original dimensions.

Eigenvalues

In the previous section we calculated the eigenvalue without realizing it: it is none other than λ. When calculating eigenvectors, we also calculate eigenvalues. Computing these values can be done by hand as shown in this tutorial. However once the dimensions of A exceed 3x3 then getting the eigenvalues and eigenvectors can be very tedious and time consuming.

PCA Implementation

Photo by Lauren Mancke on Unsplash

We now know the necessary ingredients to implement PCA to a an example of our choice. In this section we will go through each step and see how the knowledge in the previous sections apply.

Step 1: Get the Data

For this exercise we will create a 3D toy data. This is arbitrary data so we can guess in advance how our results will look. To make things easier in the following sections we will convert the data into a toy dataframe.

Please note that our data has no missing values whatsoever, as this is key for PCA to function properly. For missing data we can replace the empty observations with the mean, variance, mode, zeros, or any value of our choosing. Each technique will impute variance to the dataset and is up to us to make the adequate judgement call depending on the situation.

Looking at the data in a 3D scatterplot:

Step 2: Subtract the Mean

For PCA to work we need to have dataset with a mean of zero. We can subtract the average across each dimension easily with the code below:

Step 3: Calculate the Covariance Matrix

Picking pandas again makes our lives easier and we make use of the cov method.

Recall that the non diagonal terms are the covariance of one dimension with another. For example in the sample output below we do not care much about the values but rather about the signs. Here X and Y are negatively correlated, whereas Y and Z are positively correlated.

Step 4: Calculate Eigenvectors and Eigenvalues of Covariance Matrix

We compute the eigenvectors v and eigenvalues w with numpy’s linear algebra package: numpy.linalg.eig. The eigenvectors are normalized such that the column v[:, i] is the eigenvector corresponding to the eigenvalue w[i]. The computed eigenvalues are not necessarily ordered, this will be relevant in the next step.

Step 5: Choosing Components and New Feature Vector

Now that we have eigenvectors and eigenvalues we can begin the dimensionality reduction. As it turns out, the eigenvector with the highest eigenvalue is the principle component of the dataset. In fact the eigenvector with the largest eigenvalue represents the most signficant relationship between the data dimensions.

Our task now is to sort the eigenvalues from highest to lowest, giving us the components by order of significance. We take the eigenvectors we want to keep from the feature vector which is a matrix of vectors:

Let’s look at how we might do that in code. Here we decide to ignore the smallest eigenvalue, therefore our indexing starts at 1.

Step 6: Deriving the New Dataset

In the last part we take the transpose of the vector and multiply it on the left of the original dataset, transposed.

In the code above, the transposed feature vector is the row feature vector where the eigenvectors are now in the rows such that the most significant eigenvectors are at the top. The row data adjust is the mean adjusted data transposed, where the each row holds a separate dimension. Our new data looks as follows:

You might notice the labeling of the axes above is not X and Y. With our transformation we have changed our data to be expressed in terms of our 2 eigenvectors.

Had we decided to keep all eigenvectors our data would be unchanged, just rotated so that the eigenvectors are the axes.

Step -1: Getting the Old Data Back

Suppose we reduced our data for image compression and now we want to retrieve it. To do so let’s review how we got the final data.

We can turn around the expression to get:

As it turns out the inverse of the row feature vector is equal to the transpose of our feature vector. This is true because the elements of the matrix are all the unit vectors of our dataset. Our equation becomes:

Finally we add the mean we subtracted from the start:

In python code this looks like:

Conclusion

Congratulations! We are at the end of the exercise and have performed PCA from scratch to reduce the dimensionality of our data, and then retraced our steps to get the data back.

Although we did not dive into the details of why it works, we can confidently interpret output from packages such as sklearn’s PCA.

For the full python code, feel free to explore this notebook."
Reinforcement Learning — Model Based Planning Methods,"Reinforcement Learning — Model Based Planning Methods

In previous articles, we have talked about reinforcement learning methods that are all based on model-free methods, which is also one of the key advantages of RL learning, as in most cases learning a model of environment can be tricky and tough. But what if we want to learn a model of environment or what if we already have a model of environment and how can we leverage that to help the learning process? In this article, we will together explore RL methods with environment as a model. The following will be structured as:

Start with basic idea of how to model an environment Implement an example in Python using the theory we just learnt Further ideas to extend the theory to more general cases

Model the Environment

An agent starts from a state, by taking an available action in that state, the environment gives it feedback, and accordingly the agent lands into next state and receive reward if any. In this general settings, the environment gives an agent two signals, one is its next state in the setting, and the other is reward. So when we say to model an environment, we are modelling a function mapping (state, action) to (nextState, reward) . For example, consider a situation in a grid world setting, an agent bashes its head into the wall, and in response, the agent stays where it is and gets a reward of 0, then in the simplest format, the model function will be (state, action)-->(state, 0) , indicating that the agent with this specific state and action, the agent will stay at the same place and get reward 0.

Algorithm

Let’s now look into how a model of environment can help improve the process of Q-learning. We start by introducing the simplest form of an algorithm called Dyna-Q:

The way Q-learning leveraging models to backup policy is simple and straight forward. Firstly, the a, b, c, d steps are exactly the same as general Q-learning steps(if you are not familiar with Q-learning, please check out my examples here). The only difference lies in step e and f , in step e , a model of the environment is recorded based on the assumption of deterministic environment(for non-deterministic and more complex environment, a more general model can be formulated based on the particular case). Step f can be simply summarised as applying the model being learnt and update the Q function n times, where n is a predefined parameter. The backup in step f is totally the same as it is in step d , and you may think it as repeating what the agent has experienced several times in order to reinforce the learning process.

Typically, as in Dyna-Q, the same reinforcement learning method is used both for learning from real experience and for planning from simulated experience. The reinforcement learning method is thus the “final common path” for both learning and planning.

The general Dyna Architecture

The graph shown above more directly displays the general structure of Dyna methods. Notice the 2 upward arrows in Policy/value functons , which in most cases are Q functions that we talked before, one of the arrow comes from direct RL update through real experience , which in this case equals the agent exploring around the environment, and the other comes from planning update through simulated experience , which, in this case, is repeating the model the agent learnt from real experience . So in each action taking, the learning process is strengthened by updating the Q function from both actual action taking and model simulation.

Implement Dyna Maze

I believe the best way to understand an algorithm is to implement an actual example. I will take the example from reinforcement learning an introduction, implement it in Python and compare it with general Q learning without planning steps(model simulation).

Game Setting

Dyna Maze Board

Consider the simple maze shown inset in the Figure. In each of the 47 states there are four actions, up , down , right , and left , which take the agent deterministically to the corresponding neighbouring states, except when movement is blocked by an obstacle or the edge of the maze, in which case the agent remains where it is. Reward is zero on all transitions, except those into the goal state, on which it is +1 . After reaching the goal state (G) , the agent returns to the start state (S) to begin a new episode.

The whole structure of this implementation is to have 2 classes, the first class represents the board, which is also the environment, that is able to

Take in an action and output the agent next state(or position) Give reward accordingly

And the second class represents the agent, which is able to

Explore around the board Keep track of a model of the environment Update the Q functions along the way.

Board Implementation

The first class of the board settings are similar with many board games we talked before, you can checkout full implementation here. I will eliminate my explanation here(you can check my previous articles to see more examples), as a result, we will have a board look like this:

Board Implementation

The board is represented in an numpy array, where z indicates the block, * indicates the agent’s current position and 0 indicates empty and available spots.

Agent Implementation

Initilisation

Firstly, in the init function we will initialise all the parameters required for the algorithm.

Besides those general Q-learning settings(learning rate, state_actions, …), a model of (state, action) -> (reward, state) is also initialised as python dictionary, and the model will only be updated along with the agent’s exploration in the environment. The self.steps is the number of time the model is used to update the Q function in each action taking, and self.steps_per_episode is used to record the number of steps in each episode(we will take it as a key metrics in the following algorithm comparison).

Choose Actions

In the chooseAction function, the agent will still take ϵ-greedy action, where it has self.exp_rate probability to take a random action and 1 — self.exp_rate probability to take a greedy action.

Model learning and Policy updating

Now let’s get to the key point of policy updating using models being learnt along agent’s exploration.

This implementation follows exactly as the algorithm we listed above. At each episode(game playing), after the first round of Q function update, the model will also be updated with self.model[self.state][action]=(reward, nxtState) , and then Q function will be repeatedly updated by self.steps number of times. Notice that in side the loop, the state and action are both randomly selected from the previously observations.

Experimenting with different steps

When the number of steps is set to 0, the Dyna-Q method is essentially Q-learning. Let’s compare the learning process with steps of 0, 5 and 50.

Dyna-Q with different steps

The x-axis is the number of episodes and y-axis is the number of steps to reach the goal. The task is to get to the goal as fast as possible. From the learning curve, we observe that the learning curve of planning agent(with simulated model) stabilises faster than non-planing agent. Referring to the words in Sutton’s book:

Without planning (n = 0), each episode adds only one additional step to the policy, and so only one step (the last) has been learned so far. With planning, again only one step is learned during the first episode, but here during the second episode an extensive policy has been developed that by the end of the episode will reach almost back to the start state

The additional model simulation and backup further reinforced the agent’s experience, thus resulted in a faster and more stable learning process.(checkout the full implementation)

How to generalise the idea?

The example we explored here surely has limited use, as the state is discretised an the action is deterministic. But the idea of modelling environment to accelerate learning process has unlimited use.

For discrete states with non-deterministic action

A probability model could be learnt rather than a straight forward 1 to 1 mapping we introduced above. The probability model should be constantly updated through the learning process, and during the backup stage, the (reward, nextState) could be chosen non-deterministically with a probability distribution.

For continuous states

The Q function update will be slightly different(I will introduce it in further articles), and the key would be to learn a more complex and general parametric model of the environment. This process could involve general supervised learning algorithms with current state, action as input and next state and reward as output.

In next post, we will learn further ideas to improve Dyna methods and talk about situations when the model is wrong!

And lastly, please check out the full code here. You are welcomed to contribute, and if you have any questions or suggestions, please raise comment below!

Reference:"
Teaching A Computer To Land On The Moon,"I spent a fair amount of time last year catching up on what’s happening in machine learning. The tools available now are really impressive — you can implement a complex neural net in just a few lines of code now with the libraries that are available.

I’ve always been fascinated by the idea of machines learning complex tasks like flying by doing them over and over and seeing what worked, so I selected the OpenAI Gym Lunar Lander environment for first experiments. The program doing the learning and control is referred to as an agent.

Agents that learn the correct approach to solving problems without being presented with lots of solved examples are doing unsupervised learning. One method of doing that is to have a training environment which rewards an agent for doing well, so that the agent can reinforce that behavior. This is called reinforcement learning.

The OpenAI Gym provides a consistent training environment simulating different problems — games, physics simulations, and others. It handles giving out the required rewards so that the agent can learn.

There is a neat library for doing this called Keras-RL, which works very nicely with OpenAI Gym.

A video of the training process, sampled at intervals, is shown below. It progresses from effectively random firing of the rockets with crashes to hesitant hovering, to smooth landing. Code for the learning agent is below."
From Cups to Consciousness (Part 3): Mapping your home with SLAM,"Introduction

“All you need is a plan, the road map, and the courage to press on to your destination” — Earl Nightingale

In the previous part of this series we talked about how the road to AGI could be divided into perception and control. Within control, navigation and grasping are a crucial part of the roadmap for building a general robot for household tasks. But as the Cheshire Cat said to Alice in Wonderland,

“if you don’t know where you want to go, does it really matter which road you take?”

This is the first of two parts where we talk about how our seemingly kidnapped robots find their bearings. What do we mean? Well, from their point of view, every house is a mysterious place they’ve been thrown into or suddenly awoken in. As one can guess, this process of discovery is almost exclusively in the domain of perception and is where, logically, we should start. One must first perceive before acting accordingly, including understanding your surroundings, mapping your environment, i.e. understanding what is where, where is “occupied” and where one can move to unimpeded.

Consider this step one in building a robot that can perform general tasks. Forget about having robots do singular tasks, e.g. a roomba to vacuum your floor, alexa to tell you the time, and a toast-robot. To perform general tasks, we need our robot to be aware of its environment, i.e. to recognise what is an obstacle, as well as where things are, and use that information to navigate and complete tasks.

Seems simple enough.

Mapping in simulation

In part 1, we introduced our philosophical motivations and desires for cup-picking and covered a set of simulated 3D house environments. In part 2, we covered our wrapper for the ai2thor house environment, some reinforcement learning (RL) experiments and two specific physics simulations called PyBullet and Gazebo, enabling us to put the robot Vector onto the moon.

Knowing that mapping was our next milestone, naturally we began by implementing initial mapping within the PyBullet simulation. Since PyBullet contains a depth and colour RGB image, as well as the pose of the camera (the position and orientation), we can use this depth information from the virtual camera to convert each 2D pixel into its 3D coordinates in space, creating what is known as a point cloud.

Using that point cloud we can create a grid data structure that defines where the robot can move and where the obstacles surrounding our robot are, i.e. an occupancy grid. This depth image, to point cloud, to occupancy grid pipeline is a very common operation within computer vision and robotics.

Extending an official PyBullet code example to create a point cloud from depth information, we were able to take all points above the floor and below the roof in our simulated kitchen. From this, we create an occupancy grid with each grid cell showing the likely traversability of an area of 10cm2. This way, each area in the grid is assigned a probability of being occupied. This is, in fact, the most common format and approach for robot navigation.

Note: Our own simulated PyBullet kitchen and living room with dynamic objects (e.g. beer bottle, bowl, cup, etc) and occupancy grid mapping with exact pose information. Within this GIF, we also present a semantic object occupancy grid map, mapping aggregation and a simulated robot with 2 wheels and a caster wheel moving around the living room with depth and segmented views displayed from the viewpoint of the robot. The semantic map describes which grid cells are occupied by specific objects. The aggregation combines the information from 3 viewpoints (local maps) into one merged map.

So what did we learn?

This taught us a lot about occupancy grids, sensor models and aggregation of multiple viewpoints into one big occupancy grid. But everything mentioned above was found using the exact pose (position and orientation) of the camera provided by PyBullet. We knew that next it was necessary to find a way to localise the agent’s pose in the world, and specifically, in the real world.

As we mentioned in our last blog, we decided to move from simulation towards getting algorithms working on real robots first. One simple reason for this; there are many more problems that occur on real cameras compared to virtual scenes.

In the future, to scale the number of tasks, cups, kettles and environments that our robots can tackle we will most certainly need simulation. But for right now, real-world cup-picking trumps matrix-style every time. As you can see, the road to AGI is long, winding and treacherous. It all begins with first finding yourself.

Simultaneous localisation and mapping (SLAM)

“Not until we are lost do we begin to find ourselves” — Henry David Thoreau

It is relatively easy to create a map using known localisation (the mapping problem), and it’s also relatively easy to localise within a known map (the localisation problem). Many solutions and variations for both exist, but what if we had neither the robot’s pose, nor a map? That is, how do we create a map of the environment when we don’t know where we are?

This, relatively speaking, isn’t so easy.

Return to our kidnapped robot, who has just been placed in an unknown room. Upon opening its eyes how could it understand where it was? Most people will naturally use reference landmarks around them, i.e. a bed, the corner of a TV, an open door, to roughly locate themselves in an environment. Mapping and localisation are inextricably linked, they need each other like toast needs butter and it’s essentially a chicken and egg problem.

In order to pick up cups at breakneck speeds and to operate in homes the robot has never seen before, it should be able to both localise itself within the environment and at the same time map it, i.e. find the 3D positions of parts of the world. This is why in our journey we inevitably arrived at the field of Simultaneous Localisation and Mapping (SLAM).

SLAM research can be categorised by the sensor suite used to approach the problem (more on this later). For example, you could use a camera (or a set of cameras) to find the pose of the robot by landmarks detected in the environment. But you may not trust the weaknesses of these cameras when the lights are off, or the visual scene might be confusing, in which case you could use a radar instead. Or, as with most companies building autonomous cars, you could use an expensive LiDAR if you need a more reliable sensor that can find objects hundreds of meters away from you in high detail.

There is no single sensor to solve all of our problems. We will need to combine many sensors together (i.e. sensor fusion) in our quest to build robots that can understand the world in a way that we can relate to while at the same time, keeping the costs low. To do this we decided to focus our efforts on visual algorithms. That is, camera based SLAM, or as named in the literature, Visual SLAM (V-SLAM).

Over 2 years ago, we mentioned SLAM in our Computer Vision report and now we’re delighted to have the chance to really dive deep into this fascinating technology. If you are looking for a good review, or details on the state of the art, we recommend:

The architecture of a typical SLAM system

A typical Visual SLAM algorithm has two main components that can be easily parallelised, meaning that they can be run independently even though both parts are interconnected. Following the literature, we will refer to these as the “front-end” and “back-end”.

The Front-End

The front-end abstracts sensor data into models that are amenable for estimation. It’s in charge of preprocessing the input, as well as the detection and tracking of relevant landmarks to allow for an estimation of the sequence of poses, from which we observed them.

In the case of VSLAM the algorithm that accomplishes this is visual odometry (VO), which essentially calculates the relative pose (the transformation) between two frames according to salient visual features.

The most common method is to extract keypoints (e.g. using SIFT, ORB) from a frame and then match these same keypoints in the next frame or track them with optical flow. Moreover, while matching/tracking those observed keypoints from different positions, the nature of these algorithms may incur errors of wrong data association, so usually another algorithm is applied afterwards to remove possible outliers that potentially propagate additional error to our estimations, e.g. random sample consensus (RANSAC).

Another important consideration is that if we keep tracking every salient point of every image we would consume an immense amount of memory including lots of redundant information. For efficiency reasons, typically only a subset of all images observed, these “key-frames” are selected by a selection algorithm while matching and tracking the observed features only between keyframes. A simple keyframe selection algorithm would be to take every 5th or 10th frame, but other methods involve only adding a keyframe if the image has changed enough from the previous one (e.g. measuring parallax changed or the distance the keypoints moved).

Source: ORB-SLAM in the KITTI dataset (Sequence 00) YouTube link Note: The top view is from the KITTI dataset with ORB features overlayed from ORB-SLAM2. On the bottom we see top-down trajectory in green but only the blue frustums are keyframes. In red are the features which are tracked.

Note: By adapting a simple VO example in Python (from here) and by using every 10th frame as a keyframe we were able to get reasonably good results on a sequence in one of our houses. The top down trajectory is shown on the right on the black background as we go around a table 3 times with the camera attached to a chair. But this example was carefully cherry-picked; other sequences didn’t show as reliable results on the trajectory estimation. In general, the algorithm suffers from severe drift problems, e.g. by using only every 5th frame as a keyframe this caused caused the drift errors to accumulate much faster. This per keyframe error can be blamed on many factors e.g. calibration, initialisation or monocular scale issues.

Additionally, we could calculate the transformation between two consecutive point clouds (e.g. point cloud registration with the Iterative Closest Point (ICP) algorithm) to localise the agent’s trajectory.

Note: Pointcloud registration is about finding the transformation from one pointcloud to the other so that they overlap and align together in a way that makes most sense. The GIF shows the resulting point cloud (yellow + blue) after the registration step is performed, i.e. one of the point clouds (yellow) is transformed to be aligned to the other one (blue: which is the reference to align to), optimising the transformation so that overlaying this transformed point cloud onto the reference represents a single scene as consistently as possible. This obviously implies some overlap and consistency between both pointclouds. This relative transformation between two frames/pointclouds can be the basis of RGB-D or LiDAR odometry. For reference, a famous algorithm for computing this registration is called Iterative Closest Point (ICP).

Also, we can use a lower dimensional representation of features found in the image instead of their full description, for example by using “Bag of visual words” methods (DBoW), which creates a dictionary of possible features and transforms an image to a vector formed by a combination of the possible features (or “words” in the dictionary) encountered for a more compressed representation. This can then be used for place recognition/relocalisation and loop closing.

Source: Bag of Visual Words in a Nutshell Note: First row are the images, the second row are the image patches from these images and the third row are the histograms “bag of visual words” in a simplified four word dictionary.

The Back-End

The back-end is usually the component that uses all the extracted information from the front-end in order to build, extend and further correct the robot’s trajectory and map. It includes several algorithms like bundle adjustment — where the goal is to correct errors by enforcing reprojective consistency over more than a pair of frames. It also extends to the generation and optimisation of a graph with the different poses estimated, as well as the comparison of the bags of visual words stored by the front-end to accomplish relocalisation and loop closure.

Loop closure consists of applying corrections to the graph when the robot recognises a previously seen place. Using this information, we can alleviate possible cumulative “drift” errors encountered within the whole SLAM process.

To give you a clearer picture of how these different parts interact with each other, here is a high-level architecture of one of our favourite SLAM systems.

Source: RTAB-Map slides Note: Without the back-end, SLAM essentially reduces to odometry. RTAB-Map is an amazing SLAM algorithm that works out of the box and creates a dense pointcloud and an occupancy grid. It can be used with an IMU, stereo camera and an RGB-D camera (more on what these sensors do below)

Sensor suite: Mono, Stereo, RGB-D, IMU

It goes without saying, that different sensor configurations can make the task of SLAM much easier or much harder. Often, the more informative your sensors are the better but with additional sensors you also need to merge and fuse their information in a clever and principled way. This can become quite expensive computationally-speaking. Ideally by using multiple sensors, their pros and cons cancel each other out and together the system is incredibly robust, but often there are trade-offs involved with this choice.

Monocular SLAM involves the use of a single camera as an input to the corresponding algorithms for SLAM. This suffers from scale issues, i.e. from a monocular SLAM system’s perspective we couldn’t tell the difference in size between a regular home and a ‘doll house’ if the camera was scaled accordingly. This scale issue can be solved through various means e.g. a good initialisation procedure or using a known length of an object or real-world distance. Although dealing with these imperfections can be algorithmically more tedious. Having a single sensor is a very simple and elegant solution in terms of robot hardware and architecture. Some famous examples are MonoSLAM, ORB-SLAM and LSD-SLAM.

Note: Example of tracking points using a monocular camera.

From a set of “n” observed points (in this case 4 on the house) in an initial position (from the red image), we move across the scene and capture a second image (highlighted in green). Using matching algorithms we can find the points observed on the first image in the second, and use that information to figure out the motion of the camera (odometry) and the structure of the scene (3D coordinates of the observed points). Structure from Motion (SfM) is another famous field with a lot of similarities to Monocular SLAM. By seeing how much a keypoint moved between two frames we can calculate the extrinsic transformation of the camera in the 2nd frame.

We found that one of our RGB cameras had a very small Field of View (FoV) of horizontal 69.4 degrees that could potentially lead to losing tracking if moving too fast. A larger FoV theoretically allows to keep track of the same keypoints even after longer displacements. In the future we will also experiment with much wider FoV cameras like the fisheye camera which observes a bigger area at a given time and therefore could potentially make our lives easier in keeping the track of the observed scene at higher speeds than a narrower FoV camera would allow.

Note: With a common laptop webcam we can barely capture the center of the living room (right), whereas the 200 degree wide angle camera (left) allows us to look at the entire living room, part of the kitchen on the left and also a small peek into the bedroom after the corridor to the right.

Stereo vision involves the use of two cameras to find the structure of the scene. Having two images from different known positions taken at the same time provides significant advantages. For example, within monocular SLAM the matching happens between two images at different times, which means that between those times any object in the scene could have moved, which would completely ruin the visual odometry calculation, whereas in stereo vision, the matching is done between images taken at the same time, i.e. no movement is needed. However, stereo SLAM and in fact most SLAMs will still have this “dynamic objects problem” since they still have to track the same features taken across multiple frames. Popular stereo SLAMs are VINS-Fusion or SOFT-SLAM.

Moreover, typically matching algorithms require a computationally expensive search for the matches, but since in this case the relative positions of the cameras is known, we can project both images to an imaginary plane in order to make the search easier and prevent some errors on the matching. This process is called stereographic rectification, and it is illustrated in the image below.

Note: The dotted rectangles represent the original unrectified images. The epipolar lines help on matching by constraining the search to the epipolar lines marked in solid red. The solid black rectangles represent the rectified images, in which the epipolar lines are all parallel to the horizon, allowing for a more efficient search of possible matches.

Note: Example of the left and right stereo infrared images (from the RealSense D435i camera we bought in the top and bottom left) with a calculated disparity image on the bottom right and colour image in top right. If you alternate closing your left eye and right eye, you should observe that objects closer to you appear to jump in position more compared to objects further away (see the pen in both infrared images being in different positions). This is essentially how stereo vision can calculate disparity/depth (bottom right). The pen is so close that it gives a blind spot to one camera from calculating disparity in certain parts of the disparity image. Calculating this disparity from two (preferably rectified) stereo images essentially computes the inverse depth map which can be visualised with a colour map going from white to yellow to red, in which white represents close distances and yellow is further away.

Passive vs active. Stereo vision doesn’t only have to use passive RGB cameras to calculate the disparity map, it’s also possible to do active stereo. Active stereo projects light-beams onto the world, and by using the deformation patterns, it can help increase the accuracy of the depth. More info on rectification and active vs passive stereo here.

RGB-D SLAM typically refers to when the input to the SLAM system is both color and depth images. The depth can be achieved by the use of a commercial depth camera which commonly contains a stereo camera but the details of how this is retrieved and optimised is left to internals of the depth camera. RGB-D SLAM is capable of calculating dense maps, i.e. which include all the visible pixels, due to having the depth information of these pixels. Compared to the sparse features mentioned above (i.e. a small number of keypoints), this represents an abundance of information which can be used and displayed in a dense map. In the next blog we’ll cover RTAB-Map (architecture shown earlier) which is an excellent RGB-D SLAM system but other famous examples include KinectFusion and Kintinuous.

Note: We bought the RealSense D435i depth camera from Intel. Above is the RealSense viewer. In the color RGB image (bottom left) we can’t see as much in dark areas compared to the infrared images (top 2) which can see better in the dark. Also enabling and disabling the emitter will show or remove the white projected dots. The emitter improves depth accuracy (you can see as it turns off and on in the depth image in the bottom right) and is good for textured areas. One can disable the IR emitter and use the infrared cameras as a typical stereo pair but it is grayscale only.

Note: The depth accuracy is high and it works out of the box. Active stereo RGB-D cameras works better at low light as well as night time. The camera also contains an internal IMU and we can see the accelerometer and gyroscope display on the right. Half way through, we switch to the 3D point cloud viewer.

Note: An example of the RealSense rs-motion program which demonstrates how a Complementary Filter can be used to estimate orientation but not position on the D435i using the inbuilt IMU only.

Inertial Measurement Units (IMU) usually contain both an accelerometer and a gyroscope (and sometimes a magnetometer). In the IMU contained in the D435i, the first measures acceleration, the second angular velocity and both measure 3 degrees of freedom (DoF) to form 6 DoF altogether in this 6 DoF IMU. Many other variants are possible, e.g. an included magnetometer would make it 9 DoF, and simpler 3 DoF IMUs are often used in autonomous vehicles.

However, to extract position from these involves double integration (since the integral of acceleration is velocity and the integral of velocity is position) and even state of the art solutions involve a lot of localisation deviation which will accumulate dramatically over time. This is called drift and why an IMU localisation system usually needs to be combined with visual feedback for less drift. The main use of the IMU comes from calculating the orientation of the robot at a high frequency from the raw data. For example, in our D435i, the accelerometer can be set to 250Hz (250 measurements per second) and gyro to 200Hz and a filter (e.g. Kalman or Complementary filter) can be used on the raw data to calculate the orientation of the IMU.

When you add an IMU to the VO system, it is called Visual Inertial Odometry (VIO). There are also many approaches (loosely-coupled and tightly-coupled) which can output almost “drift-free” localisation If you choose and carefully tune the correct VIO system. We recommend the “A Benchmark Comparison of Monocular Visual-Inertial Odometry Algorithms for Flying Robots” paper for an overview on the top VIO or SLAMs that use IMUs. In the next blog we’ll talk about some great VIO systems like VINS-Mono and ROVIO.

All SLAM fanatics likely dream of the ultimate SLAM system which can handle any situation (low light, low texture, fast motion, loop closure, occlusions, dynamic objects and scenes) and still be incredibly robust. We call this mystical mythical creature a “Parkour SLAM”.

While this might be pure hogwash and simply a ridiculous dream, algorithms which are suitable for fast drones, like these VIO systems, come close to the speed and flexibility requirements. So maybe one of these could be the Parkour SLAM we’re looking for, or maybe another day in the future the SLAM of our dreams will arrive. Part 4 of this blog series will be our attempt to find this street urchin SLAM algorithm, test it relentlessly with technical questions, and award one SLAMdog Millionaire a million dollars.

Source: ROVIO YouTube Note: Incredibly fast movements, could this be our ultimate champion SLAM, the Parkour SLAM?

Feature-based vs Direct

Source: wavelab waterloo slides. Very good slides that are worth checking out to go deeper into many direct methods and compare them on a high level with feature-based SLAMs.

From the perspective of the Deep Learning (DL) revolution, which has dominated computer vision since 2012, it’s easy to be confused by the fact that most SLAMs use “classic” keypoint or feature detectors (e.g. SIFT, ORB, FAST, Harris corners). This is surprising given all the hype and good results around DL. So why no deep-SLAM?

Firstly, it’s important to mention that in “classic” classification pipelines, keypoint descriptors were used as a feature engineering step before a typical classifier was used to output the class, e.g. a Support Vector Machine (SVM). This paradigm was completely destroyed by DL, but the main takeaway is that keypoints and keypoint descriptors are different: Keypoint descriptors are vectors of values which describe statistical properties of the image patch centred on the keypoint, keypoints refer to the points themselves, i.e. their locations.

Keypoints are very common and intuitively useful here since SLAM finds the geometric relations and the positions of the 3D points which these detected and tracked 2D keypoints represent. And in fact, this is what VSLAM does in a nutshell.

SLAM will likely greatly improve with the added use of DL — for keypoint detection, for semantics or maybe an “end-to-end SLAM”. However, classic features detectors do very well at present. For more on how DL and SLAM could help each other check out Tomasz Malisiewicz’s excellent blog on The Future of Real-Time SLAM and Deep Learning vs SLAM.

Note: This image highlights the difference between keypoints (2D pixel positions) or image descriptors of an image. In general, it’s important for feature detectors to repeatedly find re-identifiable parts of an image which can be found in successive frames e.g. corners, edges, points. Whereas the descriptor of each keypoint is a vector describing an image patch around the keypoint.

Note: Example of keypoints extracted using ORB features in ORB-SLAM2 algorithm running on a sequence from one of our houses. When referring to the word “features” it can mean a combination of the keypoint and its descriptor or just the keypoint. At the end of the GIF we see an example of loop closure fixing the accumulated drift. Keep an eye out for part 4 of this blog which covers the back-end and loop closure for greater depth on this.

There is another rising paradigm within SLAM that avoids the use of sparse keypoint detectors. These so called “direct” methods use pixel intensities — unprocessed RGB values from the image — to directly estimate the motion of the camera. This minimises a photometric loss whereas feature-based methods usually minimise a geometric loss. Photometric essentially means we are calculating how to projectively warp the first image into the second image using the pixel values directly from both images. We enforce consistency on the transformation from both the raw image, and the transformed synthetic generated image, by minimising the intensity differences.

Source: link Note: This highlights the projective transformation (T) between the first image to the second. Finding the correct T is the goal here.

There are many variations within direct methods. For example, we could use the sparse points in Direct Sparse Odometry (DSO). The DSO paper in provides a good comparison between all four combinations of dense + direct, dense + indirect, sparse + direct, sparse + indirect. We encourage you to check it out, especially if you find it confusing like we did.

Another famous paper Semi-dense Visual Odometry (SVO) is a mix of both feature-based and direct methods. Meaning that it only tracks high gradient pixels from edges and corners (using direct photometric error) but relies on feature-based methods for joint optimization of structure and motion. These combinations are why it is deemed to be only ‘semi-direct’.

Direct has the potential to be able to track parts of the image with low texture, whereas feature-based SLAM might find this more difficult. Many famous SLAM fanatics believe direct-based SLAM will eventually prevail over indirect. But the results of feature-based SLAM speak for themselves. Only time will tell.

Conclusion

In summary, we discussed everything from the motivation behind mapping, mapping in simulation, to how localisation and mapping desperately need each other (SLAM); and many related fields (e.g. Visual Odometry, SfM), and the axes of variation between different SLAM algorithms (e.g. their front vs back end, sensor suite, etc.). Not only this, but also the specific methods used, e.g. direct vs feature based, sparse versus dense maps/pointclouds, and whether these systems have global optimisation and loop closure. To illustrate the point of how different SLAM can vary, you can find a comparison between many famous SLAMs in the table below.

Source: Visual SLAM algorithms: a survey from 2010 to 2016 Note: This compares many different SLAMs across their varying dimensions. For example, the visual SLAM algorithms used with the raw image data could be feature-based (ORB-SLAM, MonoSLAM) vs direct (DTAM, LSD-SLAM) vs semi-direct (SVO) vs RGB-D (KinectFusion, SLAM++).

This blog, we hope, should largely introduce and conclude the front-end component of SLAM. In some sense, this component is mostly responsible for calculating the odometry of the camera. In a few cases we saw, it can produce highly accurate localisation without any back-end component needed.

However, the longer the robot’s trajectory, the more drift will accumulate, even if just 1 millimetre in translation or 1/100 of a rotational degree per 10 kilometres, this will compound. Because of this, in our opinion, a back-end system and the ability to recognise previous places is needed. Fortunately, we reserved the details of this component for the next part of this blog series which will include bundle adjustment, pose graph creation and optimization, as well as loop closure.

But the most exciting part is yet to come! In the next instalment of C2C (#cuplife) we will display our selection and evaluation of a number of state of the art systems in SLAM, including some algorithms that we tested and where we see the future of the field potentially heading. Stay tuned!"
AI Fairness — Explanation of Disparate Impact Remover,"AI Fairness — Explanation of Disparate Impact Remover

AI Fairness is an important topic for machine learning practitioners. We must be aware that there can be both positive and negative implications for users when they interact with our models. Although our metric of success tends to be a performance metric (e.g. accuracy), those that interact with our models may consider other values as well. Tools using AI are being built to: approve or deny loans; decide if a person should be considered for an interview, and; determine if someone’s a good candidate for treatment. These outcomes all have high impact repercussions for the individual. This is why fairness is such an important value to consider.

In order to ensure fairness, we must analyze and address any bias that may be present in our training data. Machine learning discovers and generalizes patterns in the data and could, therefore, replicate bias. When implementing these models at scale, it can result in a large number of biased decisions, harming a large number of users.

Introducing Bias

Data collection, processing, and labeling are common activities where we introduce bias in our data.

Data Collection

Bias is introduced due to technologies, or humans, used in collecting the data, e.g. the tool is only available in a specific language

It could be a consequence of the sampling strategy, e.g. insufficient representation of a sub-population is collected

Processing and Labeling

Discarding data, e.g. a sub-population could more commonly have missing values and by dropping those examples, result in under-representation

Human labelers, or decision makers, may favor the privileged group or reinforce stereotypes

Disparate Impact

Disparate Impact is a metric to evaluate fairness. It compares the proportion of individuals that receive a positive output for two groups: an unprivileged group and a privileged group.

The calculation is the proportion of the unprivileged group that received the positive outcome divided by the proportion of the privileged group that received the positive outcome.

The industry standard is a four-fifths rule: if the unprivileged group receives a positive outcome less than 80% of their proportion of the privileged group, this is a disparate impact violation. However, you may decide to increase this for your business.

Mitigation with Pre-Processing

One approach for mitigating bias that some people often suggest is simply to remove the feature that should be protected. For example, if you are concerned of a model being sexist and you have gender available in your data set, remove it from the features passed to the machine learning algorithm. Unfortunately, this rarely fixes the problem.

Opportunities experienced by the privileged group may not have been presented to the unprivileged group; members of each group may not have access to the same resources, whether financial or otherwise. This means their circumstances, and consequently, their features for a machine learning model, are different and not necessarily comparable. This is a consequence of systematic bias.

Let’s take a toy example with an unprivileged group, Blue, and a privileged group, Orange. Due to circumstances out of their control, Blue tend to have lower values for our feature of interest, Feature.

We can plot the distribution of Feature for each of the two groups and visually see this disparity.

If you were to randomly pick a data point, you could use its value of Feature to predict which group you selected from.

For example, if you select a data point with Feature value 6 you would most likely assume the corresponding individual belonged in the Orange group. Conversely, for 5, you’d assume they belonged in Blue.

Feature may not necessarily be a useful attribute to predict the expected outcome. However, if the labels for your training data favor group Orange, Feature will be weighted more highly as it can be used to infer grouping.

As an example, a person’s name doesn’t necessarily impact their ability to do a job and, therefore, shouldn’t impact whether or not they are hired. However, if the recruiter is unconsciously biased, they may infer the candidate’s gender or race from the name and use this as part of their decision making.

Disparate Impact Remover

Disparate Impact Remover is a pre-processing technique that edits values, which will be used as features, to increase fairness between the groups. As seen in the diagram above, a feature can give a good indication as to which group a data point may belong to. Disparate Impact Remover aims to remove this ability to distinguish between group membership.

The technique was introduced in the paper “Certifying and removing disparate impact” by M. Feldman, S. A. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian.

The algorithm requires the user to specify a repair_level , this indicates how much you wish for the distributions of the groups to overlap. Let’s explore the impact of two different repair levels, 1.0 and 0.8.

Repair value = 1.0

This diagram shows the repaired values for Feature for the unprivileged group Blue and privileged group Orange after using DisparateImpactRemover with a repair level of 1.0.

You are no longer able to select a point and infer which group it belongs to. This would ensure no group bias is discovered by a machine learning model.

Repair value = 0.8

This diagram shows the repaired values for Feature for the unprivileged group Blue and privileged group Orange after using DisparateImpactRemover with a repair level of 0.8.

The distributions do not entirely overlap but you would still struggle to distinguish between membership, making it more difficult for a model to do so.

In-Group Ranking

When features show a disparity between two groups, we assume that they’ve been presented with different opportunities and experiences. However, within group, we are assuming that their experiences are similar. Consequently, we wish for an individual’s ranking within their group to be preserved after repair. Disparate Impact Remover preserves rank-ordering within groups; if an individual has the highest score for group Blue, it will still have the highest score among Blues after repair.

Building Machine Learning Models

Once Disparate Impact Remover has been implemented, a machine learning model can be built using the repaired data. The Disparate Impact metric will validate if the model is unbiased (or within an acceptable threshold).

Bias mitigation may result in a lower performance metric (e.g. accuracy) but this doesn’t necessarily mean the final model would be inaccurate.

This is a challenge for AI practitioners: when you know you have biased data, you realize that the ground truth you’re building a model with doesn’t necessarily reflect reality nor the values you wish to uphold.

Example Notebook

As part of my investigation into DisparateImpactRemover , I created an example notebook using a toy dataset. It demonstrates the following:

Calculating Disparate Impact (in Python and with AIF360)

Building a simple Logistic Regression model

Creating a BinaryLabelDataset

Implementing DisparateImpactRemover with two different repair levels

with two different repair levels Validating preservation of in-group ranking

This is available on GitHub here. The library we are using to implement this algorithm is AI Fairness 360.

Final Remark

The concept of fairness is incredibly nuanced and no algorithmic approach to bias mitigation is perfect. However, by considering our users’ values, and implementing these techniques, we are stepping in the right direction to a fairer world."
A deeper look at descent algorithms,"Core requirements to understand this article

Linear Algebra

Multi-variable calculus

Basic idea of convex functions

As we all know, optimization is one of the most important factors in machine learning. Thus it is of our interest to find an algorithm that optimizes functions in a reasonable time. One of the most common algorithms used today is gradient descent. Today, we will take a look at other optimization algorithms and have a theoretical understanding of them.

The core algorithms that will be discussed in this article are:

Newton’s method

Steepest Descent

Gradient Descent

You can learn more about these algorithms from the textbook Convex Optimization: Part 3. We will mostly focus on Quadratic/Polynomial functions in this article

An assumption made on our functions

We always make the assumption that the functions we are dealing with along with their derivatives are continuous (i.e. f ∈ C¹). In the case of Newton’s method, we also need to assume that the second derivatives are continuous. (i.e. f ∈ C²). The last assumption that we make is that the functions we are trying to minimize are convex. Thus, if our algorithm converges to a point (typically known as the local minimum), then we are guaranteed that it is a global optimizer.

Newton’s method

Algorithm for a single-variable function

x_n = starting point

x_n1 = x_n - (f'(x_n)/f''(x_n))

while (f(x_n) != f(x_n1)):

x_n = x_n1

x_n1 = x_n - (f'(x_n)/f''(x_n))

The idea behind Newton’s method is that the function f being minimized is approximated locally by a quadratic function. We then find the exact minimum of that quadratic function and set the next point to be that. We then repeat the procedure.

The case for multi-variable functions

So far, it looks like the Newton’s method is a solid candidate. But typically, we won’t be dealing with a lot of single variable functions. Most of the time, we will be need to optimize functions that have a lot of parameters (i.e., functions in ℝn). So here is the algorithm for the multi-variable case:

Assuming x∈ ℝn , we have:

x_n = starting_point

x_n1 = x_n - inverse(hessian_matrix) (gradient(x_n)) while (f(x_n) != f(x_n1)):

x_n = x_n1

x_n1 = x_n - inverse(hessian_matrix) (gradient(x_n))

where gradient(x_n) is the gradient vector at x_n and hessian_matrix is an n×n hessian symmetric matrix whose entries consist of second derivatives at x_n .

As we all know, inverting a matrix is expensive ( O(n³) ) and thus this method is not commonly used.

Gradient Descent

This is by far the most popular method of optimization used in machine learning and other approximate optimizations. It is an algorithm that involves taking a step in the direction of the gradient at every iteration. It also involves a constant alpha that factors the size of the step to be taken at every iteration. Here is the algorithm:

alpha = small_constant

x_n = starting_point

x_n1 = x_n - alpha * gradient(x_n) while (f(x_n) != f(x_n1)): # May take a long time to converge

x_n = x_n1

x_n1 = x_n - alpha * gradient(x_n)

Here, alpha is a value that you have to pick to update x_n at every iteration (This is called a hyperparameter). We will analyze the values for alpha we pick

If we choose a large value for alpha, we tend to overshoot and go further from the optimizer point. In fact, you can diverge if you choose it to be too big.

Gradient Descent after 10 iterations with an alpha value that is too large.

On the other hand, if we choose alpha to be small, then it will take a lot of iterations to converge to the optimum value. As you get closer to the optimum value, the gradient tends to approach to zero. So, if your alpha value is too small then it may take almost forever to converge to the minimum point.

Gradient Descent after 10 iterations with an alpha value that is too small.

Thus, as you can see, you have a burden of choosing a good constant for alpha. However, if you choose a good alpha, then you can save a lot of time at every iteration

Gradient Descent after 10 iterations with an alpha value that is good enough.

Steepest Descent

Steepest descent is very similar to gradient descent, except it’s more rigorous in that the steps taken at every iteration are guaranteed to be the best steps. Here’s how the algorithm works:

x_n = starting_point

alpha_k = get_optimizer(f(x_n - alpha * gradient(x_n)))

x_n1 = x_n - alpha_n * gradient(x_n) while (f(x_n) != f(x_n1)):

x_n = x_n1

alpha_k = get_optimizer(f(x_n - alpha * gradient(x_n)))

x_n1 = x_n - alpha_n * gradient(x_n)

where x_n and x_n1 are the input vectors in ℝn , gradient is the gradient of f at x_n and alpha_k is such that:

So while optimizing our original function, we need to optimize an inner function at every iteration. Now the good news is that this function is a single-variable function, which means that it’s not as complicated (for example we can use Newton’s method here). However, in most cases, it does tend to get a bit expensive to optimize such a function at every step.

A special case for Quadratic Functions

Consider the squared error loss function:

where I is the identity matrix and y=Qw+b

For simplicity, we’ll only consider finding the optimal value of the weight w (assume b is constant). Through substituting y and simplifying everything, we get the following:

Taking a look back at g(α), we know that if we take the gradient at αk, it should be 0 since it is the minimizer. Taking advantage of that, we have the following:

Simplifying the above mess and substituting for the gradient of f at the two points, we get the following for αk

Now we have a concrete value for αk in the case of quadratic functions.

Convergence analysis for quadratic functions

In an example for a quadratic function in ℝ², steepest descent typically tends to get very close to the optimal within less than ten steps.

Steepest descent in 2 dimensions after 4 iterations.

In the above figure, notice that the change in direction is perpendicular at every iteration. After 3 to 4 iterations, we notice that the change in derivatives is almost negligible.

Why isn’t steepest descent used?

So why isn’t this algorithm used quite often? It clearly gets rid of the need of a hyperparameter to tune and it is guaranteed to converge to a local minimum. The issue here is that at every iteration, we need to optimize alpha_k which is a little expensive considering that we have to do it at every step.

For example, in the case of the quadratic function, we had to compute multiple matrix multiplications and vector dot products at every iteration. Compare this to Gradient Descent, where at every step, we just need to compute the derivative and update the new value which is way less expensive.

Steepest descent is also difficult to generalize in the case of non-quadratic functions where there may not be a concrete value for alpha_k

A comparison between Gradient Descent and Steepest Descent

We will make a comparison between gradient descent and steepest descent and analyze their time complexities. First, we will make a comparison in timing between the two algorithms. We will create a quadratic function f:ℝ²⁰⁰⁰→ℝ (Which involves a 2000×2000 matrix). We will then optimize the function and restrict the number of iterations to 1000. Then, we will compare the time taken and how close value x_n is is to the optimizer between the two algorithms.

Let’s take a look at steepest descent first:

0 Diff: 117727672.56583363 alpha value: 8.032725864804974e-06

100 Diff: 9264.791000127792 alpha value: 1.0176428564615889e-05

200 Diff: 1641.154644548893 alpha value: 1.0236993350903281e-05

300 Diff: 590.5089467763901 alpha value: 1.0254560482036439e-05

400 Diff: 279.2355946302414 alpha value: 1.0263893422517941e-05

500 Diff: 155.43169915676117 alpha value: 1.0270028681773919e-05

600 Diff: 96.61812579631805 alpha value: 1.0274280663010468e-05

700 Diff: 64.87719237804413 alpha value: 1.027728512597358e-05

800 Diff: 46.03102707862854 alpha value: 1.0279461929697766e-05

900 Diff: 34.00975978374481 alpha value: 1.0281092917213468e-05

Optimizer found with x = [-1.68825261 5.31853629 -3.45322318 ... 1.59365232 -2.85114689 5.04026352] and f(x)=-511573479.5792374 in 1000 iterations

Total time taken: 1min 28s

Here’s the output from gradient descent, with alpha = 0.000001

0 Diff: 26206321.312622845 alpha value: 1e-06

100 Diff: 112613.38076114655 alpha value: 1e-06

200 Diff: 21639.659786581993 alpha value: 1e-06

300 Diff: 7891.810685873032 alpha value: 1e-06

400 Diff: 3793.90934664011 alpha value: 1e-06

500 Diff: 2143.767760157585 alpha value: 1e-06

600 Diff: 1348.4947955012321 alpha value: 1e-06

700 Diff: 914.9099299907684 alpha value: 1e-06

800 Diff: 655.9336211681366 alpha value: 1e-06

900 Diff: 490.05882585048676 alpha value: 1e-06

Optimizer found with x = [-1.80862488 4.66644055 -3.08228401 ... 2.46891076 -2.57581774 5.34672724] and f(x)=-511336392.26658595 in 1000 iterations

Total time taken: 1min 16s

As you can see, gradient descent tends to be faster although not by a lot (seconds or minutes). But more importantly, although the value for α was not chosen as the best parameter in gradient descent, steepest descent takes steps that are much better than gradient descent. In the above example, the difference between f(xprev) and f(xcurr) in the 900th for gradient descent was 450. That difference was passed way earlier in steepest descent (Roughly between iteration 300 and 400).

Thus, if we factor in taking only 300 iterations for steepest descent, we get the following:

0 Diff: 118618752.30065191 alpha value: 8.569151292666038e-06

100 Diff: 8281.239207088947 alpha value: 1.1021416896567156e-05

200 Diff: 1463.1741587519646 alpha value: 1.1087402059869253e-05

300 Diff: 526.3014997839928 alpha value: 1.1106776689082503e-05 Optimizer found with x = [-1.33362899 5.89337889 -3.31827817 ... 1.77032789 -2.86779156 4.56444743] and f(x)=-511526291.3367646 in 400 iterations

Time taken: 35.8s

Therefore, steepest descent is actually faster. It just goes to show that you really need fewer steps per iteration if you want to approximate the optimum. In fact, if your goal is to approximate the optimum, then steepest descent generates values closer to the optimum for small dimension functions in just 10 steps compared to 1000 in gradient descent!

Here’s an example where we have a quadratic function from ℝ³⁰→ℝ. With 10 steps, steepest descent generated f(x) = -62434.18 . Within 1000 steps, gradient descent generated f(x) = -61596.84 . In just 10 steps, steepest descent decrease to an f-value lower than that of gradient descent in a 1000 steps!

Keep in mind that the above works really well only because we are dealing with quadratic functions. In general, it does get difficult to find the value for αk at every iteration. Optimizing g(α) doesn’t always lead you to find a concrete value for αk. Usually, we tend to use iterative algorithms to minimize such functions. In this case, things get tedious and much slower compared to gradient descent. This is why steepest descent isn’t as popular.

Conclusion

In conclusion, we learned three types of algorithms:

Newton’s method

Newton’s method provides a quadratic approximation to a function and optimizes that at every step. The biggest disadvantage is that it involves inverting a matrix for the multi-variable case(which can be expensive when dealing with a vector with a lot of features)

Gradient Descent

Gradient descent is the most common optimization algorithm. It is very quick in that the most expensive thing to do at each step is just computing the derivative. However, it does involve “guessing” or “tweaking” a hyperparameter that tells you how far you should go at each step.

Steepest Descent

Steepest descent is an algorithm that finds the best step to take given the gradient vector of the function. The only issue is that it involves optimizing a function at every iteration which typically tends to be expensive. In the case of quadratic functions, steepest descent performs generally well, although it does involve a lot of matrix calculation per step.

A notebook version of this article can be found here"
Five tips for getting started in data science programming,"Five tips for getting started in data science programming

Put the time and effort in early and it will make you a great programmer later on Keith McNulty · Follow Published in Towards Data Science · 7 min read · Aug 11, 2019 -- 1 Share

If you want to be a genuine data scientist, you need to be able to code. There’s no getting around it. Some people don’t like this idea, and a number of companies are already tapping into that discomfort by offering ‘automated data science’ products — we do the coding so you don’t have to. If these are your only toolkit, you are not a data scientist.

The litmus test of a strong data scientist is that they are not scared of any data set or any problem. They might not know how to handle it straight away, and in fact it’s quite common that they don’t. But they know they can find out how, and they can eventually produce neat, efficient, reproducible code to handle the problem if it comes their way again. If you want to be a great data scientist, that’s the mindset you need to aim for.

So much of the inner confidence and quiet competence of a strong data scientist comes from how they learned to code in the first place. If you are just starting out, how you go about those early weeks and months of learning are critical to whether or not you will flourish further down the line. If you take the lazy approach — the how but not the why — you’ll develop habits that will make you less confident and efficient later. If you put the work in early — understand the how AND the why — you’ll gradually start to feel that confidence build and your capability expand faster and faster as the months go by.

Here are five tips to help you make a great start as you embark on your learning.

1. Choose the right learning sources

People learn in different ways. For example, I am not great at video learning. I need a detailed written narrative that I can carefully analyze and understand at a pace that I am happy with.

Avoid sources that are too practical — this means that they show you what to do but don’t explain why it works. If you are copy-pasting a method to solve a coding problem, and you have no idea why the method worked, then you haven’t really learned anything because you will have no idea how to apply that method later if a similar problem pops up again.

Good learning sources invest time in breaking down the underlying logic of a method. The best ones actually encourage you to…"
CNN vs fully-connected network for image processing,"Introduction

The objective of this article is to provide a theoretical perspective to understand why (single layer) CNNs work better than fully-connected networks for image processing. Linear algebra (matrix multiplication, eigenvalues and/or PCA) and a property of sigmoid/tanh function will be used in an attempt to have a one-to-one (almost) comparison between a fully-connected network (logistic regression) and CNN. Finally, the tradeoff between filter size and the amount of information retained in the filtered image will be examined for the purpose of prediction. For simplicity, we will assume the following:

The fully-connected network does not have a hidden layer (logistic regression) Original image was normalized to have pixel values between 0 and 1 or scaled to have mean = 0 and variance = 1 Sigmoid/tanh activation is used between input and convolved image, although the argument works for other non-linear activation functions such as ReLU. ReLU is avoided because it breaks the rigor of the analysis if the images are scaled (mean = 0, variance = 1) instead of normalized Number of channels = depth of image = 1 for most of the article, model with higher number of channels will be discussed briefly The problem involves a classification task. Therefore, C > 1 There are no non-linearities other than the activation and no non-differentiability (like pooling, strides other than 1, padding, etc.) Negative log likelihood loss function is used to train both networks

Symbols and Notation

Symbols used are:

x: Matrix of 2-D input images W₁, b₁: Weight matrix and bias term used for mapping

Raw image to output in fully-connected network

Filtered image to output in CNN p: Output probability X₁: Filtered image x₁: Filtered-activated image

Two conventions to note about the notation are:

Dimensions are written between {} Different dimensions are separated by x. Eg: {n x C} represents two dimensional ‘array’

Model definition

Fully-connected network

FC1: Pre-ouptut layer

FC2: Estimated probability

Convolution neural network

C1: Filtered image

C2: Filtered-activated image

Activation functions

C3: Pre-output layer

C4: Estimated probability

The Mathematics

Reducing the CNN to a fully-connected network

Let us assume that the filter is square with kₓ = 1 and K(a, b) = 1. Therefore, X₁ = x. Now the advantage of normalizing x and a handy property of sigmoid/tanh will be used. It is discussed below:

Required property of sigmoid/tanh

Sigmoid activation as a function of input. Courtesy: ResearchGate article [1]

We observe that the function is linear for input is small in magnitude. Since the input image was normalized or scaled, all values x will lie in a small region around 0 such that |x| < ϵ for some non-zero ϵ. Therefore, for some constant k and for any point X(a, b) on the image:

This suggests that the amount of information in the filtered-activated image is very close to the amount of information in the original image. All the pixels of the filtered-activated image are connected to the output layer (fully-connected).

Let us assumed that we learnt optimal weights W₁, b₁ for a fully-connected network with the input layer fully connected to the output layer. We can directly obtain the weights for the given CNN as W₁(CNN) = W₁/k rearranged into a matrix and b₁(CNN) = b₁. Therefore, for a square filter with kₓ = 1 and K(1, 1) = 1 the fully-connected network and CNN will perform (almost) identically.

Since tanh is a rescaled sigmoid function, it can be argued that the same property applies to tanh. This can also be observed in the plot below:

tanh activation as a function of input. Courtesy: Wolfram MathWorld [2]

Filter — worst-case scenario

Let us consider a square filter on a square image with kₓ = nₓ, and K(a, b) = 1 for all a, b. Firstly, this filter maps each image to one value (filtered image), which is then mapped to C outputs. Therefore, the filtered image contains less information (information bottleneck) than the output layer — any filtered image with less than C pixels will be the bottleneck. Secondly, this filter maps each image into a single pixel equal to the sum of values of the image. This clearly contains very little information about the original image. Let us consider MNIST example to understand why: consider images with true labels ‘2’ and ‘5’. Sum of values of these images will not differ by much, yet the network should learn a clear boundary using this information.

Relaxing the worst-case part 1: filter weights

Let us consider a square filter on a square image with kₓ = nₓ but not all values are equal in K. This allows variation in K such that importance is to give to certain pixels or regions (setting all other weights to constant and varying only these weights). By varying K we may be able to discover regions of the image that help in separating the classes. For example — in MNIST, assuming hypothetically that all digits are centered and well-written as per a common template, this may create reasonable separation between the classes even though only 1 value is mapped to C outputs. Consider this case to be similar to discriminant analysis, where a single value (discriminant function) can separate two or more classes.

Relaxing the worst-case part 2: filter width

Let us consider a square filter on a square image with K(a, b) = 1 for all a, b, but kₓ ≠ nₓ. For example, let us consider kₓ = nₓ-1. The original and filtered image are shown below:

Original image

Filtered image

Notice that the filtered image summations contain elements in the first row, first column, last row and last column only once. All other elements appear twice. Assuming the values in the filtered image are small because the original image was normalized or scaled, the activated filtered image can be approximated as k times the filtered image for a small value k. Under linear operations such as matrix multiplication (with weight matrix), the amount of information in k*x₁ is same as the amount of information in x₁ when k is non-zero (true here since the slope of sigmoid/tanh is non-zero near the origin). Therefore, the filtered-activated image contains (approximately) the same amount of information as the filtered image (very loosely written for ease of understanding, because [Fisher] ‘information’ is the variance of the score function, which is related to the variance of the RV. A better version of this statement is: “the scaled/normalized input image and scaled/normalized filtered will have approximately the same amount of information”).

Assuming the original image has non-redundant pixels and non-redundant arrangement of pixels, the column space of the image reduced from (nₓ, nₓ) to (2, 2) on application of (nₓ-1, nₓ-1) filter. This causes loss of information, but it is guaranteed to retain more information than (nₓ, nₓ) filter for K(a, b) = 1. As the filter width decreases, the amount of information retained in the filtered (and therefore, filtered-activated) image increases. It reaches the maximum value for kₓ = 1.

In a practical case such as MNIST, most of the pixels near the edges are redundant. Therefore, almost all the information can be retained by applying a filter of size ~ width of patch close to the edge with no digit information.

Putting things together

A peculiar property of CNN is that the same filter is applied at all regions of the image. This is called weight-sharing. The total number of parameters in the model = (kₓ * kₓ) + (nₓ-kₓ+1)*(nₓ-kₓ+1)*C.

Larger filter leads to smaller filtered-activated image, which leads to smaller amount of information passed through the fully-connected layer to the output layer. This leads to low signal-to-noise ratio, higher bias, but reduces the overfitting because the number of parameters in the fully-connected layer is reduced. This is a case of high bias, low variance. Smaller filter leads to larger filtered-activated image, which leads to larger amount of information passed through the fully-connected layer to the output layer. This leads to high signal-to-noise ratio, lower bias, but may cause overfitting because the number of parameters in the fully-connected layer is increased. This is a case of low bias, high variance.

It is known that K(a, b) = 1 and kₓ=1 performs (almost) as well as a fully-connected network. By adjusting K(a, b) for kₓ ≠ 1 through backpropagation (chain rule) and SGD, the model is guaranteed to perform better on the training set. It also tends to have a better bias-variance characteristic than a fully-connected network when trained with a different set of hyperparameters (kₓ).

Summing up

A CNN with kₓ = 1 and K(1, 1) = 1 can match the performance of a fully-connected network. The representation power of the filtered-activated image is least for kₓ = nₓ and K(a, b) = 1 for all a, b. Therefore, by tuning hyperparameter kₓ we can control the amount of information retained in the filtered-activated image. Also, by tuning K to have values different from 1 we can focus on different sections of the image. By doing both — tuning hyperparameter kₓ and learning parameter K, a CNN is guaranteed to have better bias-variance characteristics with lower bound performance equal to the performance of a fully-connected network. This can be improved further by having multiple channels.

Extending the above discussion, it can be argued that a CNN will outperform a fully-connected network if they have same number of hidden layers with same/similar structure (number of neurons in each layer).

However, this comparison is like comparing apples with oranges. An appropriate comparison would be to compare a fully-connected neural network with a CNN with a single convolution + fully-connected layer. Comparing a fully-connected neural network with 1 hidden layer with a CNN with a single convolution + fully-connected layer is fairer.

MNIST data set in practice: a logistic regression model learns templates for each digit. This achieves good accuracy, but it is not good because the template may not generalize very well. A CNN with a fully connected network learns an appropriate kernel and the filtered image is less template-based. A fully-connected network with 1 hidden layer shows lesser signs of being template-based than a CNN.

References

Sigmoid: https://www.researchgate.net/figure/Logistic-curve-From-formula-2-and-figure-1-we-can-see-that-regardless-of-regression_fig1_301570543

Tanh: http://mathworld.wolfram.com/HyperbolicTangent.html"
Neural Network for Satellite Data Classification Using Tensorflow in Python,"Deep Learning has taken over the majority of fields in solving complex problems, and the geospatial field is no exception. The title of the article interests you and hence, I hope that you are familiar with satellite datasets; for now, Landsat 5 TM. Little knowledge of how Machine Learning (ML) algorithms work, will help you grasp this hands-on tutorial quickly. For those who are unfamiliar with ML concept, in a nutshell, it is establishing the relationship between a few characteristics (features or Xs) of an entity with its other property (value or label or Y) — we provide plenty of examples (labelled data) to the model so that it learns from it and then predicts values/ labels for the new data (unlabelled data). That is enough of theory brush-up for machine learning!

The general problem with satellite data:

Two or more feature classes (e.g. built-up/ barren/ quarry) in the satellite data can have similar spectral values, which has made the classification a challenging task in the past couple of decades. The conventional supervised and unsupervised methods fail to be the perfect classifier due to the aforementioned issue, although they robustly perform the classification. But, there are always related issues. Let us understand this with the example below:

In the above figure, if you were to use a vertical line as a classifier and move it only along the x-axis in such a way that it classifies all the images to its right as houses, the answer might not be straight forward. This is because the distribution of data is in such a way that it is impossible to separate them with just one vertical line. However, this doesn’t mean that the houses can’t be classified at all!

Let us say you use the red line, as shown in the figure above, to separate the two features. In this instance, the majority of the houses were identified by the classifier but, a house was still left out, and a tree got misclassified as a house. To make sure that not even a single house is left behind, you might use the blue line. In that case, the classifier will cover all the house; this is called a high recall. However, not all the classified images are truly houses, this is called a low precision. Similarly, if we use the green line, all the images classified as houses are houses; therefore, the classifier possesses high precision. The recall will be lesser in this case because three houses were still left out. In the majority of cases, this trade-off between precision and recall holds.

The house and tree problem demonstrated above is analogous to the built-up, quarry and barren land case. The classification priorities for satellite data can vary with the purpose. For example, if you want to make sure that all the built-up cells are classified as built-up, leaving none behind, and you care less about pixels of other classes with similar signatures being classified as built-up, then a model with a high recall is required. On the contrary, if the priority is to classify pure built-up pixels only without including any of the other class pixels, and you are okay to let go of mixed built-up pixels, then a high precision classifier is required. A generic model will use the red line in the case of the house and the tree to maintain the balance between precision and recall.

Data used in the current scope

Here, we will treat six bands (band 2 — band 7) of Landsat 5 TM as features and try to predict the binary built-up class. A multispectral Landsat 5 data acquired in the year 2011 for Bangalore and its corresponding binary built-up layer will be used for training and testing. Finally, another multispectral Landsat 5 data acquired in the year 2011 for Hyderabad will be used for new predictions. To know more about how to create training data for the model, you can check out this video.

Since we are using labelled data to train the model, this is a supervised ML approach.

Multispectral training data and its corresponding binary built-up layer

We will be using Google’s Tensorflow library in Python to build a Neural Network (NN). The following other libraries will be required, please make sure you install them in advance (check out this video for installation instructions):

pyrsgis — to read and write GeoTIFF scikit-learn — for data pre-processing and accuracy checks numpy — for basic array operations Tensorflow — to build the neural network model

Without further delay, let us get started with coding.

Place all the three files in a directory — assign the path and input file names in the script, and read the GeoTIFF files.

The raster module of the pyrsgis package reads the GeoTIFF’s geolocation information and the digital number (DN) values as a NumPy array separately. For details on this, please refer to the pyrsgis page.

Let us print the size of the data that we have read.

Output:

Bangalore multispectral image shape: 6, 2054, 2044

Bangalore binary built-up image shape: 2054, 2044

Hyderabad multispectral image shape: 6, 1318, 1056

As evident from the output, the number of rows and columns in the Bangalore images is the same, and the number of layers in the multispectral images are the same. The model will learn to decide whether a pixel is built-up or not based on the respective DN values across all the bands, and therefore, both the multispectral images should have the same number of features (bands) stacked in the same order.

We will now change the shape of the arrays to a two-dimensional array, which is expected by the majority of ML algorithms, where each row represents a pixel. The convert module of the pyrsgis package will do that for us.

Schemata of restructuring of data

Output:

Bangalore multispectral image shape: 4198376, 6

Bangalore binary built-up image shape: 4198376

Hyderabad multispectral image shape: 1391808, 6

In the seventh line of the code snippet above, we extract all the pixels with the value one. This is a fail-safe to avoid issues due to NoData pixels that often has extreme high and low values.

Now, we will split the data for training and validation. This is done to make sure that the model has not seen the test data and it performs equally well on new data. Otherwise, the model will overfit and perform well only on training data.

Output:

(2519025, 6)

(2519025,)

(1679351, 6)

(1679351,)

The test_size (0.4) in the code snippet above signifies that the training-testing proportion is 60/40.

Many ML algorithms including NNs expect normalised data. This means that the histogram is stretched and scaled between a certain range (here, 0 to 1). We will normalise our features to suffice this requirement. Normalisation can be achieved by subtracting the minimum value and dividing by range. Since the Landsat data is an 8-bit data, the minimum and maximum values are 0 and 255 (2⁸ = 256 values).

Note that it is always a good practice to calculate the minimum and maximum values from the data for normalisation. To avoid complexity, we will stick to the default rage of the 8-bit data here.

Another additional pre-processing step is to reshape the features from two-dimensions to three-dimensions, such that each row represents an individual pixel.

Output:

(2519025, 1, 6) (1679351, 1, 6) (1391808, 1, 6)

Now that everything is in place, let us build the model using keras. To start with, we will use the sequential model, to add the layers one after the other. There is one input layer with the number of nodes equal to nBands. One hidden layer with 14 nodes and ‘relu’ as the activation function is used. The final layer contains two nodes for the binary built-up class with ‘softmax’ activation function, which is suitable for categorical output. You can find more activation functions here.

Neural Network architecture

As mentioned in line 10, we compile the model with ‘adam’ optimiser. (There are several others that you can check.) The loss type that we will be using, for now, is the categorical-sparse-crossentropy. You can check details here. The metric for model performance evaluation is ‘accuracy’.

Finally, we run the model on xTrain and yTrain with two epochs (or iterations). Fitting the model will take some time depending on your data size and computational power. The following can be seen after the model compilation:

Let us predict the values for the test data that we have kept separately, and perform various accuracy checks.

The softmax function generates separate columns for each class type probability values. We extract only for class one (built-up), as mentioned in the sixth line in the code snippet above. The models for geospatial-related analysis become tricky to evaluate because unlike other general ML problems, it would not be fair to rely on a generalised summed up error; the spatial location is the key to the winning model. Therefore, the confusion matrix, precision and recall can reflect a clearer picture of how well the model performs.

Confusion matrix, precision and recall as displayed in the terminal

As seen in the confusion matrix above, there are thousands of built-up pixels classified as non-built-up and vice versa, but the proportion to the total data size is less. The precision and recall as obtained on the test data are more than 0.8.

You can always spend some time and perform a few iterations to find the optimum number of hidden layers, the number of nodes in each hidden layer, and the number of epochs to get accuracy. Some commonly used remote sensing indices such as the NDBI or NDWI can also be used as features, as and when required. Once the desired accuracy is reached, use the model to predict for the new data and export the GeoTIFF. A similar model with minor tweaks can be applied for similar applications.

Note that we are exporting the GeoTIFF with the predicted probability values, and not its thresholded binary version. We can always threshold the float type layer in a GIS environment later, as shown in the image below.

Hyderabad built-up layer as predicted by the model using the multispectral data

The accuracy of the model has been evaluated already with precision and recall — you can also do the traditional checks (e.g. kappa coefficient) on the new predicted raster. Apart from the aforementioned challenges of satellite data classification, other intuitive limitations include the inability of the model to predict on data acquired in different seasons and over different regions, due to variation in the spectral signatures.

The model that we used in the present article is a very basic architecture of the NN, some of the complex models including Convolution Neural Networks (CNN) have been proven by researchers to produce better results. To get started with CNN for satellite data classification, you can check out this post “Is CNN equally shiny on mid-resolution satellite data?”. The major advantage of these methods is the scalability once the model is trained.

Please find the data used and the full script here."
Data Science Trends for 2020,"Data Science Trends for 2020

Right now, people are already celebrating the end of the decade (fun fact: the 2020s decade actually begins on Jan. 1, 2021). As last year, I decided to thinking over what has happened and what can we expect, in my opinion, regarding Data Science for 2020.

I’ve never expected my last year’s reflection post to have so many reads. From the feedback, people found it interesting and without further due, let’s dive into this year’s thoughts and keep the forum open!

Data Science arrive to conservative areas

In 2019, we began to see machine learning solutions being applied to conservative (and much more regulated) fields, like healthcare — and, now, taken seriously!

One of the most interesting Kaggle challenges I’ve noticed this year was the identification of Pneumothorax disease in chest x-rays. In essence, a collapsed lung leading to severe breathing difficulties. It can be a very difficult and subtle condition to identify. This would be just another Kaggle challenge like any other, but now it included the participation of the actual institutions that study this condition. I’m sure these machine learning algorithms could have been handy for me four years ago… A system like this was already approved by FDA.

Pneumothorax impacts nearly 74,000 Americans every year [source: Venture Beat, GE Healthcare]

The main outcome of this step is not just to help affected individuals, but also, the remarkable participation of the higher hierarchy entities — the decision makers —to enable the use of these kind of technologies in areas that were before very wary on embracing these technologies. As many data scientists know, this is usually one of the most difficult bottlenecks to overcome in new projects.

Another example, not just from 2019, is the use of data science to identify drug-supplemental interactions (supp.ai). If you know combination, you can have an idea of the magnitude of what we’re talking about.

Data privacy by design

As users become more concerned about the handling and the ownership of their data, engineers and data scientists have to find ways to satisfy these new user requirements. Countless data breaches (you can take a look at this impressive 2019’s summary) can be mitigated if companies do not actually own the user data for their business needs.

Already introduced a couple of years ago, Federated Learning has become a hot topic regarding data privacy. In essence,

Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model with training data distributed over a large number of clients each with unreliable and relatively slow network connections. [Google AI]

It might not be the silver bullet, but it may help software engineers and data scientists to architect systems following privacy by design. We can see this as an opportunity to use less data, while still being able to create products that are useful.

Federated Learning is a new a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging their data samples [source: Google AI, Wikipedia]

Doing more with less (data). Kind of counter intuitive for DATA Science, right? Yes, it may be. But trust will be a key topic in 2020. How do you make sure users trust software companies with their data? For example, using Federated Learning to train prediction models for mobile keyboards without uploading sensitive typing data to servers (Hard, et al., 2018).

Servers (companies) do not need to know about every piece of data you generate. [Hard, et al., 2018]

If Federated Learning is a new concept for you, or you wish to understand it a little bit better, I totally recommend starting with this fantastic comic. You can even start creating your own first system with the open source framework Tensorflow Federated!

Mitigate model biases and discrimination

One of the biggest headlines in November 2019 was related with Apple / Goldman Sachs credit card (most likely) gender discrimination. Users noticed that it seemed to offer smaller lines of credit to women than to men. Even if it all started with a viral tweet from David Heinemeier Hansson, it reminded us the same old problem of fully automated machine decisions: what were the main drivers that leaded to the final decision? Nobody explained so far, and it is difficult to get help from a human nowadays.

As soon as a model is put into production it is up for scrutiny of users, specially if the decisions directly affect high stakes personal issues.

As scary as it may seem, data scientists know that not including gender as an input of the model does not mean you cannot be accused of gender discrimination. Now read the official tweet from Goldman Sachs... With these kind of responses, you can see the current ingenuity persisting in some companies when using new pieces of technology. It is important to remember that,

[This is the risk that] modern algorithms will result in “proxy discrimination.” Proxy discrimination is a particularly pernicious subset of disparate impact. Like all forms of disparate impact, it involves a facially-neutral practice that disproportionately harms members of a protected class. [Proxy Discrimination in the Age of Artificial Intelligence and Big Data]

We should always keep in mind that sophisticated models trained on biased data can learn discriminatory patterns, which leads to skewed decisions. And these models can find proxies that lead to discrimination, even if you don’t actually include the actual features you think may have historical biases.

It’s up to the data scientists (including machine learning engineers) to come up with new bias identification and mitigation algorithms to help mitigate harm on users [image source: Adeli, et al., 2019]

As a path forward, bias discrimination will continue to be one of the main topics, and unfortunately we’ll see many more headlines like this, before companies take this issue seriously. Data scientists can help by performing exemplary exploratory data analysis, making sure the data is representative of the whole population, and exploring new architectures that can identify and mitigate these biases.

Python as the de facto language for Data Science

When I switched to the data science field five years ago, the question — What programming language should I focus on [for data science]? — was a really difficult one to answer. As the years go by, with the massive growth of Python, it has become easier to answer. As stated in Stack Overflow’s annual developer survey, released in April 2019:

Python, the fastest-growing major programming language, has risen in the ranks of programming languages in our survey yet again, edging out Java this year and standing as the second most loved language (behind Rust).

It is not just Stack Overflow, other websites also mention Python as one of the best bets you can do if you’re planning to start programming, having a huge supporting community on your side, or making a quick prototype. And the last aspect is usually the most relevant for data scientists nowadays. It is difficult to have any other programming language helping data scientists more.

Recall that Data Science is a concept to unify statistics, data analysis, machine learning and their related methods (this gets back to 1996 with Hayashi!). I like to add to it “as fast as possible”. That’s where Python fits in.

Python can safely be confirmed as the “de facto” data science programming language

I believe Python enabled the huge growth of Data Science (or is it the other way around?). If this is still a dubious decision for you, just go make some research to find out.

Focus as “leave me alone, I just want to get things done”

Focus in 2020! As so many new articles get published everyday — by the end of 2018 it was already around ~100 ML papers per day were published on arXiv.org (yes, they are not peer reviewed, but even though…) — focus must be something you add to your goals.

As the idea of data scientist unicorn (fortunately) vanishes, the concept of specialization in data science has matured. Like the first split in a Decision Tree (sorry about that…), you can go two ways:

Heavy engineer path, with focus on data pipelines, productionization, software engineering. If you come from computer science, this may be easier for you. Heavy analytical path, with focus on statistics, data analysis, business knowledge. If you come from applied math, physics, or even a social field, this may be an easier start for you.

It does not mean the two paths never cross, but you should pursue one of them. If you want to enter into more detail, you can even find more specific job names as I mentioned last year.

At the end of the day, what will be more important is to get things done! Remember you don’t have to create things from scratch to make great things. Build on top of other great open source tools such as Tensorflow Extended or PyTorch. Both tools enable you to overcome many initial banging head on the wall moments.

Keep your boat well steered as you enter 2020. Focus on what you really want to do. [source: Evelyn Paris]

Have a great 2020!

2019 was a crazy rollercoaster ride… Having joined Google NYC was one of the loops for me. Have a great 2020!

Hugo Lopes"
Dating App Matching Optimization Development,"PyCon JP 2019 is held in 2019/9/16~ 2019/9/17 for two days. This post is about one talk on the second day.

Takashi Morioka is the CEO of the dating App Dine. His talk mainly focuses on his development experiences for the matching optimization. The data processing is under Google Cloud Dataflow (Apache Beam Python SDK) and the ML part is done by Cloud ML Engine. Slides are here.

Background

The goal of a dating app is to match two users. The main matching approaches have two methods, the searching method, and the recommendation method.

Searching method: User search by their preference

Recommendation method: The dating site will recommend the candidate for user

Takashi Morioka told Dine mainly uses the recommendation method.

To solve the stable marriage problem, Dine uses the Gale–Shapley algorithm. But this algorithm has a big problem, the computing cost is huge.

Reduce the computing cost

See we have total N users. We first calculate the score from use A against other N-1 users. Then, we rank N users and recommend for user A. The runtime will be O(N² log N).

To reduce the computing cost, Dine split users into different groups (the speaker doesn’t mention the split standard). The advantages of grouping are it can reduce the user number from N to group numbers and calculate the score parallel.

But still, there are two problems."
The Case for Data Science as the Modern Liberal Arts,"The Case for Data Science as the Modern Liberal Arts

The liberal arts bring together insights from various fields and data. That’s exactly what modern data science requires of us. Alexander Titus · Follow Published in Towards Data Science · 5 min read · Aug 11, 2019 -- Share

Liberal arts education has been around for a long time. Back in a time when our understanding of natural phenomena was limited, it took integrated training across the arts, sciences, and humanities to be able to make sense of the world. As we began to understand more about specific fields, we required more depth of expertise to make advances in that field.

Modern science has moved back toward benefiting from a liberal arts education. It’s why data science should be considered as the modern liberal arts. The breadth of expertise required of data scientists is often criticized as the kind of training that simply can’t exist. In reality, however, the liberal arts have provided broad integrated education to young minds around the world, and modern data science is the emergence of a new form of liberal arts careers.

The demands of being a data scientist

Data science has been widely accepted as one of the hottest jobs of the 21st century. To be good at your job, you’re expected to be an expert in computer science, statistics, data engineering, data visualization, executive presentations, and persuasive writing. The world is flustered at all the expectations, largely because they expect “expertise” in all of that. What the technical world is coming to terms with is that when you have such breadth of application, you end up not knowing where to apply your skills. Similarly, when you are presented with an abundance of data, you have to systematically cull through it all to make sense of it.

Liberal arts education has been teaching breadth and synthesis for decades"
How Microsoft Azure Machine Learning Studio Clarifies Data Science,"How Microsoft Azure Machine Learning Studio Clarifies Data Science

Two great tastes that taste great together — Azure model construction + data science knowledge

I’ve been dying to test drive one of the many recent tools on the market targeted at “citizen data scientists” like DataRobot, H20 Driverless AI, Amazon SageMaker and Microsoft’s new product in the cloud called Microsoft Azure Machine Learning Studio (Studio). These tools promise to accelerate the time to value of data science projects by simplifying machine learning model construction. Ultimately, this will allow data engineers, programmers, business analysts and others without PhDs to start chipping away at the massive modeling opportunity companies are eager to tap into but are limited in their ability to address due to data science skills shortage.

So I opened up an Azure account and spent some hours building a few machine learning models from the ground up using their sample data. I’ll describe my experiences here to show you how easy this tool really is to use in the hopes that others can quickly grasp its strengths and weakness. I think I am a representative candidate to be conducting this review as I am not a working data scientist today. I however am a graduate student in data science at UC Berkeley, have a CS degree, have taken several graduate level statistics and machine learning courses and can program in Python.

Let me start out by saying, I really, really…. like Microsoft Azure Machine Learning Studio. It makes the process of doing data science work, that is building, testing and deploying a predictive model for your data much easier and visual for both those getting started and for more experienced data science users. Studio clarifies the whole process by visually walking you through thinking about your data sources, connecting the data to potential model algorithm candidates, doing data cleansing and transformations, choosing features, training the model, testing it, selecting the best model and even deploying your new shiny working machine learning model as a web service in Azure for others to use. In the end, you are left with both a working model accessible via APIs and a visual, documented representation of your model for others to see and for you to continue to tune. Wow!

As you will also see, Studio is so easy to use, it makes data science seem almost deceptively simple. Studio does make the process simpler for sure, but just like Reese’s Peanut Butter cups take two great tastes to make America’s best-selling candy (according to data by Nielsen), you need both a simpler process of model construction (the chocolate) and the peanut butter inside. And that peanut butter is a healthy dose of statistical knowledge for feature determination, model selection and interpretation and even some programming skill (for more sophisticated data adaptations).

My First Experiment

To take the product for a test drive, I just jumped right in and followed some well thought through recipes Microsoft has for creating models and didn’t bother reading documentation until I needed to. You can create a machine learning modeling experiment from scratch, or you can use an existing sample experiment as a template form the Azure AI Gallery. For more information, see: Copy example experiments to create new machine learning experiments. We will walk through the process of creating an experiment from scratch.

My first model experiment was very simple and used data from one of the 39 data sets supplied from UC Irvine, Amazon, IMDB etc… It is a linear regression model to predict car prices based upon different variables such as make and technical specifications.

You enter Studio in the interactive workspace. To develop a predictive analysis model, you will use data from one or more sources, transform and analyze that data through various data manipulation and statistical functions, and generate a set of results. Developing a model with Studio is an iterative process. As you modify the various functions and parameters, your results converge until you are satisfied that you have a trained, effective model by evaluating its score results.

Azure Machine Learning Studio is beautifully interactive and visual. You drag-and-drop datasets and analysis modules onto an interactive canvas, connecting them together to form an experiment, which you run in Machine Learning Studio. To iterate on your model design, you edit the experiment, save a copy if desired, and run it again. When you’re ready, you can convert your training experiment to a predictive experiment, and then publish it as an Azure web service API so that your model can be accessed by others.

To get started, I first went to Azure Machine Learning Studio at https://studio.azureml.net/ where I was asked to sign in using a Microsoft account, work or school account. Once signed in, you get to a home page which looks like this.

The basic layout is represented in the following tabs on the left:

· PROJECTS — Collections of experiments, datasets, notebooks, and other resources representing a single project

· EXPERIMENTS — Experiments that you have created or saved

· WEB SERVICES — Web services models that you have deployed from your experiments

· NOTEBOOKS — Jupyter notebooks that you have created

· DATASETS — Datasets that you have uploaded into Studio

· TRAINED MODELS — Models that you have trained in experiments and saved

· SETTINGS — A collection of settings that you can use to configure your account and resources.

At the top level, the recommended workflow for conducting an experiment and ultimately publishing it as a web service is as follows:

Create a model

Get the data

Prepare the data

Define features

2. Train the model

Choose and apply an algorithm

3. Score and test the model

Predict new automobile gas prices

4. Publish the model as a cloud service

Get the Data

Create a new experiment by clicking +NEW at the bottom of the Machine Learning Studio window. Select EXPERIMENT > Blank Experiment and I named the experiment Automobile Price Prediction. There are many other pre-built experiments you can choose from but I chose this as a first look.

To the left of the experiment canvas is a palette of sample datasets and modules which you can search. I chose the dataset labeled Automobile price data (Raw) and then dragged this dataset to the experiment canvas. Of course Studio supports uploading your own dataset in many formats too.

One really nice feature that data scientists appreciate is the ability to get a quick look at the data columns and distribution to understand the data we are dealing with. To see what this data looks like, you can simply click the output port at the bottom of the dataset, then select Visualize.

Datasets and modules have input and output ports represented by small circles — input ports at the top, output ports at the bottom. To create a flow of data through your experiment, you’ll connect an output port of one module to an input port of another. At any time, you can click the output port of a dataset or module to see what the data looks like at that point in the data flow.

In this dataset, each row represents an automobile, and the variables associated with each automobile appear as columns. We’ll predict the price in far-right column (column 26, titled “price”) using the variables for a specific automobile. Note the histograms of each column that are given and the details on the distribution of the data in the right pane. This quick look seemed more time-consuming to find in other tools I have used.

Prepare the data

As any experienced data scientists knows, datasets usually require some preprocessing before they can be analyzed. In this case, there are missing values present in the columns of various rows. These missing values need to be cleaned so the model can analyze the data correctly. We’ll remove any rows that have missing values. Also, the normalized-losses column has a large proportion of missing values, so we’ll exclude that column from the model altogether.

Studio makes this process very easy. They supply a module that removes the normalized-losses column completely (Select Columns in Dataset) and then we’ll add another module that removes any row that has missing data.

First, we type in “select columns” in the search bar to the left and drag onto the canvas the Select Columns in Dataset module. Then, we connect the output port of the Automobile price data (Raw) dataset to the input port of the Select Columns in Dataset module by simply clicking and drawing a line between the two dots.

By clicking on the Select Columns in Dataset module we Launch the column selector in the Properties pane of this module. By using the WITH RULES and begin with ALL COLUMNS settings, with a few steps, we can exclude a column name, in this case the normalized-losses column and the module will still pass through all other columns. Studio lets you double-click add a comment to a module by entering text so you see at a glance what the module is doing in your experiment. In this case I added the comment “Exclude Normalized Losses.”

Similarly, to remove rows with missing data, drag the Clean Missing Data module to the experiment canvas and connect it to the Select Columns in Dataset module. In the Properties pane, select Remove entire row under Cleaning mode. These options direct Clean Missing Data to clean the data by removing rows that have any missing values. I then double-click the module and typed the comment “Remove Missing Value Rows.”

Define features

Defining features simply means that we will select the columns (features) that we will use in the model to predict the price. Defining features requires experimentation as some features will have more predictive power than others. Some features will be highly correlated with other features and therefore not add to the predictive power of the model and these features should not be included to make the model as parsimonious as possible. A parsimonious model is a model that accomplishes a desired level of explanation or prediction with as few predictor variables as possible.

For our walk-through, we’ll keep to Microsoft’s example and assume a subset of features that may allow us to predict price:

{make, body-style, wheel-base, engine-size, horsepower, peak-rpm, highway-mpg, price}

To add these features we drag the Select Columns in Dataset module to the canvas and connect the output of the Clean Missing Data column to its input. We double click the module and type Select Features for Prediction as our descriptor. Next, click Launch column selector in the Properties pane and select with rules. We can begin with No Columns and one by one we add the column names (features) to the model’s list. Click the check mark (OK) button when done. This module produces the filtered dataset of only those features (and associated data) that we want to pass to the learning algorithm that we will add next.

This sure sounds a lot simpler than the reality and complexity of optimal feature selection when you start to dig into the documentation and the data science behind it all. And in a first walk-through, we do want it to be simple so we can experience the overall flow of building a model. But let me just give you some insights so readers won’t walk away assuming this is really easy and that they should roll out Studio to every analyst in the company right away.

As you work with this product more, you’ll find that there are modules that you should use to select features and that this should be a step in your process flow after cleaning the data. Studio provides these modules for feature selection:

Filter Based Feature Selection: Identifies the features in a dataset that have the greatest predictive power.

Fisher Linear Discriminant Analysis: Identifies the linear combination of feature variables that can best group data into separate classes.

Permutation Feature Importance: Computes the permutation feature importance scores of feature variables for a trained model and test dataset.

Microsoft includes this article about the feature selection modules and how to use them if you want to see more.

https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/feature-selection-modules#bkmk_howto

As a real-life story of how feature selection complexity can play out in modeling, here is an example of feature selection done by two enterprise architects at Microsoft’s Data Insight’s Center of Excellence. It describes the journey they went through to migrate their Excel linear regression model for financial forecasts to Studio. They ultimately succeeded in getting better forecasts from Studio and were able to publish the model as a web service making it more accessible — three cheers for Studio!

https://docs.microsoft.com/en-us/azure/machine-learning/studio/linear-regression-in-azure

But, keep in mind they had some learning and tweaking to do by understanding whether to use “Online Gradient Descent” or “Ordinary Least Squares” methods of regression and also discovered that they needed to tweak the L2 Regularization Weight depending on their data set size. They also described how they began using the Filter-Based Feature Selection to improve on their selection of initial data elements and that they also intended to test additional algorithms like Bayesian or Boosted Decision Trees to compare performance with linear regression which we will discuss in the next section. It takes time, testing and data science training to really produce the best results — even with a simple tool like Studio.

One area of improvement I would like to see is more help from Studio in this area of automatically selecting the optimal features. The terminology above does not sound like what citizen data scientists would understand. I did discover that some of the machine learning algorithms (discussed in the next section) in Studio do use feature selection or dimensionality reduction as part of the training process. When you use these algorithms, you can skip the feature selection process and let the algorithm decide the best inputs which is a step in the right direction. But knowing which algorithms do this and which don’t as well as actually performing feature selection will be a difficult, time consuming process for citizen data scientists who may not understand how to best select features.

Training the model — Chose and Apply an Algorithm

Now we are ready to train the model. In this example we are doing what is called supervised machine learning and there are many algorithms available that may offer the predictive power we seek. For example, there are classification models that will predict which category a subject row is likely to be in (is it a car or a truck?), there are regression algorithms that predict a numeric answer (like future stock price). There are 25 types of models baked into MLS for anomaly detection, classification, regression and clustering and many more that appear to be open library modules also available.

Because we want to predict price, which is a number, we’ll use a regression algorithm — linear regression in this case.

We train a model by giving it a set of data that includes the answer we want to predict (the price.) The model scans the data and looks for correlations between an automobile’s features and its price and captures them in the model (a mathematical equation).

Studio will allow us to use our data for both training the model and testing it by splitting the data into separate training and testing datasets. This is done by dragging the Split Data Module to the experiment canvas and connecting it to the output of the Select Columns in Dataset module.

Click the Split Data module to select it and spilt the data into two parts using 75% of the data for training and the remaining 25% for scoring as shown.

At this point we should run the experiment so that the Select Columns in Dataset and Split Data modules have the data and latest column definitions to pass forward when we score the model. This is done by pushing the run button at the bottom of the screen.

Now it’s time to add the Learning algorithm we wish to use by expanding the Machine Learning Category on the left side of the canvas and expanding Initialize Model. Select the linear regression module and drag it to the canvas. Also find and drag the Train Model module to the experiment canvas. Connect the output of the Linear Regression module to the input of the Train Model and connect the training data (left port) of the Split Data module to the Train Model as shown.

Click the Train Model module, and then click Launch column selector in the Properties pane and then select the price column. Price is the value that our model is going to predict.. Move price from available columns to selected columns list.

At last we can Run the experiment. We now have a trained regression model that can make price predictions.

Predict new automobile prices

Now that we’ve trained the model, we can use it to score the other 25 percent of the data to see how well our model functions. We do this by dragging the Score Model module to the experiment canvas and connecting the output of the Train Model to it. We then connect the test data output (right port) of the Split Data module to the Score Model module as shown.

Now run the experiment and view the output for Score Model by clicking the bottom port and selecting Visualize. The predicted prices are shown in the column Scored Labels along with all of the known feature data used by the model. The column price is the actual known price from the data set.

Deciding if our model is a good model?

The last step before we publish any model for use is to test the quality of the results. To do this we drag the Evaluate Model module onto the canvas and simply connect it to the output of the Score Model. Now when you run this experiment again, you will be able to visualize statistical results.

This area is where I think this service still could be better. To select the model that is best, a user will have to iteratively run many different experiments, save the results and compare them until you get a best fit model. That requires statistical knowledge and an understanding of when to use linear regression vs. classification vs. logistical regression or some of the many open source algorithms which might be great for modeling the data. Further, if the user is really a citizen data scientist, do they really understand how to interpret these results and use them in context to decide on the best model?

In this case, for each of the reported results, being smaller is better as that indicates predictions that more closely match the data (less error). The exception being the Coefficient of Determination (also called R squared) which we seek to make as close to 1.0 as an indication of model accuracy. This model is a .91 predictive accuracy to fitting the data to the line. I’d love to tell you that is pretty good, but really the answer is it depends on your data and what you are trying to predict and the consequences of being wrong. To get an idea of how complicated this is you can read the following from Duke https://people.duke.edu/~rnau/rsquared.htm. Citizen data scientists will need more help from Microsoft in this area.

Publish the model as a Cloud Service

One really nice feature is how easy Studio makes it to get your model into production. For those of us who are not data engineers or IT and skilled at publishing a model in the cloud as an API for others to use, this makes getting your work out much easier.

The simplest way to publish a model is to use the Set up Web Service button and simply publish the model as a web service classic. This option converts your model from an experiment to a predictive experiment by eliminating data splits, training and other unnecessary steps in a model after you have decided on its features and algorithms. You run the model one last time to check the results and you are ready to go with an API key for others to use on Azure.

You can learn more about how to deploy your model here:

Scalability & Performance

Deploying does raise the issues of what am I deploying to — What are the hardware resources I can use, what is my service level guarantee, is it a dedicated or multi-tenant cloud, security etc..?

Azure Machine Learning Service is multi-tenant and compute resources used on the back end will vary and are optimized for performance and predictability. Studio can be deployed on free tier with data sets no larger than 10 GB or on the standard paid tier which allows the use of many more paid resources and BYO storage.

One of the hot issues in data science is using GPUs to achieve very fast compute performance. I do see the Azure offers GPUs for compute intensive applications, but I don’t see a way to directly specify GPUs for Studio as it is a multi-tenant service. Perhaps the way to guarantee the compute performance needed is through the paid SLA that is part of the the standard paid tier or Microsoft may have other methods of guaranteeing GPU access that aren’t obvious in what I have read.

Summary

Microsoft has done an outstanding job of building a cloud service that clarifies, simplifies and ensures the integrity of the process of building machine learning models. Their process lays out visually a simple clear method of acquiring data, offers tools for cleansing data and choices of models. Studio goes on to require training and model scoring and will not let you proceed unless prior steps are correctly executed. Ultimately, Studio even makes deployment of models as a web service easy.

But as I said in the title, Studio is not a panacea that allows anyone to build machine learning models. Machine learning is complex and data science knowledge is still required. A user would need statistical knowledge to understand which algorithms to choose, how to choose features and to interpret the scoring results as to which model is the best fit for your circumstances. Also, to allow more flexibility modules may be inserted into the flow of cleansing and transforming data that can be custom built in R or Python etc.. This requires programming skills.

To take this product the next step for citizen data scientists Microsoft must offer more data science intelligence built in. Imagine if Studio could look at your data and what you want to predict and run through a series of algorithms, try different features, score the models and then offer up the model with best recommended fit! That would save the user a lot of time, effort, cost and require less heavy data science knowledge of the user.

I think Studio will work well for many users seeking to build models including citizen data scientists seeking a drag-and-drop solution with pre-built algorithms to more advanced enterprise users who can incorporate Studio work into an even broader ecosystem of Microsoft’s Azure Machine Learning Service. This service allows data scientists to work in a Python environment, provides more control over machine learning algorithms, deployment and supports open-source machine learning frameworks like PyTorch, TensorFlow, and scikit-learn. I look forward to working with this product more.

About the Author

Steve Dille is a Silicon Valley product management and marketing leader who has been on the executive teams of companies resulting in 5 successful company acquisitions and one IPO in the data management, analytics, BI and big data sectors. Most recently, he was the CMO of SparkPost, where he was instrumental in transitioning the company from an on-premises high volume email sender to a leading predictive analytics-driven cloud email API service growing it from $13M ARR to over $50M ARR. He is currently building deep knowledge in data science, AI and machine learning by pursuing his Master’s in Information and Data Science at UC Berkeley while working. His past education includes an MBA from University of Chicago Booth School of Business and a BS in Computer Science/Math from University of Pittsburgh. He has served as a software developer at NCR, product manager at HP, Data Warehousing Director at Sybase (SAP) and VP of Product or CMO at numerous other startups and mid-size companies."
Categorizing World Wide Web,"The Internet is the world’s largest library. It’s just that all the books are on the floor. - John Allen Paulos

Let us sweep the floor and try to stack these books in bookshelves.

Common Crawl Dataset

Instead of crawling the open web, it’s a good idea to use existing Common Crawl dataset — A crawled archive of 2.95 billion webpages with 260 terabytes of total content. Of course, it’s not a full representation of web but it gives us a pretty good start.

To analyze and categorize such a large corpus, we would need lots of computation power. Therefore, we would be using hundreds of machines on Amazon Elastic MapReduce (EMR) running Apache Spark. Good news is Common Crawl dataset already lives on Amazon S3 as a part of Amazon Public Datasets program and therefore we should be able to efficiently access it.

Extracting Text from Webpages

Common Crawl WARC files contain raw data from crawl including HTTP response from the websites it has contacted. If we take a look at one such HTML document –

<!DOCTYPE html>

<html lang=”en-US”>

<head>

<meta charset=”UTF-8"">

<meta name=”viewport” content=”width=device-width, initial-scale=1"">

<link rel=”profile” href=”http://gmpg.org/xfn/11"">

...

<div class=”cover”>

<img src=”https://images-eu.ssl-images-amazon.com/images/I/51Mp9K1v9IL.jpg"" /></p>

</div>

<div>

<div id=”desc-B00UVA0J6E” class=”description”>

<p>By Richard S. Smith,Simon W. M. John,Patsy M. Nishina,John P. Sundberg</p>

<p>ISBN-10: 084930864X</p>

<p>ISBN-13: 9780849308642</p>

<div> finishing touch of the 1st section of the Human Genome venture has awarded scientists with a mountain of recent info.

...

<footer id=”colophon” class=”site-footer” role=”contentinfo”>

<div class=”site-info”>

...

</body>

</html>

As you can see, HTML documents are very complex code often in a multitude of formats. We would use a python library — Beautiful Soup to extract the text out of these documents.

def get_text(html_content):

soup = BeautifulSoup(html_content, ""lxml"")



# strip all script and style elements

for script in soup([""script"", ""style""]):

script.decompose()



return soup.get_text("" "", strip=True)

We have created BeautifulSoup object by passing raw HTML content. It contains all the data in nested structure. All the textual data can be programmatically extracted by calling get_text on this object. However, apart from the text it also extracts javascript and CSS code which we may not want, so prior to extraction, we would remove them. That should give us what we want, just the texts —

Download e-book for iPad: Systematic Evaluation of the Mouse Eye: Anatomy, Pathology, by Richard S. Smith,Simon W. M. John,Patsy M. Nishina,John P. - Gdynia Design E-books ... finishing touch of the 1st section of the Human Genome venture has awarded scientists with a mountain of recent info. the supply of all human genes and their destinations is fascinating, yet their mechanisms of motion and interplay with different genes ... the booklet then studies and illustrates nearby ocular pathology and correlates it with human eye disease.

Classification

To classify this text, we would use scikit-learn — a python library for machine learning. We would be training our classifier with 20 newsgroups dataset. It is a collection of approximately 20,000 newsgroup documents, partitioned evenly across 20 different newsgroups. We would be mapping these newsgroups in the following eight categories.

class Classifier:



# train the model

def __init__(self):

newsgroups_train = fetch_20newsgroups(subset='train', remove = ('headers', 'footers', 'quotes'))

self.target_names = newsgroups_train.target_names

self.vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')

vectors = self.vectorizer.fit_transform(newsgroups_train.data)

self.clf = LinearSVC(penalty='l2', dual=False, tol=1e-3)

self.clf.fit(vectors, newsgroups_train.target)



def predict(self, document):

x_test = self.vectorizer.transform([document])

pred = self.clf.predict(x_test)

return self.target_names[pred[0]]

The fetch_20newsgroups function downloads the data archive from the original 20 newsgroups website. We are configuring it to remove headers, signature blocks, and quotation blocks respectively which has little to do with topic classification. We will convert the text into vectors of numerical values using TfidfVectorizer in order to feed them to the predictive model. Now we will use Linear Support Vector Classification (LinearSVC) to train our model which scales better on large dataset. In fact, numerous other models can be easily plugged in like Naïve Bayes, SGD Classifier, k-nearest neighbors, Decision Trees, etc., but empirically I found SVM performing well in our case. Now we can call predict function on our model to classify document in one of the eight categories.

Analyzing Predictions

Classifier seems to do a decent job. Here are some web pages –

Beautiful Kazak carpet of distinctive colours matches with the curtains add lively touch. ... Carpet cleaning is something which can be done at home but you might damage your carpet instead of cleaning it. Often the colours of carpet run as soon as it comes in contact with water. ... Carpet Repairing If your carpet is torn or there is piece missing, AbeeRugs can help you fix that. We fix it in a way that we also complete the design no matter how intricate it is. ... Classified as E-Commerce

-------------------------------- Make more time for the moments that matter. Get expertly-curated updates and medical education, instantly. New to EasyWeb? EasyWeb is a free service for United States Healthcare Professionals. ... Specialty Anesthesiology: general Anesthesiology: adult cardiothoracic Anesthesiology: critical care Anesthesiology: pain Anesthesiology: pediatric Dermatology Emergency Medicine Family Medicine ... Classified as Medicines

-------------------------------- All Films & Events | Arab Film Festival 2014 ... Watch Trailer Watch Trailer A Stone’s Throw from Prison Raquel Castells / Documentary / Palestine, Spain / 2013 / 65 mins Growing up in the Occupied Palestinian Territory is not easy. When you leave home for school, your mother can't be sure of when you'll be back. Rami, Ahmed, Mohammed, three among thousands, this documentary is their story, but also that of courageous Israelis and Palestinians working to cut abuses, stop conflict ... Classified as Politics

Apparently, by analyzing more predictions, it’s noticeable that categories are too broad.

Tanzania farmers adopts vegetable farming to improve nutrition The farmers in Tanzania are encouraged to grow elite varieties of vegetables, enriched with high-value nutritional content, in order to fight malnutrition, hunger and double agricultural productivity and income of smallholders The Africa RISING project focuses on the need to take urgent action in achieving Sustainable Development Goals (SDGs) which aim ... Classified as Medicines

By looking at all the possible categories that we have, the predicted category seems about right, but it would have been better to categorize it as Agriculture. Also, Aeronautics would have been more suitable for the following sample.

Schofields Flying Club - N1418V Flight Schedule N1418V Flight Schedule Cessna 172M Perhaps the staple of general aviation flight training, and rightfully so. The 172 has been in production since 1956, with no end in sight! Our 172 is an ideal VFR training platform!! Because of its high wings, 18V provides a great view of the ground, ... Classified as Space

There are many examples where web pages could fall into multiple categories. For example, the following sample could also be categorized as E-Commerce.

Recently Sold Vehicles Here are some vehicles that we recently sold. It's just a sample of the variety and quality of our vehicle selection. Please feel free to browse our Internet showroom and contact us to make an appointment if you would like to see any of our vehicles in person. ... Classified as Automobile

And there are also quite a few samples where classifier seems to get it wrong.

... We all make mistakes. That is a dictum of life. It’s surprising how ignored it is and how discouraged one is to fail in the world. Then again current with this theme of artificial organizing is that the soul is now completely detached from the world it swims in. The title Greenberg centers around […] To the dilettante the thing is the end, while to the professional as such it is the means; and only he who is directly interested in a thing, and occupies himself with it from love of it ... Support Joseph A. Hazani on Patreon! ... Misclassified as Sports

-------------------------------- ... Fast, Reliable Everytime ABC Messenger Local Flower Hill NY Delivery Service ABC Messenger & Transport, Inc. has the ability to offer rush and pre-scheduled Delivery Service at competitive prices and excellent Flower Hill NY service. Their Delivery Service is recognized as fast and dependable. Through highly experienced and skilled veteran Flower Hill NY dispatchers and seasoned couriers we are able to predict traffic patterns easily and avoid or overcome ... Misclassified as Medicines

All in all, it gives decent predictions that we can rely on for our starter project.

Language Detection

Since classifier is trained on English dataset, we will be using langdetect library to skip non-English web pages.

from langdetect import detect lang = detect(text)

if lang != 'en':

# skip non-English pages

return

Profanity Filter

We will also use profanity filter to ignore adult content.

from profanity_check import predict if predict([text])[0]:

# adult content

Spark on Amazon EMR

As discussed earlier, since the Common Crawl dataset is huge, we will run our application on Spark on Amazon EMR. We will adapt to Common Crawl example code to process data on Spark.

class WebClassifier(CCSparkJob):



name = ""WebClassifier""

def __init__(self):

CCSparkJob.__init__(self)

self.output_schema = StructType([

StructField(""topic"", StringType(), True),

StructField(""count"", LongType(), True)

])

self.classifier = Classifier()



def process_record(self, record):

# html record

if self.is_html(record):

# extract text from web page

text = self.get_text(record)

else:

return # skip non-English pages # filter profane content



topic = self.classifier.predict(text)

yield topic, text



if __name__ == '__main__':

job = WebClassifier()

job.run()

process_record function would be called on each record in the dataset where we will classify it to one of the categories listed earlier. Full source is uploaded here.

To further speed up, we will use WET files instead of WARC, which contains extracted plaintext. Also, we will load the pre-trained serialized classifier to avoid training it on every machine. I used 100 ec2 instances and it took about an hour and a half to process 1% of Common Crawl corpus. It cost me about 25 USD. And here’s what we got:

Web Categories

We started with approximately 1% of Common Crawl data or about 25.6 million webpages. We filtered 1.4 million web pages since they didn’t contain enough textual information for classification purpose. Later we skipped about 13.2 million non-English pages. We were able to classify rest of 10.9 million pages. Apparently, there is plenty of E-Commerce content and lot of content talking about Computer and Electronics followed by Politics, Medicines, Religion, Automobile, and Space. Note that we also filtered about quarter millions of adult webpages.

In conclusion, we made a gentle attempt to categorize the current state of the web. I hope this also serves as a good starter for further web analysis."
Bite-Sized Python Recipes,"Bite-Sized Python Recipes

Photo by Jordane Mathieu on Unsplash

Disclaimer: This is a collection of small useful functions I’ve found around the web, mainly on Stack Overflow or Python’s documentation page. Some may look, but one way or another, I have used them all in my projects and I think they are worth sharing. You can find all of them, with some additional comments, in this notebook which I try to keep up to date.

Unless necessary, I intend not to over-explain the functions. So, let’s begin!

Create a Dictionary From Two Lists:

>>> prod_id = [1, 2, 3]

>>> prod_name = ['foo', 'bar', 'baz']

>>> prod_dict = dict(zip(prod_id, prod_name)) >>> prod_dict

{1: 'foo', 2: 'bar', 3: 'baz'}

Remove Duplicates From a List and Keep the Order:

>>> from collections import OrderedDict >>> nums = [1, 2, 4, 3, 0, 4, 1, 2, 5]

>>> list(OrderedDict.fromkeys(nums))

[1, 2, 4, 3, 0, 5] # As of Python 3.6 (for the CPython implementation) and

# as of 3.7 (across all implementations) dictionaries remember

# the order of items inserted. So, a better one is:

>>> list(dict.fromkeys(nums))

[1, 2, 4, 3, 0, 5]

Create a Multi-Level Nested Dictionary:

Create a dictionary as a value in a dictionary. Essentially, it’s a dictionary that goes multiple levels deep.

from collections import defaultdict def multi_level_dict():

"""""" Constructor for creating multi-level nested dictionary. """"""



return defaultdict(multi_level_dict)

Example 1:

>>> d = multi_level_dict()

>>> d['a']['a']['y'] = 2

>>> d['b']['c']['a'] = 5

>>> d['x']['a'] = 6 >>> d

{'a': {'a': {'y': 2}}, 'b': {'c': {'a': 5}}, 'x': {'a': 6}}

Example 2:

A list of products is given, where each product needs to be delivered from its origin to its distribution center (DC), and then to its destination. Given this list, create a dictionary for the list of products that are shipped through each DC, coming from each origin and going to each destination.

import random

random.seed(20) # Just creating arbitrary attributes for each Product instance

class Product:

def __init__(self, id):

self.id = id

self.materials = random.sample('ABCD', 3)

self.origin = random.choice(('o1', 'o2'))

self.destination = random.choice(('d1', 'd2', 'd3'))

self.dc = random.choice(('dc1', 'dc2'))



def __repr__(self):

return f'P{str(self.id)}' products = [Product(i) for i in range(20)] # create the multi-level dictionary

def get_dc_origin_destination_products_dict(products):

dc_od_products_dict = multi_level_dict()

for p in products:

dc_od_products_dict[p.dc][p.origin].setdefault(p.destination, []).append(p)

return dc_od_products_dict dc_od_orders_dict = get_dc_origin_destination_products_dict(products)

>>> dc_od_orders_dict

{'dc1': {'o2': {'d3': [P0, P15],

'd1': [P2, P9, P14, P18],

'd2': [P3, P13]},

'o1': {'d1': [P1, P16],

'd3': [P4, P6, P7, P11],

'd2': [P17, P19]}},

'dc2': {'o1': {'d1': [P5, P12],

'd3': [P10]},

'o2': {'d1': [P8]}}}

Note, that when you run the above two examples, you should see defaultdict(<function __main__.multi_level_dict()>...) in the output. But they were removed here for legibility of the result.

Return the Keys and Values From the Innermost Layer of a Nested Dict:

from collections import abc



def nested_dict_iter(nested):

"""""" Return a generator of the keys and values from the innermost layer of a nested dict. """"""



for key, value in nested.items():

# Check if value is a dictionary

if isinstance(value, abc.Mapping):

yield from nested_dict_iter(value)

else:

yield key, value

A few things should be explained about this function:

The nested_dict_iter function returns a generator.

function returns a generator. In each loop, the dictionary value is checked recursively until the last layer is reached.

In the condition check, collections.abc.Mapping is used instead of dict for generality. That way container objects such as dict , collections.defaultdict , collections.OrderedDict and collections.Counter are checked.

is used instead of for generality. That way container objects such as , , and are checked. Why yield from ? Short and incomplete answer: it’s designed for situations where invoking a generator from within a generator is needed. I know a brief explanation cannot do it any justice, so check this SO thread to learn more about it.

Example 1:

>>> d = {'a':{'a':{'y':2}},'b':{'c':{'a':5}},'x':{'a':6}}

>>> list(nested_dict_iter(d))

[('y', 2), ('a', 5), ('a', 6)]

Example 2: let’s retrieve keys and values from our dc_od_orders_dict above.

>>> list(nested_dict_iter(dc_od_orders_dict))

[('d3', [P0, P15]),

('d1', [P2, P9, P14, P18]),

('d2', [P3, P13]),

('d1', [P1, P16]),

('d3', [P4, P6, P7, P11]),

('d2', [P17, P19]),

('d1', [P5, P12]),

('d3', [P10]),

('d1', [P8])]

The Intersection of Multiple Sets:

def get_common_attr(attr, *args):

"""""" intersection requires 'set' objects """""" return set.intersection(*[set(getattr(a, attr)) for a in args])

Example: Find the common comprising materials, if any, among our first 5 products .

>>> get_common_attr('materials', *products[:5])

{'B'}

First Match:

Find the first element, if any, from an iterable that matches a condition.

first_match = next(i for i in iterable if check_condition(i)) # Example:

>>> nums = [1, 2, 4, 0, 5]

>>> next(i for i in nums if i > 3)

4

The above implementation throws a StopIteration exception if no match is found. We can fix that by returning a default value. Since we are here, let’s make it a function:

def first_match(iterable, check_condition, default_value=None):

return next((i for i in iterable if check_condition(i)), default_value)

Example:

>>> nums = [1, 2, 4, 0, 5]

>>> first_match(nums, lambda x: x > 3)

4

>>> first_match(nums, lambda x: x > 9) # returns nothing

>>> first_match(nums, lambda x: x > 9, 'no_match')

'no_match'

Powerset:

The powerset of a set S is the set of all the subsets of S.

import itertools as it def powerset(iterable):

s = list(iterable)

return it.chain.from_iterable(it.combinations(s, r)

for r in range(len(s) + 1))

Example:

>>> list(powerset([1,2,3]))

[(), (1,), (2,), (3,), (1, 2), (1, 3), (2, 3), (1, 2, 3)]

Timer Decorator:

Shows the runtime of each class/method/function.

from time import time

from functools import wraps



def timeit(func):

""""""

:param func: Decorated function

:return: Execution time for the decorated function

""""""



@wraps(func)

def wrapper(*args, **kwargs):

start = time()

result = func(*args, **kwargs)

end = time()

print(f'{func.__name__} executed in {end - start:.4f} seconds')

return result



return wrapper

Example:

import random # An arbitrary function

@timeit

def sort_rnd_num():

numbers = [random.randint(100, 200) for _ in range(100000)]

numbers.sort()

return numbers >>> numbers = sort_rnd_num()

sort_rnd_num executed in 0.1880 seconds

Calculate the Total Number of Lines in a File:

def file_len(file_name, encoding='utf8'):

with open(file_name, encoding=encoding) as f:

i = -1

for i, line in enumerate(f):

pass

return i + 1

Example: How many lines of codes are there in the python files of your current directory?

>>> from pathlib import Path >>> p = Path()

>>> path = p.resolve() # similar to os.path.abspath()

>>> print(sum(file_len(f) for f in path.glob('*.py')))

745

Just For Fun! Creating Long Hashtags:

>>> s = ""#this is how I create very long hashtags""

>>> """".join(s.title().split())

'#ThisIsHowICreateVeryLongHashtags'

The following are not bite-sized recipes, but don’t get bitten by these mistakes!

Be careful not to mix up mutable and immutable objects!

Example: Initialize a dictionary with empty lists as values

>>> nums = [1, 2, 3, 4]

# Create a dictionary with keys from the list.

# Let's implement the dictionary in two ways

>>> d1 = {n: [] for n in nums}

>>> d2 = dict.fromkeys(nums, [])

# d1 and d2 may look similar. But list is mutable.

>>> d1[1].append(5)

>>> d2[1].append(5)

# Let's see if d1 and d2 are similar

>>> print(f'd1 = {d1}

d2 = {d2}')

d1 = {1: [5], 2: [], 3: [], 4: []}

d2 = {1: [5], 2: [5], 3: [5], 4: [5]}

Don’t modify a list while iterating over it!

Example: Remove all numbers less than 5 from a list.

Wrong Implementation: Remove the elements while iterating!

nums = [1, 2, 3, 5, 6, 7, 0, 1]

for ind, n in enumerate(nums):

if n < 5:

del(nums[ind]) # expected: nums = [5, 6, 7]

>>> nums

[2, 5, 6, 7, 1]

Correct Implementation:

Use list comprehension to create a new list containing only the elements you want:

>>> id(nums) # before modification

2090656472968

>>> nums = [n for n in nums if n >= 5]

>>> nums

[5, 6, 7]

>>> id(nums) # after modification

2090656444296

You can see above that id(nums) is checked before and after to show that in fact, the two lists are different. So, if the list is used in other places and it’s important to mutate the existing list, rather than creating a new one with the same name, assign it to the slice:

>>> nums = [1, 2, 3, 5, 6, 7, 0, 1]

>>> id(nums) # before modification

2090656472008

>>> nums[:] = [n for n in nums if n >= 5]

>>> id(nums) # after modification

2090656472008

That’s it for now (check the second bite-sized blog here). If you also have some bite-sized functions that you use regularly, let me know. I’ll try to keep the notebook up-to-date on GitHub and yours can end up there too!"
Deep Q Learning for the CartPole,"The purpose of this post is to introduce the concept of Deep Q Learning and use it to solve the CartPole environment from the OpenAI Gym.

The post will consist of the following components:

Open AI Gym Environment Intro Random Baseline Strategy Deep Q Learning Deep Q Learning with Replay Memory Double Deep Q Learning Soft Updates

Environment

The CartPole environment consists of a pole which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. The state space is represented by four values: cart position, cart velocity, pole angle, and the velocity of the tip of the pole. The action space consists of two actions: moving left or moving right. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.

The cell below plots a bunch of example frames from the environment:

# Demonstration

env = gym.envs.make(""CartPole-v1"") def get_screen():

''' Extract one step of the simulation.'''

screen = env.render(mode='rgb_array').transpose((2, 0, 1))

screen = np.ascontiguousarray(screen, dtype=np.float32) / 255.

return torch.from_numpy(screen) # Speify the number of simulation steps

num_steps = 2 # Show several steps

for i in range(num_steps):

clear_output(wait=True)

env.reset()

plt.figure()

plt.imshow(get_screen().cpu().permute(1, 2, 0).numpy(),

interpolation='none')

plt.title('CartPole-v0 Environment')

plt.xticks([])

plt.yticks([])

plt.show()

Dependent on the number of episodes, the output will look something like this:

Untrained Agent

As we can see, the agent is untrained yet, so it cannot make more than a couple of steps. We will soon explore some of the strategies that will drastically improve performance. But first, let’s define the plotting function that will help us analyze results:

def plot_res(values, title=''):

''' Plot the reward curve and histogram of results over time.'''

# Update the window after each episode

clear_output(wait=True)



# Define the figure

f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))

f.suptitle(title)

ax[0].plot(values, label='score per run')

ax[0].axhline(195, c='red',ls='--', label='goal')

ax[0].set_xlabel('Episodes')

ax[0].set_ylabel('Reward')

x = range(len(values))

ax[0].legend()

# Calculate the trend

try:

z = np.polyfit(x, values, 1)

p = np.poly1d(z)

ax[0].plot(x,p(x),""--"", label='trend')

except:

print('')



# Plot the histogram of results

ax[1].hist(values[-50:])

ax[1].axvline(195, c='red', label='goal')

ax[1].set_xlabel('Scores per Last 50 Episodes')

ax[1].set_ylabel('Frequency')

ax[1].legend()

plt.show()

The resulting plot consists of two subplots. The first one plots the total reward the agent accumulates over time, while the other plot shows a histogram of the agent’s total rewards for the last 50 episodes. We’ll see some of the graphs when we’ll analyze our strategies.

Baseline Random Model

Before implementing any deep learning approaches, I wrote a simple strategy where the action is sampled randomly from the action space. This approach will serve as a baseline for other strategies and will make it easier to understand how to work with the agent using the Open AI Gym environment.

def random_search(env, episodes,

title='Random Strategy'):

"""""" Random search strategy implementation.""""""

final = []

for episode in range(episodes):

state = env.reset()

done = False

total = 0

while not done:

# Sample random actions

action = env.action_space.sample()

# Take action and extract results

next_state, reward, done, _ = env.step(action)

# Update reward

total += reward

if done:

break

# Add to the final reward

final.append(total)

plot_res(final,title)

return final

One environment step returns several values, such as the next_state , reward , and whether the simulation is done . The plot below represents the total accumulated reward over 150 episodes (simulation runs):

Random Strategy

The plot above presents the random strategy. As expected, it’s impossible to solve the environment using this approach. The agent is not learning from their experience. Despite being lucky sometimes (getting a reward of almost 75), their average performance is as low as 10 steps.

Deep Q Learning

The main idea behind Q-learning is that we have a function 𝑄:𝑆𝑡𝑎𝑡𝑒×𝐴𝑐𝑡𝑖𝑜𝑛→ℝ, which can tell the agent what actions will result in what rewards. If we know the value of 𝑄, it is possible to construct a policy that maximizes rewards:

𝜋(𝑠)=argmax𝑎 𝑄(𝑠,𝑎)

However, in the real world, we don’t have access to full information, that’s why we need to come up with ways of approximating 𝑄. One traditional method is creating a lookup table where the values of 𝑄 are updated after each of the agent’s actions. However, this approach is slow and does not scale to large action and state spaces. Since neural networks are universal function approximators, I will train a network that can approximate 𝑄.

The DQL class implementation consists of a simple neural network implemented in PyTorch that has two main methods — predict and update. The network takes the agent’s state as an input and returns the 𝑄 values for each of the actions. The maximum 𝑄 value is selected by the agent to perform the next action:

class DQL():

''' Deep Q Neural Network class. '''

def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):

self.criterion = torch.nn.MSELoss()

self.model = torch.nn.Sequential(

torch.nn.Linear(state_dim, hidden_dim),

torch.nn.LeakyReLU(),

torch.nn.Linear(hidden_dim, hidden_dim*2),

torch.nn.LeakyReLU(),

torch.nn.Linear(hidden_dim*2, action_dim)

)

self.optimizer = torch.optim.Adam(self.model.parameters(), lr) def update(self, state, y):

""""""Update the weights of the network given a training sample. """"""

y_pred = self.model(torch.Tensor(state))

loss = self.criterion(y_pred, Variable(torch.Tensor(y)))

self.optimizer.zero_grad()

loss.backward()

self.optimizer.step() def predict(self, state):

"""""" Compute Q values for all actions using the DQL. """"""

with torch.no_grad():

return self.model(torch.Tensor(state))

The q_learning function is the main loop for all the algorithms that follow.

It has many parameters, namely:

- env represents the Open Ai Gym environment that we want to solve (CartPole.)

- episodes stand for the number of games we want to play.

- gamma is a discounting factor that is multiplied by future rewards to dampen these rewards’ effect on the agent. It is designed to make future rewards worth less than immediate rewards.

- epsilon represents the proportion of random actions relative to actions that are informed by existing “knowledge” that the agent accumulates during the episode. This strategy is called “Greedy Search Policy.” Before playing the game, the agent doesn’t have any experience, so it is common to set epsilon to higher values and then gradually decrease its value.

- eps_decay indicates the speed at which the epsilon decreases as the agent learns. 0.99 comes from the original DQN paper.

I will explain other parameters later on when we will get to the corresponding agents.

def q_learning(env, model, episodes, gamma=0.9,

epsilon=0.3, eps_decay=0.99,

replay=False, replay_size=20,

title = 'DQL', double=False,

n_update=10, soft=False):

""""""Deep Q Learning algorithm using the DQN. """"""

final = []

memory = []

for episode in range(episodes):

if double and not soft:

# Update target network every n_update steps

if episode % n_update == 0:

model.target_update()

if double and soft:

model.target_update()



# Reset state

state = env.reset()

done = False

total = 0



while not done:

# Implement greedy search policy

if random.random() < epsilon:

action = env.action_space.sample()

else:

q_values = model.predict(state)

action = torch.argmax(q_values).item()



# Take action and add reward to total

next_state, reward, done, _ = env.step(action)



# Update total and memory

total += reward

memory.append((state, action, next_state, reward, done))

q_values = model.predict(state).tolist()



if done:

if not replay:

q_values[action] = reward

# Update network weights

model.update(state, q_values)

break if replay:

# Update network weights using replay memory

model.replay(memory, replay_size, gamma)

else:

# Update network weights using the last step only

q_values_next = model.predict(next_state)

q_values[action] = reward + gamma * torch.max(q_values_next).item()

model.update(state, q_values) state = next_state



# Update epsilon

epsilon = max(epsilon * eps_decay, 0.01)

final.append(total)

plot_res(final, title)

return final

The most straightforward agent updates its Q-values based on its most recent observation. It doesn’t have any memory, but it learns by first exploring the environment and then gradually decreasing its epsilon value to make informed decisions. Let’s evaluate the performance of such an agent:

Deep Q Learning

The graph above shows that the performance of the agent has significantly improved. It got to 175 steps, which, as we’ve seen before, is impossible for a random agent. The trend line is also positive, and we can see that the performance increases over time. At the same time, the agent didn’t manage to get above the goal line after 150 epochs, and its average performance is still around 15 steps, so there is enough room for improvement.

Replay Memory

The approximation of 𝑄 using one sample at a time is not very effective. The graph above is a nice illustration of that. The network managed to achieve a much better performance compared to a random agent. However, it couldn’t get to the threshold line of 195 steps. I implemented experience replay to improve network stability and make sure previous experiences are not discarded but used in training.

Experience replay stores the agent’s experiences in memory. Batches of experiences are randomly sampled from memory and are used to train the neural network. Such learning consists of two phases — gaining experience and updating the model. The size of the replay controls the number of experiences that are used for the network update. Memory is an array that stores the agent’s state, reward, and action, as well as whether the action finished the game and the next state.

# Expand DQL class with a replay function.

class DQN_replay(DQN):

def replay(self, memory, size, gamma=0.9):

"""""" Add experience replay to the DQN network class. """"""

# Make sure the memory is big enough

if len(memory) >= size:

states = []

targets = []

# Sample a batch of experiences from the agent's memory

batch = random.sample(memory, size)



# Extract information from the data

for state, action, next_state, reward, done in batch:

states.append(state)

# Predict q_values

q_values = self.predict(state).tolist()

if done:

q_values[action] = reward

else:

q_values_next = self.predict(next_state)

q_values[action] = reward + gamma * torch.max(q_values_next).item() targets.append(q_values) self.update(states, targets)

DQL with Replay

As expected, the neural network with the replay seems to be much more robust and smart compared to its counterpart that only remembers the last action. After approximately 60 episodes, the agent managed to achieve the winning threshold and remain at this level. It also managed to achieve the highest reward possible — 500.

Double Deep Q Learning

Traditional Deep Q Learning tends to overestimate the reward, which leads to unstable training and lower quality policy. Let’s consider the equation for the Q value:

The Bellman Equation. Source: Link

The last part of the equation takes the estimate of the maximum value. This procedure results in systematic overestimation, which introduces a maximization bias. Since Q-learning involves learning estimates from estimates, such overestimation is especially worrying.

To avoid such a situation, I will define a new target network. The Q values will be taken from this new network, which is meant to reflect the state of the main DQN. However, it doesn’t have identical weights because it’s only updated after a certain number of episodes. This idea has been first introduced in Hasselt et al., 2015.

The addition of the target network might slow down the training since the target network is not continuously updated. However, it should have a more robust performance over time.

n_update from the q_learning loop specifies the interval after which the target network should be updated.

class DQN_double(DQN):

def __init__(self, state_dim, action_dim, hidden_dim, lr):

super().__init__(state_dim, action_dim, hidden_dim, lr)

self.target = copy.deepcopy(self.model)



def target_predict(self, s):

''' Use target network to make predicitons.'''

with torch.no_grad():

return self.target(torch.Tensor(s))



def target_update(self):

''' Update target network with the model weights.'''

self.target.load_state_dict(self.model.state_dict())



def replay(self, memory, size, gamma=1.0):

''' Add experience replay to the DQL network class.'''

if len(memory) >= size:

# Sample experiences from the agent's memory

data = random.sample(memory, size)

states = []

targets = []

# Extract datapoints from the data

for state, action, next_state, reward, done in data:

states.append(state)

q_values = self.predict(state).tolist()

if done:

q_values[action] = reward

else:

# The only difference between the simple replay is in this line

# It ensures that next q values are predicted with the target network.

q_values_next = self.target_predict(next_state)

q_values[action] = reward + gamma * torch.max(q_values_next).item() targets.append(q_values) self.update(states, targets)

Double DQL with Replay

Double DQL with replay has outperformed the previous version and has consistently performed above 300 steps. The performance also seems to be a bit more stable, thanks to the separation of action selection and evaluation. Finally, let’s explore the last modification to the DQL agent.

Soft Target Update

The method used to update the target network implemented above was introduced in the original DQN paper. In this section, we will explore another well-established method of updating the target network weights. Instead of updating weights after a certain number of steps, we will incrementally update the target network after every run using the following formula:

target_weights = target_weights * (1-TAU) + model_weights * TAU

where 0 < TAU < 1

This method of updating the target network is known as “soft target network updates” and was introduced in Lillicrap et al., 2016. This method implementation is shown below:

class DQN_double_soft(DQN_double):

def target_update(self, TAU=0.1):

''' Update the targer gradually. '''

# Extract parameters

model_params = self.model.named_parameters()

target_params = self.target.named_parameters()



updated_params = dict(target_params) for model_name, model_param in model_params:

if model_name in target_params:

# Update parameter

updated_params[model_name].data.copy_((TAU)*model_param.data + (1-TAU)*target_params[model_param].data) self.target.load_state_dict(updated_params)

DDQL with Soft Update

The network with soft target updates performed quite well. However, it doesn’t seem to be better than hard weight updates.

This gif illustrates the performance of a trained agent:

Trained Agent

Conclusion

The implementation of the experience replay and the target network have significantly improved the performance of a Deep Q Learning agent in the Open AI CartPole environment. Some other modifications to the agent, such as Dueling Network Architectures (Wang et al., 2015), can be added to this implementation to improve the agent’s performance. The algorithm is also generalizable to other environments. Feel free to test how well it solves other tasks!

Link to the notebook: https://github.com/ritakurban/Practical-Data-Science/blob/master/DQL_CartPole.ipynb

References

(1) Reinforcement Q-Learning from Scratch in Python with OpenAI Gym. (2019). Learndatasci.com. Retrieved 9 December 2019, from https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/

(2) Paszke, A., (2019). Reinforcement Learning (DQN) tutorial. Retrieved from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html

(3) Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … & Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.

(4) Van Hasselt, H., Guez, A., & Silver, D. (2016, March). Deep reinforcement learning with double q-learning. In Thirtieth AAAI conference on artificial intelligence.

(5) Wang, Z., Schaul, T., Hessel, M., Van Hasselt, H., Lanctot, M., & De Freitas, N. (2015). Dueling network architectures for deep reinforcement learning. arXiv preprint arXiv:1511.06581.

(6) Double DQN Implementation to Solve OpenAI Gym’s CartPole v-0. (2019). Medium. Retrieved 20 December 2019, from https://medium.com/@leosimmons/double-dqn-implementation-to-solve-openai-gyms-cartpole-v-0-df554cd0614d"
Linear Regression in Python,"Linear regression is arguably one of the most important and most used models in data science. In this blog post, I will walk you through the process of creating a linear regression model and show you some cool data visualization tricks.

We will be using the Ames Housing dataset, which is an expanded version of the often cited Boston Housing dataset. The dataset has approximately 1,700 rows and 81 columns, and the goal is to predict the selling price in the market.

Let’s Begin!

Just like any other project, the first step is to import all the libraries. You can import more libraries as you figure out all the tools you will need to build your model.

Now, before we process or change the data, let’s just get an idea of what we are dealing with.

The short description of the data set shows us that it contains various data types and null values. It’s important to remember this as we process the data and build our model.

Data Preprocessing

Datasets in the real world are never perfect. There will always be missing values and outliers that skew the dataset, affecting the accuracy of our predictive model. That’s why it is always a good idea to clean up the data before you start building your model.

Earlier, I had mentioned that this particular dataset had 1,700 rows and 81 columns. If a value is missing in a particular column, it would be unwise to delete that whole row because we would be losing a datapoint in every other column as well. There are two ways to solve this issue:

Replace every null value in a given column with the median value of that particular column. (This is only applicable to columns with numerical values) Calculate your statistics while ignoring all null values. (I’ll show you what methods to use later in the blog).

I opted for the second method, so I left the null values as they were in my dataset.

Calculating Outliers

There are multiple ways to calculate outliers— z-score, inter-quartile range (IQR), and the Tukey method are just a few methods out there. I chose to use the IQR. For all intents and purposes, I am assuming that you are familiar with the concepts of IQR, so I will only go over how to code it in Python. If you feel like you could use a short overview, this blog post does a pretty solid job of explaining the key ideas behind inter-quartile range.

In order to calculate the first and third quartile, I used the describe() function on the dataset.

summary = data_set.describe()

The describe() function yields a neat and concise data frame with important statistics from each numerical column of the original dataset. As you can see in the image above, I now have access to the mean, standard deviation, and percentile values with just one line of code!

Now, I store each statistic for every column in a data series. This allows me to access the percentile values in all the columns iteratively.

Now let’s dive into the deep end. I’ll first explain at a high level how I calculated the IQR before I dump my code onto here."
Discretizing 2D Images Into Polygonal and Point Models,"Discretizing 2D Images Into Polygonal and Point Models

The real world is all curvy and complicated. With burgeoning robotic technology and autonomous vehicles there is a common need to simplify objects in more manageable forms. Additionally, our world is continuous (for the most part), yet our technology interprets content digitally. So, for any world interaction there will be some level of discretization.

For a robotic grasping project I am developing, I recently ran into this problem. A 2D shape exists and surface information needs to be extrapolated in order to determine where to grab the shape. Luckily, OpenCV has methods of simplifying curvy contours into more polygonal looking shapes. Let us look into these functions and their results.

A key and the result after salience detection.

After being given an image we grab the item of interest using our favorite salience method. As in the example above, this creates a fairly decent mask of the object of interest. It can be seen that difficulties will arise if any sort of geometric manipulation is attempted. The curvature of the key’s handle will make it to difficult to determine a normal vector, for example. Additionally, noise picked up during segmentation will create erroneous edges.

The convenient OpenCV function approxPolyDP() is used to approximate a contour into fewer points. The function uses the Ramer–Douglas–Peucker algorithm to reduce the number of points. This is a recursive algorithm that takes a constant value, ε. The algorithm begins by finding the farthest point from two starting points. If the point is less than ε, then all the points between the first two starting points are discarded. Otherwise, the algorithm bisects the set of points again and recurses down to the next set of points. The process is repeated with each bisected set.

By varying the value of ε, different levels of “discretization” can be achieved. The greater the value, the more polygonal the final shape will be.

From Left to Right, Top to Bottom: ε = 0.5, 2, 5, 20

For many applications the above is a perfectly fine solution. However, some algorithms that deal with discrete…"
How to Build Your Own PyTorch Neural Network Layer from Scratch,"This is actually an assignment from Jeremy Howard’s fast.ai course, lesson 5. I’ve showcased how easy it is to build a Convolutional Neural Networks from scratch using PyTorch. Today, let’s try to delve down even deeper and see if we could write our own nn.Linear module. Why waste your time writing your own PyTorch module while it’s already been written by the devs over at Facebook?

Well, for one, you’ll gain a deeper understanding of how all the pieces are put together. By comparing your code with the PyTorch code, you will gain knowledge of why and how these libraries are developed.

Also, once you’re done, you’ll have more confidence in implementing and using all these libraries, knowing how things work. There will be no myth to you.

And last but not least, you’ll be able to modify/tweak these modules should the situation require. And this is the difference between a noob and a pro.

OK, enough of the motivation, let’s get to it.

Simple MNIST one layer NN as the backdrop

First of all, we need some ‘backdrop’ codes to test whether and how well our module performs. Let’s build a very simple one-layer neural network to solve the good-old MNIST dataset. The code (running in Jupyter Notebook) snippet below:"
Let me recall you this: accuracy isn’t everything in Machine Learning.,"Let me recall you this: accuracy isn’t everything in Machine Learning.

Everyday, data science professionals can’t stop thinking one thing: is that model really working? Data is like a live creature, and it changes and get messy almost everyday. At the end, all we want is to find a way to handle it and make good predictions, but how do we know how valuable are our results?

Well, one of the most basic things is to evaluate the success rate of our results, or how much of the test data we are predicting correctly. If this number is high, in theory our model is doing a good job. We like to call this value precision.

The starting point is to understand what are we trying to predict. It can be a number, a class, or even the answer for a yes/no question. After knowing this, we can define the parameters to evaluate performance. For this article, I’ll use a model that is trying to predict if the review left by a customer after an online shop experience will be good or not, which is a typical classification problem, with two possible outcomes:

0 = is not a good review

1 = is a good review

To answer that question, I did the traditional steps: data cleaning and feature selection, then I’ve trained several models, picked one and tried to improve it. You can find the full project, including the dataset I used, here on my GitHub.

Initially, I’ve tried 3 classification algorithms: KNN, Random Forest and Support Vector Machines (SVM), using the native functions available on scikit-learn, and tuning one parameter on each model. Let’s check the results:

Source: author

Let’s start looking at the column “F1 for Accuracy”. There we can see the results are very similar, around 80% each, what means that from all the predictions, the algorithm got it right 80% of the time, which can seem good if we just have that information. But when we look at recall for classes “0” and “1”, the values are substantially different. But what that means?"
Real Talk with the Director of Data Science at Columbia University,"Interviewer: Haebichan Jung, Project Lead at TowardsDataScience.com. Data Scientist at Recurly, SF.

Interviewee: Dr. Jeannette Wing, Avanessians Director of the Data Science Institute and Professor of Computer Science at Columbia University (2017-). Former Corporate Vice President at Microsoft Research (2013–2017). Former Head of the Computer Science Department at Carnegie Mellon University (Twice).

What were your main responsibilities at Microsoft Research?

There were two main functions for running the corporate research lab.

I was in charge of the basic research labs. It was about pushing state-of-the-art in all forms of research. By then Microsoft Research (MR) was covering more than just computer science (CS). We had biologists, social scientists, economists, etc. I was speaking to the core business of Microsoft and making sure that great technologies that MR researchers produced were available to business units and ensuring that business units would always be one step ahead in terms of technologies and anticipating the future.

I was in charge of both of these responsibilities across all our research labs across the globe, such as ones in Redmond, New York City, Cambridge (England), Bangalore (India), and Beijing (China).

Were you involved in Microsoft’s investments in AI projects? If so, what were some challenges you faced?

I was involved in the investing in research related to machine learning, AI, and data science more generally in terms of people we hired and projects we supported, as well as our engagements with the business units. I was in an actual business unit called “AI and Research”. The AI part of that business unit worked with other business units to make sure that other business units were aware of the latest ML and AI techniques and how to take advantage of the research ideas that were coming out from the research arm of the…"
Implementing Random Forest in R. A Practical Application of Random…,"Implementing Random Forest in R

Photo by Rural Explorer on Unsplash

What is Random Forest (RF)?

In order to understand RF, we need to first understand about decision trees. Rajesh S. Brid wrote a detailed article about decision trees. We will not go too much in details about the definition of decision trees since that is not the purpose of this article. I just want to quickly summarise a few points. A decision tree is series of Yes/No questions. For each level of the tree, if your answer is Yes, you fall into a category, otherwise, you will fall into another category. You will answer this series of Yes/No questions until you reach the final category. You will be classified into that group.

Taken from here

Trees work well with the data we use to train, but they are not performing well when it comes to new data samples. Fortunately, we have Random Forest, which is a combination of many decision trees with flexibility, hence resulting in an improvement in accuracy.

Here I will not go too much into details about RF because there are various sources outside we can get to understand what is the mathematics behind it. Here is one of them.

This article is more about practical application of RF in classifying cancer patients, so I will jump straight into the coding part. Now let’s open Rstudio and get our hands dirty :)

Implementing RF in R

First of all, we need to load the following packages. If you cannot load them, chances are you have not installed them yet. So please do so first before loading the following packages.

I read the data directly from the web link and name the dataset as Wisconsin. Let’s inspect the data a little bit"
How to Adjust DetectNet,"How to Adjust DetectNet

Photo by Christian Wiediger on Unsplash

DetectNet is an object detection architecture created by NVIDIA. It can be ran from NVIDIA’s Deep Learning graphical user interface, DIGITS, which allows you to quickly setup and start training classification, object detection, segmentation, and other types of models.

There are two basic DetectNet prototxt files provided by NVIDIA:

one for single class (which is the original) which can be found here, and one for two classes which can be found here.

DetectNet’s original architecture is written in Caffe. I could not find much documentation on the architecture besides 2 blog posts present in NVIDIA’s website and few tutorials which (mostly) reiterate the blogs’ content. I did find that a lot of information has been accumulated under one particular GitHub issue, issue #980 under the NVIDIA/DIGITS repository.

Here are the highlights I collected from the GitHub issue:

The images in your training set should not be of different sizes. If they are, you should pad them or resize them to be of equal dimensions. The resizing or padding can be done in the DIGITS dataset creation step.

DetectNet is sensitive to bounding boxes in the size range of 50x50 pixels to 400x400 pixels. It has difficulty identifying bounding boxes which are outside of this range.

If you want to detect objects smaller than the size DetectNet is sensitive to, you may either resize the images to be larger so most bounding boxes will fit DetectNet’s preferred range, or you may change the model’s stride to be smaller.

The image dimensions must be divisible by the stride. For example, 1248 and 384 (DetectNet’s default image size) are divisible by 16.

If you are training a model using an image resolution different from the original architecture (which expects images of width 1248 and height 384), you need to change the specified image dimensions within the architecture on the lines 57, 58, 79, 80, 118, 119, 2504, 2519, and 2545 (these lines refer to the single class DetectNet prototxt).

To change the model stride, you must change the default stride value (16) to your desired value in the lines 73, 112, 2504, 2519, and 2545 (these lines refer to the single class DetectNet prototxt).

If you specify a smaller stride, you will need to reduce the layers in the network in order to adjust the dimensionality. One way to reduce the dimensionality is by changing the kernel and stride parameters of the pool3/3x3_s2 layer to be 1. This layer is present from line 826 to 836 (these lines refer to the single class DetectNet prototxt).

For multiclass object detection in which you want to detect more than 2 classes, you may change the 2-class DetectNet prototxt [5]. Lines dependent on the number of classes are:"
Predicting Unknown Unknowns,"Reference Paper: Reducing Network Agnostophobia: https://arxiv.org/pdf/1811.04110.pd

For classification models for many domains and scenarios it is important to predict when the input given to the model does not belong to the classes it was trained on.

For computer vision / object detector models author provide following justification:

Object detectors have evolved over time from using feature-based detectors to sliding windows [34], region proposals [32], and, finally, to anchor boxes [31]. The majority of these approaches can be seen as having two parts, the proposal network and the classification network. During training, the classification network includes a background class to identify a proposal as not having an object of interest. However, even for the state-of-the-art systems it has been reported that the object proposals to the classifier “still contain a large proportion of background regions” and “the existence of many background samples makes the feature representation capture less intra-category variance and more inter-category variance (...) causing many false positives between ambiguous object categories” [41]. In a system that both detects and recognizes objects, the ability to handle unknown samples is crucial. Our goal is to improve the ability to classify correct classes while reducing the impact of unknown inputs.

It is also an important problem to tackle in many domains including healthcare, robotics irrespective of vision or NLP.

Nomenclature:

Past approaches to handle this have relied on two fundamental approaches:

Given an input provide uncertainty score based on how close is the input to inputs seen in training. ==> P (U | x) Given an input x predict probability of that input belonging to all classes Ci (i=1 to n) the model was trained on. We then threshold on having a minimum probability for class with highest probability to reject input as out of set or unknown.

Here are summary of some of the related approaches:"
An Intuitive Explanation of NeoDTI,"An Intuitive Explanation of NeoDTI

Introduction

In the previous stories of this series, we have discussed DeepWalk and GraphSAGE. Both of them were proposed to learn node representations in networks with a single entity and a single link type, i.e. homogenous networks. If you are unaware of these methods, you can check the previous stories.

In this story, we will explain NeoDTI [1], which has a different perspective. NeoDTI diverges from DeepWalk and GraphSAGE in two aspects. Firstly, NeoDTI is capable of operating on heterogeneous networks, i.e. networks with multiple link and entity types. Secondly, unlike DeepWalk and GraphSAGE, NeoDTI learns task-specific node embeddings, rather than general-purpose ones. In this manner, NeoDTI embeddings are specialized for a task, namely link prediction.

NeoDTI is proposed to utilize heterogeneous data for the drug-target interaction (DTI) prediction task. In the DTI task, each drug and target is represented with a node and two nodes are adjacent if they are known to be interacting. Therefore, link prediction corresponds to predict whether a drug and a protein interact. Below we can see a toy heterogeneous network that consists of three drugs and four targets.

A drug-target network with three drugs and four targets.

Although NeoDTI is proposed for DTI, link prediction on heterogeneous networks is a cross-domain problem. For instance, consider the Medium network where users and stories are represented as nodes. In this network, an edge between users denotes follower relation, whereas an edge between user and story denotes clapping. In this scenario, the link prediction task can be used for either friend or story recommendation to users."
Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric,"In my last article, I introduced the concept of Graph Neural Network (GNN) and some recent advancements of it. Since this topic is getting seriously hyped up, I decided to make this tutorial on how to easily implement your Graph Neural Network in your project. You will learn how to construct your own GNN with PyTorch Geometric, and how to use GNN to solve a real-world problem (Recsys Challenge 2015).

In this blog post, we will be using PyTorch and PyTorch Geometric (PyG), a Graph Neural Network framework built on top of PyTorch that runs blazingly fast. It is several times faster than the most well-known GNN framework, DGL.

Aside from its remarkable speed, PyG comes with a collection of well-implemented GNN models illustrated in various papers. Therefore, it would be very handy to reproduce the experiments with PyG."
Exploring Indian Premier League with Interactive Visualizations,"The art and practice of visualizing data is becoming ever more important in bridging the human-computer gap to mediate analytical insight in a meaningful way.

―Edd Dumbill

INTRODUCTION

Indian Premier League (IPL), Indian professional Twenty 20 (T20) cricket league established in 2008. The league, which is based on a round-robin group and knockout format, has teams in major Indian cities.

This is an exploration of IPL data visualization challenge (2008–2018) available in machinehack.com. I have done the visualization using Tableau.

ABOUT THE DATA

The dataset contains the following information from 2008 to 2018.

Match_date — Date and year of the match Match_number — Number of the match for each season Macth_venue — Stadium name and location Match_time — Night or Day/night Toss_winner — Team name who won the toss Toss_Decision — Decision whether to bat or field Team1 — Team who bat first Team1_score — Team 1 score Team 2 — Team who bat second Team2_score — Team 2 score Winning_team — Team who won the match Winning_margin — How many runs or wickets each team won?

In this article, we sought to understand IPL teams, matches and winning pattern through visualizations. I have done the preliminary analysis using this dataset, but a lot can be done.

Following are the steps followed for analysis:

Read data, pre-processing and cleanup of data in Python. The complete source code is available here, in case you want to have a look.

I made a good amount of worksheets with visualizations exploring various aspects of the data in Tableau. Medium doesn’t give us an option to embedded HTML or javascript code, hence what you see in this blog is a screenshot. However, all my visualizations and dashboards can be seen here

Interactive visualizations and analysis

Who is the most winning IPL Team?

From this rounded bar graphs, we can see that Mumbai Indians and Chennai Super Kings are the most successful teams in the IPL history followed by Kolkata Night Riders. Mumbai Indians won 97 matches, Chennai Super Kings won 90 matches and Kolkata Knight Riders won 86 match from 2008 to 2018.

Top Toss winners in IPL seasons?

From this packed bubbles, it can be seen that Mumbai Indians won the highest tosses in the IPL followed by Kolkata Night Riders and Delhi Daredevils.

A number of matches played across India?

The below map shows that a number of matches played across India. The highest number of matches were played in Mumbai, Kolkata, Bangalore and Delhi. Also, season 2009 and 2014 happened in South Africa and UAE and the same was not shown in the below map.

IPL Champions?

Chennai Super Kings and Mumbai Indias won the IPL Championships for the three times. Kolkata Knight Riders won the championship for two times. There are twelve teams in the dataset. Out of 12, only six teams won the championship. The championship has never been won by Royal Challengers, Delhi Daredevils, King XI Punjab, Pune Warriors, Gujarat Lions and Kochi Tusker.

How many matches were played in each stadium?

As per the below lollipop graph, M Chinnaswamy stadium has conducted the highest number of matches (76)followed by Eden Gardens, Feroz stadium and Wankhede stadium.

What are the average scores in each city?

The following butterfly chart shows that the highest scores achieved in Bengaluru followed by Dharamsala and Rajkot. It obviously shows favouring the batting pitch of these cities.

What is the average Team 1_ score?

As per the donut chart, Chennai Super Kings and Mumbai Indians had attained high scores when they batting first.

What is the team scores in IPL Finals — Batting first?

Sunrisers Hyderabad holds the highest score in the IPL finals followed by Chennai Super Kings and Mumbai Indians. Mumbai Indians holds the lowest score in the IPL finals for two seasons followed by Deccan Chargers.

How many matches conducted in each season?

The lollipop graph indicates the highest number of matches happened in the 2012 & 2013 season followed by the 2011 season.

What is the Team’s lowest and highest score in each season?

The following two charts show an individual team’s lowest and highest scores in the IPL history. Delhi daredevils hold the lowest scores (56) in two seasons 2008 & 2011. Kolkata Night Riders hold 67 in 2008 and Royal challengers hold 56 in 2011.

Royal Challengers hold the highest score in the IPL history of 263 & 248 in the year 2013 and 2016.

Most successful IPL Team?

Chennai Super Kings is the most successful team in IPL history. They are three times champions, five times Runners-up and two times qualified for Playoffs despite a two-year suspension.

The above visualizations were created in Tableau and only a preliminary one. However, more can be achieved. You can find all the visualizations here"
The Struggle of Modern Day Intrusion Detection Systems,"There are four cases when traffic attempts to pass through an IDS. The first two cases are normal traffic passing through and malicious traffic being rejected, but there are two cases where traffic can be misclassified. A false positive is when good traffic is considered malicious and rejected before entering the system, and a false negative is when malicious traffic is considered good and is allowed into the system, where positive is assumed to be malicious and negative is assumed to be normal. Adversarial AI focuses on these last two cases; by fooling the system, the adversarial AI system can choose which traffic can enter and which can be rejected.

IDS’s and adversarial AI are two major players fighting to outsmart the other, and the potential for both of these are essentially limitless. That being said, I look forward to seeing how adversarial AI will affect the development of not only IDS but AI as a whole."
Regularization for Machine Learning Models,"Regularization for Machine Learning Models

A common problem in machine learning is overfitting, where a model falsely generalizes noise in the training data:

A popular approach to remedy this problem and make the model more robust is regularization: A penalty term is added to the algorithm’s loss function. This changes the model’s weights which result from minimizing the loss function.

The most popular regularization techniques are Lasso, Ridge (aka Tikhonov) and Elastic Net. For the exemplary case of simple linear regression with only one weight parameter w (the slope of the linear fit), their penalty terms look like this (including a scaling parameter λ):

Lasso (L1) : λ·|w|

: λ·|w| Ridge (L2) : λ·w²

: λ·w² Elastic Net (L1+L2): λ₁·|w| + λ₂·w²

The different terms have different effects: Compared to L1, the quadratic L2 regularization becomes negligible at small weights (close to zero), but stronger at large weights. This leads to the following behaviours, casually phrased:"
The Ingenious Idea of Shirley Almon,"I think for folks from an economics background, probably the Shirley Almon distributed lag model is common-place, but I must admit that I came across this model (that dates back to the 1960s) pretty recently and was quite impressed by the ingenuity and learnt something which I think could be applied in context of other problems too.

Before we get to the model, Shirley Almon was a researcher in economics with just two publications to her credit, with one of them being the distributed lag model that she put forth. So the fact that she is considered among the most distinguished economists of her time, should tell the tale of the brilliance of these works. The sad part of the story though is that she got diagnosed with brain tumor in her early thirties, curtailing what would have otherwise been a long illustrious career in this field, culminating in her premature demise at the age of 40.

Shirley Almon of the now famous “Almon Lag model’

Let’s get to the lag model.

Equation 1

Essentially y is a linear function of the values x takes in the past n epochs (x1, x2 … xT). So the regression problem is one of estimating the weights (coefficients) of the values x took in the past {β₀, β₁, β₂ …, βn}

There are two issues with this model.

When n is large, estimating n parameters is time consuming. For linear regression, the assumption that the various values of x in the last ’n’ epochs are independent — i.e. uncorrelated may not hold true in a lot of the cases. Note that for a robust regression model, this would be a requirement.

Shirley Almon proposed a fantastic solution to the problem by applying the Weierstrass’s Approximation Theorem dating back to 1885.

Weierstrass’s Approximation Theorem states that in a closed interval [a,b] any continuous function f(x) can be approximated by a polynomial p(x) of some degree P.

Let’s take an example — Suppose below is the scatter plot of the βs to be estimated through regression. Each βi is represented as a point (i, β). You may wonder why this representation — you will see the significance in a moment. As one can imagine, the βi values seem scattered with no real visible connection between them.

However Shirley Almon, instead of looking at these coefficients as unconnected random values scattered across, she thought of an imaginary curve passing through these different βi in order.

Applying the Weierstrass’s Approximation Theorem, she figured that the curve could be approximated by a polynomial function of degree P.

Equation 2

In other words, she came up with a polynomial function from which the values of different βi can be found by substituting the right i.

Substituting Equation 2 in Equation 1 and simplifying it gives us something like this:

Rearranging terms and simplifying yields the following:

The reformulated task now is to estimate P regression coefficients {a₀, a1, a2 .. aP} instead of n coefficients and P is much less than n (P << n). And there is lesser chance of collinearity or correlation between the z parameters paving way for a more robust model.

However, the catch is to come up with the right value of P though.

That said, this is an easily usable model and has extensive applications. But I have to admit that I cannot stop being amazed at Shirley Almon’s display of enterprise back then to use Weierstrass’s Approximation Theorem to achieve this model simplification and robustness!

References

https://davegiles.blogspot.com/2017/01/explaining-almon-distributed-lag-model.html"
Wild Wide AI: responsible data science,"Wild Wide AI: responsible data science

Data Science can do good things for us: it improves life, it makes things more efficient, more effective and leads to a better experience. There are however some miss-steps that data-driven analysis has already exhibited. Here are few examples where data science tools were intentionally or unintentionally misused:

In 2012 a team of investigative journalists from The Wall Street Journal found out that Staples - a multinational supply retailing corporation — offered lower prices to buyers who live in more affluent neighborhoods. Staples’ intention was to offer discounts to customers who lived closer to their competitors’ stores. However their competitors tended to build stores in richer neighborhoods. Based on the correlation between location and social status, this resulted in price discrimination based on race. Neither Staples nor customers did not know about this side effect until a team of investigative journalists brought it to light (source).

In 2015, the AdFischer project demonstrated via simulation that synthetic men online profiles were being shown ads for high paying jobs significantly more frequently than female profiles. The result was a clear employment discrimination based on gender (source). The study started surfacing the problem due to lack of responsibility intentionally or not in data-driven algorithmic pipelines.

In 2016, investigators from ProPublica discovered that the software used by judges in court to predict future crimes was often incorrect, and it was racist: blacks were almost twice as likely as whites to be labeled a higher risk but less likely to re-offend. The tool made the opposite mistake among whites: they were much more likely than blacks to be labeled lower risk but went on to commit other crimes. ProPublica’s study was very influential. They published the dataset, the data methodology, as well as the data processing code in the form of a Jupyter Notebook on GitHub. This striking result really speaks about the opacity and the lack of fairness in these types of tools, especially when they were used in the public sector, in governments, in the juridical system (source).

Is Data Science impartial?

It is often claimed that data science is algorithmic and therefore cannot be biased. And yet, we saw examples above where all traditional evils of discrimination exhibit themselves in the data science ecosystem. Bias is inherited both in the data and in the process, is propelled and amplified.

Transparency is an idea, a mindset, a set of mechanisms that can help prevent discrimination, enable public debate and establish trust. When we make data science, we interact with society. The way we do decisions has to be in an environment where we have trust from the participants, from the public. Technology alone won’t solve the issue. User engagement, policy efforts are important.

Data responsibility

Aspects of responsibility in the data science ecosystem include: fairness, transparency, diversity and data protection. The area of responsible data science is very new but is already at the edge of all the top machine learning conferences because these are difficult but interesting and relevant problems.

Moritz Hardt

What is Fairness?

Philosophers, lawyers, sociologists have been asking this question for many years. In the data science context we usually solve the task of predictive analytics, predicting future performance or behavior based on some past or present observation (dataset). Statistical bias occurs when models used to solve such tasks do not fit the data very well. A biased model is somehow imprecise and does not summarize the data correctly. Societal bias happens when the data or the model does not represent the world correctly. An example occurs when the data is not representative. This is the case if we only used the data for police going to the SAME neighborhood over and over, and we use this information only about crime from those particular neighborhoods. Societal bias can also be caused by how we define world. Is it the world as it is that we are trying to impact with predictive analytics or the world as it should be? Who should determine what the world should be like?

What is discrimination?

In most legal systems, there are two concepts defining discrimination:

Disparate treatment is the illegal practice of treating an entity, such as a creditor or employee, differently based on a protected characteristic such as race, gender, age, religion, sexual orientation, or national origin. Disparate treatment comes in a context where there is some benefit to begin or some harm to be brought to the individual being treated, for example sentencing them or admitting them to college or granting them credit. It is something where there is actually tangible positive or negative impact.

Disparate impact is the result of systematic disparate treatment, where disproportionate adverse impact is observed on members of

a protected class. Different countries protect different classes or sub-populations.

When we talk about discrimination, we are using terms which could be uncomfortable, such as racism, gender, sexual orientation. Political correctness in the extreme sense has no place in these debates about responsible data science. We have to be able to name concepts to be able to talk about them. Once we can talk about those concepts, we can take corrective action.

Technical definition of fairness

Let’s consider vendors who are assigning outcomes to members of a population. This is the most basic case, a binary classification. Positive outcomes may be: offered employment, accepted to school, offered a loan, offered a discount. Negative outcomes may be: denied employment, rejected from school, denied a loan, not offered a discount. What we worry about in fairness is how outcome is assigned to members of a population. Let’s assume that 40% got the positive outcome. Some sub-population however may be treated differently by this process. Let’s assume that we know ahead of time what the sub-population is, for example red haired people. Thus we can divide our population into two groups: people with red hair, and people without red hair. In our example we observe that while 40% of the population got the positive outcome, only 20% of red haired received the positive outcome. 60% of the other received the positive outcome. Here, according to some definition, we observe disparate impact on the group of red haired individuals. Another way to denote this situation is that statistical parity fails. This is a baseline definition of fairness without conditioning. There is quite some sophistication about using such assessment in real-life, for example in courts. This basic definition of fairness, written into many laws around the globe, dictates that demographics of the individuals receiving any outcome are the same as demographics of the underlying population.

Assessing disparate impact

The vendor could say that he actually did not intend or did not look at all at hair color, which happens to be the sensitive attribute in the dataset. Instead the vendor would say that he decided to give the positive outcome to people whose hair is long. The vendor is denying the accusation and saying that he is not discriminating based on hair color. The thing is that the vendor has adversely impacted red haired people. It is not the intention that we care about, but the effect on the sub-population. In other words, blinding is not a legal or ethical excuse. Removing hair color from vendor’s process on outcome assignment does not prevent discrimination from occurring. Disparate impact is legally assessed on the impact, not on the intention.

Mitigating disparate impact

If we detect a violation of statistical parity, we may want to mitigate. In an environment in which we have a number of positive outcomes which we can assign, we have to swap some outcomes. We have to take a positive outcome from somebody in the not-red haired group and give it to someone else in the red haired group. Not everyone will agree with swapping outcomes. An individual who used to get the positive outcome would stop getting it any more. This would lead to individual fairness. It stipulates that any two individuals who are similar within a particular task should receive similar outcomes. There is a tension between group and individual fairness that is not easy to resolve.

Individual vs group fairness

An example in which individual fairness and group fairness was taken to the supreme court appears in the Ricci v. DeStefano case in 2009. Firefighters took a test for promotion, and the department threw out the test results because none of the black firefighters scored high enough to be promoted. The fire department was afraid that they could be sued for discrimination and disparate impact if they were to admit results and not promote any black firefighter. But then the lawsuit was brought by the firefighters who would have been eligible for promotion but who weren’t promoted as a result of this. There was an individual fairness argument, a disparate treatment argument. They argued that race was used to negatively impact them. This case was ruled in favor of white firefighters, in favor of individual fairness.

Individual fairness is equality, everybody gets the same box to reach the tree. Group fairness is the equity view, everybody gets as many boxes as they need to be able to reach the tree. Equity costs more because society has to invest more. These are two intrinsically different world views that we cannot logically decide which one is better. These are just two different points of view, there isn’t a better one. They go back to what we believe a world as it is, is a world as it should be. The truth is going to be somewhere in the middle. It is important to understand which kinds of mitigation are consistent with which kinds of belief systems.

Formal definition of fairness

Friedler et. al. tease out the difference between beliefs about fairness and

mechanisms that logically follow from those beliefs in their paper from 2016. The construct space is intrinsically the state of the world. It is made of things we cannot directly measure such as intelligence, grit, propensity to commit crime and risk-adverseness. We however want to measure intelligence and grit when we decide who to admit to college. We want to know the propensity of a person to recommit crime and his risk-adverseness in justice. These are raw properties which are exhibited and not directly accessible. Instead we look at the observed space where there are proxies, which are to a greater or lesser degree aligned with the properties that we want to measure. For intelligence the proxy would be SAT score, grit would be measured by high-school GPA, propensity to commit crime by family history and risk-adverseness by age. The decision space is then made of what we would like to decide: performance in college and recidivism.

Fairness is defined here as a mapping from the construct space to the decision space, via the observe space. Individual fairness (equality) believes that the observed space faithfully represents the construct space. For example high-school GPA is a good measure of grit. Therefore the mapping from construct to decision space has low distortion. Group fairness (equity) however says that there is a systematic distortion caused by structural bias, society bias when going from the construct space to observed space. Furthermore this distortion aligns with groups structure, with membership in protected groups in our society. In other words the society systematically discriminates.

to be continued …

References: lecture on responsible data science at Harvard University by Prof. Julia Stoyanovich (New York University) — selected chapters from “ The Age of Surveillance Capitalism” book by Shoshana Zuboff — thoughts from “What worries me about AI” post by François Chollet."
Neural Network Algorithms — Learn How To Train ANN,"Neural Network Algorithms — Learn How To Train ANN

Top Neural Network Algorithms

Learning of the neural network takes place on the basis of a sample of the population under study. During the course of learning, compare the value delivered by the output unit with actual value. After that adjust the weights of all units so to improve the prediction.

There are many Neural Network Algorithms are available for training Artificial Neural Network. Let us now see some important Algorithms for training Neural Networks:

Gradient Descent — Used to find the local minimum of a function.

Evolutionary Algorithms — Based on the concept of natural selection or survival of the fittest in Biology.

Genetic Algorithm — Enable the most appropriate rules for the solution of a problem and select it. So, they send their ‘genetic material’ to ‘child’ rules. We will learn about them in details below.

Get the introduction of learning rules in Neural Network for more understanding of Neural Network Algorithms.

Gradient Descent

We use the gradient descent algorithm to find the local smallest of a function. The Neural Network Algorithm converges to the local smallest. By approaching proportional to the negative of the gradient of the function. To find local maxima, take the steps proportional to the positive gradient of the function. This is a gradient ascendant process.

In linear models, the error surface is well defined and well known mathematical object in the shape of a parabola. Then find the least point by calculation. Unlike linear models, neural networks are complex nonlinear models. Here, the error surface has an irregular layout, crisscrossed with hills, valleys, plateau, and deep ravines. To find the last point on this surface, for which no maps are available, the user must explore it.

In this Neural Network Algorithm, you move over the error surface by following the line with the greatest slope. It also offers the possibility of reaching the lowest possible point. You then have to work out at the optimal rate at which you should travel down the slope.

The correct speed is proportional to the slope of the surface and the learning rate. Learning rate controls the extent of modification of the weights during the learning process.

Hence, the moment of a neural network can affect the performance of multilayer perceptron.

Evolutionary Algorithms

This algorithm based on the concept of natural selection or survival of the fittest in Biology. The concept of natural selection states that — for a given population, environment conditions use a pressure that results in the rise of the fittest in that population.

To measure fittest in a given population, you can apply a function as an abstract measure.

In the context of evolutionary algorithms, refer recombination to as an operator. Then apply it to two or more candidates known as parents, and result in one or more new candidates known as children. Apply the mutation on a single candidate and results in a new candidate. By applying recombination and mutation, we can get a set of new candidates to place in the next generation based on their fittest measure.

The two basic elements of evolutionary algorithms in Neural Network are:

Variation operators (recombination and mutation)

Selection process (selection of the fittest)

The common features of evolutionary algorithms are:

Evolutionary algorithms are population-based.

Evolutionary algorithms use recombination mix candidates of a population and create new candidates.

On random selection evolutionary algorithm based.

Hence, on the basis of details and applied problems, we use various formats of evolutionary algorithms.

Some common evolutionary algorithms are:

Genetic Algorithm Genetic Algorithm — It provides the solution for optimization problems. It provides the solution with the help of natural evolutionary processes. Like mutation, recombination, crossover, and inheritance.

Genetic Programming — genetic programming provides a solution in the form of computer programs. By the ability to solve computational problems accuracy of a program measures.

Evolutionary Programming — In a simulated environment to develop the AI we use it.

Evolution Strategy It is an optimization algorithm. Grounded on the concepts of the adaptation and the evolution in biological science.

Neuroevolution — To train neural networks we use Neuroevolution. By specifying structure and connection weights genomes uses to develop neural networks.

In all these Neural Network Algorithms, a genetic algorithm is the most common evolutionary algorithm.

Genetic Algorithm

Genetic algorithms, developed by John Holland’s group from the early 1970s. It enables the most appropriate rules for the solution of a problem to be selected. So that they send their ‘genetic material’ (their variables and categories) to ‘child’ rules.

Here refer a like a set of categories of variables. For example, customers aged between 36 and 50, having financial assets of less than $20,000 and a monthly income of more than $2000.

A rule is the equal of a branch of a decision tree; it is also analogous to a gene. You can understand genes as units inside cells that control how living organisms inherit features of their parents. Thus, Genetic algorithms aim to reproduce the mechanisms of natural selection. By selecting the rules best adapted to prediction and by crossing and mutating them until getting a predictive model.

Together with neural networks, they form the second type of algorithm. Which mimics natural mechanisms to explain phenomena that are not necessarily natural.

The steps for executing genetic algorithms are:

Step 1: Random generation of initial rules — Generate the rules first with the constraint being that they must be all distinct. Each rule contains a random number of variables chosen by a user.

Step 2: Selection of the best rules — Check the Rules in view of the aim by the fitness function to guide the evolution toward the best rules. Best rules maximize the fitness function and retain with the probability that increases as the rule improves. Some rules will disappear while others select several times.

Step 3: Generation of new rules by mutation or crossing — First, go to step 2 until the execution of the algorithm stops. Chosen rules are randomly mutated or crossed. The mutation is the replacement of a variable or a category of an original rule with another.

A crossing of 2 rules is the exchange of some of their variables or categories to produce 2 new rules. A crossing is more common than mutation.

Neural Network Algorithms ends when 1 of the following 2 conditions meets:

A specified number of iterations that reached.

Starting from the generation of rank n, rules of generations n, n-1, and n-2 are (almost) identical.

So, this was all about Neural Network Algorithms. Hope you like our explanation.

Conclusion

Hence, Artificial Neural Network is typically difficult to configure and slow to train, but once prepared are very fast in the application. They are generally designed as models to overcome the mathematical, computational, and engineering problems. Since, there is a lot of research in mathematics, neurobiology and computer science.

If you’d like to share your opinion and have any query about Artificial Neural Network Algorithms, please do so in the comment section.

I also recommend you to read my previous articles-

Top Machine Learning Algorithms You Should Know to Become a Data Scientist

Dimensionality Reduction in Machine Learning

Artificial Neural Network for Machine Learning — Structure & Layers"
Multi-Class Imbalance,"Made this in Microsoft Paint

So, for my first write-up, I am tackling a problem I encountered whilst working on my first data science project at my company. I work as a machine learning researcher at ALC- Innovative Transportation Solutions and my problem arose during a predictive modeling project. Multi-class imbalance. I had encountered class imbalance before in classroom projects and had employed the use of the ROSE package but never had I been exposed to a multi-class imbalance issue.

Google Images: Binary class imbalance. I’ve dealt with binary class imbalance before and there are plenty of tools and articles about tackling this common data issue.

Binary class-imbalance is a common headache in data science but can be easily solved(great article about it: https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/). Unfortunately for this project I had on my hands, there weren’t as many resources. I wanted to share my thought process for overcoming multi-class imbalance so someone else might find it useful in mediating their own distribution conflicts.

The Distribution

My company’s data is not open for sharing but here is a simulated version of the issue I was facing:

This data distribution across classes is completely imbalanced. This is bad because the goal of my project is to build a multi-class classifier that can correctly assign what ‘Class’ a data point belongs to. Specifically, this imbalance is an issue because the predictive model, whichever I end up pursuing, would be biased towards Class1 and, to less of a degree but still, Class2. It would achieve decent accuracy by classifying the majority of the train and test set as Class1 or Class2 because of the imbalance; this is the ‘Accuracy Paradox.’ My model might achieve good accuracy on classification but that’s because the model would only be modelling the imbalanced distribution.

Google Images: Confusion Matrix. If a ML model trains on an imbalanced data set it will over-classify towards the majority class. In the image, for reference, the model would predict all classes to be ‘P’ while some of those should have been ‘N.’ In the case of multi-class imbalance the effects would be even more drastic where the model would predict ‘P’ (because it’s the majority class in this example) when the actual class was ’N’ or ‘O’ or ‘M’ etc.

This issue is compounded by the fact that the distinguishing characteristics between the classes is quite thin, in the case of my actual work project. Their longitude/latitude features are only different by a minuscule amount. Sure, there are other predictive features but Geo-spatial data makes up the bulk of the predictive and interpret-ability aspects of the model. Yet, this difference in classes is important to the company and the decision making of the model and therefore this issue must be tackled.

Google Images: Imbalanced Classifier. This image is an example of how an imbalanced data set would create an ‘accurate’ classifier that, in production, would really be a weak classification model hiding behind the guise of ‘High Accuracy.’

ROSE

ROSE, or the Random Over Sampling Experiment, is a fantastic R package that deals with class imbalance quite well, but only binary class imbalance (two classes). How could I use this package to fix the multi-class data I was looking at?

After trial-and-error, researching of options, and dropping a quick email to one of my great professors I determined the best course of action: writing a function that takes the whole data set, divides it up into 6 subsets using ‘Class’ as the dividing feature, and then use ROSE to balance those subsets out to my desired distribution. Then they would be compiled back into one, reasonably balanced data set. Each subset contains Class1 and then one of the minority classes. Class2 was left out because it’s not under-represented.

The subset is then given to a ROSE argument to over-sample the minority class. The ROSE code for doing so can be seen below. The identity of the minority class is used as the formula and the ‘p’ argument is the probability of sampling the rare class, an alternative is setting N to the desired size of the data set. Below, with p=0.5, these arguments return a data set where the minority class is now represented in 50% of the data.

library(ROSE) BalancedData <- ovun.sample(MinorityClassi~, ImbalancedData, method=""over"", p=0.5, subset=options(""subset"")$subset, na.action=options(""na.action"")$na.action, seed) index = createDataPartition(y=BalancedData$Class, p=0.7, list=FALSE)

train = BalancedData[index,]

test = BalancedData[-index,] BalTrain <- droplevels.data.frame(train)

BalTest <- droplevels.data.frame(test)

After over-sampling the minority class and bringing the distribution out of imbalanced hell I ran this balanced data set through my classifier. The best model was a Random Forest (compared against a logistic regression model as the baseline, a gini-criteria decision tree, an information-gain decision tree, and a neural network). The results were great, but not so great on the test set. Why? Because the over-sampled minority class data weren’t new data points. So the model couldn’t conceive that new points could fit into that class.

I considered using SMOTE, synthetic-minority over-sampling technique but it’s results were negligible at best. I settled on using the ROSE formula in the ROSE package which: “creates a sample of synthetic data by enlarging the features space of minority and majority class examples” (cran.r-project.org/web/packages/ROSE/ROSE.pdf).

SynthBalData <- ROSE(MinorityClassi~, ImbalancedData, p=0.5, hmult.majo=1, hmult.mino=1, subset=options(""subset"")$subset, na.action=options(""na.action"")$na.action, seed) index = createDataPartition(y=SynthBalData$Class, p=0.7, list=FALSE)

SynthBalTrain = SynthBalData[index,]

SynthBalTest = SynthBalData[-index,] SynthBalTrain <- droplevels.data.frame(SynthBalTrain)

SynthBalTest <- droplevels.data.frame(SynthBalTest)

By sticking to my original method of subset creation and applying ROSE I got back synthetic yet balanced data samples and compiled a new data set. I trained all the models on a simple random sample 70:30 train/test split of the data. The accuracy was high, and the models responded very well to new data points as they come in from my company’s server."
A.I. For Filmmaking,"Originally published at https://rsomani95.github.io.Visit the link for a better formatted, interactive version of the post with many more images.

GitHub: https://github.com/rsomani95/shot-type-classifier

Table of Contents

-What is Visual Language, and Why Does it Matter?

-Neural Networks 101 (Read if you don’t know what neural networks are)

-The Dataset

— — Data Sources

— — Shot Types

-Methodology

-Results

— — Training Performance

— — Confusion Matrix

— — Heatmaps (Highlight of the post)

— — Robustness

-Conclusion

Analysing cinema is a time-consuming process. In the cinematography domain alone, there’s a lot of factors to consider, such as shot scale, shot composition, camera movement, color, lighting, etc. Whatever you shoot is in some way influenced by what you’ve watched. There’s only so much one can watch, and even lesser that one can analyse thoroughly.

This is where neural networks offer ample promise. They can recognise patterns in images that weren’t possible until less than a decade ago, thus offering an unimaginable speed up in analysing cinema. I’ve developed a neural network that focuses on one fundamental element of visual grammar: shot types. It’s capable of recognising 6 unique shot types, and is~91% accurate. The pretrained model, validation dataset (the set of images used to determine its accuracy), code used to train the network, and some more code to classify your own images is freely available here.

W hat is Visual Language, and Why Does it Matter?

When you’re writing something — an email, an essay, a report, a paper, etc, you’re using the rules of grammar to put forth your point. Your choice of words, the way you construct the sentence, correct use of punctuation, and most importantly, what you have to say, all contribute towards the effectiveness of your message.

Cinema is about how ideas and emotions are expressed through a visual form. It’s a visual language, and just like any written language, your choice of words (what you put in the shot/frame), the way you construct the sentence (the sequence of shots), correct use of punctuation (editing & continuity) and what you have to say (the story) are key factors of creating effective cinema. The comparison doesn’t apply rigidly, but is a good starting point to start thinking about cinema as a language.

The most basic element of this language is a shot. There’s many factors to consider while filming a shot — how big should the subject be, should the camera be placed above or below the subject, how long should the shot be, should the camera remain still or move with the subject, and if it’s moving, how should it move? Should it follow the subject, observe it from a certain point while turning right/left or up/down and should the movement be smooth or jerky. There are other major visual factors, such as color and lighting, but we’ll restrict our scope to these factors only. A filmmaker chooses how to construct a shot based on what he/she wants to convey, and then juxtaposes them effectively to drive home the message.

Let’s consider this scene from Interstellar. To give you some context, a crew of two researchers and a pilot land on a mysterious planet to collect crucial data from the debris of a previous mission. This planet is very different from Earth — it is covered in an endless ocean, and its gravity is 130% stronger than Earth’s.

This scene consists of 89 shots, and the average length of each shot is 2.66 seconds.

For almost all the shots showing Cooper (Matthew McConaughey) inside the spacecraft, Nolan uses a Medium Close Up, showing Cooper from the chest up. This allows us to see his facial expressions clearly, as well as a bit of the spacecraft he’s in and his upper body movements. Notice how the camera isn’t 100% stable. The camera moves slightly according to Cooper’s movements, making us feel more involved in this scene."
Advice for New and Junior Data Scientists,"Who Do I Think I Am

My name’s Darrin, and I’ve been a Data Scientist at Outcome Health for close to 2 years now. We’re a health tech company focused on patient education at the point of care. In English, that means we put devices in doctor’s offices with content for patients. We have over 100,000 devices in the field — which means lots of data.

Who This Article is For

This article is for the many Junior/Associate Data Scientists out there. If you’re just beginning your journey, if you’re a bit confused and lost, if you’re not always certain what you’re supposed to be doing or how to best contribute to your new company, you’ve come to the right place.

You. Me. Your C-suite, probably.

Why I Wrote This Article

I’ve seen many data science articles explaining the ins-and-outs of what doing more advanced data science work is like, the skills great data scientists need, the types of projects such masters tackle. But as I went through my Associate Data Scientist journey, I had a hell of a time navigating what to do to actually be useful as a more junior team member.

This article is an attempt to alleviate that issue for other budding Associate Data Scientists.

My main goal is to provide a basic blueprint, with examples, of how an Associate Data Scientist can quickly add significant value to an organization.

I was lucky enough to figure things out along the way to the degree that I am now a Data Scientist — I’m hoping this will help some of you along the same path!

Step 1: Finding Low-Risk, High-Reward Projects

The Main Issue

The first problem every Associate Data Scientist encounters is probably the same: How do you…"
Most Philadelphia schools have had a nearby shooting this year,"As of May 1, 2019, more than half of Philadelphia schools have had a shooting occur fewer than 500 meters away. Nineteen schools — 15 of which are in North Philadelphia — have had a shooting less than a football field away.

The median distance between a Philadelphia school and the nearest shooting is a 421 meters — roughly a quarter mile. This data includes district, charter and private schools.

| distance (meters) | schools |

|-------------------|---------|

| 0 - 500 | 316 |

| 501 - 1,000 | 114 |

| 1,001 - 1,500 | 39 |

| > 1,500 | 78 |

The vast majority of students attend a school where a shooting has occurred fewer than 500 meters away (a little less than a mile).

| distance (meters) | students |

|-------------------|----------|

| 0 - 500 | 90,896 |

| 501 - 1,000 | 33,715 |

| 1,001 - 1,500 | 12,695 |

| > 1,500 | 31,264 |

Data: Shooting victims and school locations, by provided by the City of Philadelphia. Chart by Jared Whalen.

Here are the five schools that are closest to a shooting."
How to land an internship in Machine Learning,"Getting an internship in Machine Learning as an undergraduate student is tough. Really tough… Most of the well-known companies are looking for Ph.D. students with publications in prestigious journals. How to increase the chances of getting an internship in ML if you can’t satisfy the above?

Photo by Hunters Race on Unsplash

Last year, I have spent hours sending applications to apply for Machine Learning/AI internships. As an Electronic Engineering student, I’ve found it particularly difficult to get even an interview, even though I had some relevant experience in Machine Learning. During my search for internships, I have noticed a few common things which companies are looking for. Based on these facts, I have decided to change my strategy for sending CVs, which finally resulted in me getting a job as a Machine Learning Research Intern. If you are planning to apply for Machine Learning internships, or you are struggling to get one, I hope that this article will help you land your dream job!

I also wrote another article that helps you prepare for the interview. Link is here.

Big companies = Big competition

Photo by Paweł Czerwiński on Unsplash

The first mistake I made, was to apply to big, well-known companies. Companies such as Google, Amazon or Apple are getting hundreds of applications per day and it is very difficult to get even through the first recruitment stage for an internship. If you feel that you have goods skills in Machine Learning, with substantial experience in this field then go for it. Otherwise, it might be better to focus on targeting smaller, less known companies to maximize your chances of getting hired. For example, have a look at the required basic qualifications for Machine Learning Intern at Amazon:

Internship requirements for Machine Learning Internship at Amazon, found on Linkedin

Woah! You see, that is what I meant by saying tough requirements. Preferably, you should be a P.h.D. student with several publications in Machine Learning. If you are undergraduate, like me, you are clearly at the disadvantage here."
Cleaning Web-Scraped Data with Pandas (Part II),"Member-only story Cleaning Web-Scraped Data with Pandas (Part II)

This post is a continuation of my previous discussion on cleaning Web-Scraped data. You can access Part I with the link below:

|| I || Quantity vs. Quality (of data)

As I mentioned in my previous post, cleaning data is a prerequisite to machine learning. Measuring the sanity of your data can also give you a good indication of how precise or accurate your model would be. When it comes to web-scraped data, you would often lose a lot of information in the process of cleaning. So what should it be? Quantity or Quality?

Photo by JOSHUA COLEMAN on Unsplash

It’s not easy to answer the question, as it can really depend on the case and the process mapped out by a data scientist.

If you end up working with data that requires less specificity to work with its variables, then you may choose to go with quantity, and should be fine with using data cleansing methods that may replace values using inferential data.

However, if you’re working with data that does require specificity. For example, in this case, we’re working with Laptop data. It wouldn’t make sense to use mean, median, or mode to replace missing values in our data since there are multiple categories of laptops with different combinations of specs. For example, if i5 is the mode of column “processor”, you can’t let all the missing values be equal to i5 as this would skew the data, creating a large bias.

Therefore, you would clean the data by removing the rows with missing values. This would reduce your data from 440 values to 153 values. I know that’s not great for a machine learning model but as a data scientist, I understand that the quality of my results will be tied to the quality of my…"
"Don’t blame the AI, it’s the humans who are biased.","These datasets include things like your name or personal information, a photograph, documents, or other historical artifact. Because these data have been collected by humans they are subjective and often unrepresentative[3]. As outlined in my earlier post, there are problems with how women (and minorities) have been represented (or suppressed) in historical records. Women and people of color may be disproportionately affected by AI bias, due to intersections of economics and gender, race, and ethnicity within the datasets[1,4]. Buolamwini and Gebru[5], in their analysis of commercially available facial recognition AI, find that gender of white men was properly identified by the computer more than 99% of the time, but accuracy is barely 65% for black women.

How societal bias makes its way into AI is complex, but after researching this topic at length, I find they can roughly group into 3 main issues. Bear with me as a non-engineer in my attempt to articulate them.

First, the data used to train the AI system may itself carry systemic social biases. Some even have outlined how our language itself carries biases, impacting many data points at the root. Imagine a hypothetical scenario of a university using AI to help rank freshman applicants, using previous student records as its training data (e.g., high school GPA, SAT scores, or other state testing results correlated with college success). There are widely discussed inequities in the U.S. school system and standardized testing that…"
Using SQL to Improve Missing Values,"Missing data points can sometimes feel like missing jigsaw pieces (photo: author)

For a few months now, we’ve been looking at different ways to use SQL in data science — most recently on using SQL to detect outliers (not the same as detecting anomalies, although that’s in the future).

In the first post in this sporadic series of posts, we looked at how to assess missing data by counting the proportion of nulls in a column. However, simply finding and counting missing data points is of limited use — you need to have tools to use if you find these missing data points.

The core ways to handle missing values should be familiar to all data scientists, a phrase which here means ‘if you aren’t familiar, you should memorise the following list’:

Listwise deletion: if a variable has so many missing cases that it appears useless, delete it. Casewise deletion: if there are too many factors missing for a particular observation, delete it. Dummy Variable Adjustment: if the variable is missing for a particular case, use an assumed value in its stead. Depending on the problem the median may appear the intuitive choice or a value that represents a ‘neutral’ setting. Imputation: use an algorithm to fill in the value, from a simple random number at the most basic end of the spectrum, to a value imputed by its own model at the more complex end.

SQL is clearly better at some of these than others. Listwise deletion is as simple as leaving the column name out of the SELECT statement; casewise deletion may be as simple as a WHERE clause testing for nulls.

Using these two methods comes with risks — you can easily introduce bias if the data are not Missing Completely at Random. It’s not hard to think of situations where measurements aren’t collected due to specific circumstances reflected in the data. An example given by Frank Harrell in his book ‘Regression Modeling Strategies’ is blood pressure measurements which are not taken on patients expected to die shortly — hence excluding patient observations missing blood pressure measurements in a model of patient outcomes could selectively exclude patients who died, creating an obvious skew.

While the above example illustrates the risk of casewise deletion, the disadvantage of listwise deletion can also be seen from the…"
DeepPiCar — Part 2: Raspberry Pi Setup and PiCar Assembly,"1 x Set of Miniature Traffic Signs ($15) and a few Lego figurines. You may not need to buy them if your younger ones have some of these toy signs and Lego figurines in the playroom. You can use whatever signs you find to train the model, just make sure they are not TOO BIG!

(Optional) 1 x 170 degree Wide Angle USB Camera ($40). This is an optional accessory. I bought it to replace the stock camera that came with the SunFounder PiCar so that the car can have a wide field of vision. The stock camera is great, but not as wide angle as I like and it can’t see lane lines that are 3–4 inches in front of the front wheels. I wrote the lane following code in Part 4 with the stock camera initially. After trying a few lens, I found that the lane following accuracy and stability greatly increased with this wide angle camera. It is nice to have control of both your hardware and software (vs running a car in car simulator), because you may resort to a hardware fix if a problem can’t be easily solved via software alone.

USB Keyboard/Mouse and Monitor that takes HDMI input. You only need these during the initial setup stage of the Pi. Afterward, we can remote control the Pi via VNC or Putty.

A desktop or laptop computer running Windows/Mac or Linux, which I will refer to as “PC” here onwards. We will use this PC to remote access and deploy code to the Pi computer.

Sometimes, it surprises me that Raspberry Pi, the brain of our car is only about $30 and cheaper than many of our other accessories. Indeed, the hardware is getting cheaper and more powerful over time, and software is completely free and abundant. Don’t we live in a GREAT era?!

This is the end product when the assembly is done. I am using a wide-angle camera here.

Raspberry Pi Set up

Raspberry Pi Operating System Set up (1 Hour)

Follow this excellent step-by-step guide to install the NOOBS Raspbian Operating System (a variate of Linux) onto a micro SD card. It would take about 20 min and about 4GB of disk space. After installation and reboot, you should see a full GUI desktop like below. This feels like you are in a Windows or Mac GUI environment, doesn’t it?

During installation, Pi will ask you to change the password for the default user pi . Let’s set the password to rasp , for example.

. Let’s set the password to , for example. After the initial installation, Pi may need to upgrade to the latest software. This may take another 10–15 minutes.

Setup Remote Access

Setting up remote access allows Pi computer to run headless (i.e. without a monitor/keyboard/mouse) which saves us from having to connect a monitor and keyboard/mouse to it all the time. This video gives a very good tutorial on how to set up SSH and VNC Remote Access. Here are the steps, anyways.

Open the Terminal application, as shown below. The Terminal app is a very important program, as most of our command in later articles will be entered from Terminal.

Find the IP address of the Pi by running ifconfig . In this case, my Pi’s IP address is 192.168.1.120 .

pi@raspberrypi:~ $ ifconfig | grep wlan0 -A1

wlan0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500

inet 192.168.1.120 netmask 255.255.255.0 broadcast 192.168.1.255

Run sudo raspi-config in Terminal to start the “Raspberry Pi Software Configuration Tool”. You may be prompted to type in the password for user pi

Enable SSH Server: Choose 5. Interface Options -> SSH -> Enable

-> -> Enable VNC Server: Choose 5. Interface Options -> VNC -> Enable

-> -> Download and install RealVNC Viewer onto your PC.

Connect to Pi’s IP address using Real VNC Viewer. You will see the same desktop as the one Pi is running.

At this point, you can safely disconnect the monitor/keyboard/mouse from the Pi computer, leaving just the power adapter plugged in.

Setup Remote File Access

Since our Pi will be running headless, we want to be able to access Pi’s file system from a remote computer so that we can transfer files to/from Pi computer easily. We will install Samba File Server on Pi.

sudo apt-get update && sudo apt-get upgrade -y

Get:1

Packages [45.0 kB]

[omitted...]

Unpacking lxplug-ptbatt (0.5) over (0.4) ...

Setting up lxplug-ptbatt (0.5) ... pi@raspberrypi:~ $Get:1 http://archive.raspberrypi.org/debian stretch InRelease [25.4 kB]Packages [45.0 kB][omitted...]Unpacking lxplug-ptbatt (0.5) over (0.4) ...Setting up lxplug-ptbatt (0.5) ... pi@raspberrypi:~ $ sudo apt-get install samba samba-common-bin -y

Reading package lists... Done

Building dependency tree

[omitted...]

Processing triggers for libc-bin (2.24-11+deb9u4) ...

Processing triggers for systemd (232-25+deb9u11) ... pi@raspberrypi:~ $ sudo rm /etc/samba/smb.conf pi@raspberrypi:~ $ sudo nano /etc/samba/smb.conf

Then paste in the following lines into the nano editor

[global]

netbios name = Pi

server string = The PiCar File System

workgroup = WORKGROUP



[HOMEPI]

path = /home/pi

comment = No comment

browsable = yes

writable = Yes

create mask = 0777

directory mask = 0777

public = no

Save and exit nano by Ctrl-X, and Yes to save changes.

Then set up a Samba Server password. For simplicity, we will use the same rasp as the Samba server password. After the password is set, restart the Samba server.

# create samba password

pi@raspberrypi:~ $ sudo smbpasswd -a pi

New SMB password:

Retype new SMB password:

Added user pi. # restart samba server

pi@raspberrypi:~ $ sudo service smbd restart

At this point, you should be able to connect to the Pi computer from your PC via Pi’s IP address (My Pi’s IP is 192.168.1.120). Go to your PC (Windows), open a Command Prompt (cmd.exe) and type:

# mount the Pi home directory to R: drive on PC

C:\>net use r: \\192.168.1.120\homepi

The command completed successfully.

C:\Users\dctia>r: C:\>dir r:

Volume in drive R is HOMEPI

Volume Serial Number is 61E3-70FF Directory of R:\ 05/02/2019 03:57 PM <DIR> .

04/08/2019 04:48 AM <DIR> ..

04/08/2019 05:43 AM <DIR> Desktop

04/08/2019 05:43 AM <DIR> Documents

04/08/2019 05:43 AM <DIR> Downloads

04/08/2019 05:15 AM <DIR> MagPi

04/08/2019 05:43 AM <DIR> Music

05/02/2019 03:43 PM <DIR> Pictures

04/08/2019 05:43 AM <DIR> Public

04/08/2019 05:43 AM <DIR> Templates

04/08/2019 05:43 AM <DIR> Videos

0 File(s) 0 bytes

11 Dir(s) 22,864,379,904 bytes free

Indeed this is our Pi Computer’s file system that we can see from its file manager. This will be very useful since we can edit files that reside on Pi directly from our PC. For example, we can use PyCharm IDE to edit Python programs on Pi first, and then just use Pi’s terminal (via VNC) to run these programs.

If you have a Mac, here is how to connect to the Pi’s file server. Hit Command-K to bring up the “Connect to Server” window. Enter the network drive path (replace with your Pi’s IP address), i.e. smb://192.168.1.120/homepi, and click Connect. Enter the login/password, i.e. pi/rasp and click OK to mount the network drive. Then the drive will now appear on your desktop and in the Finder Window sidebar. For more in-depth network connectivity instructions on Mac, check out this excellent article.

Install USB Camera

The device driver for the USB camera should already come with Raspian OS. We will install a Video Camera Viewer so we can see live videos.

Take the USB Camera out of PiCar kit and plug into Pi computer’s USB port

Run sudo apt-get install cheese from the terminal to install “Cheese”, the camera viewer.

pi@raspberrypi:~ $ sudo apt-get install cheese -y

Reading package lists... Done

Building dependency tree

Reading state information... Done

....

cheese is the newest version (3.22.1-1).

Launch Cheese app by Raspberry Pi button(Top Left Corner) -> Sound & Video -> Cheese You should see a live video feed displayed like the picture above.

SunFounder PiCar-V Software Configuration (Deviations from the manual)

Before assembling PiCar, we need to install PiCar’s python API. SunFounder release a server version and client version of its Python API. The Client API code, which is intended to remote control your PiCar, runs on your PC, and it uses Python version 3. The Server API code runs on PiCar, unfortunately, it uses Python version 2, which is an outdated version. Since the self-driving programs that we write will exclusively run on PiCar, the PiCar Server API must run in Python 3 also. Fortunately, all of SunFounder’s API code are open source on Github, I made a fork and updated the entire repo (both server and client) to Python 3. (I will submit my changes to SunFounder soon, so it can be merged back to the main repo, once approved by SunFounder.)

For the time being, run the following commands (in bold) instead of the software commands in the SunFounder manual. You shouldn’t have to run commands on Pages 20–26 of the manual.

# route all calls to python (version 2) to python3,

# pip (version 2) to pip3, even in sudo mode

# note: `sudo abcd` runs `abcd` command in administrator mode

alias python=python3

alias pip=pip3

alias sudo='sudo ' # Download patched PiCar-V driver API, and run its set up

pi@raspberrypi:~ $ cd git clone https://github.com/dctian/SunFounder_PiCar.git

Cloning into 'SunFounder_PiCar'...

remote: Enumerating objects: 9, done.

remote: Counting objects: 100% (9/9), done.

remote: Compressing objects: 100% (9/9), done.

remote: Total 276 (delta 0), reused 2 (delta 0), pack-reused 267

Receiving objects: 100% (276/276), 53.33 KiB | 0 bytes/s, done.

Resolving deltas: 100% (171/171), done.

pi@raspberrypi:~ $ cd ~/SunFounder_PiCar/picar/



pi@raspberrypi:~/SunFounder_PiCar/picar $ git clone https://github.com/dctian/SunFounder_PCA9685.git

Cloning into 'SunFounder_PCA9685'...

remote: Enumerating objects: 7, done.

remote: Counting objects: 100% (7/7), done.

remote: Compressing objects: 100% (5/5), done.

remote: Total 87 (delta 2), reused 6 (delta 2), pack-reused 80

Unpacking objects: 100% (87/87), done. pi@raspberrypi:~ $Cloning into 'SunFounder_PiCar'...remote: Enumerating objects: 9, done.remote: Counting objects: 100% (9/9), done.remote: Compressing objects: 100% (9/9), done.remote: Total 276 (delta 0), reused 2 (delta 0), pack-reused 267Receiving objects: 100% (276/276), 53.33 KiB | 0 bytes/s, done.Resolving deltas: 100% (171/171), done.pi@raspberrypi:~ $pi@raspberrypi:~/SunFounder_PiCar/picar $Cloning into 'SunFounder_PCA9685'...remote: Enumerating objects: 7, done.remote: Counting objects: 100% (7/7), done.remote: Compressing objects: 100% (5/5), done.remote: Total 87 (delta 2), reused 6 (delta 2), pack-reused 80Unpacking objects: 100% (87/87), done. pi@raspberrypi:~/SunFounder_PiCar/picar $ cd ~/SunFounder_PiCar/

pi@raspberrypi:~/SunFounder_PiCar $ sudo python setup.py install

Adding SunFounder-PiCar 1.0.1 to easy-install.pth file

Installing picar script to /usr/local/bin

[omitted....]

# and install depedent software

pi@raspberrypi:~/SunFounder_PiCar/picar $ cd

pi@raspberrypi:~ $ git clone https://github.com/dctian/SunFounder_PiCar-V.git

Cloning into 'SunFounder_PiCar-V'...

remote: Enumerating objects: 969, done.

remote: Total 969 (delta 0), reused 0 (delta 0), pack-reused 969

Receiving objects: 100% (969/969), 9.46 MiB | 849.00 KiB/s, done.

Resolving deltas: 100% (432/432), done. # Download patched PiCar-V applications# and install depedent softwarepi@raspberrypi:~/SunFounder_PiCar/picar $pi@raspberrypi:~ $Cloning into 'SunFounder_PiCar-V'...remote: Enumerating objects: 969, done.remote: Total 969 (delta 0), reused 0 (delta 0), pack-reused 969Receiving objects: 100% (969/969), 9.46 MiB | 849.00 KiB/s, done.Resolving deltas: 100% (432/432), done. pi@raspberrypi:~ $ cd SunFounder_PiCar-V pi@raspberrypi:~/SunFounder_PiCar-V $ sudo ./install_dependencies

Adding SunFounder-PiCar 1.0.1 to easy-install.pth file

Installing picar script to /usr/local/bin Installed /usr/local/lib/python2.7/dist-packages/SunFounder_PiCar-1.0.1-py2.7.egg

Processing dependencies for SunFounder-PiCar==1.0.1

Finished processing dependencies for SunFounder-PiCar==1.0.1

complete

Copy MJPG-Streamer to an Alternate Location. complete

Enalbe I2C. complete Installation result:

django Success

python-smbus Success

python-opencv Success

libjpeg8-dev Success

The stuff you have change may need reboot to take effect.

Do you want to reboot immediately? (yes/no)yes

Answer Yes, when prompted to reboot. After reboot, all required hardware drivers should be installed. We will test them after the car assembly.

PiCar Assembly

The assembly process closely reassembles building a complex Lego set, and the whole process takes about 2 hours, a lot of hand-eye coordination and is loads of fun. (You may even involve your younger ones during the construction phase.) PiCar Kit comes with a printed step-by-step instructional manual. But I recommend these two additional resources.

PDF version of the instructional manual. The print manual is small, and diagrams may not be printed very clearly, whereas the PDF version is crystal clear, can be searched and zoomed in for more details. I found it very helpful with the PDF on my laptop during the assembly phase.

YouTube 4-part instructional videos published by SunFounder. Unfortunately, these videos are for an older version of PiCar, so some parts (like the servo motor assembly) are different. But most parts and assembling techniques are the same. So if you are scratching your head at a particular diagram in the assembly manual, you may want to take a look at the relevant parts of the videos. I wish SunFounder would publish a new set of videos for the new PiCar-V kit.

Assembly Videos (4 Parts) for an Older Version of PiCar, a Useful reference

When Rubber Meets the Road!

Now that all the basic hardware and software for the PiCar is in place, let’s try to run it!

Connect to PiCar via VNC from PC

Make sure fresh batteries are in, toggle the switch to ON position and unplug the micro USB charging cable. Note that your VNC remote session should still be alive.

In a Pi Terminal, run the following commands (in bold). You should:

see the car going faster, and then slow down when you issue picar.back_wheel.test() see the front wheels steer left, center and right when you issue picar.front_wheel.test() . To stop these tests, press Ctrl-C. To exit the python program, press Ctrl-D.

pi@raspberrypi:~/SunFounder_PiCar/picar $ python3

Python 3.5.3 (default, Sep 27 2018, 17:25:39)

[GCC 6.3.0 20170516] on linux

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information. >>> import picar

>>> picar.setup() >>> picar.front_wheels.test()

DEBUG ""front_wheels.py"": Set debug off

DEBUG ""front_wheels.py"": Set wheel debug off

DEBUG ""Servo.py"": Set debug off

turn_left

turn_straight

turn_right >>> picar.back_wheels.test()

DEBUG ""back_wheels.py"": Set debug off

DEBUG ""TB6612.py"": Set debug off

DEBUG ""TB6612.py"": Set debug off

DEBUG ""PCA9685.py"": Set debug off

Forward, speed = 0

Forward, speed = 1

Forward, speed = 2

Forward, speed = 3

Forward, speed = 4

Forward, speed = 5

Forward, speed = 6

Forward, speed = 7

Forward, speed = 8

Forward, speed = 9

Forward, speed = 10

Forward, speed = 11

If you run into errors or don’t see the wheels moving, then either something is wrong with your hardware connection or software set up. For the former, please double check your wires connections, make sure the batteries are fully charged. For the latter, please post a message in the comment section with detailed steps you followed and the error messages, and I will try to help.

What’s Next

Congratulations, you should now have a PiCar that can see (via Cheese), and run (via python 3 code)! It is not quite a Deep Learning car yet, but we are well on our way to that. Whenever you are ready, head on over to Part 3, where we will give PiCar the superpower of computer vision and deep learning.

Here are the links to the whole guide:

Part 1: Overview

Part 2: Raspberry Pi Setup and PiCar Assembly (This article)

Part 3: Make PiCar See and Think

Part 4: Autonomous Lane Navigation via OpenCV

Part 5: Autonomous Lane Navigation via Deep Learning

Part 6: Traffic Sign and Pedestrian Detection and Handling"
Survival Modeling — Accelerated Failure Time — XGBoost,"Introduction

Survival analysis is a “censored regression” where the goal is to learn time-to-event function. This is similar to the common regression analysis where data-points are uncensored. Survival modeling is not as equally famous as regression and classification. Therefore, I would explain it more in detail with example. This is a modeling task that has censored data. Censored data are the data where the event of interest doesn’t happen during the time of study or we are not able to observe the event of interest due to some reasons.

Censoring Types and Examples

Left Censoring occurs when we start the study/campaign and the event has already happened leading to higher Example — We want to study about users buying new iPhone given offers, but some users might have already bought it is not useful to throw this data out.

Interval Censoring is used when we don’t know the exact time of the event and it could happen between time interval. Example — Users buying Insurance where users not sure about the exact date of purchase.

Right Censoring is used when an event has not happened during the time of the study. Example — Treatment received to the cancer patients.

Censored Data-Types

Time-to-event modeling is critical for understanding users/companies' behaviors not limited to credit, cancer, and attrition risks.

Cox-Proportional Hazard model is a semi-parametric model where we model hazard ratio using predictors while in accelerated Failure time log of survival time is modeled using predictors. It is parametric survival modeling as we are assuming the distribution of response data.

Below is the Flow-Chart of Survival Modeling-

Flow Chart — Survival Machine Learning

State of Art

Tree-based models have improved the supervised learning to lead to many prizes on Kaggle and performed better in the industry as well. We have gradient boosting models implemented in R and python both for Cox-Proportional Hazard Function and Accelerated Failure Time. It is natural to develop more tree-based models for survival modeling as well. For Example — GBM, mboost, Scikit-survival and etc. Currently, XGBoost supports the Cox-Ph model without baseline prediction. Therefore, we have included Accelerated Failure Time in Xgboost to improve survival modeling support.

Data Type Support

Most of the survival models support only the right censoring data types. We have generally 4 types of data — Right, Left, Interval, and Uncensored. Right censored is the most used censored data type. It is also important to understand other censoring types.

Distribution Type Support

Accelerated Failure Time Modeling is parametric survival modeling where survival time follows certain distribution, unlike Cox-Proportional Hazard Model. We support 3 types of distribution — Normal, Logistic, and Extreme. The normal distribution is the most common distribution type but it has a thin tail which might not be the right choice in case of the fat tail events or of extreme events.

It's better to visualize it."
Tackling Climate Change with Machine Learning,"If the title of this blog post seems somewhat familiar, it’s likely because you’ve heard of or read this thoroughly-sourced paper released back in June of this year. The paper lays out an overview of the myriad areas machine learning can provide impactful solutions to mitigate the effects of climate change. While the entire paper is worth summarizing (and reading!), for this blog post I will focus on two specific areas that I found interesting: carbon emission capture/reduction and climate prediction.

Carbon Emission Capture/Reduction

In 2018, the Intergovernmental Panel on Climate Change (IPCC) estimated that, within 30 years, the world will be facing catastrophic consequences if we do not limit and severely reduce global greenhouse gas emissions. Despite international accords, global protests, and the overwhelming scientific consensus that we need to reduce our emissions if we are to avoid catastrophe, our global emissions continue to increase. If governments are unwilling to act as quickly as necessary to decrease emissions, then investment in carbon-capturing technologies is a necessity. While the technology itself exists, it is in its infancy. But machine learning can aid this new technology in a variety of ways.

The paper outlines three options for reducing carbon emissions. It also admits that, while these technologies do currently exist, the applications they outline, specifically as it pertains to machine learning, are speculative.

1) Natural or Semi-Natural Methods

While global emissions have been increasing year over year, deforestation has also added fuel to the fire. About half of the world’s tropical forests have already been cleared. Even worse, estimates put total deforestation at 18.7 million acres of forest per year, the equivalent of 27 soccer fields worth of forest being cleared every minute. In total, deforestation accounts for roughly 15% of all greenhouse gas emissions.

Given all the deforestation occurring, providing tools to help track deforestation can provide valuable data for policy-makers and law enforcement. According to the paper, machine learning can help “differentiate selective cutting from clearcutting using remote sensing imagery.” It can…"
How to Teach Code,"Member-only story How to Teach Code

Learning to code is really hard and teaching code is even harder. There is no bootcamp in ‘How to Teach Code’. Most teachers do not have any formal training in education and come from a software background.

I’ve had the pleasure of working with several bootcamps in different ways (as a student, teacher or experience partner). I’ve also:

Ran a code Meetup club teaching code

Delivered a Hackathon series

Helped out with various learning code events.

The Problems

It’s amazing how little regulation there is in teaching code. The result is students can receive wildly different experiences. I’ve seen some of the best education on the planet being given away for free. Yet I’ve also seen other bootcamps delivering their students 3 months of confusion and charged £9,000 for the pleasure.

I’ve seen bootcamps struggle along until they inevitably fail whilst others just continue to grow and grow. Taking students money to deliver them a poor education is wrong but I also don’t think it was anyone’s intention to just steal from people.

With potential huge swathes of current jobs being taken over by software, there are more students than ever looking to transfer into a technical role.

Stories like the Mined Minds fiasco are just the surface of a very big problem. Rather than just talking about how big the problem is it makes sense to do something to fix it.

Solutions

Below is what I’ve learnt myself about the difficulties in teaching code. How to teach code it in a way that gives the best experience for students. And how to grow a sustainable business by delivering a great experience.

(Note — as a student on a bootcamp it is worth reading this. If your course if making some of these mistakes you might be able to help save your education by encouraging some better practices.)

Part 1 — Why Learning…"
Web Scrape Twitter by Python Selenium (Part 1),"Web Scrape Twitter by Python Selenium (Part 1) WY Fok · Follow 7 min read · Nov 9, 2019 -- Share

(PS: My Twitter https://twitter.com/mydatasciencej1)

Introduction

One popular application of Python is web scraping. Web scraping means extracting data from a website by software or programming. There are tons of data on the Internet that can be used for data analytics. However, it is impractical, if not impossible, to manually copy and store data. At this moment, web scraping is what we need.

There are some famous Python packages that perform web scraping like requests (an inbuilt package), BeautifulSoup, Scrapy, and Selenium. Today I will demonstrate how to use Selenium to web scrape. And the objective is to web scrape all tweet traffics of my data science twitter account link.

(PS: There is a more direct way connecting Twitter by using Tweepy package. I use Twitter as an example only in this article. There are many different applications by using web scraping. Moreover, as far as I know, there is no function of checking the tweet analytics.)

More information about Selenium can be found via:

Content

Prerequisites

Begin of tutorial

Ending

Prerequisites

Install Selenium

pip install Selenium

2. Download a Driver and include the path in PATH

https://selenium-python.readthedocs.io/installation.html#drivers

A Driver is used to control your browser by using the Python program. Depending on your browser, select a suitable driver. Then you either place the path of this driver in PATH environment or you declare the path of the driver every time in your Python script. I would recommend to include in your PATH environment directly so you don’t need to set the path every time.

3. Basic HTML language"
Neural Style Transfer and Visualization of Convolutional Networks,"Neural Style Transfer

NST was first published in the paper “A Neural Algorithm of Artistic Style” by Gatys et al, originally released to ArXiv 2015 [7].

Several mobile apps use NST techniques, including DeepArt and Prisma.

Here are some more examples of stylizations being used to transform the same image of the riverbank town that we used earlier.

Neural style transfer combines content and style reconstruction. We need to do several things to get NST to work:

choose a layer (or set of layers) to represent content — the middle layers are recommended (not too shall, not too deep) for best results.

Minimize the total cost by using backpropagation.

Initialize the input with random noise (necessary for generating gradients).

Replacing max-pooling layers with average pooling to improve the gradient flow and to produce more appealing pictures.

Code Implementation

Now for the moment you’ve all been waiting for, the code to be able to make these images yourself. For clearer relationship between the code and the mathematical notation, please see the Jupyter notebook located in the GitHub repository.

Part 1: Import Necessary Functions

Part 2: Content Loss

We can generate an image that combines the content and style of a pair with a loss function that incorporates this information. This is achieved with two terms, one that mimics the specific activations of a certain layer for the content image, and a second term that mimics the style. The variable to optimize in the loss function will be a generated image that aims to minimize the proposed cost. Note that to optimize this function, we will perform gradient descent on the pixel values, rather than on the neural network weights.

We will load a trained neural network called VGG-16 proposed in 1, who secured the first and second place in the localization and classification tracks of ImageNet Challenge in 2014, respectively. This network has been trained to discriminate over 1000 classes over more than a million images. We will use the activation values obtained for an image of interest to represent the content and styles. In order to do so, we will feed-forward the image of interest and observe it’s activation values at the indicated layer.

The content loss function measures how much the feature map of the generated image differs from the feature map of the source image. We will only consider a single layer to represent the contents of an image.

Part 3: Style Loss

The style measures the similarity among filters in a set of layers. In order to compute that similarity, we will compute the Gram matrix of the activation values for the style layers. The Gram matrix is related to the empirical covariance matrix, and therefore, reflects the statistics of the activation values.

The output is a 2-D matrix which approximately measures the cross-correlation among different filters for a given layer. This, in essence, constitutes the style of a layer.

Part 4: Style Loss — Layer’s Loss

In practice we compute the style loss at a set of layers rather than just a single layer; then the total style loss is the sum of style losses at each layer:

Part 5: Total-Variation Regularizer

We will also encourage smoothness in the image using a total-variation regularizer. This penalty term will reduce variation among the neighboring pixel values.

Part 6: Style Transfer

We now put it all together and generate some images! The style_transfer function below combines all the losses you coded up above and optimizes for an image that minimizes the total loss. Read the code and comments to understand the procedure.

Part 6: Generate Pictures

Now we are ready to make some images, run your own compositions and test out variations of hyperparameters and see what you can come up with, I will give you an example below. The list of hyperparameters to vary is as follows:

The base_img_path is the filename of content image.

is the filename of content image. The style_img_path is the filename of style image.

is the filename of style image. The output_img_path is the filename of the generated image.

is the filename of the generated image. The convnet is for the neural network weights, VGG-16 or VGG-19.

is for the neural network weights, VGG-16 or VGG-19. The content_layer specifies which layer to use for content loss.

specifies which layer to use for content loss. The content_weight weights the content loss in the overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content).

weights the content loss in the overall composite loss function. Increasing the value of this parameter will make the final image look more realistic (closer to the original content). style_layers specifies a list of which layers to use for the style loss.

specifies a list of which layers to use for the style loss. style_weights specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller-scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image.

specifies a list of weights to use for each layer in style_layers (each of which will contribute a term to the overall style loss). We generally use higher weights for the earlier style layers because they describe more local/smaller-scale features, which are more important to texture than features over larger receptive fields. In general, increasing these weights will make the resulting image look less like the original content and more distorted towards the appearance of the style image. tv_weight specifies the weighting of total variation regularization in the overall loss function. Increasing this value makes the resulting image look smoother and less jagged, at the cost of lower fidelity to style and content.

The following code will generate the front image of this article if run for 50 iterations.

Here are a couple of rough examples from my own implementation after 50 iterations:

Style of ‘Escher Sphere’ used to transform an image of the Goldengate Bridge.

Style of ‘Seated Nude’ used to transform an image of the riverbank town image.

I recommend taking some of the images in the GitHub repository (or your own) and playing around with the hyperparameters and seeing what images you can make. However, to warn you, the training times are quite high unless you have access to a GPU, possibly taking several hours for one image."
So what is Machine Learning?,"Photo by fabio on Unsplash

I am sure by now you must have heard about this term, and if surprisingly not, just have a look at the presentations by Apple or Google; even McDonald’s is doing something. In fact not only them, if you were to look at the Google Trends page for Machine Learning, you’ll notice the upward trend in popularity and interest over time. So everyone’s talking about it…

You can easily find many popular use-cases of Machine Learning. I am sure you check Amazon for when you need to buy new clothes or shoes. And then you see a list of recommended items for you. This is, in fact, machine learning at play. You might have also heard about autonomous vehicles which are being tested out, or maybe you have interacted with a chatbot. Almost every other brand is rethinking their strategy to involve machine learning and leverage its immense power in some way or the other.

But what is Machine Learning?

I will start with a popular definition given by Tom Mitchell:

A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.

wait, what? | Photo by Emily Morter on Unsplash

Don’t worry if you didn’t get that, even I didn’t completely at first. What it essentially says is that suppose you have lots and lots of data, with machine learning you can find patterns in that data hidden in plain sight which a computer program can learn. This computer program, which is transformed into a machine learning model, when faced with new data can now identify with a certain confidence whether that pattern can be found here or not.

Basically what happens is that when you build a machine learning model, you train the model with huge amounts of historical data which has the same context as the problem you are trying to solve. Certain patterns are learnt by the model which helps in identification, recognition or prediction. For instance, you can build a machine learning model to identify entities in an image…"
Beautiful visual model interpretation of classification strategies— Kannada MNIST Digits Recognition,"Beautiful visual model interpretation of classification strategies— Kannada MNIST Digits Recognition

Kannada handwritten digits

The Kannada MNIST dataset is a great recent work (details here), and I’m delighted that it is available to the public as well. I’m sure pretty soon the community here would be posting state of the art accuracy numbers on this dataset. Which is why, I’m doing something different.

Instead, we will try to visualize, try to see what the model sees, assess things pixel by pixel. Our goal would be interpretability. I’ll start with the ‘simplest’, easiest to interpret algorithm in this article. Hopefully, I’ll post results with other modeling techniques in later article.

To reiterate and clarify: I will not be focusing on getting best possible performance. Rather, I’ll focus on visualizing the output, making sense of the model, and understanding where it failed and why. Which is more interesting to assess when the model isn’t working extremely well. :)

Visualizing the digits data

Function to plot one random digit along with its label

def plot_random_digit():

random_index = np.random.randint(0,X_train.shape[0])

plt.imshow(X_train[random_index], cmap='BuPu_r')

plt.title(y_train[random_index])

plt.axis(""Off"") plt.figure(figsize=[2,2])

plot_random_digit()

A random Kannada digit plotted as image

Looking at 50 samples at one go

plt.figure(figsize=[10,6])

for i in range(50):

plt.subplot(5, 10, i+1)

plt.axis('Off')

if i < 10:

plt.title(y_train[i])

plt.imshow(X_train[i], cmap='BuPu_r')

As someone who is not good at reading Kannada script, to me the symbols seem somewhat similar for -

3 and 7

6 and 9

At the onset, I would expect that the predictors could be somewhat confused between these pairs. Although this isn’t necessarily true — maybe our model can identify the digits better than I can.

Reshaping the datasets for predictive model building

The individual examples are 28 X 28. For most predictive modeling methods in scikit learn, we need to get flatten the examples to a 1D array.

We’ll use the reshape method of numpy arrays.

X_train_reshape = X_train.reshape(X_train.shape[0], 784)

X_test_reshape = X_test.reshape(X_test.shape[0], 784)

Building and understanding the Logistic regression model

Let’s build a Logistic regression model for our multiclass classification problem.

Note again that we’ll not be focusing on getting the best possible performance, but on how to understand what the model has learnt.

A logistic regression model will be easy and interesting to analyse the coefficients to understand what the model has learnt.

The formulation of a multi-class classification can be done in a couple of ways in SciKit-learn. They are -

One vs Rest

Multinomial

1. One vs Rest:

Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. One advantage of this approach is its interpretability.

Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multi-class classification and is a fair default choice.

For our case, it would mean building 10 different classifiers.

Read more about it here:

https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html

2. Multinomial:

In this strategy, we model the logarithm of the probability of seeing a given output using the linear predictor.

For multinomial the loss minimised is the multinomial loss fit across the entire probability distribution. The softmax function is used to find the predicted probability of each class.

Read more about this here:

https://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_log-linear_model

Note: This distinction is important, and needs you to interpret the coefficients differently for the models.

First, let’s built our model using the One vs. Rest scheme

from sklearn.linear_model import LogisticRegression

lr1 = LogisticRegression(solver=""liblinear"", multi_class=""ovr"")



# Fitting on first 10000 records for faster training

lr1.fit(X_train_reshape[:10000], y_train[:10000])

Assessing performance on the train set

The predictions of the model for the training data

from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

y_train_pred = lr1.predict(X_train_reshape[:10000]) cm = confusion_matrix(y_train[:10000], y_train_pred[:10000])



plt.figure(figsize=[7,6])

sns.heatmap(cm, cmap=""Reds"", annot=True, fmt='.0f')

plt.show()

That’s VERY high training accuracy! Overfitting?

Also, looks like the model is NOT very confused between 3 and 7, 6 and 9, at least not on the train set.

Error Analysis: Checking out the mis-classified cases

We’ll covert to a Pandas series for ease of indexing, isolate the mis-classification cases, plot some examples.

11 cases were mis-classified

Studying some cases

Picking 9 random cases — we’ll plot the digits, along with the true and predicted labels

The mis-classified cases

Can you see why the model was confused?

Let’s see how the model fares on the test set.

Confusion matrix on the test set

Making predictions on the test data, and plotting the confusion matrix.

Confusion Matrix on test set — smells like over-fitting

Looking at the confusion matrix and the classification report -

Recall is least for 3, 7 — model is confused between them significantly. Similarly, there is confusion between 4 and 5. Also, many 0s are mistaken for 1 and 3.

Okay! So it looks like the performance has fallen sharply on the test set. There’s a very good chance we’re over-fitting on the train set.

We acknowledge that the model could be improved.

But, let’s not worry about that for now. Let’s focus on the way to understand what the model learnt."
Creating Graphs in Python using Networkx,"Creating Graphs in Python using Networkx

If you’re interested in doing Graph Theory analysis in Python and wondering where to get started then this is the blog for you. We’ll start by presenting a few key concepts and then implementing them in Python using the handy Networkx Package.

Some Graph Theory Terminology

A Graph G(V, E) is a data structure that is defined by a set of Vertices (V) and and a set of Edges (E).

G(V, E) is a data structure that is defined by a set of Vertices (V) and and a set of Edges (E). Vertex (v) or node is an indivisible point, represented by the lettered components on the example graph below

(v) or node is an indivisible point, represented by the lettered components on the example graph below An Edge (vu) connects vertex v and vertex u together.

(vu) connects vertex v and vertex u together. A Complete Graph of n vertices is a graph in which every vertex shares an edge with every other vertex and therefore contains the maximum number of edges.

of n vertices is a graph in which every vertex shares an edge with every other vertex and therefore contains the maximum number of edges. The size of the Maximum Possible Edge Set of any simple graph of n vertices is equal to (n*(n-1))/2. This is because it is equivalent to the number of vertex pair combinations ie. (n choose 2) = n! /( (n-2)!(2!)).

of any simple graph of n vertices is equal to (n*(n-1))/2. This is because it is equivalent to the number of vertex pair combinations ie. (n choose 2) = n! /( (n-2)!(2!)). An Induced Subgraph G[S] on vertices S of graph G(V, E) is a graph such that S ⊂ V and the edge set of G[S] consists of all of the edges in E that have both endpoints in S.

G[S] on vertices S of graph G(V, E) is a graph such that S ⊂ V and the edge set of G[S] consists of all of the edges in E that have both endpoints in S. A Clique C of graph G is any Induced Subgraph of G that is also a Complete Graph

Installing the package and creating your first graph

The first thing you’ll need to do is install the Networkx package on your machine. Using Pip its as easy as:

pip install networkx

Once installed import the package and Initialize a graph object

import networkx as nx G = nx.Graph()

Add the first two nodes and an edge between them

G.add_node(1)

G.add_node(2)

G.add_edge(1, 2)

At this point our graph is just two connected nodes

Fig. 1 A two vertex Graph

Adding edges one at a time is pretty slow but luckily we can also add lists of nodes and lists of edges where each edge is represented by a node tuple.

G.add_nodes_from([2,3,4,5,6])

G.add_edges_from([(1,2),(4,5),(3,5),(2,3),(5,6)])

Our Graph should now look something like this

Fig 2. Our graph now has seven vertices

Accessing and storing information

We can see a list of nodes or edges by printing these attributes of our graph.

print(G.nodes())

>>>[0, 1, 2, 3, 4, 5, 6]

print(G.edges())

>>>[(0, 1), (1, 2), (2, 3), (3, 4), (3, 5), (5, 6)]

It is also possible to define nodes as strings.

G.add_node('ib')

G.add_edge(3,'ib')

Most importantly each node can be assigned any number of attributes which are then stored in dictionaries.

G.nodes[1]['color'] = 'red'

G.nodes[1]['count'] = 10

print(G.nodes.data())

>>>[(0, {}), (1, {'color': 'red', 'count': 10}), (2, {}), (3, {}), (4, {}), (5, {}), (6, {}), ('ib', {})]

To make this data more manageable feed the output of nodes.data() into a dict() function to get it into a lovely nested dictionary where each node is a key.

print(dict(G.nodes.data()))

{0: {}, 1: {'color': 'red', 'count': 10}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}, 'ib': {}}

Max Clique estimation

In order to find the max clique we’ll need to import the approximation package from networkx since it is not included in the default import

from networkx.algorithms import approximation as aprx

Now in order to test the max clique we’ll create a clique of size 3 in our current graph by adding edge (4,6)

G.add_edge(4,6)

Fig 3. Now with 8 vertices and a clique of size 3

So the set of vertices {4,5,6} contain our max clique of size 3 and that vertex set is exactly what the max_clique function will return.

print(aprx.max_clique(G))

>>>{4, 5, 6}

print(len(approximation.max_clique(G)))

>>>3

To see the induced subgraph of that vertex set, we need to combine the above with the subgraph method

max_clique = G.subgraph(approximation.max_clique(G))

Which will give us the following complete 3-vertex graph.

The induced subgraph of the maximum clique

Final Thoughts and Questions

There are a number of graph libraries out there for Python but I chose Networkx for its readability, ease of setup and above all for its excellent documentation. If you have any further question or wish to explore the library more please refer to the official documentation.

Sources

https://networkx.github.io/

https://en.wikipedia.org/wiki/Induced_subgraph

https://en.wikipedia.org/wiki/Clique_(graph_theory)"
Autonomous Agents And Multi-Agent Systems 101: Agents And Deception,"Autonomous Agents And Multi-Agent Systems 101: Agents And Deception

This article provides a brief introduction to the area of autonomous agents and multi-system agents. Furthermore, a perspective of deception mechanisms used by agents is presented. Rafael Belchior · Follow Published in Towards Data Science · 7 min read · Nov 9, 2019 -- Listen Share

Photo by Debby Hudson on Unsplash

Humans use deception mechanisms to gain an advantage over other humans. Some of the most typical mechanisms are (1), not sharing their beliefs (2), pretending to be able to perform certain actions or even pretending not to be able to perform a certain action. In the Autonomous Agents and Multi-Agent Systems, jargon, (1) corresponds to hidden utilities, (2) to hidden actions and (3) to decoy actions.

One could ask how can an agent use deception to maximize his own reward? Can even an agent use deception techniques? Can an agent can use deception in a cooperative setting?

To answer these questions, we first introduce the notion of an agent [P.Maes, 1993]:

A (rational) agent is a system that tries to fulfill a set of goals in a complex environment, by choosing the action with the optimal expected outcome for itself from among all possible actions (the one that has greater utility). Agents need to be autonomous and adaptive, in order to cope with the vicissitudes of environments. In a multi-agent scenario, agents can compete, being their interactions described by a game encounter. For maximising the utility, a utility function ranks different options according to their utility to an individual. Cooperating agents have the same goal and can work together to achieve a common goal.

Nice looking agent. Photo by Alex Knight on Unsplash

In fact, we behave similarly to agents: rational beings wanting to fulfill a set of goals, by choosing the actions that maximise our chances of success. At least sometimes.

To formalize the interaction between agents, which is mostly when deception takes place, let us consider the notions of game and game encounter:

A game is a competitive activity, where players compete with each other (interactions), according to defined rules (strategy) [Wooldridge M., Paterson S., 2002]. Players can only take certain actions defined by the environment. Assuming that agents simultaneously choose an action to perform, the result of their actions depends on the combination of actions. The environment is then changed, according to the cumulative set of actions performed by all agents. This fact arises one question: if all agents influence the environment, and if all agents want to maximise their utility, how should they act? The choice of the appropriate action has some relevant issues, including but not limited to the nature of the goal (what kind, static or dynamic), re-usability, the depth of understanding of actions leading to emergent behaviors and the relationship between perceptions and actions. Game theory studies interactions between rational agents who aim to maximise their utility. Agents can negotiate in order to achieve a position favorable to both of them.

Before picturing a game encounter between agents, let us dig deeper into the concept of deception:

Deception is often used by humans to raise the probability of success when negotiating. The ability of an agent to negotiate effectively in the presence of conflicting goals is related to the information that the adversary holds. Let us assume an encounter between two competing agents, with the possibility of them to use the following deception mechanisms i. hidden actions, ii. hidden utilities and iii. decoy actions. Deception techniques can occur on inter-agent negotiation with incomplete information, as negotiation typically assume trustworthy agents, which is not always the case [Zlotkin G., Josenschein J., 1991]. As agents co-exist and might interfere with the outcomes of the actions performed, there is the possibility of cooperation, to help each other and achieve both goals with a lower overall cost.

Let us assume that a game encounter takes place. The game is represented by the bi-matrix b, where agent i and agent j have different goals, gi and gj, respectively. The values of an entrance correspond to the yielded utility to agent i and agent j, respectively. Both agents want to transform the world from the initial to a state si or sj that satisfies its goal. Agent i is the row player and agent j the column player. Both players can perform the same actions, A and B. Deception techniques can be used to maximise the overall utility for one of the agents. The game is expressed on the following table:

Let us suppose that agent i knows that agent j is going to perform an action a. The result yields:

In this case, agent i should also perform action A, because the utility delivered is the greatest. If But if agent j is taking action B, we have:

Therefore, agent i would also choose action B, as one is greater than zero. The optimal strategy of agent i is determined by the choice of agent j. If Agent j informs agent i that he can only take action B (hidden actions), it leads agent i to perform action B (as one is greater than zero and therefore that action is the one delivering the most utility). Nonetheless, agent i can perform action A, yielding utility two for him and utility zero for agent i.

Hidden utility mechanisms are used by agents that do not want to share its utility regarding their actions. If an agent does not share its utility associated with each action, the other will be picking actions (at least initially) guided solely by its utility. Such decisions can lead to sub-optimal options, for instance, when agent j picks action A and agent i picks action B. Let us now suppose that agent j can only perform action A, but is using a decoy action mechanism, pretending that he can perform action B. Cooperative agents rationally always choose action A, as it yields the highest outcome possible. Nonetheless, if agent i is a competing agent that not only aims to maximise its utility but also aims to minimise its adversary utility (zero-sum game), he can rationally choose action B. Decoy actions are a way to protect against agents that are competitive and want to minimise other’s utility. It is clear that an agent can use several deception techniques to maximise its reward, in a competing scenario.

Conversely, in contexts where agents want to minimize the overall cost (maximise the overall utility), typically it does not make sense to use deception mechanisms, as they comprise extra difficulties on the task. In scenarios with no strictly dominant strategy, more rules are needed to solve the game.

The notion of Nash Equilibrium [Nicola G., 1967] and Pareto Optimality [Pareto V., 1906] are important to make conclusions. Nash equilibrium is a set of strategies for each player, such as each player does not have an incentive to unilaterally change its strategy. Nash equilibria are inherently stable and can help solve Game 1. In Game 1, there are two Nash equilibria: when both agents pick the same action. An outcome is Pareto efficient if no other outcome improves a player’s utility without making someone else worse off. The impact of deception influences agents, as they believe an outcome is in Nash equilibrium or is Pareto efficient when in reality it is not.

Nash Equilibrium. Neither of the rocks that compose the pile has the motivation to move. Photo by Debby Hudson on Unsplash

Let us assume that in Game 1, agent i can only pick action A, but tells agent j that he can pick all actions. Given the Nash equilibrium when both agents choose action B, there is room for deception. If Agent i choose action A, while agent j chooses action B, it yields two utility points for agent i and zero utility points for agent j. Conversely, agent i can tell agent j he cannot pick action B, thus suggesting agent j can always pick action A (Nash equilibrium). Agent i can pick action B, thus not earning so much utility but, at the same time, minimising the utility of his adversary. Deception can give the illusion of a Nash equilibrium. This reasoning is analogous to Pareto optimal.

PRINCIPLES TO DESIGN RATIONAL AGENTS From the analysis above, we can extract some principles to design rational agents under self-interest and cooperative contexts.

In self-interest scenarios:

💢 One should hide their utility, to obtain an initial advantage.

💢 One can use decoy actions to protect against the other agent (forcing situations such as Nash Equilibrium).

💢 One can hide their actions if the goal is to minimise the opponent’s utility

Even though these principles are rational, in theory, in practice competition instead of cooperation yields to worse results.

✔️ In cooperative scenarios, in most cases, deception mechanisms do not make sense, as they difficult communication and thus complicate achieving the common goal.

CONCLUSIONS Deception mechanisms can be used by competing agents to maximise their utility, yielding better results in competitive, zero-sum scenarios. Often, a good deal can be obtained through cooperation or strategies such as the Nash Equilibrium.

ACKNOWLEDGEMENTS Thanks to Professor Rui Henriques for the available course materials, which were the basis of this essay, and also for the guidance and suggestions."
Logistic Regression in Machine Learning using Python,"Logistic Regression in Machine Learning using Python

Learn how logistic regression works and how you can easily implement it from scratch using python as well as using sklearn. Adarsh Menon · Follow Published in Towards Data Science · 7 min read · Dec 27, 2019 -- 2 Listen Share

In statistics logistic regression is used to model the probability of a certain class or event. I will be focusing more on the basics and implementation of the model, and not go too deep into the math part in this post.

The value of weights b0 and b1 are updated at each iteration

This is a written version of this video. Watch the video if you prefer that.

Logistic regression is similar to linear regression because both of these involve estimating the values of parameters used in the prediction equation based on the given training data. Linear regression predicts the value of some continuous, dependent variable. Whereas logistic regression predicts the probability of an event or class that is dependent on other factors. Thus the output of logistic regression always lies between 0 and 1. Because of this property it is commonly used for classification purpose.

Logistic Model

Consider a model with features x1, x2, x3 … xn. Let the binary output be denoted by Y, that can take the values 0 or 1.

Let p be the probability of Y = 1, we can denote it as p = P(Y=1).

The mathematical relationship between these variables can be denoted as:

Here the term p/(1−p) is known as the odds and denotes the likelihood of the event taking place. Thus ln(p/(1−p)) is known as the log odds and is simply used to map the probability that lies between 0 and 1 to a range between (−∞,+∞). The terms b0, b1, b2… are parameters (or weights) that we will estimate during training.

So this is just the basic math behind what we are going to do. We are interested in the probability p in this equation. So we simplify the equation to obtain the value of p:

The log term ln on the LHS can be removed by raising the RHS as a power of e:

2. Now we can easily simplify to obtain the value of p :

This actually turns out to be the equation of the Sigmoid Function which is widely used in other machine learning applications. The Sigmoid Function is given by:

The sigmoid curve (Wikipedia)

Now we will be using the above derived equation to make our predictions. Before that we will train our model to obtain the values of our parameters b0, b1, b2… that result in least error. This is where the error or loss function comes in.

Loss Function

The loss is basically the error in our predicted value. In other words it is a difference between our predicted value and the actual value. We will be using the L2 Loss Function to calculate the error. Theoretically you can use any function to calculate the error. This function can be broken down as:

Let the actual value be yᵢ. Let the value predicted using our model be denoted as ȳᵢ. Find the difference between the actual and predicted value. Square this difference. Find the sum across all the values in training data.

Now that we have the error, we need to update the values of our parameters to minimize this error. This is where the “learning” actually happens, since our model is updating itself based on it’s previous output to obtain a more accurate output in the next step. Hence with each iteration our model becomes more and more accurate. We will be using the Gradient Descent Algorithm to estimate our parameters. Another commonly used algorithm is the Maximum Likelihood Estimation.

The loss or error on the y axis and number of iterations on the x axis.

The Gradient Descent Algorithm

You might know that the partial derivative of a function at it’s minimum value is equal to 0. So gradient descent basically uses this concept to estimate the parameters or weights of our model by minimizing the loss function. Click here for a more detailed explanation on how gradient descent works.

For simplicity, for the rest of this tutorial let us assume that our output depends only on a single feature x. So we can rewrite our equation as:

Thus we need to estimate the values of weights b0 and b1 using our given training data.

Initially let b0=0 and b1=0. Let L be the learning rate. The learning rate controls by how much the values of b0 and b1 are updated at each step in the learning process. Here let L=0.001. Calculate the partial derivative with respect to b0 and b1. The value of the partial derivative will tell us how far the loss function is from it’s minimum value. It is a measure of how much our weights need to be updated to attain minimum or ideally 0 error. In case you have more than one feature, you need to calculate the partial derivative for each weight b0, b1 … bn where n is the number of features. For a detailed explanation on the math behind calculating the partial derivatives, check out my video.

3. Next we update the values of b0 and b1:

4. We repeat this process until our loss function is a very small value or ideally reaches 0 (meaning no errors and 100% accuracy). The number of times we repeat this learning process is known as iterations or epochs.

Implementing the Model

Import the necessary libraries and download the data set here. The data was taken from kaggle and describes information about a product being purchased through an advertisement on social media. We will be predicting the value of Purchased and consider a single feature, Age to predict the values of Purchased. You can have multiple features as well.

We need to normalize our training data, and shift the mean to the origin. This is important to get accurate results because of the nature of the logistic equation. This is done by the normalize method. The predict method simply plugs in the value of the weights into the logistic model equation and returns the result. This returned value is the required probability.

The model is trained for 300 epochs or iterations. The partial derivatives are calculated at each iterations and the weights are updated. You can even calculate the loss at each step and see how it approaches zero with each step.

Since the prediction equation return a probability, we need to convert it into a binary value to be able to make classifications. To do this, we select a threshold, say 0.5 and all predicted values above 0.5 will be treated as 1 and everything else will be 0. You can choose a suitable threshold depending on the problem you are solving.

Here for each value of Age in the testing data, we predict if the product was purchased or not and plot the graph. The accuracy can be calculated by checking how many correct predictions we made and dividing it by the total number of test cases. Our accuracy seems to be 85%.

Accuracy = 0.85

Implementing using Sklearn

The library sklearn can be used to perform logistic regression in a few lines as shown using the LogisticRegression class. It also supports multiple features. It requires the input values to be in a specific format hence they have been reshaped before training using the fit method.

The accuracy using this is 86.25%, which is very close to the accuracy of our model that we implemented from scratch !"
Correlation between score and comments on the front page of Reddit,"I’m not a huge Reddit user but when I’m there I usually hangout in just a few subreddits (r/dataisbeautiful, r/geography, r/philadelphia, r/emo and a couple others). Other than the popular data viz subreddit, none of these generate the scores you see on the front page of Reddit.

I only say all that to point out that I am not tapped into all the nuances of Reddit culture. I did however have a question after perusing the front page of the internet one day — is there a correlation between a popular post’s score and the amount of conversation that garners around it?

It looks like r/worldnews really struck up some conversations!

First things first, getting the data. I took a look at the Reddit homepage api but didn’t see a way to pull the top posts for a given date and time. So I turned to the Wayback Machine’s API which can use a specific date and time as an endpoint and will return the url of the closest web capture.

The front page is pretty well archived it appears.

Feeling pretty confident that I could scrape a good amount of data for 2018, I jumped over to R and generated a list of complete urls to call the API.



dateRange <- gsub(“-”,””,seq(ymd(‘20180101’),ymd(‘20181231’), by = ‘1 day’, truncated=2))

base_url <- "" library(lubridate)dateRange <- gsub(“-”,””,seq(ymd(‘20180101’),ymd(‘20181231’), by = ‘1 day’, truncated=2))base_url <- "" https://archive.org/wayback/available?url=reddit.com #create list of api urls

url_list <- c()

for (date in dateRange) {

full_url <- paste(base_url, ""×tamp="",date, ""120000"", sep="""")

url_list <- c(url_list, full_url)

}

Now we can call the Wayback Machine in order to get a list of web captures.

#create list of archive links

archive_list <- c()

archive_times <- c() for (url in url_list) {

#get raw result from api call

raw.result <- GET(url = url)

raw.result$status_code

#get raw content from results

this.raw.content <- rawToChar(raw.result$content)

#put content into list

this.content <- fromJSON(this.raw.content)

#extract archive url from list and add to archive_list

archive_times <- c(archive_times, this.content$archived_snapshots$closest$timestamp)

archive_list <- c(archive_list, this.content$archived_snapshots$closest$url)

}

This gives us a list of 365 urls to captures from around noon of each day. Now onto the actual web scraping. There are probably quicker ways to do this, but I went with an old fashioned for loop and used the rvest package to scrape the score, number of comments, and r/subreddit of each of the page’s 25 posts.

I included some simple error handling by checking to make sure the length of the r/subreddit value is greater than 0 (i.e. any posts were actually pulled) before adding it to the datalist variable.

After the loop is complete, I use rbind to fill the data frame and filter out any problematic data.

#create empty list

datalist = list()

for (i in 1:length(archive_list)) {

#get all the html from the webpage

webpage <- read_html(archive_list[i])

#filter all the .things

things <- webpage %>%

html_node(""#siteTable"") %>%

html_nodes("".thing"")

#get votes

score <- things %>%

html_node("".score"") %>%

html_text()

#get number of comments

comments <- things %>%

html_node("".comments"") %>%

html_text()

#remove "" comments"" and convert to number

comments <- as.numeric(gsub("" comments"","""", comments))

# get post subreddit

subreddit <- things %>%

html_node("".subreddit"") %>%

html_text()

#get date of page

date <- gsub("" #loop through archive urlsfor (i in 1:length(archive_list)) {#get all the html from the webpagewebpage <- read_html(archive_list[i])#filter all the .thingsthings <- webpage %>%html_node(""#siteTable"") %>%html_nodes("".thing"")#get votesscore <- things %>%html_node("".score"") %>%html_text()#get number of commentscomments <- things %>%html_node("".comments"") %>%html_text()#remove "" comments"" and convert to numbercomments <- as.numeric(gsub("" comments"","""", comments))# get post subredditsubreddit <- things %>%html_node("".subreddit"") %>%html_text()#get date of pagedate <- gsub("" http://web.archive.org/web/|/https://www.reddit.com/ "", """", archive_list[i]) if (length(subreddit) > 0) {

print(paste(unique(date),length(subreddit),sep="" ""))

#create temp df

temp <- data.frame(date = date, score = score, comments = comments, subreddit = subreddit)

#add it to the list

datalist[[i]] <- temp

}

} #make a df from the datalist

main_data = do.call(rbind, datalist)

#remove incomplete posts

reddit_posts <- main_data %>%

filter(score != ""•"",

!is.na(score),

!is.na(comments)

) %>%

mutate(score = as.numeric(sub(""k"", ""e3"", score, fixed = TRUE)),

subreddit = gsub("".*r/"",""r/"",subreddit))

How did the scrape do? Not too bad. The scrape successfully pulled daily posts for 75% of the year. I didn’t investigate this too thoroughly since I had enough data to work with, but I think the Wayback Machine had some issues with the Reddit site redesign.

Now we have a freshly minted dataset, but in order to produce the visualization I want it needs some wrangling.

Identify the eight subreddits that sent the most posts to the front page Change the subreddit value of posts that came from non-top subs as “other” Reclassify the subreddit factor levels so that they are in descending order with “other” at the end.

#get top 8 subreddits

top_subs <- reddit_posts %>%

group_by(subreddit) %>%

summarise(count=n()) %>%

top_n(8, count) %>%

ungroup() #create vector of top_subs

top_subs <- as.character(top_subs$subreddit) #make notin operator

'%!in%' <- function(x,y)!('%in%'(x,y)) reddit_posts_reduc <- reddit_posts %>%

mutate(subreddit = case_when(

subreddit %!in% top_subs ~ 'other',

TRUE ~ as.character(.$subreddit)

)) #get list of factors in descending order

factor_order <- reddit_posts_reduc %>%

group_by(subreddit) %>%

summarise(count=n()) %>%

arrange(desc(count)) %>%

select(subreddit) #overwrite with list

factor_order <- as.vector(factor_order$subreddit)

#remove ""other"" from first position

factor_order <- factor_order[-1]

#create new factor level list

factor_order2 <- factor_order

#update new factor list with ordering info

for (i in 1:length(factor_order)) {

factor_order2[[i]] <- paste(""#"",i,"" "",factor_order[[i]], sep = """")

}

#append other to both factor lists

factor_order <- append(factor_order, ""other"")

factor_order2 <- append(factor_order2, ""other"") #update dataframe levels with update factor levels

reddit_posts_reduc$subreddit_f <- mapvalues(reddit_posts_reduc$subreddit, from = factor_order, to = factor_order2)

levels(reddit_posts_reduc$subreddit_f)

Now, time to plot. I plotted the number of comments on the x-axis and the score on the y-axis. I used axis limits to account for outliers, The end result was a plot of small multiples grouped by subreddit and labeled with its correlation coefficient.

#plot data

reddit_posts_reduc %>%

ggplot(aes(

x=score,

y=comments,

color=subreddit_f)

) +

geom_point(size=3, alpha=0.4) +

facet_wrap(~subreddit_f, ncol = 3) +

geom_smooth(se=F) +

theme_fivethirtyeight() +

theme(axis.title=element_text()) +

# labs(title = ""Correlation between score and comments on front page"",

# subtitle = ""Posts from the front page of Reddit in 2018 plotted to show correlation between score and the number of comments. Posts are grouped by the eight subreddits that sent the most posts to the front page with all other posts grouped in other."",

# caption = ""Data from Reddit via Archive.org

Chart by @jared_whalen""

# ) +

theme(legend.position=""none"") +

stat_cor(method = ""pearson"", label.x = 110000, label.y = 9000) +

scale_y_continuous(label=unit_format(unit = ""K"", scale = 1e-3, sep=""""),

limits=c(0,10000)) +

scale_x_continuous(label=unit_format(unit = ""K"", scale = 1e-3, sep=""""),

limits=c(0,150000)) +

xlab(""Score"") +

ylab(""Number of comments"")

Things I gained from this project

How to use the Wayback Machine’s API to scrape archived pages

A better understanding of reassigning factor levels in order to customize ordering when plotting

Here is a gist of the entire source code."
The Akaike Information Criterion,"Implementing the regression strategy using Python, pandas and statsmodels

Import all the required packages.

import pandas as pd from patsy import dmatrices from collections import OrderedDict import itertools import statsmodels.formula.api as smf import sys import matplotlib.pyplot as plt

Read the data set into a pandas data frame.

df = pd.read_csv('boston_daily_temps_1978_2019.csv', header=0, infer_datetime_format=True, parse_dates=[0], index_col=[0])

The data set contains daily average temperatures. We want monthly averages. So let’s roll up the data to a month level. This turns out to be a simple thing to do using pandas.

df_resampled = df.resample('M').mean()

We are about to add lagged variable columns into the data set. Let’s create a copy of the data set so that we don’t disturb the original data set.

df_lagged = df_resampled.copy()

Add 12 columns, each one containing a time-lagged version of TAVG.

for i in range(1, 13, 1):

df_lagged['TAVG_LAG_' + str(i)] = df_lagged['TAVG'].shift(i)

Print out the first 15 rows of the lagged variables data set.

print(df_lagged.head(15))

This prints out the following output:

The data set containing 12 lagged variables (Image by Author)

The first 12 rows contain NaNs introduced by the shift function. Let’s remove these 12 rows.

for i in range(0, 12, 1):

df_lagged = df_lagged.drop(df_lagged.index[0])

Print out the first few rows just to confirm that the NaNs have been removed.

print(df_lagged.head())

Before we do any more peeking and poking into the data, we will put aside 20% of the data set for testing the optimal model.

split_index = round(len(df_lagged)*0.8) split_date = df_lagged.index[split_index] df_train = df_lagged.loc[df_lagged.index <= split_date].copy() df_test = df_lagged.loc[df_lagged.index > split_date].copy()

Now let’s create all possible combinations of lagged values. For this, we’ll create a dictionary in which the keys contain different combinations of the lag numbers 1 through 12.

lag_combinations = OrderedDict() l = list(range(1,13,1))



for i in range(1, 13, 1):

for combination in itertools.combinations(l, i):

lag_combinations[combination] = 0.0

Next, we will iterate over all the generated combinations. For each lag combination, we’ll build the model’s expression using the patsy syntax. Next we’ll build the linear regression model for that lag combination of variables, we’ll train the model on the training data set, we’ll ask statsmodels to give us the AIC score for the model, and we’ll make a note of the AIC score and the current ‘best model’ if the current score is less than the minimum value seen so far. We’ll do all of this in the following piece of code:

#Current minimum AIC score

min_aic = sys.float_info.max #Model expression for the best model seen so far

best_expr = '' #OLSResults objects for the best model seen so far

best_olsr_model_results = None



expr_prefix = 'TAVG ~ ' #Run through each lag combination

for combination in lag_combinations:

expr = expr_prefix



i = 1 #Build the model's expression in patsy notation, for e.g. 'TAVG ~ TAVG_LAG_1 + TAVG_LAG_2' represents a model containing two lag variables and TAVG_LAG_1 and TAVG_LAG_2 plus the intercept for lag_num in combination:

if i < len(combination):

expr = expr + 'TAVG_LAG_' + str(lag_num) + ' + '

else:

expr = expr + 'TAVG_LAG_' + str(lag_num)



i += 1



print('Building OLSR model for expr: ' + expr)



#Given a model expression, patsy makes it easy to carve out the X,y matrices from the data set. We will use X_test, y_test later for testing the model. y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')



#Build and fit the OLSR model using statsmodels

olsr_results = smf.ols(expr, df_train).fit()



#Store away the model's AIC score

lag_combinations[combination] = olsr_results.aic print('AIC='+str(lag_combinations[combination])) #If the model's AIC score is less than the current minimum score, update the current minimum AIC score and the current best model

if olsr_results.aic < min_aic:

min_aic = olsr_results.aic

best_expr = expr

best_olsr_model_results = olsr_results

Finally, let’s print out the summary of the best OLSR model as per our evaluation criterion. This is the model with the lowest AIC score.

print(best_olsr_model_results.summary())

This prints out the following output. I have highlighted a few interesting areas in the output:

Model summary generated by statsmodels OLSResults.summary() (Image by Author)

Let’s inspect the highlighted sections.

Choice of model parameters

Our AIC score based model evaluation strategy has identified a model with the following parameters:

Model parameters and their regression coefficients (Image by Author)

The other lags, 3, 4, 7, 8, 9 have been determined to not be significant enough to jointly explain the variance of the dependent variable TAVG. For example, we see that TAVG_LAG_7 is not present in the optimal model even though from the scatter plots we saw earlier, there seemed to be a good amount of correlation between the response variable TAVG and TAVG_LAG_7. The reason for the omission might be that most of the information in TAVG_LAG_7 may have been captured by TAVG_LAG_6, and we can see that TAVG_LAG_6 is included in the optimal model.

Statistical significance of model parameters (the t-test)

The second thing to note is that all parameters of the optimal model, except for TAVG_LAG_10, are individually statistically significant at a 95% confidence level on the two-tailed t-test. The reported p-value for their ‘t’ score is smaller than 0.025 which is the threshold p value at a 95% confidence level on the 2-tailed test.

The t value and the p-value of the model parameters (Image by Author)

Joint significance of model parameters (the F-test)

The third thing to note is that all parameters of the model are jointly significant in explaining the variance in the response variable TAVG.

This can be seen from the F-statistic 1458. It’s p value is 1.15e-272 at a 95% confidence level. This probability value is so incredibly tiny that you don’t even need to look up the F-distribution table to verify that the F-statistic is significant. The model is definitely much better at explaining the variance in TAVG than an intercept-only model.

F-statistic and its p-value. All mdoel parameters are jointly significant (Image by Author)

To know more about how to interpret the F-statistic, please refer to my article on the F-test.

The AIC score and the Maximized Log-Likelihood of the fitted model

Finally, let’s take a look at the AIC score of 1990.0 reported by statsmodels, and the maximized log-likelihood of -986.86.

Maximized Log-likelihood and the AIC score (Image by Author)

We can see that the model contains 8 parameters (7 time-lagged variables + intercept). So as per the formula for the AIC score:

AIC score = 2*number of parameters —2* maximized log likelihood

= 2*8 + 2*986.86 = 1989.72, rounded to 1990. 0

Which is exactly the value reported by statmodels."
Automated and Manageable Pipelines: Key Components of a Data Science Factory,"Data science can be a messy endeavor, with the constant influx of raw data from countless sources pumping through ever-evolving pipelines attempting to satisfy shifting expectations. To harness all of this chaotic potential, businesses strive to create data science factories that streamline the process while reducing inefficiencies; however, data isn’t going to wait for companies to catch up. Producing a highly-functional data science factory while processing torrents of data is like trying to build an airplane while trying to fly it.

The key to building an effective data science factory is implementing intelligent automation and scoring pipelines into each step of the process to produce analytic products like APIs, scored files, and data enrichment for business partners and customers. Each component must produce sound results for the operation to be scalable and to generate reliable insights. Let’s take a look at the contributing components and how to maximize each one.

The three types of pipelines

Data pipelines: Data has a long and harrowing journey from the point of origin to its final resting place in a beautiful graphic, or eventually a data warehouse. Data pipeline software moves data from one point to another and often involves a transformation in the process. An efficient data pipeline reduces manual steps and relies on automation for each step: extraction, cleaning, transformation, combinations, validation, and loading for further analysis. Transporting data increases the risk of corruption and potential latency, and the more effort applied to mitigate risks on a small scale, the higher the quality of the output when the process expands.

Machine learning scoring pipelines: Clean, prepared data is ready to be fed into machine learning scoring algorithms, where scores are generated that inform business decisions. Effective ML scoring pipelines rely heavily on the quality of their models.

Feedback and response pipelines: The prescribed decisions produced by the ML pipelines must be logged and returned for further learning via feedback and response pipelines. This process can either take place in real-time — such as website product recommendations — or could require latent responses for products with longer acquisition life cycles such as mortgage applications or life insurance quotes.

Three speeds of data pipelines

Data pipelines can process at three unique speeds, with each option offering distinct advantages and limitations.

Batch: Batch processing is an effective way of handling high volumes of data. Transactions collected over a period of time are processed as a batch. This method is commonly used for modeling predictive analytics, as the large volume of data ensures more accurate results and stronger insights.

Real-time: Many digital operations require immediate action, so contemporary data scientists often rely on real-time data processing. This method requires constant input, processing, and output. Streaming created the phenomenon of fast data, and many businesses provide critical real-time services such as fraud detection, speech recognition, and recommendations.

Event-driven: In an effort to conserve resources and limit redundancy, some pipelines apply event-driven processing. An event could be a smart machine indicating a specific temperature, a period of time, or a point-of-sale notification related to inventory. Event-driven pipelines are optimized to produce real-time results, but only under specific, predetermined circumstances.

Critical elements of highly scalable pipelines

1. Underlying infrastructure

Infrastructure refers to the technology stack required to produce machine learning algorithms. Successful operations require air-tight solutions and solid infrastructure. Unruly pipeline-systems can lead to unrecoverable technical debt, which is consistently an issue in ML development or entangled “pipeline jungles” that make it impossible to reproduce results and workflows.

2. Automated quality control

AI is revolutionizing quality control across industries, but it is equally crucial that the technology can monitor the quality of its own output. Implementing both in-line and over-time automated quality control solutions ensures more reliable outcomes and reduces the amount of time spent manually reviewing damaged data.

3. Automate drift and anomaly detection

Concept drift is a common phenomenon in machine learning and can lead to inaccurate outcomes; however, changes in the target variable can be automatically flagged, triggering a retraining to protect the integrity of the model. Additionally, when data points fall outside of predicted patterns, automated anomaly detection can trigger appropriate action or further investigation.

4. Integrate modern data catalogs for data governance and self-documenting pipelines

Data is increasingly recognized as invaluable to companies, causing the management, storage, and governance of that data to become a top priority. Pipelines capable of self-documenting increase the functionality and value for future projects, and integrating modern data catalogs improves the relevance of any algorithm’s predictions.

5. Implement robust logging and diagnostic capabilities

As the old English proverb states: A stitch in time saves nine. Once data is in motion, it becomes challenging to debug. It is essential to build logging and diagnostic capabilities during the development and deployment stages to avoid surgical data-repair later in the process.

Establishing expectations

In 1790 Samuel Slater built America’s first factory to produce processed cotton. Freshly-picked cotton went in; processed cotton came out. Nearly 230 years later, with data being the world’s most valuable resource, the concept of “factory” has evolved. The days of a single, static input are history, and the new normal is figuring out how to transform the 2.5 quintillion bytes of data produced each day into actionable insights. Building an efficient data science factory requires a constant work-in-progress, even at the highest levels of the enterprise. While it’s impossible to communicate the countless dynamic variables involved, integrating these basic components is a step in the right direction.

For more components of a highly-functioning data science factory, read about feature stores.

This post was originally published on Quickpath’s blog."
[NLP] Basics: Measuring The Linguistic Complexity of Text,"Photo by Nong Vang on Unsplash

Determining the linguistic complexity of text is one of the first basic steps you learn in natural language processing. In this article, I take you through what linguistic complexity is and how to measure it.

The pre-processing steps

First, you need to proceed with the tokenisation of your corpora. In other words, you need to break the sentences in your corpus of text into separate words (tokens). In addition, you should also remove punctuation, symbols, numbers and transform all words to lowercase. In the line of code below, I show you how to this using the quanteda package.

# Creating a corpus

mycorpus <- corpus(data, text_field = ""text"") # Tokenisation

tok <- tokens(mycorpus, what = ""word"",

remove_punct = TRUE,

remove_symbols = TRUE,

remove_numbers = TRUE,

remove_url = TRUE,

remove_hyphens = FALSE,

verbose = TRUE,

include_docvars = TRUE)

tok <- tokens_tolower(tok)

I then add one more step to the pre-processing of my corpora: removing stopwords. This allows you to get more meaningful results when you measure the lexical complexity of text. Why? Because stopwords have little lexical richness and are only used to bind words in a sentence. As such, it is better to remove them in order not to create any noise in your analysis. However, keep in mind that doing this will significantly drop the number of words.

tok <- tokens_select(tok, stopwords(""english""), selection = ""remove"", padding = FALSE)

Note that some measures of lexical complexity may work well on your corpus object (as you’ll see below), and hence you won’t need to go through the pre-processing steps.

Types vs. tokens

In natural language processing, you will often hear the words “types” or “tokens” and understanding what they refer to is important.

Tokens refer to the number of words in your corpus. It refers to any kind of word, even stopwords. The line of code below shows you how to find out the number of tokens in your corpus, using your tokenised object.

ntoken(tok)

Types refer to the number of unique words found in your corpus. In other words, while tokens count all the words regardless of their repetition, types only show you the frequency of unique words. Logically, the number of types should therefore be lower than the number of tokens.

ntype(tok)

Complexity measures — what are they?

In linguistics, complexity is a characteristic of a text but there are multiple measures and hence multiple implied definitions in practice. In natural language processing, these measures are useful for descriptive statistics. I will show you the two most popular ways of assessing textual complexity: how readible your text is (textual readability) and how rich it is (textual richness).

Lexical readability

Readability measures attempt to quantify how hard a text is to read. The usual benchmark taken is children’s books — classified as “simple”. Using the quanteda package, a useful way to start is with the basic function for textual readability shown in the code below.

readability <- textstat_readability(mycorpus, c(""meanSentenceLength"",""meanWordSyllables"", ""Flesch.Kincaid"", ""Flesch""), remove_hyphens = TRUE,

min_sentence_length = 1, max_sentence_length = 10000,

intermediate = FALSE) head(readability)

Note that this function allows you to use the corpus object instead of the tokenised one. In your argument, you have to specify which measure of lexical readability you wish to use (click here to find out all the ones you can use). You can include very simple and straightforward ones such as “meanSentenceLength” calculating the average sentence length and “meanWordSyllable” calculating the average word syllables.

Otherwise, you can chose more statistically robust and complicated measures. Popular ones are the Flesch-Kincaid readability score or Flesch’s reading ease score. These two tests use the same core measures (word length and sentence length) but they have different weighting factors. And the results of the two tests correlate approximately inversely: a text with a high score for Flesch’s reading ease should have a lower score in the Flesch-Kincaid.

A higher score in Flesch’s reading ease test indicates material that is easier to read; lower numbers mark passages that are more difficult to read. As a benchmark: a high score should be easily understood by an average 11 year old, while a low score is best understood by university graduates.

Lexical richness

Similar to textual readability, the quanteda package also has a function to assess how rich a text is using lexical diversity measures.

The most popular measure is the Type-Token Ration (TTR). The basic idea behind that measure is that if the text is more complex, the author uses a more varied vocabulary so there’s a larger number of types (unique words). This logic is explicit in the TTR’s formula, which calculates the number of types divided by the number of tokens. As a result, the higher the TTR, the higher the lexical complexity.

dfm(tok) %>%

textstat_lexdiv(measure = ""TTR"")

Although TTR is a useful measure, you should however keep in mind that it may unfortunately be affected by text length. The longer the text, the less likely it is that novel vocabulary will be introduced. Therefore, longer texts might lean more towards the tokens side of the equation: more words (tokens) are added but less and less represent unique words (types). So should you have a large corpus to analyse, you may want to use additional measures of lexical richness to verify the results of your TTR.

Another measure of lexical richness you may use is Hapax richness, defined as the numbre of words that occur only once divided by the number of total words. To calculate this, simply use a logical operation on the document-feature matrix to return a logical value for each term that occurs once and then sum up the rows to get a count. Last but not least, calculate it as a proportion of the overall number of words (ntokens) for better interpretation within your overall corpora."
Distributed Vector Representation : Simplified,"Arguably the most essential feature representation method in Machine Learning

Let’s play a simple game. We have 3 “people” with us — Micheal, Lucy and Gab. We wanna know which person’s playlist matches the most with Lucy, is it Micheal or Gab?? Let me give you some hints. Lucy loves classic rock, and so does Gab but Micheal is not a fan. Lucy prefers instrumental versions and Micheal is also the same, but Gab does not fancy them at all. Lucy does not like Pop, Micheal outright hates it but Gab is definitely a Pop fanatic!!

Was this representation of the information helpful? Can you conclusively say whose playlist matches more with Lucy? Lucy and Micheal have common interest in 2 different song genres while Lucy and Gab share only one common song genre. But can you be sure that the one genre Lucy and Gab share will not outweigh the other two? (I mean comeon .. it’s classic rock!!!)

What if I told you that I have some magic formula with which I can represent their music interests as one single value? Lucy can be represented as -0.1, Gab can be represented as -0.3 and Micheal can be represented as 0.7. That certainly makes the job easier because if you trust the formula, I think it is clear that Lucy’s playlist will match the most with Gab. But how did I come up with the formula? Let’s take it from the top…

What is Distributed Representation?

Distributed Representation refers to feature creation, in which the features may or may not have any obvious relations to the original input but they have comparative value i.e. similar inputs have similar features. Converting inputs to numerical representation (or features) is the first step to any Machine Learning algorithm in every domain.

Why is non-distributed representation not enough?

Non-distributed representation (also called one-hot vector representation) adds a new vector dimension for every new input possibility. Obviously, the more number of unique possible inputs, the longer the feature vector will be. This kind of representation has 2 major flaws,"
Reinforcement Learning Introduction,"Reinforcement Learning Introduction

An introduction to reinforcement learning problems and solutions Y Tech · Follow 4 min read · Jul 25, 2019 -- Share

This post will be an introductory level on reinforcement learning. Throughout this post, the problem definitions and some most popular solutions will be discussed. After this article, you should be able to understand what is reinforcement learning, and how to find the optimal policy for the problem.

The Problem Description

The agent-environment interaction in reinforcement learning

The Setting

The reinforcement learning (RL) framework is characterized by an agent learning to interact with its environment.

learning to interact with its environment. At each time step, the agent receives the environment’s state (the environment presents a situation to the agent), and the agent must choose an appropriate action in response. One time step later, the agent receives a reward (the environment indicates whether the agent has responded appropriately to the state) and a new state .

(the environment presents a situation to the agent), and the agent must choose an appropriate in response. One time step later, the agent receives a (the environment indicates whether the agent has responded appropriately to the state) and a new . All agents have the goal to maximize the expected cumulative reward.

Episodic vs. Continuing Tasks

Continuing tasks are tasks that continue forever, without end.

are tasks that continue forever, without end. Episodic tasks are tasks with a well-defined starting and ending point.

* In this case, we refer to a complete sequence of interaction, from start to finish, as an episode.

* Episodic tasks come to an end whenever the agent reaches a terminal state.

Cumulative Reward

The discounted return at time step t is G(t) = R(t+1) + γ*R(t+2) + γ^2*R(t+3) + ...

The agent selects actions with the goal of maximizing expected (discounted) return.

The discount rate γ is something that you set, to refine the goal that you have the agent.

* It must satisfy 0 ≤ γ ≤ 1 .

* If γ = 0 , the agent only cares about the most immediate reward.

* If γ = 1 , the return is not discounted.

* For larger values of γ , the agent cares more about the distant future. Smaller values of γ result in more extreme discounting.

MDPs and One-Step Dynamics"
Clearing the Water Around A.I.,"Clearing the Water Around A.I.

Nearly everyone today has been experiencing some effects and new ideas about artificial intelligence. Most companies, banks, retail stores, etc, are focusing on ways artificial intelligence will expand their market and lead them to more successful ventures. I’m sure you’ve all once had or heard a conversation with people, with no technical knowledge about artificial intelligence, apprehensively talking about how their lives are going to be changed by it. I’m sitting in this hole in the wall breakfast place by Wall Street writing this and experiencing it right now. Just picture a couple of classic NYC execs talking about the A.I. strategies they’ve started implementing at their firm; “It’s going far better than we assumed, I’ve got no clue what’s going on, I just hope we’ll be able to start making staff cuts ASAP”.

Numerous startups today are trying to slap A.I. right on their face, aiming, and usually succeeding, to generate large amounts of funding. There is this large investment into a field that people see as today’s ‘Gold Rush’, but there lays a murky public understanding of how it functions (pun intended).

ALGORITHMS ARE THE LEGOS

“An algorithm is a set of step-by-step instructions so explicit even something as literal-minded as a computer can follow them.”

By itself, other than impressing or intimidating people just from the sound of it, an algorithm alone is no smarter than a power saw. (A.I. solving power saw kickbacks with Destin, SmarterEveryDay)

Try and think of something that just does one thing extremely well, over and over again. It’s very useful for sorting lists of numbers or searching the internet for pictures of the mother of dragons."
I Made a Dynamic Hurricane Map with Excel!,"These days I have been obsessed with researching data maps. I challenged myself and made a data map using Excel. The graphic below is the dynamic map of Hurricane Irma that I have drawn with Excel.

If you are interested, I will be happy to share with you the process of making a hurricane map with Excel. It is to use the bubble chart to outline the dynamic path of the hurricane and to show the change in wind strength. Here are the specific steps for making a hurricane map.

1. Prepare Materials

① Find a map of the US Atlantic that includes lines of latitude and longitude.

Note that the map we need must have latitude and longitude lines. Many maps provided on the public network are inaccurate. I downloaded this map from National Hurricane Center and Central Pacific Hurricane Center.

② Download the storm track statistics of Hurricane Irma from Weather Underground, including date, time, latitude, longitude, wind, and so on.

2. Process Data

① Remove the units of data such as latitude, longitude, wind speed, wind pressure, etc. And the date and time should be converted into a format that is easy for Excel to process.

② We can see that the hurricane statistics are recorded every six or three hours in the data material. Here we keep the data recorded every six hours.

3. Draw a Bubble Chart

① Insert the above map into the table, and then draw a bubble chart on it. The X axis of the bubble chart represents the longitude data, the Y axis represents the latitude data, and the bubble size depends on the value of the wind pressure.

② Design the format of Chart Area. The maximum and minimum values of the coordinate axes in the bubble chart are set according to the latitude and longitude readings. And make the spacing on the coordinates coincide with the spacing of the latitudes and longitudes on the map. Here I set the spacing to 5, which ensures that the data points drawn by the bubble chart match the actual latitude and longitude on the map.

③ Drag and drop the border of the plot area to make it coincide with the coordinate axes in the bottom Atlantic map. Then hide the axis data and set the border to “No line”.

4. Make Dynamic Effects

To achieve the dynamic effects of the hurricane trajectory, we can use Slider Control to control the time. Each time we slide, the time is increased by 6 hours. The cell linked by the slider passes the data to the table area on the left side of the chart. And then it obtains the corresponding latitude and longitude data and wind data by querying the time point. At the same time, two sets of data sources of the chart are generated from this time data. The entire control chain is thus formed and the final control of the chart by the slider is completed.

Further than that, if I want to implement automatic control of the slider, I will need VBA code to make the chart automatically show dynamic effects.

Operation Process:

① Write two macros with VBA. One macro controls the start, the other controls the stop.

② Draw 4 controls, representing start, stop, loop, and slider. Specify the above macros.

③ The format of the slider control specifies the left cell, which records the sliding data. And the upper cell “Data & Time” calls the data of this cell.

④ Prepare dynamic data.

In order to achieve the above dynamic effects, two sets of data are actually prepared in the bubble chart. One set shows all the path points that the hurricane has traveled before the current time point, and the other set marks the location of the hurricane at the current time point.

Here we need to use the LOOKUP function and call the data of the cell “Data & Time”.

For the first set of data, we select all the data less than or equal to the current time point and set the other data points to #N/A.

For the second set, we select the data that matches the current time, and the other data is also set to #N/A. Here, #N/A doesn’t display data points in the chart.

⑤ Bind the bubble chart to the data source.

In the final step, we bind the bubble chart to the data source and the hurricane trajectory map is complete.

The textbox in the chart can get the information of the data points directly from the cells by linking with them. In addition to the bubble chart, a set of column charts is added to the map to show the wind, and its data binding operation form is the same as that of the bubble chart.

Well, the general idea of making a hurricane map with Excel is like this. Below is an official map of Hurricane Irma. Is it very similar to the one I made with Excel?

From Weather Underground

Tips

Excel is very powerful, but if you want to use it to make some complicated charts, you must have a code base and learn VBA language, which is time consuming. I shared an article 4 Uses of Data Maps in Business Analysis, in which all maps are made with the zero-code visualization tool FineReport. The operation is very simple. If you don’t understand the code, you can try this tool to create data visualization charts.

You might also be interested in…

Top 16 Types of Chart in Data Visualization

How Can Beginners Design Cool Data Visualizations?

A Beginner’s Guide to Business Dashboards"
What is a perceptron?,"A neural network is an interconnected system of perceptrons, so it is safe to say perceptrons are the foundation of any neural network. Perceptrons can be viewed as building blocks in a single layer in a neural network, made up of four different parts:

Input Values or One Input Layer Weights and Bias Net sum Activation function

A neural network, which is made up of perceptrons, can be perceived as a complex logical statement (neural network) made up of very simple logical statements (perceptrons); of “AND” and “OR” statements. A statement can only be true or false, but never both at the same time. The goal of a perceptron is to determine from the input whether the feature it is recognizing is true, in other words whether the output is going to be a 0 or 1. A complex statement is still a statement, and its output can only be either a 0 or 1.

Following the map of how a perceptron functions is not very difficult: summing up the weighted inputs (product of each input from the previous layer multiplied by their weight), and adding a bias (value hidden in the circle), will produce a weighted net sum. The inputs can either come from the input layer or perceptrons in a previous layer. The weighted net sum is then applied to an activation function which then standardizes the value, producing an output of 0 or 1. This decision made by the perceptron is then passed onto the next layer for the next perceptron to use in their decision.

Together, these pieces make up a single perceptron in a layer of a neural network. These perceptrons work together to classify or predict inputs successfully, by passing on whether the feature it sees is present (1) or is not (0). The perceptrons are essentially messengers, passing on the ratio of features that correlate with the classification vs the total number of features that the classification has. For example, if 90% of those features exist then it is probably true that the input is the classification, rather than another input that only has 20% of the features of the classification. It’s just as Helen Keller once said, “Alone we can do so little; together we can do so much.” and this is very true for perceptrons all around."
The Importance of Ethics in Artificial Intelligence,"Photo by ThisIsEngineering on Pexels

Member-only story The Importance of Ethics in Artificial Intelligence

“Just because we can, doesn’t mean we should” could be something to keep in mind when it comes to innovating with technology. The arrival of The Internet has 10x’d the speed of innovation and allows us to pretty much create anything we can think of. Artificial Intelligence is a great example of a space in which we can build whatever we like and then some, but should we?

As ethical as its developer

Ethics (noun): moral principles that govern a person’s behavior or the conducting of an activity. ( “many scientists question the ethics of cruel experiments”)

We, humans, have something called “a moral compass”. It’s an agent that sits in our brain and basically tells right from wrong. When you see an injustice, your brain tells you something isn’t right. The actions that come from it are up to you, but you can tell right from wrong. The standards of your moral compass strongly depend on your upbringing and environment, but most people have one of these compasses. It’s also what companies build their ethics and compliances on, what’s right and what is wrong and how do we set rules based upon that.

Artificial Intelligence is lacking such a compass. As a matter of fact, it’s lacking any kind of compass. Artificial Intelligence can only separate right from wrong based on data that has the label “right” and the label “wrong” attached to it. AI doesn’t have awareness of itself, nor does it have something called “empathy” which is the fundament of ethics. The only moral compass there is when talking about AI, is that of its developer who then sets the bar for what is right and what is wrong. If the developer has a low moral compass, he/she might develop AI with bad intentions, and vice versa. That doesn’t mean AI will actually always live by those standards, as AI isn’t coded, it’s trained. Meaning it could be made with good intentions, but still, draft into something less morally approved or “for good” as one might have hoped.

So, why is Ethics in technology such a big deal?

Well, if we don’t build technology based on ethics and make sure we understand the outcome…"
The Rise of the Term “MLOps”,"The Rise of the Term “MLOps”

“MLOps (a compound of Machine Learning and “information technology OPerationS”) is [a] new discipline/focus/practice for collaboration and communication between data scientists and information technology (IT) professionals while automating and productizing machine learning algorithms.” — Nisha Talagala (2018)

For folks interested, I’ll also be teaching a Machine Learning in Production class in 2022–23!

Torrey Pines Gliderport in San Diego for intro aesthetic

The understanding of the machine learning lifecycle is constantly evolving. When I first saw graphics illustrating this “cycle” years ago, the emphasis was on the usual suspects (data prep and cleaning, EDA, modeling etc…). Less notice was given to the more elusive and less tangible final state — often termed “deployment”, “delivery” or in some cases just “prediction”.

At the time, I don’t think a lot of rising data scientists really considered the sheer scope of that last term (I sure as hell didn’t). “Prediction” didn’t just mean .predict() , it implied true scale, production-level deployment, monitoring and updating — a true cycle. Without the engineering skills needed to make this vague concept reality, the data scientist was stuck in the notebook. Models lived as .pickle files on a data scientist’s local machine, performance was reported with Powerpoint, and the ML lifecycle was broken.

A Straightforward, but Incomplete Data Science Lifecycle with the word “Modeling” spelled incorrectly

While the end-to-end ML lifecycle has always been pitched as an actual “cycle”, to date there has been limited success in actually managing this end-to-end process at enterprise level scale for what I see as the following reasons:"
How well does a value-based regression model perform in the 2016 Presidential Election?,"How well does a value-based regression model perform in the 2016 Presidential Election?

Using multiple regression to investigate values predictive of Donald Trump voters in the 2016 election Zach Alexander · Follow Published in Towards Data Science · 6 min read · Dec 30, 2019 -- Share

A D3.js map comparing the actual 2016 election results to the model predictions. Visit www.zach-alexander.com/pres2016-regression to interact with the full visualization.

For some, it may be hard to believe, but the 2020 presidential election is roughly 300 days away. In what is shaping up to be another contentious year in politics, forecasts predict an unprecedented number of voters will turn out to vote for nominees that will undoubtedly have very different agendas for our country’s future.

As the Democratic party seeks to win over ex-Trump voters over the next year, I thought it would be interesting to take a look back at the 2016 election results to identify factors that led to his success. During my analysis, it became quite clear that certain values and ideologies were predictive in estimating the percent of voters that cast their vote for Donald Trump on a state-wide level.

By using linear regression, I created a model that captured close to 87 percent of the variability in the proportion of votes for Donald Trump.

Now, for those that are more statistically savvy, this model would by no means be effective at predicting the outcome of the 2020 election (or even the 2016 election), but it does tell us some valuable things about the way voters feel about certain “hot-button” issues and the effect they have on their decision to vote for President Trump or not.

Datasets used for this analysis

The 2016 election results were ultimately obtained from Townhall.com after scraping their html-formatted table and doing a bit of tidying. If interested, you can read more about this on GitHub. Credit for the web-scraping is attributed to Tony McGovern.

The second dataset that I worked with contained responses to the 2017 American Values Survey. This dataset was found on the Public Religion Research Institute (PRRI) website. It holds a large number of questions related to values ranging from respondent’s views on immigration, gun control laws, health care, and much more.

The topline version of the survey can be found here and more information about the surveying methodology can be found…"
Introduction to Git Data Extraction and Analysis in Python,"Is there anyone in the software industry who has never used or at least heard of Git?

Git is a revolutionary tool that is quite ubiquitous in software teams nowadays. This article’s purpose is not to provide an introduction to git, there are a ton of resources that can guide you through that. Its purpose is rather to analyze git relevant data in order to get important insights from those data.

Throughout this article, we are going to extract Git related data by using the Github REST API and then analyze those data by leveraging Python’s top data analysis library, Pandas as well as an interactive data visualization library that is gaining massive popularity, Plotly. We are going to take as example data the repository of Apache Spark.

Git Data Extraction

Git repositories are generally stored in source code hosting facilities. The most popular of these are Github and Bitbucket, but many others are available, like Gitea, GitLab, etc. In this article, we shall focus on Github, but the data extraction process should be similar for the other hosting facilities too.

Github provides a REST API that contains endpoints for all git related resources. In order to be able to consume Github APIs, we need to generate an access token at the Developer Settings on the Github Profile page. After having done that, we should be all set. We start our Jupyter Notebook and we begin by importing the necessary libraries:"
How many engineers does it take to fix a lightbulb — Optimizing Incident Response,"How many engineers does it take to fix a lightbulb — Optimizing Incident Response

Introduction

How many engineers does it take to replace a lightbulb? This satirical hypothetical will be used to demonstrate how businesses can optimize their incident response for mission critical tasks.

We’re often found workplace situations that require us to work with others to solve time-sensitive, business-critical, and customer facing issues. Whether a software developer trying to resolve a down website for a eCommerce website or first responders responding to a wildfire incident, time is money (and sometimes human life), thus the quicker the issue is resolved, the happier and sometimes safer your customers will be.

To Add or Not to Add?

Imagine being a manager of an engineering team that manages lightbulbs. A particular lightbulb goes out and the engineering team has been notified. Your boss asks you frantically:

How many engineers are needed to replace the lightbulb?

Well, that depends:

Cost of downtime (i.e., for every minute the lightbulb is out, what is the impact to our business?) Cost of engineering resources Tolerance for risk

Risk

Without spending too much time on risk, in this context it refers to a rare chance that something really bad happens. Both rare and really bad are relative/subjective terms. One standard method is to measure the 95th percentile of the distribution of all possible outcomes. The risk value will be defined as a 5% of chance of something worse occurring than the risk value.

95th-percentile of Bad Outcomes. Risk is the location of the red vertical line

The black vertical line represents the average outcome whereas the red vertical line represents the 95th percentile. The fatter the tail, the farther apart the vertical and red line will be, and thus, the higher the risk.

Approach"
How to Create a Cancer Survival Prediction Model with EDA,"How to Create a Cancer Survival Prediction Model with EDA

A Thorough Walkthrough of Exploratory Data Analysis Techniques with Haberman’s Cancer Survival Dataset using Python to Create a Simple Predictive Cancer Survival Model Jessica Phillip · Follow Published in Towards Data Science · 16 min read · Dec 30, 2019 -- Listen Share

Illustration by John Flores

The impetus for this blog and the resultant cancer survival prediction model is to provide a glimpse into the potential of the healthcare industry. Healthcare continues to learn valuable lessons from the current success of machine learning in other industries to jumpstart the utility of predictive analytics (also known as “health forecasting” ) and to improve patient diagnosis, care, chronic disease management, hospital administration and supply chain efficiency. [1]

This classification project is an introduction to the exploratory data analysis (EDA) of the Haberman dataset in order to determine the attributes necessary to develop a simple predictive cancer survival model. The model’s forecast will determine, what is the likelihood a patient will survive beyond 5 years after surgery? This write up ultimately automates the prognosis for patients based on three attributes; age, year of surgery and the number of positive axillary nodes removed from the patient during surgery.

1. What is Exploratory Data Analysis?

In the field of machine learning, exploratory data analysis (EDA) is a philosophy or rather an approach for analyzing a dataset. It is a technique for summarizing, visualizing and becoming intimately familiar with the important characteristics of a dataset. EDA is useful in order to maximize insights, uncover underlying structure, extract important variables, detect outliers and anomalies as well as test unconscious/unintentional assumptions.

And while this process can be a bit tedious, one does not simply, [insert classic LOTR meme], skip the EDA process to rush into the machine learning stage. In fact, EDA techniques are the precursor to machine learning as it’s used to answer questions like:

How to define feature variables that can potentially be used for machine learning? How to choose the most suitable algorithms for your data set?

In essence, EDA is for unearthing what the data can tell us beyond formal modeling or general hypothesis testing to create applicable business insights, models and procedure enhancements that may have not been evident or worth investigating to business stakeholders. How? By finding patterns in the data and thus insights into something interesting that’s worth investigating.

2. Understanding the Dataset

The Haberman’s survival dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago’s Billings Hospital on the survival of patients who had undergone surgery for breast cancer.

You can easily download the dataset from the Kaggle link below:

Attribute Information:

Age: Age of patient at time of operation (numerical). Year: Patient’s year of operation to remove the nodes (year — 1900, numerical) Nodes: Number of positive axillary nodes detected (numerical) Status: Survival status (class attribute)

1 = the patient survived 5 years or longer

2 = the patient died within 5 years

Note: Just a reminder! It’s always a great idea to do a bit of background research on your dataset and the problem at hand before you delve into EDA. I did so with ‘nodes’ and here’s what I found.

Background: Positive axillary lymph nodes are small, bean-shaped organs located in the armpit (axilla), which act as filters along the lymph fluid channels. As lymph fluid leaves the breast and eventually returns to the bloodstream, the lymph nodes catch and trap cancer cells before they reach other parts of the body. Thus, having cancer cells in the lymph nodes under your arm suggests an increased risk of the cancer spreading. When lymph nodes are free of cancer, test results are negative. However, if cancer cells are detected in axillary lymph nodes they are deemed positive.

3. Environment Configuration: Importing Libraries and Loading Data File

Below, you’ll find details on how to set up your environment to repeat this EDA process and cancer survival model. The purpose for importing the required libraries for analysis are:

Pandas is used for manipulating the dataset

NumPy for imposing mathematical calculations and statistical on the dataset

Matplotlib and Seaborn are used for visualization

# Import the necessary packages

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import numpy as np # Load the dataset

haberman = pd.read_csv(""haberman.csv"")

4. Data Details

# Print first 5 lines of dataset

haberman.head()

Output:

# Personal preference: convert year format from 'YY' to 'YYYY'

haberman['year'] = haberman['year'] + 1900 # Again, personal preference: convert survival status 1 and 2 to #'yes' and 'no', respectively

haberman['status'] = haberman['status'].map({1:'yes', 2: 'no'}) # Print first 5 lines of altered dataset

haberman.head()

Output:

# Print the number of rows and columns for the dataset

haberman.shape

Output:

(306, 4)

Observations:

The .csv file contains data from 306 patients stored in the rows and 4 columns describing the features of the data set.

# Print the column names in dataset and the data type

haberman.columns

Output:

Index([‘age’, ‘year’, ‘nodes’, ‘status’], type = ‘object’)

# Details about the dataset

haberman.info()

Output:

<class 'pandas.core.frame.DataFrame'>

RangeIndex: 306 entries, 0 to 305

Data columns (total 4 columns):

age 306 non-null int64

year 306 non-null int64

nodes 306 non-null int64

status 306 non-null int64

dtypes: int64(4)

memory usage: 9.6 KB

Observations:

There are no missing values in this data set. The 3 attributes are of int data type and the last attribute was converted to a categorical datatype. In the ‘status’ column, the initial int-type was mapped to ‘yes’, which means the patient survived beyond 5 years. And the value ‘2’ was mapped to ‘no’, which means the patient passed within 5 years.

# Statistically describe the dataset

haberman.describe()

Output:

Observations:

Count: Total number of data points present in each respective column was 306. Mean: The average value for each column in respect to age, year and nodes. Std (Standard Deviation): The measurement of how far the group of values are spread out from the average (mean), or expected value. Min (Minimum): The minimum value present in each column. 25% Quartile: 1/4th of the data points are below the provided value. 50% Quartile: 1/2 of the data points are below the provided value. 75% Quartile: 3/4th of the data points are below the provided value. Max (Maximum): The maximum value present per column.

# Count each type of status in the dataset

haberman[""status""].value_counts()

Output:

Yes 225

No 81

Name: status, dtype: int64

Observations:

The value_counts function details how many data points for each class are present. This code snippet describes how many patients survived and how many did not after a period of 5 years. Out of 306 patients, 225 survived and 81 did not live beyond 5 years.

# Create a 'yes status' dataset to store 'yes' values for the patients that survived

status_yes = haberman[haberman[""status""] == 'yes']

status_yes.describe()

Output:

# Create a 'no status' dataset to store values for the patients that did not survive

status_no = haberman[haberman[""status""] == 'no']

status_no.describe()

Output:

Observations:

On Age: The difference between the mean age and year of no vs yes dataset isn’t statistically significant. Note: There is however, a trend that describes a general increase in the age of the patients who did not survive surgery after 5 years. This increase can be seen in the minimum age, the percent quartiles and the maximum age of the non-survived patients. On Year: The difference between the mean year and of the no vs yes dataset isn’t statistically significant. On Nodes: There is a noticeable increase in the mean number of nodes found for the ‘yes dataset’ and the ‘no dataset’. There is also an increase in the maximum number of nodes found in the ‘no dataset’. The percent quartiles were also higher for the no dataset than the yes dataset. Note: For example, for the 75% of patients who died within 5 years after surgery they were found to have 11 nodes or more compared to only 3 nodes for the patients that did survive.

Model Insight:

On average, those who survived have about 2.5 times less nodes than those who did not survive. For those who survived they held an average of 2.79 nodes versus the 7.46 nodes for the patients that did not survive.

5. Univariate Analysis

Univariate analysis is the simplest form of analyzing data. This process does not deal with causes or relationships, as only one variable is involved. Instead, it’s major motive is to describe; it takes data, summarizes that data and finds patterns that exists within a single feature.

5.1 Probability Density Functions

Probability Density Functions (PDF) are a statistical measure used to gauge the likely outcome of a discrete value. PDF’s are plotted on a graph typically resembling a bell curve, with the probability of the outcome lying below the curve.

Here the height of the bar denotes the percentage of the data points under the corresponding group.

What is the intuition behind PDF?

# Create a function for PDF analysis

def histoPDF(featureVar, classVar):

sns.FacetGrid(data = haberman, hue = classVar, height = 5) \

.map(sns.distplot, featureVar) \

.add_legend();

plt.title(""Histogram for "" + featureVar)

plt.ylabel(""density"")

plt.plot('histogram_2.png') # PDF histogram for age v. status

histoPDF('age', 'status')

Output:

Observations:

Major overlapping is observed, which suggests that age isn’t a major determining factor in the patients likihood of survival. Differences between the age of the yes dataset and no dataset are barely observable given the amount of overlap in the PDF. Perhaps another statistical method can unearth a pattern between age and survival status.

Model Insights:

Ages 30–40 had a higher chance of survival, whereas ages 40–60 did not. For ages 60+ the chances of survival were about 50/50.

# PDF histogram for year v. status

histoPDF('year', 'status')

Output:

Observations:

Major overlapping continues again suggesting that the year of the patient’s surgical procedure did not affect their survival rate/outcome after 5 years. There was a spike in the death rate for patients whose surgery was in year 1965 and a decrease for procedures done in 1960. Patient’s likelihood of survival was up between 1960–1962.

# PDF histogram for nodes v. status

histoPDF('nodes', 'status')

Output:

Observations:

Complete separation would be ideal to distinguish the exact number of nodes for patients who survived. Patients with 0 nodes or 1 node are more likely to survive. There are very few chances of surviving if there are 25 or more nodes. This plot has shown that the number of nodes seem to influence the survival rate of patients more so than age and year of operation.

Model Insight:

Patient non-survival increasingly likely after 5 nodes.

6. Cumulative Distribution Function

CDF creates a plot of the empirical cumulative distribution function. Use the CDF plot to determine the percent of data that is at or below a given value on the x-axis.

# CDF analysis

count1, bin_edges1 = np.histogram(status_yes['nodes'], bins = 10, density = True)

pdf1 = count1/(sum(count1))

print(pdf1, bin_edges1);

cdf1 = np.cumsum(pdf1)

plt.plot(bin_edges1[1:], pdf1)

plt.plot(bin_edges1[1:], cdf1, label = 'Yes')

plt.xlabel('nodes') print(""---------------------------------------------------------"") count2, bin_edges2 = np.histogram(status_no['nodes'], bins = 10, density = True)

pdf2 = count2/(sum(count2))

print(pdf2, bin_edges2);

cdf2 = np.cumsum(pdf2)

plt.plot(bin_edges2[1:], pdf2)

plt.plot(bin_edges2[1:], cdf2, label = 'No')

plt.xlabel('nodes')

plt.legend() plt.show()

Output:

Observations:

Approximately 83.55% of patients who survived had nodes in the 0 to 4.6 range as per the CDF summary stats.

7. Box & Whisker Plots and Violin Plots

A box and whisker plot — also called a box plot — displays the five-number summary of a set of data. The five-number summary is the minimum, first quartile, median, third quartile, and maximum. The box extends from the lower to upper quartile values of the data, with a line at the median. The whiskers extend from the box to show the range of the data. Outlier points are those past the end of the whiskers.

Violin plot is the combination of a box plot and probability density function (CDF). Violin Plots allow to visualize the distribution of a numeric variable for one or several groups. It’s a close form of the boxplot, but allows a deeper understanding of the density.

# Create box and whisker plot for each feature

plt.figure(1)

plt.figure(figsize = (15, 5))

plt.subplot(131)

sns.boxplot(x = 'status', y = 'age', data = haberman)

plt.subplot(132)

sns.boxplot(x = 'status', y = 'year', data = haberman)

plt.subplot(133)

sns.boxplot(x = 'status', y = 'nodes', data = haberman)

plt.show()

Output:"
Bist-Parser : an end-to-end implementation of a Dependency Parser,"This article is the 2nd and last article on Dependency Parsing. We will give you some easy guidelines for implementation and the tools to help you improve it.

Vocabulary

A TreeBank is a parsed text corpus that annotates syntactic or semantic sentence structure. Dependency TreeBanks are created using different approaches : either thanks to human annotators directly, or using automatic parsers to provide a first parse, then checked by annotators. A common approach consists in using a deterministic process to translate existing TreeBanks into new language through head rules. Producing a high-quality TreeBank is both time-consuming and expensive.

is a parsed text corpus that annotates syntactic or semantic sentence structure. Dependency TreeBanks are created using different approaches : either thanks to human annotators directly, or using automatic parsers to provide a first parse, then checked by annotators. A common approach consists in using a deterministic process to translate existing TreeBanks into new language through head rules. Producing a high-quality TreeBank is both time-consuming and expensive. CoNLL-U — Computational Natural Language Learning-Universal is a revised version of the CoNLL-X format. Sentences from TreeBanks are separated, and each word or punctuation mark is disposed on a distinct line. Each of the following items follows the word, separated by tabulations:

–ID: word index in the sentence, starting at 1

–FORM: word form or punctuation

–LEMMA: Lemma or stem of word form

–UPOS: Universal part of speech tag

–XPOS: Language-specific part of speech tag; will not be used in our model

–FEATS: Unordered list of morphological features, defined by Universal Dependencies; indicates the gender and number of a noun, the tense of a verb, etc.

–HEAD: Head of the word, indicates the index of the word to which the current one is related

–DEPREL: Universal Dependencies relation; indicates the relation between two words (subject or object of a verb, determiner of a noun, etc.)

–DEPS: Language-specific part dependencies; will not be used in our model

–MISC: Commentary or other annotation

An example of CoNLL-U format

An Entry is a word, or a punctuation mark in a sentence. It has multiple attributes, defined above. A sentence is typically a concatenation of entries (a word itself is an attribute of an entry: its form), separated by space.

The Implementation

The implementation of the Bist-Parser comes from the authors of its paper. An update has been published on GitHub by Xiezhq Hermann. You can find it here. It works on Python 3.x, with torch 0.3.1 (with or without Cuda). It is very complete and can be used as is. However, in order to adapt the code to your data or upgrade it, you must get through every module, which can be a difficult task. This part of the article will lead you through all files and processes.

Universal Dependencies (UD) is an open-community framework for grammatical annotation. It provides corpora and tools that greatly help to develop a Dependency Parser.

From UD, you can download a corpus of sentences of your choice (in any language available, even Old French!), use them as is, and start training your Bist-Parser with this type of command:

python src/parser.py --outdir [results directory] --train training.conll --dev development.conll --epochs 30 --lstmdims 125 --lstmlayers 2 [--extrn extrn.vectors]

You can detail hyperparameters here, caught by the model thanks to the file parser.py

As you may know, when you train a model on a corpus, the model is biased towards this corpus. You could train your model on multiple corpora in order to generalize it more. Several techniques allow you to increase scores, with TreeBank Embedding as an example. Here, we have just concatenated some TreeBanks, without any further processing.

utils

Create a ConllEntry class: every entry has well-known attributes: id, form, lemma, Universal PoS tag, language Specific PoS tag, morphological features, head of current word, dependency relation, enhanced dependency relation and commentary. These attributes are defined from the Universal Dependencies CoNLL-U format. This format is useful for the model to understand what its inputs are, and what it should predict.

class: every entry has well-known attributes: id, form, lemma, Universal PoS tag, language Specific PoS tag, morphological features, head of current word, dependency relation, enhanced dependency relation and commentary. These attributes are defined from the Universal Dependencies CoNLL-U format. This format is useful for the model to understand what its inputs are, and what it should predict. Read a CoNLL-U file and transform each sentence into a ConllEntry.

Count vocabulary: This function creates a Counter of ConllEntry attributes and allows you to know how these attributes are distributed through your dataset. If you want to determine the most frequent words or relations in your dataset, this function can be useful.

mstlstm

This file contains your model. All your hyper-parameters and most of your monitoring work happen in this file.

The method forward iterates through each entry in the sentence. It first computes the vectors for each entry attribute. With our model, we get multiple vectors that describe the word, the PoS tag and the feats. Those vectors are then concatenated to form a vector with a bigger dimension for each entry. These entries are then concatenated together to form the sentence vector.

First, it converts entries into vectors. Here, the principal attributes are the embedding of words, lemmas (onto) and PoS tags (pos). However, we advise you to add as many features as possible. For example, you may have access to features of words that indicate whether the noun is singular or plural, its gender, or tense… Embedding these features allows your BiLSTM to find many more patterns.

Evolution of PoS embedding on two dimensions

Then, it feeds the BiLSTM with these vectors (for = forward, back = backward). Line 52 evaluates the scores of the sentence. This is the part where the full Weighted Digraph is created. On line 57, it evaluates the relation score. This is an interesting trick in this model: rather than evaluating all the possibilities at the same time (|possibilities|=|arcs|.|labels|, which is way too high), it predicts the dependencies first, then the relations.

We will see about errs, lerrs and e later.

In the illustrations below, you can see the evolution of dependency evaluation through batches. A dark blue cell corresponds to a weighted arc. The example comes from a typical French sentence, “Les commotions cérébrales sont devenu si courantes dans ce sport qu’on les considère presque comme la routine.” You can spot spelling mistakes in the sentence; this is not rare in TreeBanks."
Predict Employee Retention,"Photo by Mario Gogh on Unsplash

Introduction

We know that larger companies contain more than thousand employees working for them, so taking care of the needs and satisfaction of each employee is a challenging task to do, it results in valuable and talented employees leave the company without giving the proper reason.

Employee churn is a major problem for many firms these days. Great talent is scarce, hard to keep and in high demand. Given the well-known direct relationship between happy employees and happy customers, it becomes of utmost importance to understand the drivers of employee dissatisfaction.

This post emphasizes on predicting retention of an employee within an organization such that whether the employee will leave the company or continue with it. It uses the data of previous employees who have worked for the company and by finding a pattern it predicts the retention in the form of yes or no.

The parameters we are using, such as salary, number of years spent in the company, promotions, number of hours, work accident, financial background, etc. Through this paper, an organization can choose its strategies to keep great representatives from leaving the organization. The data has 14,999 examples (samples). Below are the features and the definitions of each one:

satisfaction_level: Level of satisfaction {0–1}.

last_evaluationTime: Time since last performance evaluation (in years).

number_project: Number of projects completed while at work.

average_montly_hours: Average monthly hours at the workplace.

time_spend_company: Number of years spent in the company.

Work_accident: Whether the employee had a workplace accident.

left: Whether the employee left the workplace or not {0, 1}.

promotion_last_5years: Whether the employee was promoted in the last five years.

sales: Department the employee works for.

salary: Relative level of salary {low, medium, high}.

The source code that created this post can be found here.

Data Set

loading the data into the data frame and separating the result column.

FinalCode.py hosted by GitHub

Data Preprocessing

The dataset has ‘salary’ and ‘sales’ column as categorical data, So we have to perform OneHotEncoding & LabelEncoding to convert this data into numerical form and To create dummy features we have to drop the first one to avoid linear dependency where some learning algorithms may struggle.

After that, we will split the data into training and testing datasets.

Preprocessing

Regression Models

Because we want results in the form of ‘Yes’ or ‘No’ such that whether an employee will leave the company or not, So the best suitable regression model is Logistic Regression for this dataset. Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign.

To calculate the accuracy of the result our model has generated we will be going to use Confusion Matrix as an evaluation parameter.

Logistic Regression

Classifiers

A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition based on the attribute value. It partitions the tree in a recursive manner call recursive partitioning.

Random forests is a supervised learning algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests create decision trees on randomly selected data samples, gets a prediction from each tree and selects the best solution through voting. It also provides a pretty good indicator of the feature importance.

Here in our dataset, we will use these two classifiers to classify our result in the form of Yes and No.

Decision Tree and Random Forest Classifier

Conclusion

Factors affecting the employment

Here in the above graph numbers on x-axis from 0 to 6 are representing WithHigherProjects,WithLowerSalary,WithHigherTime,WithPromotion,WithWorkAccident,WithorNotWorkAccident,WithoutWorkAccident. Each of these are the factors which can affect employment as WithHigherTime represents, employees who have more than four year of work experience but still haven’t got any promotion is 1750 which is a significant amount, WithLowerSalary represents employees whose salary level is low even when their evaluation score was higher than 3 such employees are 750.

Thus after evaluating this dataset, we get to know that lower salary levels, no promotions even when employees are working more than 4 years are the two main reasons for the employees to leave the organization."
Make your Pandas apply functions faster using Parallel Processing,"Make your Pandas apply functions faster using Parallel Processing

Super Panda

Parallelization is awesome.

We data scientists have got laptops with quad-core, octa-core, turbo-boost. We work with servers with even more cores and computing power.

But do we really utilize the raw power we have at hand?

Instead, we wait for time taking processes to finish. Sometimes for hours, when urgent deliverables are at hand.

Can we do better? Can we get better?

In this series of posts named ‘Python Shorts,’ I will explain some simple constructs provided by Python, some essential tips and some use cases I come up with regularly in my Data Science work.

This post is about using the computing power we have at hand and applying it to the data structure we use most.

Problem Statement

We have got a huge pandas data frame, and we want to apply a complex function to it which takes a lot of time.

For this post, I will use data from the Quora Insincere Question Classification on Kaggle, and we need to create some numerical features like length, the number of punctuations, etc. on it.

The competition was a Kernel-based competition and the code needed to run in 2 hours. So every minute was essential, and there was too much time going in preprocessing.

Can we use parallelization to get extra performance out of our code?

Yes, we can.

Parallelization using just a single function

Can we make all our cores run?"
Build Your First Computer Vision Project — Dog Breed Classification,"Build Your First Computer Vision Project — Dog Breed Classification

Photo by Joe Caione on Unsplash

For us, humans, it is pretty easy to tell one dog breed from another. That is if you are talking about 10–20 popular dog breeds. When we are talking about more than 100 kinds of dogs, it is an entirely different story. For a human to correctly and consistently classify a large number of breeds, we need a different approach than just pure memorization. We need to start extracting “features” that correspond to different breeds such as fur color, ear shape, facial shape, tail length, etc. Even then, we need to memorize what breed has what features, and it’s not an easy or fun task.

Image credit: It is hard to classify a large number of dog breeds

Humans having difficulties identifying a large number of species is a perfect example of situations where computers are better than us. This is not to say that Skynet is coming to kill us all; I believe that we should not worry about that just yet. Instead of machines replacing humans in the workforce, many believe that a hybrid approach of the two races working together can outperform any single race working alone. You can read more about the topic in the book titled Machine, Platform, Crowd by Andrew McAfee.

In this project, I will walk you through the steps to build and train a convolutional neural network (CNN) that will classify 133 different dog breeds. This project is part of my Data scientist Nanodegree, and you can find out more about it here. The code for this project is available on my GitHub repo, make sure that you install the required packages if you want to follow along.

What you need to know

How computer understand an image

We are very good at recognizing things thanks to millions of years of evolution that went into perfecting our eyes and brains for vision capability. A computer “sees” an image as a series of numbers. A pixel on your screen is represented as three numbers from 0 to 255. Each number is represented as the intensity of red, green, and blue. A…"
Confusion Matrix and Class Statistics,"Co-author: Maarit Widmann

With this post I am going back to the classics. Here explained the confusion matrix and some accuracy measures associated with it.

— — — — — — —

A classification model assigns data to two or more classes. Sometimes, detecting one or the other class is equally important and bears no additional cost. For example, we might want to distinguish equally between white and red wine. At other times, detecting members of one class is more important than detecting members of the other class: an extra investigation of a non-threatening flight passenger is tolerable as long as all criminal flight passengers are found.

Class distribution is also important when you’re quantifying performances of classification models. In disease detection, for example, the number of disease carriers can be minor in comparison with the class of healthy people.

The first step in evaluating a classification model of any nature is to check its confusion matrix. Indeed, a number of model statistics and accuracy measures are built on top of this confusion matrix.

Email Classification: spam vs. useful

Let’s take the case of the email classification problem. The goal is to classify incoming emails in two classes: spam vs. useful (“normal”) email. For that, we use the Spambase Data Set provided by UCI Machine Learning Repository. This dataset contains 4601 emails described through 57 features, such as text length and presence of specific words like “buy”, “subscribe”, and “win”. The “Spam” column provides two possible labels for the emails: “spam” and “normal”.

Figure 1 shows a workflow that covers the steps to build a classification model: reading and preprocessing the data, partitioning into a training set and a test set, training the model, making predictions by the model, and evaluating the prediction results.

The workflow shown below is downloadable from on the KNIME Hub page and from the EXAMPLES Server under: 04_Analytics / 10_Scoring / 01_Evaluating_Classification_Model_Performance.

Fig. 1: Workflow building, applying and evaluating a supervised classification model: data reading and preprocessing, partitioning, model training, prediction, and model evaluation. This workflow predicts whether emails are “spam” or “normal”.

The last step in building a classification model is model scoring, which is based on comparing the actual and predicted target column values in the test set. The whole scoring process of a model consists of a match count: how many data rows have been correctly classified and how many data rows have been incorrectly classified by the model. These counts are summarized in the confusion matrix.

In the email classification example we need to answer several different questions:

How many of the actual spam emails were predicted as spam?

How many as normal?

Were some normal emails predicted as spam?

How many normal emails were predicted correctly?

These numbers are shown in the confusion matrix. And the class statistics are calculated on top of the confusion matrix. The confusion matrix and class statistics are displayed in the interactive view of the Scorer (JavaScript) node as shown in Figure 2.

Fig. 2: Confusion matrix and class statistics in the interactive view of the Scorer (JavaScript) node.

Confusion Matrix

Let’s see now what these numbers are in a confusion matrix.

The confusion matrix was initially introduced to evaluate results from binomial classification. Thus, the first thing to do is to take one of the two classes as the class of interest, i.e. the positive class. In the target column, we need to choose (arbitrarily) one value as the positive class. The other value is then automatically considered the negative class. This assignment is arbitrary, just keep in mind that some class statistics will show different values depending on the selected positive class. Here we chose the spam emails as the positive class and the normal emails as the negative class.

The confusion matrix in Figure 3 reports the count of:

The data rows (emails) belonging to the positive class (spam) and correctly classified as such. These are called True Positives (TP). The number of true positives is placed in the top left cell of the confusion matrix.

The number of true positives is placed in the top left cell of the confusion matrix. The data rows (emails) belonging to the positive class (spam) and incorrectly classified as negative (normal emails). These are called False Negatives (FN) . The number of false negatives is placed in the top right cell of the confusion matrix.

. The number of false negatives is placed in the top right cell of the confusion matrix. The data rows (emails) belonging to the negative class (normal) and incorrectly classified as positive (spam emails). These are called False Positives (FP). The number of false positives is placed in the lower left cell of the confusion matrix.

The data rows (emails) belonging to the negative class (normal) and correctly classified as such. These are called True Negatives (TN). The number of true negatives is placed in the lower right cell of the confusion matrix.

Therefore, the correct predictions are on the diagonal with a gray background; the incorrect predictions are on the diagonal with a red background:

Fig. 3: A confusion matrix showing actual and predicted positive and negative classes in the test set.

Measures for Class Statistics

Now, using the four counts in the confusion matrix, we can calculate a few class statistics measures to quantify the model performance.

The class statistics, as the name implies, summarizes the model performance for the positive and negative classes separately. This is the reason why its value and interpretation changes with a different definition of the positive class and why it is often expressed using two measures.

Sensitivity and Specificity

Fig. 4: Sensitivity and specificity values and their formulas, which are based on the values in the confusion matrix, for a classification model predicting emails as “spam” or “normal”

Sensitivity measures how apt the model is to detecting events in the positive class. So, given that spam emails are the positive class, sensitivity quantifies how many of the actual spam emails are correctly predicted as spam.

We divide the number of true positives by the number of all positive events in the dataset: the positive class events predicted correctly (TP) and the positive class events predicted incorrectly (FN). The model in this example reaches the sensitivity value of 0.882. This means that about 88 % of the spam emails in the dataset were correctly predicted as spam.

Specificity measures how exact the assignment to the positive class is, in this case, a spam label assigned to an email.

We divide the number of true negatives by the number of all negative events in the dataset: the negative class events predicted incorrectly (FP) and the negative class events predicted correctly (TN). The model reaches the specificity value of 0.964, so less than 4 % of all normal emails are predicted incorrectly as spam.

Recall, Precision and F-Measure

Fig. 5: Recall and precision values and their formulas, which are based on the values shown in the confusion matrix, for a classification model predicting emails as “spam” or “normal”

Similarly to sensitivity, recall measures how good the model is in detecting positive events. Therefore, the formula for recall is the same as for sensitivity.

Precision measures how good the model is at assigning positive events to the positive class. That is, how accurate the spam prediction is.

We divide the number of true positives by the number of all events assigned to the positive class, i.e. the sum of true positives and false positives. The precision value for the model is 0.941. Therefore, almost 95 % of the emails predicted as spam were actually spam emails.

Recall and precision are often reported pairwise because these metrics report the relevance of the model from two perspectives, also called type I error as measured by recall and type II error as measured by precision.

Recall and precision are often connected: if we use a stricter spam filter, we’ll reduce the number of dangerous emails in the inbox, but increase the number of normal emails that have to be collected from the spam box folder afterwards. The opposite, i.e. a less strict spam filter, would force us to do a second manual filtering of the inbox where some spam mails land occasionally.

Alternatively, recall and precision can be reported by a measure that combines them. One example is called F-measure, which is the harmonic mean of recall and precision:

Multivariate Classification Model

In case of a multinomial classification model, the target column has three or more values. The emails could be labeled as “spam”, “ad”, and “normal”, for example.

Similarly to a binomial classification model, the target class values are assigned to the positive and the negative class. Here we define spam as the positive class and the normal and ad emails as the negative class. Now, the confusion matrix looks as shown in Figure 6.

Fig. 6: Confusion matrix showing the distribution of predictions to true positives, false negatives, false positives, and true negatives for a classification model predicting emails into three classes “spam”, “ad”, and “normal”

To calculate the class statistics, we have to re-define the true positives, false negatives, false positives, and true negatives using the values in a multivariate confusion matrix:

The cell identified by the row and column for the positive class contains the True Positives , i.e. where the actual and predicted class is spam

, i.e. where the actual and predicted class is spam Cells identified by the row for the positive class and columns for the negative class contain the False Negatives , where the actual class is spam, and the predicted class is normal or ad

, where the actual class is spam, and the predicted class is normal or ad Cells identified by rows for the negative class and the column for the positive class contain the False Positives , where the actual class is normal or ad, and the predicted class is spam

, where the actual class is normal or ad, and the predicted class is spam Cells outside the row and column for the positive class contain the True Negatives, where the actual class is ad or normal, and the predicted class is ad or normal. An incorrect prediction inside the negative class is still considered as a true negative

Now, these four statistics can be used to calculate class statistics using the formulas introduced in the previous section.

Summary

In this article, we’ve laid the first stone for the metrics used in model performance evaluation: the confusion matrix.

Indeed, a confusion matrix shows the performance of a classification model: how many positive and negative events are predicted correctly or incorrectly. These counts are the basis for the calculation of more general class statistics metrics. Here, we reported those most commonly used: sensitivity and specificity, recall and precision, and the F-measure.

Confusion matrix and class statistics have been defined for binomial classification problems. However, we have shown how they can be easily extended to address multinomial classification problems.

— — -

As first published in the KNIME Blog"
Objects of Desire: A conversation with Edgar Yau,"Edgar Yau: It’s been about three, four years since I’ve actually been a part of these communities. Now I’m more of an observer, but I’ve stayed really up-to-date with the controversies and the outrage that goes on in them. In middle school I played video games and it’s the way I connected with my friends. It’s an interesting subculture for a boy to get all caught up in. The gamerspeak is very distinctive, games are very irreverent, they just kind of say whatever and if you get offended, you’re offended and everyone makes fun of you for it.

It’s not a first person shooter thing, it’s just a gamer thing. I wasn’t primarily playing first person shooters at that time, I was playing a lot of Minecraft with my friends and going on all these random servers. Even then it was like, who can build the biggest swastika? It was this whole idea of, I’m not getting offended, so if other people get offended they’re stupid, they’re vulnerable, they’re idiotic. What right do they have to control the way that I speak? The concept of freedom of speech is huge in these communities.

I started researching this because I was playing Call of Duty with my friend one day online, and within two minutes I heard a prepubescent voice say the n-word. I was catapulted into my adolescent years.

Andrew Thompson: Angela Nagle talked about this in Kill All Normies where she discusses the idea of extending cultural capital into subcultural capital. The distinction between groups in this case is based on one’s knowledge of these very esoteric references and memes and language, like gamerspeak, and I would say also on your ability to withstand insults. If you bristle, you’re sort of being filtered out of the subculture.

EY: The instant some feminist critique of games comes out, it feels like the enemy outside is coming into my space and telling me that I shouldn’t like something. There are only two options: you either double down on your opinions, or you kind of acknowledge that maybe they’re right. I don’t want to say that I was too young-I was around 16-but I was probably just too certain of what I thought and how I felt that I didn’t consider just for a second that those criticisms might not be totally baseless. The interesting thing is that I’m not from here, I grew up in Hong Kong and I only came to the States for college, but I was still extremely involved in this stuff.

AT: I really think of people in these subcultures expressing this fear about threats against “the West”. To hear that you are actually having these experiences in Hong Kong I wouldn’t have expected.

EY: I went to an international school so their first language was English. It was a very Western atmosphere. Out of all my friends, I was the deepest into this stuff. I was bringing a lot of these ideas to them, especially my guy friends, and they would latch onto it. Then as I’d talk about it around my female friends they’d freak out and I’d feel vindicated. Because like, Oh, I triggered someone. I have the power to deliver this uncomfortable truth them and make them emotionally react.

I can’t speak to how similar or different it was from an American high school experience, but it was certainly a very Western experience. It’s embarrassing to admit, but a lot of the guys would just say the n-word and we would get reprimanded for it. All of these moral norms in the US are the same over there. Obviously we’re consuming American pop culture, we’re watching American TV shows, we’re listening to American music. There was no connection of that word for me to race, it was just the idea that I get to use it and I can just kind of say that and trigger a reaction in people. And that’s kind of a global feeling.

AT: I was talking to somebody recently who told me he was really into this stuff for a time during his adolescence. This is somebody who if you met him you would not guess in a million years that any of this materiel ever appealed to any part of him, and it did. I’m almost 33 now-this subculture as I witnessed it in my own adolescence was more or less the same. The platforms were different, I don’t think 4chan existed when I was in high school, but the texture of it was all pretty much identical in the sense of using racial slurs, maximizing offense, and being jarring in the worst possible way. I didn’t really participate in that, but my teenage rebellion certainly had a digital form. This was when we had AOL Instant Messenger. I would just find people and troll them. I was just like a little shitposter. The line dividing my own form of trolling and more outwardly hateful activity is razor thin. Between you and talking to this other person and thinking about me, and thinking about other people I know from high school, I’ve realized that this transgressive online behavior is not this fringe practice that a few troubled youth drift into, this is pretty pervasive among males of a certain age to the extent that I would almost say you can assume that if you see a young male who isn’t exceedingly attractive, he is probably involved in this in some way, or is adjacent to it. It’s just the culture they default to. What the media has done is framed it as this marginal subculture when in fact, it’s actually this mainstream culture that just hasn’t been identified as such until very recently.

EY: I think that’s spot on and that’s something that I’ve been looking into a lot, how it’s actually the norm. Often the media doesn’t treat the internet with the respect that it needs to. There’s no discussion of it with an actual understanding that these are real people and this is a serious phenomenon. I remember on CNN they were like, “Who is this hacker 4chan?” It reminds me of the kinds of things that I was thinking, because a lot of this is kind of a rejection of the mainstream media, mainstream ideals, partially to be edgy, but also partially because you don’t feel like it reflects or represents you.

AT: There’s just more people engaging on /pol today than there were just a few years ago. So now it’s even more mainstreamed than it was and has developed a critical mass.

EY: There’s almost like no meaningful, actual alternative to these monolithic places of discussion. If you go on Reddit, for example, there is still a voting system where the most popular ideas go to the top and the most controversial ideas [within a given subreddit] go away. And so if you want to express controversial ideas that don’t necessarily line up exactly with the way everyone around you is supposed to think, there’s no way to do that.

The way that 4chan is laid out, it feels like you’re forced to engage with as many ideas as are thrown at you. There are conservative Reddits, but then those don’t really get as far as 4chan. And you don’t really get exposed to as many ideas that have the potential for that thrill of revelation. When you’re first starting out engaging in political ideas, you get those feelings of revelation over and over again. And it’s an interesting feeling, it feels like you’re learning. And so once you find something like Reddit, once you get a good grasp on say, if you go on r/gaming, like if you’re on the default gaming subreddit, it’s like okay, I get what these people are saying now. This is boring to me. What’s next? Oh, let’s find more and more specific, niche things. A platform like Reddit, is, I think, eventually not enough for someone who’s looking for that kind of discussion. There’s almost no alternative but 4chan to go to consistently have those provocative feelings. You don’t really see that on Reddit once you go through it.

I think that that is what that’s indicative of. Not that I’m saying that people are using 4chan as a substitute for Reddit. I just think that that is a general trend, where 4chan is interesting because it’s constantly presenting you with new ideas.

AT: You bring up a good point which is that it’s popular because it’s not one of these big platforms. It isn’t owned. I don’t even know who runs it. Who maintains the 4chan servers? I don’t know who the people are behind it. I think that there’s an appeal to that.

I’ve also been considering what Reddit is. Is it good, is it bad, what’s good, what’s bad, and so on. Reddit has basically subsumed all message boards at this point. You don’t go to message boards anymore. You go on different kinds of social media, and Reddit is the closest form of social media we have to a message board. But it doesn’t mimic message boards a lot. 4chan feels more like an old forum to me, it’s more Web 1.0 than something like Reddit is. I think that there’s no other places off these platforms to go, really. What are the other non-alt-right message boards that aren’t owned by venture capitalists?

Wherever you spend your time, the more time you spend there, the more you just absorb the ideas you’re surrounded by. I find myself absorbing ideas even when I’m approaching them from a distance. Even when researching things, I can feel them trying to work on me in someway.

EY: I use the word “Chad” in conversation sometimes. The language seeps into your brain, and you start to think of things in that same framework. It’s really ridiculous.

AT: The YouTuber ContraPoints did that incels video, and she actually said that just in the course of researching her video, she started using the words like Chad and Stacy. I think the reason she did that, and the reason you do that, and the reason I do it, is because the word Chad is funny and is a great word for a man of bulletproof fuckability.

It’s also indicative of more pernicious and more destructive ideas that can seep in just by sheer proximity. The way that there’s research indicating that if you force yourself to smile, you feel happier. Who knows, it’s pop psychology bullshit, I don’t know if it’s real or not. But I do think it’s real in the sort of digital-political realm, where just by touching these ideas, you are susceptible to them in some way.

This is data from r/Braincels and r/TheRedPill. These are the most popular subreddits for both people. The one that really stuck out to me in the r/Braincels results was the subreddit NEET, which stands for “Not in Employment, Education or Training”, and I have kind of a long thought I want to launch into for a minute.

This overlap between the sexual and economic outcasts is I think brought up really well in Nagle’s book. She’s talking about this guy Roger Devlin in this passage:

His essay “Sexual Utopia and Power” argues against “today’s sexual dystopia with its loose morals and confused sexual roles. It explores “ female hypergamy”, mating up, narcissism, infidelity, deceptiveness, and masochism.” It also argues that “the breakdown of monogamy results in promiscuity for the few, loneliness for the majority.” On this last point, I think he’s getting to the central issue driving this kind of reactionary sexual politics, perhaps even the central personal motivation behind the entire turn to the far right among young men. The sexual revolution that started the decline of lifelong marriage has produced great freedom from the shackles of loveless marriage and selfless duty to the family for both men and women, but this ever-extended adolescence has also brought with it the rise of adult childlessness and a steep sexual hierarchy. Sexual patterns that have emerged as a result of the decline of monogamy have seen a greater level of sexual toys for an elite of men and a growing celibacy among the large male population at the bottom of the pecking order. Their own anxiety and anger about their lower ranking status in this hierarchy is precisely what it’s produced that are hard line rhetoric about asserting hierarchy in the world politically when it comes to women and non-whites. The pain of relentless rejection has festered in these forums and allowed them to be the masters of the cruel natural hierarchies that bring them so much humiliation.

From Dataclysm by Christian Rudder

I thought of two things when I read this passage. The first was a graph from this book Dataclysm by Christian Rudder, who founded OkCupid. Rudder ultimately published this book that drew on a lot of different datasets but also used the OkCupid dataset. This is one of the analyses that was published in Dataclysm. OkCupid doesn’t work like this anymore as I understand, but for a long time when you came across another user, you would rate them from one to five stars. It’s probably good we don’t do that anymore. But that was how the platform operated. And I remember that if you were in a certain echelon of people, if you were in the top 10 percent or whatever-and I wasn’t-you would get this email that congratulated you on being the creme de la creme of attractiveness.

According to the data on OkCupid, women find men less attractive than men find women. And I think you can synthesize what Nagle is saying with this graph and just see this swelling discontent on incels.co and Braincels and TheRedPill as essentially the casualties of this new sexual system we’ve been developing since the 1970s where relations are reduced to this sexual marketplace.

The second thing I thought of was basically the entire worldview of Michel Houellebecq, whose ideas I see throughout Kill All Normies. His first big book has written a lot about how the sexual revolution, despite being associated with the left, was really the beginning of a cultural neoliberalism. Just as institutions under neoliberalism have dissolved and been taken over by market forces, our social relations are unmediated by anything other than sheer desire and you end up with nothing but a bazaar of flesh. One result of that is that you end up with a sexual underclass, the sexual counterpart to the NEET subreddit. Just as people are cast away from the job market through automation and are no longer able to adapt to the world necessary to survive economically, it’s more difficult for people to survive in this new sexual reality, especially one that provides a method of meeting people on digital platforms that optimize for quick, weak interactions that give little opportunity for anything other than physicality to dominate the selection process.

I think the bearing that reality has on people differs on geography. I don’t know how much Tinder is used in Laramie, Wyoming, where I used to live, but I can tell you that it is the dominant method by which people meet each other in places like New York City. And I have watched people move here and thrive or die in that system. I have seen people go in completely opposite directions. I can think of two people off the top of my head where this happened. One of them was a very attractive girl who moved to Brooklyn from Philadelphia, and I think Philadelphia is less dictated by this ruthless superficiality than New York, which makes a lot of sense when you think about New York as a neoliberal center, both economically and in this cultural sense we’re talking about. She moved here and became this #bestself member of this culture’s sexual elite who saw her own sexual activity as basically the extent of her politics.

The other friend also moved here from Philadelphia and I guess he split with his girlfriend or something, and I found these Red Pill leanings creeping into his personality. He believed women’s brains were hackable. He would call women ugly to me. It was really weird. And I would tell him not to do it and he would laugh and find it funny. To him, that was his own brand of 4chan offensiveness, he was upsetting these feminist sensibilities of the left. And what was so fucking bizarre was that he was kind of a leftist. He worked as an environmental organizer for years and lived in Brooklyn and had a bookshelf of Chuck Klosterman and whatever you would find on a Brooklyn bookshelf. But I think the longer he went without sex, the more hateful he became. He actually referred to himself as celibate. He would say, “Well I’m celibate now.” He never said incel. I had never heard that term actually until 2018, but he was the first person I ever knew who referred to himself as celibate.

My friendships with both these people reached their conclusions for these very reasons, which sucks because in New York City friendships don’t abound. But it was bizarre to see both of them sort of convert to this worldview.

EY: It seems like that worldview is being forced upon her, not her reinforcing the assumptions of that ideology.

AT: Absolutely, and that that’s my whole point. Even if incels engage in toxic behavior, they exist in a macro historical moment in which that behavior is cultivated. That absolutely goes for her, and it goes from my other friend as well. It goes for all of us, not to completely dismiss any kind of agency. And I think that this is an environment that I’m always relieved when people acknowledge, whether it be Houellebecq or Nagle.

EY: But then I think that things like the Amazon documentary The Red Pill and places like Rolling Stone and all of them almost never investigate this, the moment that we find ourselves in that is creating this kind of hyper-sexual frustration.

AT: And I will say as well, so not only have things been replaced by this sexual marketplace but we are daily inundated with images establishing what sort of conventional existence looks like. There’s a fascinating paper written by Jonah Peretti, who founded Buzzfeed, wrote when he was at MIT. It’s one of the most interesting papers I think I’ve ever read. His whole idea was that in this current stage of capitalism, we cycle through identities more quickly.

Lacan had this idea in his psychoanalysis of the Mirror Stage where the child develops a sense of itself when it looks at the mirror and it recognizes itself. But this phenomenon is extended past the mirror and into images. So we don’t just identify with the reflection of ourselves in the mirror. We then identify with the image on the movie screen or the TV screen or phone screen or whatever sort of picture is looking back at us. And that sort of becomes our sense of self as well.

Because we are so inundated with these images and these narratives, we find ways to reconcile our own sense of identity with this identity that is projected back at us from the screen. You can think of a child after they watch The Avengers, they go and they play Avengers with their friends and they think of themselves as Captain America and Black Widow and whatever. That’s kind of a very rudimentary, basic example of this idea of the child taking on the identify of the screen.

But we do this all the time as adults. So extrapolate that behavior to an adult looking at an image in a Chanel advertisement or watching The Social Network, and they see in the mirror a model or a machinating startup CEO. The adult takes on the identity of what is projected at them by the image and then reconciles their own sense of self with what they see projected at them.

With people like incels you have something of an identity crisis. Because of this new sexual marketplace, they are no longer able to close the gap between their own identities and the identities projected back at them in the image. And because of the endless white noise of media, they are drowning in reminders of their very inability to close that gap.

Something the activist-turned-red-piller/black-piller I mentioned observed that I think is very true is that we don’t really have an outlet for men who don’t have sex or can’t have sex for whatever reason. There’s no monastery for them to join. We have created this system where you are either getting laid or you’re not. Those are your two options. And if you can’t reconcile your identity with that of the image, which is almost universally an image of someone having lots of sex, we don’t really give you any other options. There’s no image of, like, a monk.

I sent you that Oneohtrix Point Never video, I don’t know if you watched it or not.

EY: Yes, I watched it. I watched the shit out of it.

AT: To me it captures everything about what we’re saying in the span of four minutes, for the most part. Mostly what it captures is that alienation and ostracization and totally unfulfillable desire among these people. That there’s just no way to satiate these urges, and the urges become ever more extreme the more time you spend feeding them and dwelling within them. Nagle’s quote about the unsatiated desire being the fundament of everything is captured in that OPN video better than anything I’ve seen.

EY: Yeah, I think so. And something interesting that stood out to me about that video is that there are no “real women” in that video, it’s all furries or hentai. It makes me wonder whether the phenomenon of incels loving hentai is almost self-fulfilling-they pine after these girls and they have these waifus and they love these things just because part of them knows that this isn’t an obtainable, real thing. It’s another form of comfort and safety for them. Like, “I can fall in love with a fictional character, and some part of me knows that will never happen, so committing to that is safe.”

AT: I also think that it’s a sign of just how alien an actual woman is to this culture. It is so distant and abstract that it becomes unreal. In a way, the furries and the hentai girls are both literal depictions of what these people end up desiring, but also these non literal representations of what women are to them.

I looked up “red pill” and I looked up “black pill” on /pol. People say “red pill” more, but it’s interesting to see the climb of “black pill” relative to “red pill” and I’m wondering in your research if you’ve gotten the sense that the use of term is becoming more pervasive. Do you think that it indicates a deeping nihilism among these people, or is this just kind of a coincidence?

EY: To me it just feels like “red pill” is becoming a normie concept now. It’s mainstream. So they’re kind of getting backed into a corner and I don’t know where you can go past the black pill. There’s no darker black pill.

AT: The jet black pill.

EY: Seeing this makes me feel like, Okay there is this kind of … “ I’m trying to reconcile this rejection of the mainstream with all the other things you’re talking to me about. Like because it is … because the mainstream idea is the sexual marketplace, right? And then there’s the belief that the sexual marketplace can’t benefit you. And so I feel like those ideas kind of intertwined like the wires get crossed or something. Where once an idea starts to become part of a more mainstream cultural consciousness it becomes almost, yuck, you’re deterred from it.

It reminds that I was talking to this guy who went viral talking about his alt-right journey and one word that he kept using I actually had never used before was “boomer”. He was saying like, “Oh they’re just a bunch of Boomers.” “That’s just Boomer talk.” And so that really had me thinking about the way all of this stuff is situated in their minds. It all can come back to that enemy outside thing. And so once the Red Pill ideas are hitting mainstream and if … let’s say Chads start building in TheRedPill, which they can do, the Red Pill isn’t an exclusively incel thing, then where do you go. You have to start moving into this space where it’s like, Okay this ideology is now not somewhere that I feel safe. I have to take it further. And that just gets encouraged by everyone else who feels the same way and feels like you have to be edgy. You have to be on the edge, you have to be on the fringe.

AT: I feel like that pertains to this other result here. I looked at all three-word combinations used in any /pol post with the words “red pill” and the one that stuck out to me was any terms that include the word “ultimate”. This idea of the ultimate red pill, and how commonly that factors into these discussions, this idea of like taking the final red pill. Nick Land’s decision to title his manifesto the “Dark Enlightenment” bears on this as well, I think, which implies a forbidden knowledge.

It makes me think of something Mark Fisher said in this essay he wrote about Joy Division. “The depressive is always confident of one thing: that he is without illusions.” There’s this idea that the more nihilistic the idea is, the truer is must be. And I wonder if that becomes its own reinforcement to people who traffic in the language of the red pill and the black pill and the dark enlightenment. That because it’s so nihilistic, it is this form of realism.

EY: It reminds me of how incels will go and they’ll post pictures of themselves to be rated on, fully expecting and knowing for a fact that all they’re going to get is criticism and reaffirmation that their worldview, that they’re never going to be able to fuck, is true. It’s almost comforting in the most depressing sense: Everything I’m thinking is right, I am worthless, I am terrible, any hope is illusion."
5 Minute Guide to Detecting Holidays in Python,"If you decide to dive deep into data analysis and want to, for example, analyze purchasing trends, you’ll probably want to construct a new attribute in your dataset having only two possible outcomes — 0 or 1 — is a holiday, isn’t a holiday.

Photo by Artem Sapegin on Unsplash

Take a moment to think about why this derived variable might be useful. You probably won’t uncover a greater volume of purchases on holidays itself, but how did the sales do in the week leading to the holiday?

A whole another animal, isn’t it?

Anyway, in today’s rather quick post I want to discuss one Python library I’ve found some time ago — holidays — yes, that’s the name. It allows you to get dates and names of major holidays for specific year(s) and for a specific country. You might be wondering now how many countries does it support, and the answer will positively surprise you — around 50-ish (with the ability to specify individual regions for some countries).

You can find the whole list here:

The goal of this article is to do some exploration of the library and showcase functions you’ll use most often. We’ll create a list of dates for the whole year of 2019, and from there declare another column which will have the value of 1 if there is (or was) a holiday on that date, and 0 otherwise.

Before jumping into code you’ll need to install the library, so fire up the terminal and execute the following:

pip install holidays

Is everything good? I mean it should, it’s only a pip install for Pete’s sake. Nevertheless, let’s do some real stuff now.

The Imports

Ass always, your Notebook should start with imports. Nothing special here, only Numpy, Pandas, Datetime, and yeah, newly installed Holidays library:

You probably won’t need Numpy to follow along, but I always like to have it imported.

Extracting Holidays"
How Much Data Engineering Does A Data Scientist Need To Know?,"“It is a capital mistake to theorize before one has data.” Sherlock Holmes

Data Engineering BEFORE Data Science

Data Science has become a huge buzzword in the last 5 years. Every company wants to do some of this famous Data Science and the topic remains one of the highest-priorities for companies turning to AI in 2019. However, in practice, the challenge proves to be very difficult and Data Science often remains a very distant goal that many never seem to reach.

The last few years have shown that Data Engineering, an essential step prior to Data Science & AI, remains a step that many companies totally miss, rush or ignore. In consequence, many Data Scientists end up doing the job of Data Engineers and barely have any time or opportunities to build valuable Data Science projects.

When it comes to separating the job of a Data Engineer & a Data Scientist, the line can be a little blurry. It can confuse companies into hiring the wrong person for the job, thereby frustrating their Data Scientists.

The Overlap between the two roles

Data Scientists & Data Engineers frequently have overlaps in their knowledge. Overlapping knowledge is good, since both need to be able to communicate and help each other. They should understand each other’s decision and each other’s tasks for the cooperation to be as smooth as possible.

While no one should expect for a Data Scientist to cover the job of a Data Engineer (nor the other way around!), some Data Engineering knowledge can be essential for Data Scientists.

Cloud Computing"
5 New Year’s Resolutions to Amp Up Your Programming Career,"January 1st is looming, along with the promises that come with the stroke of midnight. So many things to commit to starting or conversely stop. But sometimes our career goals are omitted from our new year resolutions. This is not very prudent since work is such an important part of our lives that should be in our best interest to become better at what we do for living.

We should carefully be considering where we want to be ten years down the line and plan 🅝🅞🅦 how to get there. The end of the decade is a great time to reflect on what has or has not worked for us, and to think about what we can do differently to achieve our goals in the next one.

So do take a breather from programming and before you immerse yourself completely in the same old grind, take a few moments to reflect and resolve. With that in mind, here are some resolution ideas and a few accompanied resources to set yourself on a new track.

⭐️ Pro Tip: As you approach this list, do not be vague. Think to yourself: ""What shall I do, by when"". You need to make a plan: As you approach this list, do not be vague. Think to yourself: ""What shall I do, by when"". You need to make a plan: SMART goals are specific, measurable, achievable, relevant and time bound.

Let’s go through the list together:

— 1: Learn the art of software crafting

It doesn’t take a huge amount of knowledge and skill to get a program working. Everyone can do it. Getting something to work is not a hard thing — getting it right is! Imagine the very first program you wrote when you took your first programming class. And compare it to now. See how far you have come?

Let me now introduce you to the magical world of design patterns, solid principles and software architecture paradigms to see how you can position yourself with the elect few that write elegant software that lasts, has high cohesion and loose coupling and most notably is easy to comprehend, extend and maintain.

“Architecture is about the important stuff. Whatever that is.” — Ralph Johnson

Here are a few all-time classics you should strive to read this year:"
100 Days of Artificial Intelligence,"100 Days of Artificial Intelligence

100 days ago I decided to write one article about artificial intelligence every day for 500 days. Since this is day 100, I am now a fifth of the way towards my goal. I have written more than I have ever done before, and been able to focus intently on one topic. It has very much been a humbling experience. The highlights of this goes far beyond writing and in this article I will mention the benefit of thinking aloud; a summary of a few threads followed in my writing; and of what I will write going forward. I have reached a milestone and I am proud that I managed to take this small step towards understanding artificial intelligence better.

Thinking aloud on a topic you care about

The immediate benefit that I thought about when writing regularly was becoming a better writer. Now that I have written regularly for 100 days I do in no way think that is the greatest benefit I have received. There is a Chinese saying that resonates with me in this regard.

请教别人一次是5分钟的傻子，从不请教别人是一辈子的傻子— Qǐngjiào biérén yīcì shì 5 fēnzhōng de shǎzi, cóng bù qǐngjiào biérén shì yībèizi de shǎzi

He who asks a question is a fool for five minutes; he who does not ask a question remains a fool forever.

I do not think however that one remains a fool forever — listening is important. Yet I feel less foolish for having started to ask such a wide range of questions to this technological development while learning programming alongside different subjects in the field of social science.

Thinking aloud while reading and doing has clear benefits I will outline a few that in my experience is contributing to a learning process:

Friends, professors and colleagues sending me links to articles related to the topic of my interest. Being asked critical questions in regards to your current thinking by a series of different people from different backgrounds. Approaching text can mean being approached by other writers in different capacities for conversations or writing to another audience. Challenging yourself can inspire others to express themselves through writing which may reveal wonderful sides you had yet to discover. Judging your writing in retrospective and present looking back on who you were in a given moment and what you are right now.

Wanting to become a better writer can lead to so much more than grammatical correctness or textual flourish. Writing is interpretation of who you are and a reading by other of what you think. It is as much as words can convey your intentions as it is connecting to the reader beyond the reading. I have been so lucky and fortunate to experience what writing can be beyond my immediate expectations and for that I am grateful.

Feeling artificially intelligent

It is hard to know where to start when you are faced with the two words ‘artificial intelligence’. I spent the first 50 days considering a general approach to artificial intelligence writing about everything I could find. The next 50 days I have spent focusing on AI Safety attempting to connect thoughts from this area to that of the climate crisis. Along the way I have without a doubt felt worthless or like an idiot several times over, at times in passing.

It is a process that is very likely to repeat itself if I continue exploring this field. I have to embrace not knowing to get to know what I do not know — or what I do not know in the unknown unknowns. Discovery within the lack of clarity makes it exciting every day when I write, uncertainty is in some sense a certainty. I do not have to grow fast as long as I continue growing.

不怕慢, 就怕停 — bù pà màn, jiù pà tíng

Be not afraid of growing slowly, be afraid of standing still.

Understanding takes time, and I am not done yet. I still have 400 days left, however beyond this lies a lifetime. I have met several researchers that have studied artificial intelligence almost since before I was born or has been programming actively for a lifetime. Academics and professionals I appreciate have imparted their learnings to me and I will do my best to carry this forward and contribute with everything I am to society. To give everything I can.

As such to summarise the two topics that I have explored now each in 50 days I would say the following.

Artificial intelligence (AI) is a loved and hated concept, an ambiguous suitcase word that is in no way close to being human or not human. A human is more than a series of programmed actions, yet we are also this. Questions are arising of sustainability in regards to artificial intelligence alongside new techniques in machine learning. The climate crisis can be worsened by irresponsible use of technology, particularly when large technology companies self-police or self-evaluate.

is a loved and hated concept, an ambiguous suitcase word that is in no way close to being human or not human. A human is more than a series of programmed actions, yet we are also this. Questions are arising of sustainability in regards to artificial intelligence alongside new techniques in machine learning. The climate crisis can be worsened by irresponsible use of technology, particularly when large technology companies self-police or self-evaluate. AI Safety has to go beyond cybersecurity and possible existential threats. It is additionally a focus on environmental consequences, externalities or fallout from the proliferation of machine learning techniques and growing amounts of data. We talk of safety in a narrow sense in the context of technology often without these perspectives nor governance. That change in dialogue is critical when some companies drift back from a ‘user’ to a ‘citizen’ focus. We have to develop new methods, regulations and practices to address these concerns in an appropriate manner.

It is strange to attempt reducing 100 articles down to two paragraphs, but I did my best and I hope it is worth something to you. You could read through all 100 articles, however I do not think most have the chance or time to do so. I am working on a small project to compile these learnings in a better format that should be announced soon. I ask kindly for your patience and assistance if you are willing to help in any way.

Where do I go or write from here?

I was telling myself I should do more programming every day and relate it to my writing. As such making the next 100 days about Python, one of the most well known programming languages makes sense to me. 100 days dedicated to machine learning for social scientists with qualitative as well as quantitative methods sounds like a fun challenge that I want to undertake.

100 days of Python and math does challenge my boundaries a lot, in fact pushing them. I am in no way comfortable navigating in Python, writing about Python or programming in Python. Therefore this seems to me to be one of the best possible ways to proceed. Exploring code repositories and looking through some Python code every single day for the next 100 days will not make me a great programmer, however it will make me a slightly better programmer.

If you in this context going forward feel disappointed with my writing and that my articles are not at the level you feel necessary I apologise. I would ask if perhaps it would be better with your help in making me understand and learn if you read this.

What is the most important when writing?

The most important when writing is: to not be too obsessed. I cannot say I live this line fully or that it is consistent with my current actions, as you may have guessed. Yet I think it is the most important aspect I can think of in this particular moment.

You can obsess over your mistakes, worrying that everyone will notice how terrible you are. You can obsess over your lack of knowledge on a topic, being almost sure that someone is talking about your writing unfavourably. You can obsess over what you should have written once a piece is finally published and go back again for endless revisions. All this is well and fine.

Most of all however you can obsess about writing instead of love.

This is the most devastating of all obsessions relating to writing. When you type away on your keyboard and you see someone you love wanting your attention. It is the gravest sin for a writer to ignore love.

To forget to be proud of the person you love and tell them how great they are through your actions is not okay. I am guilty on all these accounts, and it is something I have to change if I am to continue writing for another 100 or 400 days.

When reaching a milestone it is customary to thank someone, and there are plenty of people that has made this learning experience rewarding.

I want to first say a thank you to my wife who always understands and is patient beyond my understanding. I want to thank my family for supporting me through avid reading of my articles. I want to thank my fellow students for challenging my assumptions and my professors for questioning the foundations of my ideas. Lastly to my friends you give me the confidence to share and challenge myself: thank you.

This is the first 100 days of #500daysofAI.

100 days of artificial intelligence

1. Defining AI — #500daysofAI:

https://medium.com/@alexmoltzau/500-days-of-artificial-intelligence-1-191cb486921b?source=friends_link&sk=887e8744ed51e477458ae60d2859dbad

2. An Essay On Applied Artificial Intelligence and Sustainable Cities:

https://medium.com/@alexmoltzau/an-essay-on-applied-artificial-intelligence-and-sustainable-cities-463fae866d77?source=friends_link&sk=23a53c2db129bdda4f243bd1cee9188a

3. Some Current Issues In Funding of Artificial Intelligence:

https://medium.com/@alexmoltzau/some-current-issues-in-funding-of-ethical-artificial-intelligence-a7be9ca1a178?source=friends_link&sk=70ba1570388643fc8334f305d4ebd706

4. Patents In The Field of Artificial Intelligence:

https://medium.com/@alexmoltzau/patents-in-the-field-of-artificial-intelligence-2019-52db9b03abe6?source=friends_link&sk=effdefe9c85481794fedad27d053990e

5. AI for Good and AI for Bad — published in The Startup:

https://medium.com/swlh/ai-for-good-and-ai-for-bad-71627e3d7849?source=friends_link&sk=7ed4065fd7913db8b71354f3480f2b25

6. AI Governance in Argentina and Uruguay:

https://medium.com/swlh/ai-governance-in-argentina-and-uruguay-aeb68417db93?source=friends_link&sk=d352d3f641fcafc9b45649a9035cf15b

7. Three Writers In The Field of AI:

https://medium.com/@alexmoltzau/three-writers-in-the-field-of-ai-44f14c0e3402?source=friends_link&sk=3610e4980c85f65d70e791d5af082248

8. Scandinavian AI Strategies 2019 — published In Becoming Human:

https://medium.com/@alexmoltzau/scandinavian-ai-strategies-2019-16ecec9f17dc?source=friends_link&sk=46ecb92fa21edd4a3268037c7fd7b926

9. Social Scientists and AI — published in AI Social Research:

https://medium.com/ai-social-research/social-scientists-and-ai-1d9d97a5246?source=friends_link&sk=fb29f5178e3c4b34b7ec56082bfc3e47

10. 10 reflections on AI:

https://medium.com/@alexmoltzau/10-thoughts-on-artificial-intelligence-e94c6c533270?source=friends_link&sk=1d9868554318868dce78d1d7e973e58c

11. The Rise and Rise of Spacemaker AI — published in Towards Data Science:

https://towardsdatascience.com/the-rise-and-rise-of-spacemaker-ai-5c800a001caa?source=friends_link&sk=f820a79f1d7e5a3a9b6708099e31c363

12. Inequalities and AI — published in The Startup:

https://medium.com/@alexmoltzau/inequalities-and-ai-255c28a706d4?source=friends_link&sk=540641bd952b87c2f7b8672acc70ca0b

13. Artificial Intelligence and Religion:

https://medium.com/@alexmoltzau/artificial-intelligence-and-religion-7aebd77f95f1?source=friends_link&sk=e765eb857cc905410efa1884433db94d

14. AI Contextual Awareness Engine:

https://medium.com/@alexmoltzau/ai-contextual-awareness-engine-283cfbbf2280?source=friends_link&sk=64d482122c97895a68eb57f679935fab

15. A Noob Is Critiquing My Algorithm:

https://medium.com/@alexmoltzau/a-noob-is-critiquing-my-algorithm-7c0bf585758f?source=friends_link&sk=ce1165e59aec6f8665b246a8be177ee9

16. IRIS.AI Your Science Assistant — published in The Startup:

https://medium.com/swlh/iris-ai-your-science-assistant-60eefd3628ab?source=friends_link&sk=ade9119c43dd4634e1f10ca37efe4b15

17. Digital Insecurities:

https://medium.com/@alexmoltzau/ai-cybersecurity-blockchain-and-the-climate-crisis-a10880046cd?source=friends_link&sk=d1a7ff36975422c8aa05b0e1dd293496

18. Artificial Intelligence in Switzerland:

https://medium.com/@alexmoltzau/artificial-intelligence-in-switzerland-a2d6391b7c4?source=friends_link&sk=4f3de129c60eec480a6f632008524980

19. Water and Artificial Intelligence

https://medium.com/@alexmoltzau/water-and-artificial-intelligence-a7cf2ac23c17?source=friends_link&sk=bfb2c3e8c81a558023f3232e46473e74

20. Artificial Intelligence and Music:

https://medium.com/@alexmoltzau/artificial-intelligence-and-music-f3f3b1421529?source=friends_link&sk=bc218c8f48fb7ea3ca43962b5dcd3473

21. #500daysofAI after 20 days:

https://medium.com/@alexmoltzau/500daysofai-after-20-days-99daf69a8f02?source=friends_link&sk=afa3992d5e96befd4c318e3f6efe48c2

22. Everyday Life and Microprediction:

https://towardsdatascience.com/everyday-life-and-microprediction-f76c1721872a?source=friends_link&sk=a5ae0af21348e80b0cf482fc30856a73

23. Biased Bananas? 🍌

https://medium.com/zero-equals-false/biased-bananas-957f335ec2?source=friends_link&sk=2acd54689734e95dcd50022dc40beffe

24. AI and Nine Forms of Intelligence

https://towardsdatascience.com/ai-and-nine-forms-of-intelligence-5cf587547731?source=friends_link&sk=62970b5ed8c29279e1069657304c526b

25. Multimodal Elderly Care Systems Using Artificial Intelligence to Improve Quality of Life:

https://medium.com/@alexmoltzau/multimodal-elderly-care-systems-using-artificial-intelligence-to-improve-quality-of-life-992de2980787?source=friends_link&sk=ed622b591d2445fb0dbeb39fe9b4ebb7

26. Tales From Goodwin

https://medium.com/@alexmoltzau/tales-from-goodwind-d4403c1da48b?source=friends_link&sk=7b59cf627b8cabc3df0476cd09285c23

27. The Centre for Artificial Intelligence Research (CAIR)

https://medium.com/@alexmoltzau/the-centre-for-artificial-intelligence-research-cair-5c1f4a0a440d?source=friends_link&sk=33e3b01d9d180a1de8f2e0b605c09968

28. The Qualitative Data Scientist

https://towardsdatascience.com/the-qualitative-data-scientist-e0eb1fb1ceb9?source=friends_link&sk=816b5c2278239abd478346ec5f3023af

29. AI in Physical Product Design

https://medium.com/@alexmoltzau/ai-in-physical-product-design-e67c02a8c2c1?source=friends_link&sk=9fcad1f438c0bfdcb082622786925e37

30. Very Artificial Poetry

https://medium.com/@alexmoltzau/very-artificial-poetry-1e690f8f097?source=friends_link&sk=c36e9f4e1f226d966b918b97913b1222

31. 30 Days of Artificial Intelligence

https://medium.com/@alexmoltzau/30-days-of-artificial-intelligence-96b56b1966e6?source=friends_link&sk=2ed9253d507be094357dd4c0c07b3fe5

32. AI and The Imposter Syndrome

https://towardsdatascience.com/ai-and-the-imposter-syndrome-bf66e1daf8d8?source=friends_link&sk=8e168fe5d71b939d5f0ac43557883623

33. The New Chinese Startup Smart Speaker Startup Maybe

https://medium.com/@alexmoltzau/the-new-chinese-smart-speaker-startup-maybe-3b290d6cc5c3?source=friends_link&sk=80389fe70dfa8b0e500b724c80f892e7

34. Artificial Intelligence of Things

https://medium.com/@alexmoltzau/artificial-intelligence-of-things-97278f810d8a?source=friends_link&sk=592b357a75d3e76dc0c0adcc2fe81760

35. Databaiting

https://towardsdatascience.com/databaiting-d26cad4c49ca?source=friends_link&sk=ddc5a5ed46c4947fb1010e10e50c112d

36. AI as a Service

https://towardsdatascience.com/ai-as-a-service-b465ddc0c7e0?source=friends_link&sk=cd53b7a877301333d4f7671540c8e910

37. Digital Violence and AI

https://medium.com/@alexmoltzau/digital-violence-and-ai-8300dee795c9?source=friends_link&sk=bca08914176abbe5c8c983cc408c1486

38. Stop in The Name of AI!

https://medium.com/towards-artificial-intelligence/stop-in-the-name-of-ai-490b1cda26b?source=friends_link&sk=dbf7cdd8b35da8b06a3fd9b31762ed37

39. Google AI and Developments in Semi-Supervised Learning

https://towardsdatascience.com/google-ai-and-developments-in-semi-supervised-learning-5b1a4ad29d67?source=friends_link&sk=ddc7b1d106980109b8699b187ee39191

40. Advancements in Semi-Supervised Learning with Unsupervised Augmented Data

https://towardsdatascience.com/advancements-in-semi-supervised-learning-with-unsupervised-data-augmentation-fc1fc0be3182?source=friends_link&sk=cbac033d5f98b5ff54daf4547a3d0ace

41. How is the Quiet Revolution in Semi-Supervised Learning Changing the Industry?

https://towardsdatascience.com/how-is-the-quiet-revolution-in-semi-supervised-learning-changing-the-industry-4a25f211ce1f?source=friends_link&sk=5d489e00953626ac206eceee8ac3c5fc

42. 40 days along with 500days of AI

https://medium.com/@alexmoltzau/40-days-along-with-500daysofai-a05167e9ac69?source=friends_link&sk=364ef2e465d53cf076c0ba9fb4079c64

43. Saving Wildlife with Epigram AI

https://medium.com/zero-equals-false/saving-wildlife-with-epigram-ai-32db125e4e22?source=friends_link&sk=08d4ab5ab4c951519e8d7fa5519f01f9

44. Google Federated Learning and AI

https://towardsdatascience.com/google-federated-learning-and-ai-64c6e4e4e22f?source=friends_link&sk=45176a2f883275f562669a7931190cf1

45. Artificial Intelligence and Pension

https://medium.com/swlh/artificial-intelligence-and-pension-f94c6f4915ee?source=friends_link&sk=2dca013ae1ee10728ab3b6771e0d0a8f

46. Databaiting with FaceApp

https://medium.com/zero-equals-false/databaiting-with-faceapp-1d810089f61d?source=friends_link&sk=e034427c03a753dfdd2b88fbf03877d4

47. Towards Social Data Science

https://towardsdatascience.com/towards-social-data-science-f90c5c020855?source=friends_link&sk=2c86ddf5dcdef6f055843d3f01771bed

48. Facebook vs. EU Artificial Intelligence and Data Politics

https://towardsdatascience.com/facebook-vs-eu-artificial-intelligence-and-data-politics-8ab5ba4abe40?source=friends_link&sk=756bbc8cf12d7bab4c83f6b802abdf9e

49. Artificial Intelligence and Fairness

https://medium.com/@alexmoltzau/artificial-intelligence-and-fairness-75c2490e8d57?source=friends_link&sk=32fc4151778edb664eb79dd57e712118

50. Far-Right Extremism and AI

https://medium.com/@alexmoltzau/far-right-extremism-and-ai-ef93d55c6dee?source=friends_link&sk=64f67ba00148ddfd563ad14af37d57c8

51. Towards Artificial General Intelligence

https://medium.com/dataseries/openai-or-closedai-fae7bdd0fcff?source=friends_link&sk=4b572783e9ff49add07e4aef3ded34c3

52. Debating the AI Safety Debate

https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d?source=friends_link&sk=55e9229db0464a53c5b27d715c79b1fa

53. How does Facebook define Terrorism in relation to Artificial Intelligence:

https://towardsdatascience.com/artificial-intelligence-and-terrorism-in-social-media-cf166adaf78e?source=friends_link&sk=2bd40d725ee8f99a91181272bf20cfde

54. Avoiding Side Effects and Reward Hacking in Artificial Intelligence

https://towardsdatascience.com/avoiding-side-effects-and-reward-hacking-in-artificial-intelligence-18c28161190f?source=friends_link&sk=b71f8915dc595fe44cbdeeb2f7d0a735

55. AI Safety and the Climate Crisis

https://medium.com/swlh/ai-safety-and-the-climate-crisis-dd232af145a2?source=friends_link&sk=6f039ba378b8269cad8276f3cf7858f0

56. Mitigation of Climate Change with Machine Learning

https://towardsdatascience.com/mitigation-of-climate-change-with-machine-learning-197f09c00fac?source=friends_link&sk=22c6f39c3fbf7fa34a12ab91fbb42cb7

57. Adaptation to Climate Change with Machine Learning

https://medium.com/@alexmoltzau/adaption-to-climate-change-with-machine-learning-47421cd04387?source=friends_link&sk=91aed82ad9c85320c179633ef900184c

58. AI Safety and Social Data Science

https://towardsdatascience.com/ai-safety-and-social-data-science-527c2c576a98?source=friends_link&sk=23c6f786de9571012aeec56d9adcff74

59. Artificial Intelligence and Norwegian Gender Quotas

https://medium.com/swlh/artificial-intelligence-and-norwegian-gender-quotas-11d8d9bf114a?source=friends_link&sk=1217743d41cbfc1dd4efad588143769a

60. AI + Safety with DNV-GL

https://towardsdatascience.com/ai-safety-with-dnv-gl-826500a401a7?source=friends_link&sk=a5d372109d613bae39abc3a4486e4248

61. Artificial Intelligence & The Bystander

https://medium.com/@alexmoltzau/artificial-intelligence-the-bystander-8185ed9367b7?source=friends_link&sk=770916644ec3feb299e2512ac1852a20

62. Does AI Safety Consider Job Safety or Not?

https://becominghuman.ai/does-ai-safety-consider-job-safety-or-not-f21af695824e?source=friends_link&sk=e5e9a69612afea433e31571bce0ba412

63. Center for AI Safety at Stanford University

https://medium.com/dataseries/center-for-ai-safety-at-stanford-university-fce999f12bc8?source=friends_link&sk=00bf743b0aed34d57b3c9829be33125c

64. Very Artificial Poetry #2

https://medium.com/@alexmoltzau/very-artificial-poetry-2-f0f2fde5450a?source=friends_link&sk=6fc0be6733f236ab6179c4acc0032d99

65. Artificial Intelligence Ethics vs. World Domination

https://towardsdatascience.com/artificial-intelligence-ethics-vs-world-domination-7cf2a5734151?source=friends_link&sk=43d1f42dc31c89d3b4410c3a3a0de271

66. Artificial Intelligence and Recent Billion Dollar Investments 2019

https://medium.com/dataseries/artificial-intelligence-and-recent-billion-dollar-investments-2019-759e78b042ad?source=friends_link&sk=698504869969768dd65cec8bc8cf8306

67. Artificial Intelligence and Trust

https://becominghuman.ai/artificial-intelligence-and-trust-e2bc1ac548?source=friends_link&sk=301a1d6d95f8660f48b8d9e576d55d73

68. Safe Artificial General Intelligence

https://towardsdatascience.com/safe-artificial-general-intelligence-29cb4ad0814e?source=friends_link&sk=3827586b6571324e19f63177fd5022fb

69. The Top Myths About Advanced AI

https://medium.com/@alexmoltzau/the-top-myths-about-advanced-ai-9961eb1f6987?source=friends_link&sk=7fc705f4096addfc059513067dffe383

70. AI Safety and Designing to Understand What Humans Really Want

https://medium.com/@alexmoltzau/ai-safety-and-designing-to-understand-what-humans-want-adcd3455d0c0?source=friends_link&sk=905acf0832aa754280abc3c47a9b277b

71. 70 days of Artificial Intelligence

https://medium.com/@alexmoltzau/70-days-of-artificial-intelligence-d2522f209ec7?source=friends_link&sk=caa3f87d22504c7d37b466dd147a19e1

72. Artificial Intelligence and the UNDP

https://towardsdatascience.com/artificial-intelligence-and-the-undp-779b50de9acd?source=friends_link&sk=15b4720fb4210392b6e79aab48fac223

73. Elements of AI

https://medium.com/@alexmoltzau/elements-of-ai-b68ffb60526a?source=friends_link&sk=391dee2b6b14ce1e2751dbb664c40a64

74. AI Safety — How Do You Prevent Adversarial Attacks

https://towardsdatascience.com/ai-safety-how-do-you-prevent-adversarial-attacks-ede17480a24d?source=friends_link&sk=6412985e912cb5e33a407490d342991a

75. How Safe is a Solution Within the Field of AI

https://medium.com/ai-social-research/how-safe-is-a-solution-within-the-field-of-artificial-intelligence-16d32ae45886?source=friends_link&sk=6d3fec62c29bb188baf42b98974c3d43

76. AI Safety and LawTech

https://medium.com/dataseries/artificial-intelligence-and-lawtech-4d7181a03202?source=friends_link&sk=034ca8e79398847bf9cac61c2fe53f46

77. Artificial Intelligence and Biotechnology

https://medium.com/datadriveninvestor/artificial-intelligence-and-biotechnology-c787c02f4d62?source=friends_link&sk=3f276dab183c40aa2087cc3e558b85e9

78. Evaluating Artificial Intelligence

https://medium.com/@alexmoltzau/evaluating-artificial-intelligence-5ca22e9f9bdb?source=friends_link&sk=671d0f39ee1fa7f1dfedde982ed0a714

79. AI and Collective Action

https://towardsdatascience.com/ai-and-collective-action-ce2c15632911?source=friends_link&sk=19a68a623fb02b3acd04f8758ff9ea26

80. AI Safety in the Serengeti

https://medium.com/zero-equals-false/ai-safety-in-the-serengeti-8a2c9dcf174a?source=friends_link&sk=20af45a84ab409407cd5d9587dc937af

81. United Kingdom and Artificial Intelligence

https://medium.com/@alexmoltzau/united-kingdom-and-artificial-intelligence-1a97ad37ac17?source=friends_link&sk=cfd651518c8d9ba43c6e30300dfb8cd5

82. 80 days of Artificial Intelligence

https://medium.com/@alexmoltzau/80-days-of-artificial-intelligence-3dc55749d6e6?source=friends_link&sk=00cbfcc1225b2b8e8ce88d891d604681

83. DeepMind co-founder’s Leave of absence

https://towardsdatascience.com/deepmind-co-founders-leave-of-absence-59730259f2a4?source=friends_link&sk=e25ddb5e8e9b928418b8ee077320fc9f

84. Artificial Intelligence and Being Sustainable

https://medium.com/@alexmoltzau/artificial-intelligence-and-being-sustainable-c963e10e8a49?source=friends_link&sk=378ac80312cc6b6894b288419dd48968

85. AI Safety and Intellectual Debt

https://medium.com/datadriveninvestor/ai-safety-and-intellectual-debt-f3c6380bd0a8?source=friends_link&sk=494787995fd43de6dbb0a7a5ecadb811

86. Should Every Computer Science Degree Require a Course in Cyber Security?

https://medium.com/@alexmoltzau/should-every-computer-science-degree-require-a-course-in-cyber-security-b79ec4948e69?source=friends_link&sk=32bf4bc918e63379176f812d7d4df1e0

87. Elon Musk and Jack Ma’s Debate at World Artificial Intelligence Conference in Shanghai August 2019:

https://medium.com/@alexmoltzau/elon-musk-and-jack-mas-debate-at-world-artificial-intelligence-conference-in-shanghai-august-2019-c1d8970fac05?source=friends_link&sk=3329262f86e5a152aa2722c369acf90e

88. Exploring the Partnership on AI:

https://medium.com/@alexmoltzau/exploring-the-partnership-on-ai-9495ff845a39?source=friends_link&sk=a3b87883bb0975978bc3a8615c85dc73

89. Artificial Intelligence and Nonprofits:

https://towardsdatascience.com/artificial-intelligence-and-nonprofit-e6cdaaae166f?source=friends_link&sk=3d8b684c139ebb42f342bd49334cd92d

90. Artificial Intelligence in Participatory Planning:

https://medium.com/ai-social-research/artificial-intelligence-in-participatory-planning-5934c8779e93?source=friends_link&sk=b911da08ce85f969b39438a076b5bd49

91. Speculative Execution of Code:

https://medium.com/@alexmoltzau/speculative-execution-of-code-fd06c20bbb8?source=friends_link&sk=72243ab0cd7bc39e79808afb4a6a3ea9

92. Artificial Intelligence and Forest Management:

https://medium.com/odscjournal/artificial-intelligence-and-forest-management-50f480b56325?source=friends_link&sk=a50d1af97210032739f34c318d9711f2

93. What is Transfer Learning:

https://medium.com/@alexmoltzau/what-is-transfer-learning-6ebb03be77ee?source=friends_link&sk=fc1af22d1877d7f566f93d8c8cfb149e

94. In Between Social Science and Computer Science:

https://medium.com/ai-social-research/in-between-social-science-and-computer-science-532383de9f73?source=friends_link&sk=c865dbfe1529745e1cbda964dc266c01

95. Very Artificial Poetry #3:

https://medium.com/@alexmoltzau/very-artificial-poetry-4-8f87b6cfc408?source=friends_link&sk=81a062cb3bad63f5a55a826282909388

96. Why digital twin?

https://medium.com/@alexmoltzau/why-digital-twin-4f5796795a0d?source=friends_link&sk=7012f0eb5db69d4e632b45933f7b5dc3

97. Google’s AI for Social Good:

https://medium.com/@alexmoltzau/googles-ai-for-social-good-ab88b5bcf1b3?source=friends_link&sk=d6c8d2fc35760fc42045b98d352ce106

98. Artificial Intelligence and Understanding Time:

https://medium.com/swlh/artificial-intelligence-and-understanding-time-735711777072?source=friends_link&sk=65e48365c00041e4ce368e139edd1d8d

99. Artificial Intelligence and Data Citizenship:

https://medium.com/@alexmoltzau/artificial-intelligence-and-data-citizenship-a1602bd3b31c?source=friends_link&sk=b861805177ac6b06275d926a4e16ee7a

100. 100 days of Artificial Intelligence (today)"
Using Naive Bayes to dig into “81% of ‘suspects’ flagged by Met’s police facial recognition technology innocent”,"Sky News: London

Photo by Daniel Lincoln on Unsplash

Recently I wrote “The incidence of the disease in the population is a critical variable which many people overlook” in my Naive Bayes post.

Today when browsing the news I happened across Sky News’ article and noted they’d fallen into this exact trap.

Researchers found that the controversial system is 81% inaccurate

The incidence of the suspects in the crowd is a critical variable which Sky News has overlooked

Why is this? Let’s try varying the the incidence of suspects in the crowd to see why it is so important.

System 1: Randomly tosses a coin to assess guilt, accuses 50%

System 2: Error rate of 1 in 1,000 Crowd A: 32,000 suspects; 0 innocent people

Crowd B: 0 suspects; 32,000 innocent people

Which system is better?

System 1 on Crowd A - 16000 suspects; 100% accurate!

System 2 on Crowd B - 32 mistakes; 100% inaccurate!

The incidence of suspects in the crowd makes a huge difference. Sky News ignores crowd composition measuring only accuracy so by this methodology System 1’s coin tosses win out.

So let’s try bringing the theory from my previous naive Bayes post into this real world example.

First we need to start with this quote:

They found that, of 42 matches, only eight were verified as correct — an error rate of 81%. Four of the 42 were people who were never found because they were absorbed into the crowd, so a match could not be verified.

The math is simple:

But “Four of the 42 were people who were never found” so I don’t know why they are counted. We don’t know whether all four were incorrect, correct or some mix. It seems to me that those examples need to be discarded, leaving us with 8 matches out of 38 or 79% incorrect, but I digress.

We need more details on the system performance. In the article we can find a police force error estimate of 1 in 1000:

The force maintains its technology only makes a mistake in one in 1,000 cases — but it uses a different measurement to arrive at this conclusion.

Since they’ve quoted a single number; false negative/positive are not broken out so we’ll assume it’s the same for both. Now what are these numbers in terms of Bayes? Despite my disagreement I’ll use their 19% or 0.19:

TP = True Positive = 0.999

FP = False Negative = 0.001 P(B ∣ A) P(A)

P(A ∣ B) = ──────────────

P(B)

A = Suspect

B = Positive Neoface match P(A ∣ B) = Probability a person is a suspect given a match = 0.19

P(B ∣ A) = Probability of a match given a suspect = TP = 0.999

P(A) = Probability of a person in the crowd being a criminal

P(B) = Probability of a Neoface match = FP × (1-P(A)) + TP × P(A)

Plug in the formula and the values, solve for P(A) and working it out with a pencil (definitely not using Wolfram Alpha) you get:

19/80938

Or ~1 in 4000. Is this a reasonable estimate for the number of wanted suspects in a random crowd? The UK prison population is ~1 in 1000. That puts the number of suspects known to the police on the same order of magnitude as prisoners, but 4 times fewer. Seems reasonable.

Let’s do a sanity check on the numbers for a crowd of 32,000 people, we’ve estimated 1 in 4000 is a suspect and the system is (claimed) 99.9% reliable in detecting them. So 8 of 8 suspects detected. It is also (claimed) 99.9% reliable in rejecting non-suspects so of the 31,992 non-suspects 32 would be wrongly detected. Let’s expand our earlier examples:

System 1: Randomly tosses a coin to assess guilt, accuses 50%

System 2: Error rate of 1 in 1,000 Crowd A: 32,000 suspects; 0 innocent people

Crowd B: 0 suspects; 32,000 innocent people

Crowd C: 8 suspects; 31,992 innocent people System 1 on Crowd A - 16000 suspects; 100% accurate!

System 2 on Crowd B - 32 mistakes; 100% inaccurate!

System 2 on Crowd C - 8 suspects, 32 mistakes; 80% inaccurate!

We’ve come round a full circle and gotten back to the headline number of 80% inaccurate. So the following things can be true simultaneously:

The system has a false negative and false positive rate of 0.1% In a crowd of 32000 people 32 of 40 or 80% would be wrongly flagged

The above two things can be true if suspects occur at a rate of 1 in 4000 in a random crowd.

The incidence of the suspects in the crowd is a critical variable"
Variable types and examples,"Quantitative

A quantitative variable is a variable that reflects a notion of magnitude, that is, if the values it can take are numbers. A quantitative variable represents thus a measure and is numerical.

Quantitative variables are divided into two types: discrete and continuous. The difference is explained in the following two sections.

Discrete

Quantitative discrete variables are variables for which the values it can take are countable and have a finite number of possibilities. The values are often (but not always) integers. Here are some examples of discrete variables:

Number of children per family

Number of students in a class

Number of citizens of a country

Even if it would take a long time to count the citizens of a large country, it is still technically doable. Moreover, for all examples, the number of possibilities is finite. Whatever the number of children in a family, it will never be 3.58 or 7.912 so the number of possibilities is a finite number and thus countable.

Continuous

On the other hand, quantitative continuous variables are variables for which the values are not countable and have an infinite number of possibilities. For example:

Age

Weight

Height

For simplicity, we usually referred to years, kilograms (or pounds) and centimeters (or feet and inches) for age, weight and height respectively. However, a 28-year-old man could actually be 28 years, 7 months, 16 days, 3 hours, 4 minutes, 5 seconds, 31 milliseconds, 9 nanoseconds old.

For all measurements, we usually stop at a standard level of granularity, but nothing (except our measurement tools) prevents us from going deeper…"
Sports Analytics: an exploratory analysis of international football matches-Part 2,"In my previous article (Part 1 of this series), I’ve been implementing some interesting visualization tools for a meaningful exploratory analysis. Then, with the Python package Streamlit, I made them interactive in the form of a web app.

In this article, I’m going to continue working on the same dataset as before, this time focusing on the interaction between two teams. I will keep using Plotly as visualization tool, since it provides the possibility to interact with graphs and collect relevant information. Since I won’t attach the code of my previous article, if you are new to Streamlit I strongly recommend to read it before starting.

Now, as anticipated, I want to dwell on the matches between two teams of interest. So, let’s start by filtering our initial dataset (available here) with users’ multiselection:

import streamlit as st

import pandas as pd

import numpy as np

import plotly.express as px

import seaborn as sns

import matplotlib.pyplot as plt

import plotly.graph_objects as go

from plotly.subplots import make_subplots st.title('Internationa Football matches')

df = pd.read_csv(""results.csv"") st.subheader('Comparing 2 teams')

teams_to_compare = st.multiselect('Pick your teams', df['home_team'].unique()) comparison = df[(df['home_team'].isin(teams)) & (df['away_team'].isin(teams)) ]

comparison = comparison.reset_index(drop=True)

st.write(comparison)

st.write('Number of matches: ', len(comparison))

The object ‘teams_to_compare’ will be a list of two teams, and I’m interested in analyzing those matches where the two teams played one against the other (regardless of which one was playing at home). Then, I asked my app to show me the new filtered dataset together with the number of matches:

Here, I’m interested in all the matches England vs Scotland, and this is how my final dataset looks like.

Now let’s perform some analytics on these two teams."
How To Build a Model on SageMaker,"Data science projects tend to end at reports of accuracy and sleek plots. In this post we are going to look into the next step: How to create a model that is ready for deployment.

For this purpose we are going to use Amazon SageMaker and break down the steps to go from experimentation to production readiness. We will follow a high level approach, which means AWS will pick some parameters for us.

Before moving on, make sure you have a AWS account and access to a Jupyter Notebook.

Set Up a Notebook Instance

Photo by ASHLEY EDWARDS on Unsplash

The first step is logging into the Amazon console and look for SageMaker in case it’s not visible.

Next we click on create notebook instance.

We name the notebook and the pricing for running it is determined by the instance type. Here we use ml.t2.medium however it is recommended to see the pricing page for the best pick. For this application we can keep the elastic inference as None.

We to set the role, which is a kind of security certificate and determines permissions such as what resources the notebook has access to. We click on it and select Create a new role

Since there are no additional buckets we want our notebook to access we select None and click on create role. The most important condition here is the second line.

Finally we click on crate notebook instance at the very bottom of the page. After a few moments the status of your notebook should show InService. As soon as that happen AWS will charge us for using the instance, so make sure to turn it off when not in use.

Cloning the Deployment Notebook

From the picture above click on Open Jupyter to be redirected to a familiar setup.

On the top right click on New and open a Terminal and change directories. We then clone this git repository that contains the notebook and close it when done.

Back in the home tab we see that a sagemaker directory was added. We navigate to where the notebook is inside the sagemaker_ml folder.

Download the Data

The IMDB dataset can be like a rite of passage for data scientists in general, and NLP practitioners specifically. We are going to use this data to predict user sentiment with XGBoost.

The first step is downloading the data. We can make use of command line code within the notebook:

The GNU documentation provides ample resources for commands like the one above.

Prepare the Data

We will not go through this in detail here, and only provide a brief overview. However feel free to check the full process in the notebook.

This is a NLP exercise so we need to process the raw information into data and corresponding labels. We then strip the text of any html tags, perform stemming with NLTK, and extract a bag of words.

At the end of this processing, we should have our testing and training data ready.

Classify with XGBoost

XGBoost clasifier requires that the dataset bet written to a file and stored using Amazon S3. We further split the trainig dataset in two parts: training and validation.

We will write those datasets to a file and upload the files to S3. Furthermore we will do the same with the test set input upload it to S3. This is so that we can use SageMaker’s Batch Transform functionality to test the model once fitting is done.

The documentation for the XGBoost algorithm in SageMaker requires that the saved datasets should contain no headers or index and that for the training and validation data, the label should occur first for each sample.

At this point it is good practice to save up on memory available to us and we can set text_X, train_X, val_X, train_y and val_y to None:

Upload Training Validation Files to S3

For this part we will draw heavily on the SageMaker API documentation and the SageMaker Developer Guide.

the upload_data method uploads local file or directory to S3. It is a member of object representing our current SageMaker session. This method uploads the data to the default bucket, created for us by AWS if it doesn’t exist already, into the path described by the key_prefix variable. If we navigate to the S3 console, we should find our files there.

Create XGBoost Model

We consider a model on SageMaker to be three components:

Model Artifacts

Training Code (Container)

Inference Code (Container)

The Model Artifacts are the actual model itself. For this case the artifacts are the trees created during training.

The Training Code and the Inference Code are used to manipulate the training artifacts. The training code uses the training data that is provided plus the created model artifacts, and the inference code uses the model artifacts to make predictions on new data.

SageMaker runs the training and inference codes by making use of docker containers, a way to package code and ensure that dependencies are not an issue.

Fit XGBoost

Fitting the model is done by accessing the S3 input.

When a model is fit using SageMaker, the process is as follows.

A compute instance (a server somewhere) is started up with the properties that we specified.

When the compute instance is ready, the code, in the form of a container, that is used to fit the model is loaded and executed.

When this code is executed, it is provided access to the training (and possibly validation) data stored on S3.

Once the compute instance has finished fitting the model, the resulting model artifacts are stored on S3 and the compute instance is shut down.

Test

We use batch transform to perform inference on a large dataset in a way that is not realtime. This allows us to see how well our model performs.

The advantage of this is that we don’t need to use the model’s results immediately, instead we can perform inference on a large number of samples. The method is also useful in that we can perform inference on the entire testing set.

To perform the transform job we need to specify the type of data we are sending that it is serialized correclty in the background. Here we are providing the model with csv data so we specify text/csv.

In addition, if the data is too large to process all at once then we need to specify how the data file should be split it. Again this is a csv file, therefore each line is a single entry, we tell SageMaker to split the input on each line.

With the code above, the transform is running in the background. We call the wait method to wait until the transform job is done and receive some feedback.

The transform job is executed and the estimated sentiment of each review has been saved on S3. We want to work on this file locally and copy it to the data directory data_dir.

A convenient way to do is inside jupyter is found in the AWS CLI command reference.

Finally, we can read the output from the model.

We need to convert the output into something more usable for our purposes. We convert the sentiment to be 1 for positive and 0 for negative. Finally we can print out the accuracy: 86%, not bad!

Cleanup

Photo by Paweł Czerwiński on Unsplash

As we perform operations on larger and larger data, keeping track of how much memory we use becomes essential. We might run out of memory while performing operations and/or incur costly expenses.

The default notebook instance on SageMaker might not have a lot of excess disk space. As we repeat exercises similar to this one, we might eventually fill up the alloted disk space, leading to erros which can be difficult to diagnose.

Once we are done with a notebok, it is good practie to remove the files we created along the way. We can do this from the terminal or from the notebook hub.

After we are done, be sure to return to the SageMaker Notebook Instances and stop the instance.

Summary & Next Steps

In this post we saw how we can create a model on AWS SageMaker that is ready for deployment. The workflow should be the same as a typical machine learning exercise with some additional steps.

The main takeaway is to keep an eye on where and how the data is being stored, what decisions are being made on our behalf, and how to keep memory from overflowing.

Congratulations! We now have a sentiment analysis model ready for deployment!"
What I Learned From Interviewing With Top Data Science Teams — Tips for Aspiring Data Scientists,"What I Learned From Interviewing With Top Data Science Teams — Tips for Aspiring Data Scientists Nelson Griffiths · Follow Published in Towards Data Science · 6 min read · Nov 1, 2019 -- Share

What to be prepared for at your next interview

Photo by Daniel McCullough on Unsplash

Over the course of the last year, I spent a decent amount of time looking for a job in data science. I was lucky enough to interview with a few companies with incredible data science teams . The interview processes were stressful, but at the same time enlightening. I learned a lot of things from these interviews that can be helpful no matter where you are interviewing for a data science role.

What I am not going to cover in this article, is how to get interviews at amazing companies. If you are struggling to get interviews, go check out this article I wrote about gaining experience before your first job. Once you get an interview, the real fun begins.

Have a Breadth of Knowledge

For most entry level jobs, you are not expected to be an expert in any specific field of data science (unless you are specifically applying for an NLP position or something similar). You are expected to know a lot of things across the realm of data science however. Almost everywhere I interviewed, the first interview was a technical one. I was asked questions on topics ranging from p-values to random forests to time efficient programming and everything in-between. What companies don’t want is someone who knows how to plug and play with scikit-learn and nothing else, so make sure you have a strong fundamental understanding of the key areas of data science. If you don’t know where to start, these lectures on GitHub cover a lot of the basics. There are plenty more resources similar to this one that work just as well. A good test for if you are “interview ready” on a subject is if you can answer these three questions:

Do I know what _______? Can I explain intuitively how _______ works? If given an unlimited amount of time, could I code _______ from scratch based on my current understanding?

If you can answer yes to the first question for most topics, you won’t crash and burn, but you also won’t blow socks off. If you can answer yes to the second question, you are in a good place and will probably nail the interview question. If you can answer yes to…"
Kaplan Meier curves: an introduction,"Kaplan-Meier curves are widely used in clinical and fundamental research, but there are some important pitfalls to keep in mind when making or interpreting them. In this short post, I’m going to give a basic overview of how data is represented on the Kaplan Meier plot.

The Kaplan-Meier estimator is used to estimate the survival function. The visual representation of this function is usually called the Kaplan-Meier curve, and it shows what the probability of an event (for example, survival) is at a certain time interval. If the sample size is large enough, the curve should approach the true survival function for the population under investigation.

It usually compared two groups in a study (like a group that got treatment A vs a group that got treatment B).

Treatment B seems to be doing better than treatment A (median survival time of +/- 47 months vs 30 months with a significant p-value). In this post, I only explore treatment arm A and won’t be comparing two groups versus each other.

Basic Kaplan Meier plot

Let’s start by creating some basic data. We have 10 patients participating in a study (so called “at risk”), with a follow-up of 10 months. Every participant gets an identical treatment.

Cohort without censored data

If we take a closer look at the ‘Follow-up’ and ‘Eventtype’ columns:

The follow-up time can be any time-interval: minutes, days, months, years.

An event type of 1 equals an event. An typical event in a cancer trial can be death, but Kaplan-Meier curves can also be used in other types of studies. Ann, for example, participated in this fictional study for a new cancer drug but died at after 4 months.

An event type of 0 equals a right-censored event.

To keep it simple, there are no censored events in this first example.

The study starts. Every month, one participant experiences an event. Every time an event occurs, the survival probability drops by 10% of the remaining curve (= number of events divided by number at risk) until it reaches zero at the end of the study.

Kaplan Meier plot with censored data

Let’s add some censored data to the previous graph.

Observations are called censored when the information about their survival time is incomplete; the most commonly encountered form is right censoring (as opposed to left and interval censoring, not discussed here). A patient who does not experience the event of interest for the duration of the study is said to be “right censored”. The survival time for this person is considered to be at least as long as the duration of the study. Another example of right censoring is when a person drops out of the study before the end of the study observation time and did not experience the event. In other words, censored data is a type of missing data.

Ann, Mary and Elizabeth left the study before it was completed. Kate did not have an event at the end of the study. The curve is already looking very different compared to the “stairs” pattern from before.

Cohort with censored data (Ann, Mary, Elizabeth and Kate). Note that Andy experienced an event at 6.2 months instead of 7 months in the example above (and was not censored).

Now what is the relationship between events, censoring and the drops on the Kaplan Meier curve?

If we take a look at the first participant that has an event (John), we see that after 1 month we have a drop of 0.1, or 10% of the remaining height:

If we wait a little bit longer, we can see that at month 5, there are 6 patients at risk remaining. Two have had an event and two more have been censored. At the next event, the curve drops 16% of the remaining height (instead of 10% at the start of the study), because less people are at risk:

This goes on until the end of the study period, or until the number of patients at risk reaches 0. The last drop is the largest. At this last drop, the curve drops 50% of the remaining height (or 20% of the total height). Yet still only 1 person experiences an event, the same as at the start of the study (when the drop was only 10% of the remaining (=total) height). This is because only 2 people are at risk at this point in the study.

Importance of confidence intervals

Especially when there are very few patients at risk, the impact of a censored event can have a big impact on the appearance of the KM curve.

In the previous plot, it seems that the survival curve reaches a plateau at 20% survival probability. If we would swap the censored status between Joe and Kate (participants 9 and 10), the KM curve changes drastically and drops to 0 at the end of the study period. In this scenario (curve B), all participants either had an event or were censored.

The event type for Joe and Kate are reversed in scenario B

In other words, only one events marks the difference between the survival curve reaching 0 or reaching a plateau staying stable at 20%.

We can also see this is if we plot the 95% confidence intervals on the KM curve. The confidence intervals are very wide, giving a clue that the study contains very few participants. Furthermore, the 95% CI increases when more time elapses, because the number of censored individuals increases.

Exclude censored data: yes or no?

Small dataset

We can simulate the best case scenario (censoring is equal to no events) and the worst case scenario (censoring is equal to events) and compare this to the actual curve.

The first 3 observations for every scenario (best, worst and actual)

In the best case scenario, the curve stops at 40% survival probability at the end of the study, while in the worst case scenario the curve drops to 0. The median survival times are also very different:

Actual curve: 6.2 months

Best case: 8.1 months

Worst case: 5.5 months

Large dataset

This is even more striking if we increase the sample size. In the simulation, the sample size has increased from 10 to 100, with a follow-up time of 48 months. In this simulation, 40% of the individuals are censored (at random) somewhere between month 0 and month 48. Again, this shows that the median survival time can be substantially different.

Conclusions"
The Key Question For All Data Scientists: “So What?”,"Data scientists can be precious of their work. This isn’t a criticism — the characteristic is normally born of healthy passion. And good data scientists are an intellectually curious bunch (I’ve done plenty of analysis for its own sake in my time!)

However, in a professional context analysis is rarely an end goal in and of itself. Even the world’s great analytical powerhouses — your Facebooks, Googles, and Amazons — crunch numbers to somehow improve an aspect of their business.

In my previous life as a Consultant, my analysis was supposed to inform a business decision, or help form the basis of a client recommendation. Hence the catchphrase, “So What?” I soon learned to self-assess my work. The difference between two customer cohorts is statistically significant. So what? Can my client do anything useful with this information?

Having spent five years in Management Consultancy, conflating things that are ‘interesting’ with things that are ‘important’ is a mistake that I’ve seen all too often. I’d gently suggest that less than half of all McKinsey consultants who’ve used the phrases ‘Advanced Analytics’ or ‘Machine Learning’ in front of their clients properly know what these things are, and fewer still would know how they’d actually help to solve the business problem at hand.

Seeing The Wood For The Trees"
How To Use Deep Learning Even with Small Data,"How To Use Deep Learning Even with Small Data

You’ve heard the news — deep learning is the hottest thing since sliced bread. It promises to solve your most complicated problems for the small price of an enormous amount of data. The only problem is you are not working at Google nor Facebook and data are scarce. So what are you to do? Can you still leverage the power of deep learning or are you out of luck? Let’s take a look at how you might be able to leverage deep learning even with limited data and why I think this might be one of the most exciting areas of future research.

Start Simple

Before we discuss methods for leveraging deep learning for your limited data, please step back from the neural networks and build a simple baseline. It usually doesn’t take long to experiment with a few traditional models such as a random forest. This will help you gauge any potential lift from deep learning and provide a ton of insight into the tradeoffs, for your problem, of deep learning versus other methods.

Get More Data

This might sound ridiculous, but have you actually considered whether you can gather more data? I’m amazed by how often I suggest this to companies, and they look at me like I am crazy. Yes — it is okay to invest time and money into gathering more data. In fact, this can often be your best option. For example, maybe you are trying to classify rare bird species and have very limited data. You will almost certainly have an easier time solving this problem by just labeling more data. Not sure how much data you need to gather? Try plotting learning curves as you add additional data and look at the change in model performance.

Fine-Tuning

Photo by Drew Patrick Miller on Unsplash

Okay. Let’s assume you now have a simple baseline model and that gathering more data is either not possible or too expensive. The most tried and true method at this point is to leverage pre-trained models and then fine-tune them for your problem.

The basic idea with fine-tuning is to take a very large data set which is hopefully somewhat similar…"
Keeping Data Science Simple,"Data Science is a field filled with fancy sounding things. Concepts both simple and complex get cool names, and let you make claims like being “Powered by AI”. While this isn’t necessarily a problem, it can mislead aspiring Data Scientists. Like any field, fancy names and complex concepts get much of the attention. This can help give the impression that cutting edge modeling is where the party is at. However, Data Science isn’t all about who’s got the most convolutional neural networks or the deepest learning. Crazy AI skills may prove valuable in certain situations, but Data Science is about picking the right tool for the job and using it effectively to solve real world problems. That last part, solving real world problems, should always the ultimate goal. Consistently hitting that goal is the foundation of a Data Science career.

To help form that foundation, there are three simple steps to follow,

Focus on solving real world problems, while measuring your solution. Pick the right tool for the job, which is probably the simplest. Repeat consistently.

Keep it simple

The right tool for the job is often the simplest one, at least at first. Complex models break, their behaviour can be hard to develop an intuition for, and implementing them is time consuming. Focus on simplicity and you’ll start more projects, which will themselves probably have a higher success rate. When it comes to starting a career in Data Science, having a track record of providing real world value will give you a tremendous boost. An education in machine learning, statistics, or programming will provide you with an essential base of skills, but proving you can apply those skills to real world problems and communicate your solution effectively is far more valuable.

Starting simple projects and seeing them through to the end will help you build that track record. A project can start as simple as a SQL query, so try making a list of possible projects in and out of your company and having a go at each. Generating and measuring value should take precedent over almost everything. It’s important to remember that how you measure your solution depends on what problem you’re trying to solve. If I’m trying to create a mortgage approval model, I probably care more about correctly screening out fraudsters than mistakenly rejecting those with good intentions. How I measure my solution should reflect those priorities. This approach isn’t just the best way to build a portfolio; it’s a vital part of any Data Science work. Talk to stakeholders, get to the root of a problem, and find the best way to measure the value your solution provides.

Summary

If you’re looking to break into Data Science, find real world problems and aim to make a dent in them as quickly as you can. The simplest method will help you finish more of what you start, and get you to a result faster. It’s important to build a solid base of skills, but being able to show what impact you’ve had with your skills is far more valuable than how exciting they sound."
7 Machine Learning lessons that stuck with me this year,"Member-only story 7 Machine Learning lessons that stuck with me this year

I’ve been a student of Machine Learning for the past two years, but this past year was when I finally got to apply what I learned and solidify my understanding of it. So I decided to share 7 lessons I learned during my “first” year of Machine Learning and hopefully make this article an annual tradition.

1. Data is Queen

Think about the data before you get started

Nowadays, it is relatively easy to learn about Machine Learning thanks to the vast selection of learning resources that exist online. Unfortunately, many of them tend to gloss over the data collection and cleaning steps.

During my first serious Machine learning project, my team and I run into the BIG question of where do we get our data from? We needed thousands of images of animals and after giving up on the idea of scraping the web, we came across the CIFAR-100 dataset. But just when we thought all of our problems where solved, we realized that we needed to pre-process our data (remove background, scale down, turn into vectors etc.) before feeding it into our Deep Learning model. This turned out to be the most time consuming part of our project (aside from training and tweaking our model over night for about a week).

The data part of any Machine Learning project shouldn’t be overlooked, “where is the data coming from?” should be one of the first questions you ask before you get started.

2. Domain Knowledge Matters

Domain knowledge can sometimes matter just as much as technical skills

It is easy to get caught up on the idea that you only need technical skills to solve problems using Machine Learning. The reality is that you’ll have a hard time getting very far if you only think of the problem in front of you in terms of just numbers and algorithms. For once, it can be difficult to come up with project ideas in a domain that you don’t know much about. But even if you manage to come up with a great idea, domain knowledge can be extremely helpful when deciding what kind of data you need to collect and the type of features you should use when…"
Dive Really Deep into YOLO v3: A Beginner’s Guide,"Screenshot from a video made by Joseph Redmon on Youtube

We are a team of experts to help your business adopt AI solutions or build new AI products. Contact us at info@imaginaire.ai or visit our website https://www.imaginaire.ai

For full source code, please go to https://github.com/ethanyanjiali/deep-vision/tree/master/YOLO/tensorflow. I really appreciate your STAR that supports my efforts.

Foreword

When a self-driving car runs on a road, how does it know where are other vehicles in the camera image? When an AI radiologist reading an X-ray, how does it know where the lesion (abnormal tissue) is? Today, I will walk through this fascinating algorithm, which can identify the category of the given image, and also locate the region of interest. There’s plenty of algorithms introduced in recent years to address object detection in a deep learning approach, such as R-CNN, Faster-RCNN, and Single Shot Detector. Among those, I’m most interested in a model called YOLO — You Only Look Once. This model attracts me so much, not only because of its funny name, but also some practical design that truly makes sense for me. In 2018, this latest V3 of this model had been released, and it achieved many new State of the Art performance. Because I’ve programmed some GANs and image classification networks before, and also Joseph Redmon described it in the paper in a really easy-going way, I thought this detector would just be another stack of CNN and FC layers that just works well magically.

But I was wrong.

Perhaps it’s because I’m just dumber than usual engineers, I found it really difficult for me to translate this model from the paper to actual code. And even when I managed to do that in a couple of weeks (I gave up once put it away for a few weeks), I found it even more difficult for me to make it work. There’re so quite a few blogs, GitHub repos about YOLO V3, but most of them just gave a very high-level overview of the architecture, and somehow they just succeed. Even worse, the paper itself is too chill that it fails to provide many crucial details of implementation, and I have to read the author’s original C implementation (when is the last time did I write C? Maybe at college?) to confirm some of my guesses. When there’s a bug, I usually have no idea why it would occur. Then I end up manually debugging it step by step and calculating those formulas with my little calculator.

Fortunately, I didn’t give up this time and finally made it work. But in the meantime, I also felt really strongly that there should be a more thorough guide out there on the internet to help dumb people like me to understand every detail of this system. After all, if one single detail is wrong, the whole system would go south quickly. And I’m sure that if I don’t write these down, I would forget all these in few weeks too. So, here I am, presenting you this “Dive Really Deep into YOLO V3: A Beginner’s Guide”. I hope you’ll like it.

Prerequisite

Before getting into the network itself, I’ll need to clarify with some prerequisites first. As a reader, you are expected to:

1. Understand the basics of Convolutional Neural Network and Deep Learning

2. Understand the idea of object detection task

3. Have curiosity about how the algorithm works internally

If you need help on first two items, there’re plenty of excellent resources like Udacity Computer Vision Nanodegree, Cousera Deep Learning Specialization and Stanford CS231n

If you just want to build something to detect some object with your custom dataset quickly, check out this Tensorflow Object Detection API

YOLO V3

YOLO V3 is an improvement over previous YOLO detection networks. Compared to prior versions, it features multi-scale detection, stronger feature extractor network, and some changes in the loss function. As a result, this network can now detect many more targets from big to small. And, of course, just like other single-shot detectors, YOLO V3 also runs quite fast and makes real-time inference possible on GPU devices. Well, as a beginner to object detection, you might not have a clear image of what do they mean here. But you will gradually understand them later in my post. For now, just remember that YOLO V3 is one of the best models in terms of real-time object detection as of 2019.

Network Architecture

Diagram by myself

First of all, let’s talk about how this network look like at a high-level diagram (Although, the network architecture is the least time-consuming part of implementation). The whole system can be divided into two major components: Feature Extractor and Detector; both are multi-scale. When a new image comes in, it goes through the feature extractor first so that we can obtain feature embeddings at three (or more) different scales. Then, these features are feed into three (or more) branches of the detector to get bounding boxes and class information.

Darknet-53

The feature extractor YOLO V3 uses is called Darknet-53. You might be familiar with the previous Darknet version from YOLO V1, where there’re only 19 layers. But that was like a few years ago, and the image classification network has progressed a lot from merely deep stacks of layers. ResNet brought the idea of skip connections to help the activations to propagate through deeper layers without gradient diminishing. Darknet-53 borrows this idea and successfully extends the network from 19 to 53 layers, as we can see from the following diagram.

Diagram from YOLOv3: An Incremental Improvement

This is very easy to understand. Consider layers in each rectangle as a residual block. The whole network is a chain of multiple blocks with some strides 2 Conv layers in between to reduce dimension. Inside the block, there’s just a bottleneck structure (1x1 followed by 3x3) plus a skip connection. If the goal is to do multi-class classification as ImageNet does, an average pooling and a 1000 ways fully connected layers plus softmax activation will be added.

However, in the case of object detection, we won’t include this classification head. Instead, we are going to append a “detection” head to this feature extractor. And since YOLO V3 is designed to be a multi-scaled detector, we also need features from multiple scales. Therefore, features from last three residual blocks are all used in the later detection. In the diagram below, I’m assuming the input is 416x416, so three scale vectors would be 52x52, 26x26, and 13x13. Please note that if the input size is different, the output size will differ too.

Diagram by myself

Multi-scale Detector

Once we have three features vectors, we can now feed them into the detector. But how should we structure this detector? Unfortunately, the author didn’t bother to explain this part this his paper. But we could still take a look at the source code he published on Github. Through this config file, multiple 1x1 and 3x3 Conv layers are used before a final 1x1 Conv layer to form the final output. For medium and small scale, it also concatenates features from the previous scale. By doing so, small scale detection can also benefit from the result of large scale detection.

Diagram by myself

Assuming the input image is (416, 416, 3), the final output of the detectors will be in shape of [(52, 52, 3, (4 + 1 + num_classes)), (26, 26, 3, (4 + 1 + num_classes)), (13, 13, 3, (4 + 1 + num_classes))]. The three items in the list represent detections for three scales. But what do the cells in this 52x52x3x(4+1+num_classes) matrix mean? Good questions. This brings us to the most important notion in pre-2019 object detection algorithm: anchor box (prior box).

Anchor Box

The goal of object detection is to get a bounding box and its class. Bounding box usually represents in a normalized xmin, ymin, xmax, ymax format. For example, 0.5 xmin and 0.5 ymin mean the top left corner of the box is in the middle of the image. Intuitively, if we want to get a numeric value like 0.5, we are facing a regression problem. We may as well just have the network predict for values and use Mean Square Error to compare with the ground truth. However, due to the large variance of scale and aspect ratio of boxes, researchers found that it’s really hard for the network to converge if we just use this “brute force” way to get a bounding box. Hence, in Faster-RCNN paper, the idea of an anchor box is proposed.

Anchor box is a prior box that could have different pre-defined aspect ratios. These aspect ratios are determined before training by running K-means on the entire dataset. But where does the box anchor to? We need to introduce a new notion called the grid. In the “ancient” year of 2013, algorithms detect objects by using a window to slide through the entire image and running image classification on each window. However, this is so inefficient that researchers proposed to use Conv net to calculate the whole image all in once (technically, only when your run convolution kernels in parallel.) Since the convolution outputs a square matrix of feature values (like 13x13, 26x26, and 52x52 in YOLO), we define this matrix as a “grid” and assign anchor boxes to each cell of the grid. In other words, anchor boxes anchor to the grid cells, and they share the same centroid. And once we defined those anchors, we can determine how much does the ground truth box overlap with the anchor box and pick the one with the best IOU and couple them together. I guess you can also claim that the ground truth box anchors to this anchor box. In our later training, instead of predicting coordinates from the wild west, we can now predict offsets to these bounding boxes. This works because our ground truth box should look like the anchor box we pick, and only subtle adjustment is needed, whhich gives us a great head start in training.

Diagram by myself

In YOLO v3, we have three anchor boxes per grid cell. And we have three scales of grids. Therefore, we will have 52x52x3, 26x26x3 and 13x13x3 anchor boxes for each scale. For each anchor box, we need to predict 3 things:

1. The location offset against the anchor box: tx, ty, tw, th. This has 4 values.

2. The objectness score to indicate if this box contains an object. This has 1 value.

3. The class probabilities to tell us which class this box belongs to. This has num_classes values.

In total, we are predicting 4 + 1 + num_classes values for one anchor box, and that’s why our network outputs a matrix in shape of 52x52x3x(4+1+num_classes) as I mentioned before. tx, ty, tw, th isn’t the real coordinates of the bounding box. It’s just the relative offsets compared with a particular anchor box. I’ll explain these three predictions more in the Loss Function section after.

Anchor box not only makes the detector implementation much harder and much error-prone, but also introduced an extra step before training if you want the best result. So, personally, I hate it very much and feel like this anchor box idea is more a hack than a real solution. In 2018 and 2019, researchers start to question the need for anchor box. Papers like CornerNet, Object as Points, and FCOS all discussed the possibility of training an object detector from scratch without the help of an anchor box.

Loss Function

With the final detection output, we can calculate the loss against the ground truth labels now. The loss function consists of four parts (or five, if you split noobj and obj): centroid (xy) loss, width and height (wh) loss, objectness (obj and noobj) loss and classification loss. When putting together, the formula is like this:

Loss = Lambda_Coord * Sum(Mean_Square_Error((tx, ty), (tx’, ty’) * obj_mask)

+ Lambda_Coord * Sum(Mean_Square_Error((tw, th), (tw’, th’) * obj_mask)

+ Sum(Binary_Cross_Entropy(obj, obj’) * obj_mask) + Lambda_Noobj * Sum(Binary_Cross_Entropy(obj, obj’) * (1 -obj_mask) * ignore_mask)

+ Sum(Binary_Cross_Entropy(class, class’))

It looks intimidating but let me break them down and explain one by one.

xy_loss = Lambda_Coord * Sum(Mean_Square_Error((tx, ty), (tx’, ty’)) * obj_mask)

The first part is the loss for bounding box centroid. tx and ty is the relative centroid location from the ground truth. tx’ and ty’ is the centroid prediction from the detector directly. The smaller this loss is, the closer the centroids of prediction and ground truth are. Since this is a regression problem, we use mean square error here. Besides, if there’s no object from the ground truth for certain cells, we don’t need to include the loss of that cell into the final loss. Therefore we also multiple by obj_mask here. obj_mask is either 1 or 0, which indicates if there’s an object or not. In fact, we could just use obj as obj_mask, obj is the objectness score that I will cover later. One thing to note is that we need to do some calculation on ground truth to get this tx and ty. So, let’s see how to get this value first. As the author says in the paper:

bx = sigmoid(tx) + Cx

by = sigmoid(ty) + Cy

Here bx and by are the absolute values that we usually use as centroid location. For example, bx = 0.5, by = 0.5 means that the centroid of this box is the center of the entire image. However, since we are going to compute centroid off the anchor, our network is actually predicting centroid relative the top-left corner of the grid cell. Why grid cell? Because each anchor box is bounded to a grid cell, they share the same centroid. So the difference to grid cell can represent the difference to anchor box. In the formula above, sigmoid(tx) and sigmoid(ty) are the centroid location relative to the grid cell. For instance, sigmoid(tx) = 0.5 and sigmoid(ty) = 0.5 means the centroid is the center of the current grid cell (but not the entire image). Cx and Cy represents the absolute location of the top-left corner of the current grid cell. So if the grid cell is the one in the SECOND row and SECOND column of a grid 13x13, then Cx = 1 and Cy = 1. And if we add this grid cell location with relative centroid location, we will have the absolute centroid location bx = 0.5 + 1 and by = 0.5 + 1. Certainly, the author won’t bother to tell you that you also need to normalize this by dividing by the grid size, so the true bx would be 1.5/13 = 0.115. Ok, now that we understand the above formula, we just need to invert it so that we can get tx from bx in order to translate our original ground truth into the target label. Lastly, Lambda_Coord is the weight that Joe introduced in YOLO v1 paper. This is to put more emphasis on localization instead of classification. The value he suggested is 5.

Diagram from YOLOv3: An Incremental Improvement

wh_loss = Lambda_Coord * Sum(Mean_Square_Error((tw, th), (tw’, th’)) * obj_mask)

The next one is the width and height loss. Again, the author says:

bw = exp(tw) * pw

bh = exp(th) * ph

Here bw and bh are still the absolute width and height to the whole image. pw and ph are the width and height of the prior box (aka. anchor box, why there’re so many names). We take e^(tw) here because tw could be a negative number, but width won’t be negative in real world. So this exp() will make it positive. And we multiply by prior box width pw and ph because the prediction exp(tw) is based off the anchor box. So this multiplication gives us real width. Same thing for height. Similarly, we can inverse the formula above to translate bw and bh to tx and th when we calculate the loss.

obj_loss = Sum(Binary_Cross_Entropy(obj, obj’) * obj_mask) noobj_loss = Lambda_Noobj * Sum(Binary_Cross_Entropy(obj, obj’) * (1 — obj_mask) * ignore_mask)

The third and fourth items are objectness and non-objectness score loss. Objectness indicates how likely is there an object in the current cell. Unlike YOLO v2, we will use binary cross-entropy instead of mean square error here. In the ground truth, objectness is always 1 for the cell that contains an object, and 0 for the cell that doesn’t contain any object. By measuring this obj_loss, we can gradually teach the network to detect a region of interest. In the meantime, we don’t want the network to cheat by proposing objects everywhere. Hence, we need noobj_loss to penalize those false positive proposals. We get false positives by masking prediciton with 1-obj_mask. The `ignore_mask` is used to make sure we only penalize when the current box doesn’t have much overlap with the ground truth box. If there is, we tend to be softer because it’s actually quite close to the answer. As we can see from the paper, “If the bounding box prior is not the best but does overlap a ground truth object by more than some threshold we ignore the prediction.” Since there are way too many noobj than obj in our ground truth, we also need this Lambda_Noobj = 0.5 to make sure the network won’t be dominated by cells that don’t have objects.

class_loss = Sum(Binary_Cross_Entropy(class, class’) * obj_mask)

The last loss is classification loss. If there’re 80 classes in total, the class and class’ will be the one-hot encoding vector that has 80 values. In YOLO v3, it’s changed to do multi-label classification instead of multi-class classification. Why? Because some dataset may contains labels that are hierarchical or related, eg woman and person. So each output cell could have more than 1 class to be true. Correspondingly, we also apply binary cross-entropy for each class one by one and sum them up because they are not mutually exclusive. And like we did to other losses, we also multiply by this obj_mask so that we only count those cells that have a ground truth object.

To fully understand how this loss works, I suggest you manually walk through them with a real network prediction and ground truth. Calculating the loss by your calculator (or tf.math) can really help you to catch all the nitty-gritty details. And I did that by myself, which helped me find lots of bugs. After all, the devil is in the detail.

Implementation

If I stop writing here, my post will just be like another “YOLO v3 Review” somewhere on the web. Once you digest the general idea of YOLO v3 from the previous section, we are now ready to go explore the remaining 90% of our YOLO v3 journey: Implementation.

Framework

At the end of September, Google finally released TensorFlow 2.0.0. This is a fascinating milestone for TF. Nevertheless, new design doesn’t necessarily mean less pain for developers. I’ve been playing around TF 2 since very early of 2019 because I always wanted to write TensorFlow code in the way I did for PyTorch. If it’s not because of TensorFlow’s powerful production suite like TF Serving, TF lite, and TF Board, etc., I guess many developers will not choose TF for new projects. Hence, if you don’t have a strong demand for production deployment, I would suggest you implement YOLO v3 in PyTorch or even MXNet. However, if you made your mind to stick with TensorFlow, please continue reading.

TensorFlow 2 officially made eager mode a first-tier citizen. To put it simply, instead of using TensorFlow specific APIs to calculate in a graph, you can now leverage native Python code to run the graph in a dynamic mode. No more graph compilation and much easier debugging and control flow. In the case where performance is more important, a handy tf.function decorator is also provided to help compile the code into a static graph. But, the reality is, eager mode and tf.function are still buggy or not well documented sometimes, which makes your life even harder in a complicated system like YOLO v3. Also, Keras model isn’t quite flexible, while the custom training loop is still quite experimental. Therefore, the best strategy for you to write YOLO v3 in TF 2 is to start with a minimum working template first, and gradually add more logic to this shell. By doing so, we can fail early and fix the bug before it hides too deeply in a giant nested graph.

Dataset

Aside from the framework to choose, the most important thing for successful training is the dataset. In the paper, the author used MSCOCO dataset to validate his idea. Indeed, this is a great dataset, and we should aim for a good accuracy on this benchmark dataset for our model. However, a big dataset like this could also hide some bugs in your code. For example, if the loss is not dropping, how do you know if it just needs more time to converge, or your loss function is wrong? Even with GPU, the training is still not fast enough for you to quickly iterate and fix things. Therefore, I recommend you to build a development set which contains tens of images to make sure your code looks “working” first. Another option is to use VOC 2007 dataset, which only has 2500 training images. To use MSCOCO or VOC2007 dataset and create TF Records, you could refer to my helper scripts here: MSCOCO, VOC2007

Preprocessing

Preprocessing stands for the operations to translate raw data into a proper input format of the network. For the image classification task, we usually just need to resize the image, and one-hot encode the label. But things are a bit more complicated for YOLO v3. Remember I said the output of the network is like 52x52x3x(4+1+num_classes) and has three different scales? Since we need to calculate the delta between ground truth and prediction, we also need to format our ground truth into such a matrix first.

For each ground truth bounding box, we need to pick the best scale and anchor for it. For example, a tiny kite in the sky should be in the small scale (52x52). And if the kite is more like a square in the image, we should also pick the most square-shaped anchor in that scale. In YOLO v3, the author provides 9 anchors for 3 scales. All we need to do is to choose the one that matches our ground truth box the most. When I implement this, I thought I need the coordinates of the anchor box as well to calculate IOU. In fact, you don’t need to. Since we just want to know which anchor fits our ground truth box best, we can just assume all anchors and the ground truth box share the same centroid. And with this assumption, the degree of matching would be the overlapping area, which can be calculated by min width * min height.

During the transformation, one could also add some data augmentation to increase the variety of training set virtually. For example, typical augmentation includes random flipping, random cropping, and random translating. However, these augmentations won’t block you from training a working detector, so I won’t cover much about this advanced topic.

Training

After all these discussions, you finally have a chance to run “python train.py” and start your model training. And this is also when you meet most of your bugs. You could refer to my training script here when you are blocked. Meanwhile, I want to provide some tips that are helpful for my own training.

NaN Loss

Check your learning rate and make sure it’s not too high to explode your gradient. Check for 0 in binary cross-entropy because ln(0) is not a number. You can clip the value from (epsilon, 1 — epsilon). Find an example and walk through your loss step by step. Find out which part of your loss goes to NaN. For example, if width/height loss went to NaN, it could be because the way you calculate from tw to bw is wrong.

Loss remains high

Try to increase your learning rate to see if it can drop faster. Mine starts at 0.01. But I’ve seen 1e-4 and 1e-5 works too. Visualize your preprocessed ground truth to see if it makes sense. One problem I had before is that my output grid is in [y][x] instead of [x][y], but my ground truth is reversed. Again, manually walk through your loss with a real example. I had a mistake of calculating cross-entropy between objectness and class probabilities. My loss also remains around 40 after 50 epochs of MSCOCO. However, the result isn’t that bad. Double-check the coordinates format throughout your code. YOLO requires xywh (centroid x, centroid y, width and height), but most of dataset comes as x1y1x2y2 (xmin, ymin, xmax, ymax). Double-check your network architecture. Don’t get misled by the diagram from a post called “A Closer Look at YOLOv3 — CyberAILab”. tf.keras.losses.binary_crossentropy isn’t the sum of binary cross-entropy you need.

Loss is low, but the prediction is off

Adjusting lambda_coord or lambda_noobj to the loss based on your observation. If you are traininig on your own dataset, and the dataset is relative small (< 30k images), you should intialize weights from a COCO pretrained model first. Double-check your non max suppression code and adjust some threshold (I’ll talk about NMS later). Make sure your obj_mask in the loss function isn’t mistakenly taking out necessary elements. Again and again, your loss function. When calculating loss, it uses relative xywh in a cell (also called tx, ty, tw, th). When calculating ignore mask and IOU, it uses absolute xywh in the whole image, though. Don’t mix them up.

Loss os low, but there’s no prediction

If you are using a custom dataset, please check the distribution of your ground truth boxes first. The amount and quality of the boxes could really affect what the network learn (or cheat) to do. Predict on your training set to see if your model can overfit on the training set at least.

Multi-GPU training

Since the object detection network has so many parameters to train, it’s always better to have more computing power. However, TensorFlow 2.0 doesn’t have great support over multi-GPU training so far. To do that in TF, you’ll need to pick a training strategy like MirroredStrategy, as I did here. Then wrap your dataset loader into a distributed version too. One caveat for distributed training is that the loss coming out of each batch should be divided by the global batch size because we are going to `reduce_sum` over all GPU results. For example, if the local batch size is 8, and there’re 8 GPUs, your batch loss should divide a global batch size of 64. Once you summed up losses from all replica, the final result will be the average loss of a single example.

Postprocessing

The final component in this detection system is a post-processor. Usually, postprocessing is just about trivial things like replacing machine-readable class id with human-readable class text. In object detection, though, we have one more crucial step to do to get final human-readable results. This is called non maximum suppression.

Let’s recall our objectness loss. When is false proposal has great overlap with ground truth, we won’t penalize it with noobj_loss. This encourages the network to predict close results so that we can train it more easily. Also, although not used in YOLO, when the sliding window approach is used, multiple windows could predict the same object. In order to eliminate these duplicate results, smart researchers designed an algorithm called non maximum supression (NMS).

Photo by Python Lessons from Analytics Vidhya

The idea of NMS is quite simple. Find out the detection box with the best confidence first, add it to the final result, and then eliminates all other boxes which have IOU over a certain threshold with this best box. Next, you choose another box with the best confidence in the remaining boxes and do the same thing over and over until nothing is left. In the code, since TensorFlow needs explicit shape most of the time, we will usually define a max number of detection and stop early if that number is reached. In YOLO v3, our classification is not mutually exclusive anymore, and one detection could have more than one true class. However, some existing NMS code doesn’t take that into consideration, so be careful when you use them.

Conclusion

YOLO v3 is a masterpiece in the rising era of Artificial Intelligence, and also an excellent summary of Convolution Neural Network techniques and tricks in the 2010s. Although there’re many turn-key solutions like Detectron out there to simplify the process of making a detector, a hands-on experience in coding such sophisticated detector is really a great learning opportunity for machine learning engineers because merely reading the paper is far from enough. Like Ray Dalio said about his philosophy:

Pain plus reflection equals progress.

I hope my article could be a lighthouse in your painful journey of implementing YOLO v3, and perhaps you can also share the delightful progress with us later. If you like my article or my source code of YOLO v3, please ⭐star⭐ my repo and that will be the biggest support for me.

References"
Russian Open Speech To Text (STT/ASR) Dataset,"If you do not pay the iron price, you know someone paid it for you. It works like this in every aspect of life

Originally posted on spark-in.me on May 1, 2019

TLDR

This is an accompanying post for our release of Russian Open Speech To Text (STT/ASR) Dataset. This is meant to be a bit light-hearted and tongue in cheek. All opinions are my own, and probably opinions of my colleagues differ. This is a non-technical summary. Do not take this too seriously, probably 50% of this text is some kind of subtle joke (ping me if you find all the Easter eggs!).

Anyway, here is the dataset:

Dataset composition

TLDR:

We have collected and published a dataset with 4,000+ hours to train speech-to-text models in Russian;

to train speech-to-text models in Russian; The data is very diverse, cross domain, the quality of annotation ranges from good enough to almost perfect. Our intention was to collect a dataset that would somehow relate to real-life / business applications. Collecting only pure / clean data in academic fashion is of little interest. Ideally this dataset is a first step on a path to a repo with pre-trained STT models;

to real-life / business applications. Collecting only pure / clean data in academic fashion is of little interest. Ideally this dataset is a first step on a path to a repo with pre-trained STT models; We intend to grow this amount to around 10,000 hours or even maybe to 20,000 hours, if the stars align properly (we know how to get to 6–7k, we will improvise something);

or even maybe to 20,000 hours, if the stars align properly (we know how to get to 6–7k, we will improvise something); We have NOT invested any real money in creation of this dataset (except of course for our time and effort), so we are releasing it under cc-by-nc license. If you want to use the dataset for commercial purposes please go here;

in creation of this dataset (except of course for our time and effort), so we are releasing it under cc-by-nc license. If you want to use the dataset for commercial purposes please go here; You can see releases history here;

Speeding up the Imagenet moment

Ideally it goes like this:

Pull existing public models and datasets;

Collect some MVP dataset in your domain;

Build an MVP model;

Add more difficult cases;

Validate, test, rinse and repeat;

In domains like Computer Vision (CV) and Natural Language Processing (NLP) there is something to build on, these are only things that work in 95% of cases:

Widely shared Imagenet pre-trained models. Each framework has a repo like this — where you can get any sort of CV model with weights. Just add water;

In NLP, in my experience tools like FastText work best. Modern huge networks like BERT also work, but in real life we did not find them practical;

But in STT in Russian there is nothing really to build on:

Of course there are paid APIs / commercial last-gen products / products from government-related entities — but they have their drawbacks (besides being private or being built of less transparent technologies);

Public datasets are scarce at best (<100 hours) and non-diverse / too clean at worst;

Even English public datasets … are academic and detached from real life usage;

STT has a long history, and it has a bias towards being developed by large tech companies;

Unlike Google / Facebook / Baidu which are known to publish stellar research (FAIR’s papers are awesome and accessible, Google less so), Yandex is not really known to add anything to the community;

In STT also there are several less discussed “gotcha” perks:

There is a speculation on how much data you need for proper generalization — estimates range from 5,000 hours to 20,000 hours. For example Librispeech ( LS , one of the most popular datasets) is 1,000 hours and very “clean”. Google and Baidu report training on 10,000–100,000 hours of data in various papers for various settings;

, one of the most popular datasets) is 1,000 hours and very “clean”. Google and Baidu report training on 10,000–100,000 hours of data in various papers for various settings; If in CV your “domain” for which you learn features is for example “cats” (in certain positions that is, see explanations of Hinton’s CapsNets on this) in STT it is a bit more complicated. Your data can be clean / noisy . Also each voice is its own domain , models are amazing in adapting to voices. Also vocabulary is also an issue . So, basically when someone reports a 3 percentage point improvement on LS (i.e. 10% WER reduced to 7% WER) — you should take this with a grain of salt. My initial pipeline built in 1–2 days was able to work on LS quite well;

(in certain positions that is, see explanations of Hinton’s CapsNets on this) in STT it is a bit more complicated. Your data can be . Also , models are amazing in adapting to voices. Also . So, basically when someone reports a 3 percentage point improvement on LS (i.e. 10% WER reduced to 7% WER) — you should take this with a grain of salt. My initial pipeline built in 1–2 days was able to work on LS quite well; Cross domain transfer (i.e. when you train on books and validate on phone calls) works, but quite poorly. You can easily get +20pp WER. Also minor things like intonation / talking differently in noisy conditions matter;

So — you can think of our venture as a first step towards providing a set of public models for the community in our language (Russian, the majority of our methods scale to other languages) and bringing the Imagenet moment closer / making reasonably good STT models for the public.

You can say that now we are knee-deep in the data. To succeed we will need to go deeper! Please stop me from producing these horrible puns.

Seeking contributors

If you are willing to make a sensible contribution to our dataset / venture — you are welcome! We will make sure to publish as much as possible as friendly as possible.

Please contact us here.

You can frame it like this. If 4 people (not working 100% of time on this project only) can make a difference, probably your addition will tip the balance of scales towards building a truly flexible deployable model?

How not to share datasets

First of all, here is the list of things I dislike that people do all the time when sharing datasets:

Obviously paywalls with no way to inspect what you will get in advance;

Academic ivory tower attitude — “our dataset is cool and large, but useless in real life”;

Registration walls. Yes, “our dataset is public, but we will share it only for the chosen”. This thing. And yeah, if we decide not to share it with you, you will not be notified. Or most probably — nobody pays the moderator;

Sharing via Google Drive (btw, you can use wget with Google Drive, you need to download a cookie and use their link structure) or something that relies on temporary / dynamic links. Ideally you should just be able to use wget or better aria2c to get the data;

with Google Drive, you need to download a cookie and use their link structure) or something that relies on temporary / dynamic links. Ideally you should just be able to use or better to get the data; Poor hosting, poor CDN, poor speeds. It is laughable when people share a dataset, that probably would cost upwards of US$100k — US$1m to annotate from scratch manually, but spare US$100 per month on actually properly hosting the dataset;

Sharing via torrent … w/o having active seeders with at least 100 Mbit/s uplink speed. Academic torrents is great, but no seeders at any time;

Poor folder organization, no meta-data files, illogical structure;

Bloated, outdated tools and file formats used to load the dataset (hello xml and object detection datasets);

and object detection datasets); Convoluted and obscure code to read such data. You can pull off miracles with one liners in pandas. No, really;

No way to check your data;

Generally not caring about users;

Sharing data in proprietary formats / pushing some form of agenda;

We made a reasonable effort not to make any of these mistakes:

First of all, all links are public . We sourced the majority of our data from the Internet, so we s hare back whatever we can ;

. We sourced the majority of our data from the Internet, so we s ; We did not do this yet (help us, why not), but you can just write a script that would download everything, unpack everything, check md5 sums and files in one go;

Dataset is hosted on AWS compatible bucket with CDN — download speeds will be good. You can use aria2c with -x flag to load files faster;

with CDN — download speeds will be good. You can use with -x flag to load files faster; Data is mostly checked and written in the same format;

Data is collected in a disk DB optimized (see the repo for details) to work even on hard drives (we did not test it). I believe that a RAID array or NVME cache for your hard drive array will solve the IO problem entirely (we ourselves use SSD NVME drives);

(we did not test it). I believe that a RAID array or NVME cache for your hard drive array will solve the IO problem entirely (we ourselves use SSD NVME drives); Meta data file;

Some rudimentary hackable code snippets for easier start;

Finding a motivation, making a difference, ethics and motivation

Finally a set of points I would like to make:"
How to Choose Between Multiple Models,"How to Choose Between Multiple Models

In a previous article we discussed the concepts of underfitting and overfitting, how they can lead to models that don’t match the available data, how to identify each issue, and how to identify models that do fit the data well. These concepts can help you avoid major blunders and generate models that fit the data reasonably accurately; however, there are an incredible number of models that meet that description. This means that the next step, beyond generating a model that fits decently, is identifying which of the possible models fits best.

When determining how well a model fits the data set it’s important to calculate statistical values comparing the model predictions to the data set. This is beyond the scope of this conceptual article, but more information can be found in Data Science from Scratch or in Practical Statistics for Data Scientists. In this article we’ll discuss the process of developing, validating, and testing models.

What are the model development, validation, and testing phases and why are they necessary?

The fundamental issue to be aware of here is that you cannot trust a model that you’ve developed simply because it fits the training data well. This is for a simple reason: You forced the model to fit the training data well. If after creating a model the statistical calculations show that it matches the data well, this means that it’s possible to use mathematical methods to force a model to match the data well. What it doesn’t mean is that the model is capturing the trends that are really occurring, or that the model is able to predict other circumstances. The example of the overfit model in my previous article is a great way to highlight this.

The solution to this is model validation. Validation is the practice of using the model to predict the output in other situations for which you have data, and calculating those same statistical measures of fit on those results. Note that this means you need to divide your dataset into two different data files. The first is a training data set, which you use to generate your models. The second is a validation data set, which you use to check the accuracy of your…"
Illustration with Python: Confidence Interval,"This article uses knowledge in the central limit theorem, and also the concept of the weak law of large numbers and Chebyshev’s inequality, you can review these topics by visiting the links. If you would like to follow along, you can get the code from this link: Jupyter Notebook.

In my opinion, this topic is the most confusing theorem compare with others, even the article in Wikipedia mentions the misunderstanding about it, so I will try my best to explain it.

Before diving in, keep in mind that the mean of the population (the thing we what to estimate) is a constant, there is no randomness about the number.

The confidence interval is an estimator we use to estimate the value of population parameters. The interval will create a range that might contain the values. When we create the interval, we use a sample mean. Recall the central limit theorem, if we sample many times, the sample mean will be normally distributed.

I create the sample mean distribution to demonstrate this estimator."
Is web crawling legal?,"Photo by Sebastian Pichler on Unsplash

Web crawling, also known as web scraping, data scraping or spider, is a computer program technique used to scrape a huge amount of data from websites where regular-format data can be extracted and processed into easy-to-read structured formats.

Web crawling is commonly used:

Web crawling basically is how the internet functions. For example, SEO needs to create sitemaps and gives their permissions to let Google crawl their sites in order to make higher ranks in the search results. Many consultant companies would hire companies to specialize in web scraping to enrich their database so as to provide professional service to their clients.

It is really hard to determine the legality of web scraping in the era of the digitized era.

Why does web crawling have a negative connotation:

Web crawling can be used in the malicious purpose for example:

Scraping private or classified information. Disregard of the website’s terms and service, scrape without owners’ permission. An abusive manner of data requests would lead web server crashes under additionally heavy load.

It is important to note that a responsible data service provider would refuse your request if:

The data is private which would need a username and passcodes The TOS (Terms of Service) explicitly prohibits the action of web scraping The data is copyrighted

What reasons can be used to sue people?

Your “just scraped a website” may cause unexpected consequences if you used it inappropriately.

HiQ vs LinkedIn

You probably heard of the HiQ vs Linkedin case in 2017. HiQ is a data science company that provides scraped data to corporate HR departments. Linkedin then sent desist letter to stop HiQ scraping behavior. HiQ then filed…"
Is the AWS Big Data Certification Worth It?,"TLDR;

As a data scientist, the AWS Big Data Certification was worth it for me because I gained a more solid understanding of the base layers of the data science hierarchy of needs. The material I learned has helped me communicate better with my co-workers and made me a more flexible data professional.

Use the flowchart below to see if it might be for you:

Flowcharts are great.

Introduction

I’ve been a data scientist at Outcome Health for about 2 years now. We’ve done some interesting work, but throughout my tenure, I always had the nagging feeling that I didn’t understand enough about data infrastructure.

So, several months ago, I spent an hour a day for nearly 100 days straight studying for my AWS Big Data Certification. My AWS Big Data muscles got pretty ripped, and I was able to pass my certification exam on the first try!

But was it worth it?

For data scientists considering whether or not this certification is worthwhile, this article is for you!

The Benefits

Understanding the data science hierarchy of needs is critical to any data scientist’s job because it (1) helps you work better with your teammates and (2) enables you to pick up other job functions more quickly.

The AWS Big Data Certification can help bolster your knowledge of the lower levels of this hierarchy — typically where data engineers and DBAs step in.

Communication w/ Teammates

The primary difference I’ve seen between the before and after of working toward the certification is in my conversations with data engineers and DBAs on…"
Let’s calculate Z-scores for Airbnb prices in New York,"Let’s calculate Z-scores for Airbnb prices in New York

Z-score, also called standard score, according to wikipedia.

In statistics, the standard score is the signed fractional number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.

Translation: a measure of how far a value is from its population average.

Let’s look at the formula.

It’s pretty straightforward. Subtract the mean of the dataset from the value being evaluated, then divide by the standard deviation.

Let’s play with some data.

First, download the dataset from Kaggle and save it in the same directory as your jupyter notebook (yours filename may be different than mine). Then sample the first few records to see how the data looks.

import pandas as pd

ny = pd.read_csv('airbnb-new-york/AB_NYC_2019.csv')

ny.head(3)

Scipy has a nifty method for evaluating the z-score for every value in the dataset. See below.

from scipy import stats

stats.zscore(ny.price) #=> array([-0.01549307, 0.30097355, -0.01132904, ..., -0.15707024,

-0.4069123 , -0.2611711 ])

But we’ll write our own function as we just want to check a few values.

import numpy as np mean = np.mean(ny.price)

std = np.std(ny.price) def z_score(value, mean, std):

return (value - mean) / std

Now randomly select 5 prices from the dataset.

import random values = [] # randomly select 5 values from price

for i in list(range(0,5)):

value = random.choice(ny.price)

values.append(value) print(values)

#=> [78, 169, 53, 375, 80]

And calculate z-scores for those values."
Decision Making Is More Than Quantitative Problem Solving,"“All my best decisions in life and work have been made with heart, intuition, courage and not analysis.” — Jeff Bezos at Economic Club of Washington 2018

For years I had a mental model that equated decision making to quantitative problem solving. In this mental model, a decision can be made when a quantitative problem is solved; for example, a new drug should not get an approval (decision making) if its treatment effect is not statistically better than a comparison drug (quantitative problem solving). I have had this mental model for a long time, because this mental model not only mirrors my experience in how I approached textbook problems quantitatively at school, but also speaks directly to the popular concept of data driven decision making in workplace. This mental model treats decision making as a smooth continuum of quantitative problem solving, and makes me believe that I know decision making just because I know quantitative problem solving.

Different from my imaginary decision making, decision making on the ground is a dynamic process that can either rely on or be completely independent of quantitatively problem solving. Through my work, I am shocked by the mismatch between decision making and quantitative problem solving; many data scientists also admit their data results are not well taken by their business partners (link). Lack of data culture becomes an easy excuse, however I believe the hard truth is that decision making is not quantitative problem solving in the first place. What exactly is decision making? I have never seriously thought about it before. This time, I want to walk outside the data science garden and look hard at decision making by itself.

What’s covered in the following:

Analyze four types of decision making to inform your data science work Use a N:N model to help you navigate the complex dynamics of today’s decision making processes

Decision Makings Have Distinct Characteristics

Although we can universally define decision making as the action to select one option from alternatives, the process to agree on a selection differs greatly from one decision making to another. Using the number of distinct stakeholders, we can classify decision making into individual decision making and corporate decision making. Individual decision making is familiar to most people because it is a part of everyday life. Individual decision making has one or a small number of stakeholders, usually decision maker himself or herself. Individual decision making is focused, cost efficient, and has a clear accountability. Individual decision making takes place both in personal life and workplace; you can decide on your college major and you can decide on the best way to present at work. Individual decision making usually has limited consequences.

“Decision-making involves the selection of a course of action from among two or more possible alternatives in order to arrive at a solution for a given problem”. — Trewatha & Newport

When the number of stakeholders is large, we have corporate decision making. Corporate decision making usually takes place in various levels of organizations, and has earned a bad reputation of being slow, costly, political and lacking accountability, despite the fact that more rigorous evaluations and multiple perspectives are considered in corporate decision making. In large organizations, a small decision could end up with rounds of meetings and planning.

In data science exercises, we usually focus on individual decision making where stakeholder management presents no issue at all. However, in real world applications, we often apply data science to decision making in organizational context with many stakeholders, hence we primarily work on corporate decision making. Because dealing with stakeholders is never a thing among data scientists, those data scientists who have a channel interest in modeling techniques are likely to feel shocked by the complexity of corporate decision making. In corporate decision making, stakeholders have competing perspectives and interests, rely on both quantitative and qualitative information for decision making. Data science results may not appear sexy but rather powerless in the face of bureaucracy. When a data scientists believe data should predominately drive decision making (data-driven decision making ideology), this data scientist may end up saying, “the result is clear, they just don’t listen”.

We can further classify decisions into operation decisions and strategy decisions. Operation decisions are characterized as recurring and structured; such decisions concern day to day running of the business and are made by middle managers and front line employees. Strategy decisions are characterized as non-routine and unstructured; such decisions concern organizational policies and strategies, and are made by top management and executives. Successful data science applications today are mostly in operation decision making, because operation decisions are more quantifiable, have large available data for modeling and have less stakeholders. Think about house price estimation, loan application approval model and user retention prediction. These successful data science applications are operationalized as code and have limited stakeholders intervention. When we try to extrapolate these successes to other categories of decision making, we need to understand decision makings have distinct characteristics.

Build Influence to Convince a Village

Today’s decision making landscape is very challenging. Leadership structure is flat, projects are ran by cross functional teams, therefore we often do not have a single authority for decision making. To support decentralized decision making, data scientists have a village to convince. As we talked about corporate decision making earlier, decision making complexity increases greatly when the number of people increases. In flat leadership structure, a single decision maker or power center is less likely to retain big enough authority, therefore the final decision may not get honored completely by the workforce, which leads to half heart execution and poor decision making outcome. In a simplified world, a data scientist need to convince one decision maker; in today’s corporate, a data scientists is just one of many to convince many decision makers. We need go from a 1:1 model to a N:N model.

In N:N model, from the perspective of a data scientist, the data scientist need deliver data stories to multiple team leaders from multiple lens. In my work, a sign of success is not the applause at the end my presentation, instead a sign of success is the referral I get to present to another business leader. One more presentation sounds alright, but it is not easy to tell persuasive data stories to N stakeholders, because the data science work has to be robust enough against N dimensions of scrutiny. In the communication with N stakeholders, data scientists should not only know their different perspectives but also their different decision making preferences. Some want others to make decisions for them, some frame questions and interpret results, some can talk about database systems and modeling, and some just want data to prove existing belief. Data scientists need to be prepared to encounter N decision makers out there.

In N:N model, from the perspective of a decision maker, the decision maker takes input from multiple sources to form an opinion. Decision makers are consistently fed with information by internal teams and external consultants, through emails, PowerPoint and spreadsheets. In large corporate where teams of data scientists are employed, due to data sources variation and methodology differences, a decision maker may hear opposite decision suggestions from different data scientists, therefore a data scientist competes with other data scientists and other professionals to give the best insight for decision making. Decision makers never rely on one input, hence do not take it personally if your suggestion got ignored. Billionaire Ray Dalio perfectly summarizes this dynamics."
Development practices that data scientists should use NOW.,"You never write code for yourself, but for others.

This is the first basic rule I tell coding data scientists: you never ever write code for yourself, but always for others.

And, mind you, “other” people can be… you. If you don’t write for your colleagues, at least write for your future self.

What does that imply?

COMMENT YOUR CODE. Twice as more as you’re doing now :) Actually, commenting before writing code is a good practice, too :)

Write politely. Use explicit variable names, file names (I see you, Mr untitled09.ipynb). Delete/comment the code you’re not using anymore.

Document your code (not exactly the same as commenting). Put a README, give your reader a helping hand.

Collect versions of the modules you’re using, so that in 1 year (and who know how many JS frameworks could be created in 1 year), when you’ll have to run the project in a rush, you’ll find the versions of the modules you need.

Code that is not pushed into a repo doesn’t exist.

I am amazed by the number of data scientists working on their own GPU-packed machine, with all the bells and whistles, and a very fragile hard drive with all of their life packed in a tiny little space.

A hard drive has a density of, say, 500 Gbit per square inch. That means I tiny little movement can lead to disaster. Given my own ability to bang into furniture, that’s a lot of trust for such a small space.

So, treat code laying on your PC just as if it didn’t exist. It’s not versioned, it’s not reviewed, it’s not CIed, it’s not backed up. It. Just. Doesn’t. Exist.

Oh, and did you know Jupyter notebooks were actually versionable files?

Be as conservative as you can in your architecture choices

You’re a data scientist? Great. You would like to use the latest framework that has half a star on a Github repository that’s been published 3 years ago and never touched ever since (sorry, no links). Don’t.

You’re going to have a lot of uncertainty to deal with, so please avoid skating on thin ice by using an obscure Pytorch fork from an abandoned university.

One line of code costs 10x more the time it takes to be written

…speaking of which… you have to be aware that one line of code costs a lot more than the time it takes to write it. Even dead code.

So when you’re adding a new feature nobody asked you for, when you’re training your model to detect patterns that were not part of the specs, you’re just building technical debt.

Successful coding sessions often involve trimming unnecessary parts of your code. This is one simple way to manage complexity.

Trim your code.

You never train for yourself, neither.

Machine Learning often involves training data, evaluating your training and building a scalable, powerful inference machine.

The first part involves a lot of experimentation. And there’s always going to be a better person who could bring improvements in your machine learning design. I mean, even you, Ian Goodfellow, if you’re reading this, trust me, there is someone better than you. For example, your future self!

So, as a rule of the thumb, never train for yourself but for someone else. That means, documenting, commenting, and two major things:

give ways to others to train against your original data (version your data and give its URL, for example!)

give ways to others to evaluate against your test data

Seems obvious? Go and read some deep learning papers, and you’ll understand it’s not always that obvious :)

Bad coders copy/paste. Great coders steal shamelessly.

Half of what programmers do is looking up for answers on Google. Ah, and here comes the Stack Overflow post explaining how to solve your problems!

Now, once you’ve copy+pasted the solution, just be kind enough to reference the original Stack Overflow post, for example as a comment. It’s not much but it really helps a lot when you’re trying to figure out why you did that.

Even Microsoft does that!

It will save a lot of time when you’ll look at your code later.

Don’t use Python 2 (please)

Don’t scare the python

Naaaah, come ooooon… It’s 2019, seriously. Python 3 has so much to bring. Plus, you don’t like UnicodeErrors, do you?

All in all, following these simple rules will make you a better coder, a better data engineer and a better data scientist. If what you’re doing has a purpose, at least let other people share it!

And you? What are your best practices? Tell me in comments!"
"Why you should be a Generalist first, Specialist later as a Data Scientist?","So what’s a Generalist and a Specialist?

Before going any further, let’s first understand what we mean when we talk about being a generalist and a specialist in data science.

A generalist is someone that has knowledge in many areas whereas a specialist knows a lot in one area. Simple as that.

Particularly in data science, it’s notoriously hard to become a generalist in all phases of data science project lifecycle. It takes years to acquire all the skills in different areas, yet it’s not necessary to master all of them.

Similarly, it’s not easy to be a specialist in data science either.

Now you might define a generalist as the Jack of all trades, master of none.

I couldn’t agree more about that.

And this is precisely the reason why I’d choose to be a specialist in the later stage of my data science path.

Why be a Generalist first?

After all, being a generalist is not meant to master anything, but rather to understand the full picture of the whole data science project lifecycle.

The question is: Why is understanding the full flow of the data science project lifecycle important in the first place?

You see. As a data scientist, we don’t build a machine learning model just for the sake of building it. We first understand the business problem and frame that into a problem that can be solved through data science approach. Then you need to identify data sources and define success metrics. Depending on your company’s stage of maturity, you might also need to build a pipeline to collect data (YES, you may not even have data in place)…

We can still go on and on but here is the point — all the job scopes above are part of being a generalist. The good thing is that you’ll get to know the full picture of the data science problem as a generalist — as a data scientist in the beginning of your career.

In other words, you’ll learn and you’ll grow, tremendously.

What I’m advocating here is this: If you’re someone who is starting out in data science, my recommendation is to be a generalist first. Go join a startup and take on many hats as you will probably be the only data scientist in your company. Generalists add more value than specialists in a company’s early days, since you’re building most of your product from scratch and something is better than nothing.

Your machine learning models don’t have to be a game changer but should be able to provide actionable insights and results.

Learn how you can help the company generate more revenues. Learn how you can leverage the existing data or build some pipeline to collect data to solve some problems.

Start with the low-hanging fruit first. There isn’t always a need to go for AI if the company isn’t ready for that. In fact, normal statistical approach is typically sufficient to tackle some simple problems.

The ideal data scientist is a strong generalist who also brings unique specialties that complement the rest of the team

Be a strong generalist. Be the Jack of a trades.

Once you’ve enough experience and you’ve found your interest and passion in a specific area (say NLP), then you can deep dive into that, which leads us to the next stage.

Why be a Specialist later?

Say if you’re a NLP specialist. Your focus could be solely on building the best NLP classifier model given the data. And that’s it.

All the things are already set for you. The business problems are well defined (done by product managers). The pipeline is ready and maintained 24/7 (done by data engineers) and the data is there for collection. What you need to do is do what you’re best at. This is crucial as you can focus on your expertise and strength to add the highest values to the project.

It’s perfectly fine to be specialist in data science. Being a specialist in your niche plays an important role in a company, which is also something that makes you irreplaceable and valuable to others.

At this stage, since now you’re already experienced in different areas as a specialist in data science. Your experience and expertise are not something that can be easily substituted by others.

Even better, you’ll be able to focus on your specialization and work with others as a team with your broad knowledge and understanding of other parts of the data science workflow."
Linear Regression Models,"Arguably the best starting point for regression tasks are linear models: they can be trained quickly and are easily interpretable.

Linear models make a prediction using a linear function of the input features. Here we’ll explore some popular linear models in Scikit-Learn.

The full Jupyter notebook can be found here.

Data

Here we’ll use the SciKit-Learn diabetes dataset to review some popular linear regression algorithms.

The dataset contains 10 features (that have already been mean centered and scaled) and a target value: a measure of disease progression one year after baseline.

We import the data and prepare for modeling:

from sklearn.datasets import load_diabetes

from sklearn.model_selection import train_test_split # load regression dataset

diabetes, target = load_diabetes(return_X_y=True)

diabetes = pd.DataFrame(diabetes) # Prepare data for modeling

# Separate input features and target

y = target

X = diabetes # setting up testing and training sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)

Evaluation Metric: R²

R-Squared, or the coefficient of determination, is how much variance in the target variable that is explained by our model.

Values can range from 0 to 1. Higher values indicate a model that is highly predictive. For example, a R² value of 0.80 means that the model is accounting for 80% of the variability in the data.

In general, the higher the R² value the better. Low values indicate that our model is not very good at predicting the target. One caution, however, is that a very high R² could be a sign of overfitting.

We’ll use the following function to get cross validation scores for our models:"
"Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.","Buyers beware, fake product reviews are plaguing the internet. How Machine Learning can help to spot them.

Opinion spamming is a situation that is aggravating, for instance, CBS News reports that 52% of product reviews posted in Walmart.com are “inauthentic or unreliable”, while at least 30% of reviews posted at Amazon are fake. The problem of identifying opinion spamming remains an open topic, despite the fact that several researchers have addressed it already.

What makes businesses to incur in product deceptive reviews? The main driver is getting ahead of the competition for positioning their product or service to influence the public and organizations to make a purchase, therefore increasing their sales. The fraud takes place by posting fake negative reviews and giving unjust ratings to products from the competition.

It is known that “Amazon Mechanical Turk”, an internet crowdsourcing marketplace that allows requesters (businesses or individuals) to coordinate human labor to carry out a task, was employed to crowdsource fake reviews for a hotel chain. Given that this problem has grown to alarming rates, Yelp.com, a business directory service that publishes crowd-sourced reviews about businesses, launched a sting operation in order to unmask those businesses who buy fake reviews.

I will discuss the method that Mukherjee et al. present in their paper for detecting spam in product reviews. They called their model “Author Spamicity Model” (ASM). It is based on unsupervised learning which models spamicity as latent, shortly meaning that the model variables are “hidden”. It is also a Bayesian inference framework. The aim of the model is to cluster the categorization of this latent population distributions into spammers and not spammers.

Please note that when I refer to products, I will be including also services.

How can we identify that a review may be fake? In order to develop their model, the authors define nine variables as observed features, the first five they categorize them as author features that have values in the interval [0, 1] (denoting a Beta distribution), where a value close to 0 or 1 denotes non-spamming or spamming, respectively. In the other hand, variables 5 to 9, represent review features, and they assume a binary value, 0 for non-spamming, and 1 for spamming (denoting a Bernoulli distribution):

Content Similarity (CS). Spammers are prone to copy reviews for comparable products. Cosine similarity is used to capture content similarity in these reviews. Maximum Number of Reviews (MNR). The unusual behavior of posting several reviews in one day by the same author, can be a sign of spamming. Reviewing Burstiness (BST). Refers to the frequency (short) to which an author posts a review. This author is usually a new member of the site. Meeting this condition may signify a tendency to incur in deceptive reviews. Ratio of First Reviewers (RFR). This metric quantifies the fact that early reviews have an impact on sales of newly launched products. Duplicate/Near Duplicate Reviews (DUP). Identical or quasi identical reviews may indicate a spamming behavior. This feature is similar to CS, but in this case pertains to the review features. Extreme Rating (EXT). In order to deliver the most or the least benefit to a review, spammers usually mark the product with either one or five stars. Rating Deviation (DEV). Spammers will try to divert the average sentiment on reviews, by placing theirs. These types of reviews are identified when this quantified deviation exceeds a threshold. Early Time Frame (ETF). This feature captures how early the review was made. The rationale is, spammers are most likely to review earlier, close to the launch of the product to achieve the greatest impact. Rating Abuse (RA). Refers to the action of star-qualifying the same product multiple times.

How does ASM work? In order to illustrate the model, I have simplified its functioning in the following schema (See Figures 1-A and 1-B), for a mathematical representation please refer to the paper.

Figure 1-A

Figure 1-B

ASM commences by taking in all the reviews by all the authors, where these reviews are organized by the features that we have discussed. Each sphere represents an observable variable (i.e. feature). Once the features are collected (See Figure 1-A node A) they are processed by the model and learn the “latent behavior distributions for spam and not spam” (Murkherjee et al.). Therefore, ASM solves a clustering problem (K = 2).

The spamicity is modeled as latent as ASM functions in the Bayesian context. It is a generative process because it emits the nine features with their probability of spamming.

In order to perform an inference, the model uses “Collapsed Gibbs Sampling” (CGS) that represents a technique for approximating the posterior probability distribution. CGS belongs to the family of algorithms of Markov Chain Montecarlo.

Once the ranking functions have been inferred, they are processed using the Learning to Rank supervised technique, that basically takes the rankings obtained by ASM and generates a single aggregated ranking function (see Figure 1-A node C).

In my opinion, this paper presents a technique that can improve significantly the detection of opinion spammers in product reviews. It is innovative because presents an unsupervised method for detecting fake reviews. The authors claim having achieved a superior level of accuracy compared to strong competitors. I believe that opinion spamming will start to decrease as more businesses providing this type of information start to implement ML techniques like ASM, meanwhile consumers must be skeptical and get informed using sites that filter fake reviews.

A Mukherjee, A Kumar, B Liu, J Wang, M Hsu… — Proceedings of the 19th …, 2013 — dl.acm.org

P Resnik, E Hardisty — 2010

GEP Box, GC Tiao — 2011 — books.google.com

by JA López — ‎2017

— — — — — — — — — — — — — — — — — — — — — — — — — — — — — — — -

Dear reader, I am interested to know from you:

When you buy online, do you feel influenced to decide by the local review that is presented? Or in addition to, you look for one or more external reviews? What is the products review site that you trust more, if you use one? What do you think may be a solution to this problem that is growing at alarming rates?

Thanks for participating, you can leave a comment to respond."
Data types in R,"Introduction

This article presents the different data types in R. To learn about the different variable types from a statistical point of view, read “Variable types and examples”.

What data types exist in R?

There are the 6 most common data types in R:

Numeric Integer Complex Character Factor Logical

Datasets in R are often a combination of these 6 different data types. Below we explore in more detail each data types one by one, except the data type “complex” as we focus on the main ones and this data type is rarely used in practice.

Numeric

The most common data type in R is numeric. A variable or a series will be stored as numeric data if the values are numbers or if the values contains decimals. For example, the following two series are stored as numeric by default:

# numeric series without decimals

num_data <- c(3, 7, 2)

num_data ## [1] 3 7 2 class(num_data) ## [1] ""numeric"" # numeric series with decimals

num_data_dec <- c(3.4, 7.1, 2.9)

num_data_dec ## [1] 3.4 7.1 2.9 class(num_data_dec) ## [1] ""numeric"" # also possible to check the class thanks to str()

str(num_data_dec) ## num [1:3] 3.4 7.1 2.9

In other words, if you assign one or several numbers to an object in R, it will be stored as numeric by default (numbers with decimals), unless specified otherwise.

Integer

An integer data type is actually a special case of numeric data. Integers are numeric data without decimals. This can be used if you are sure that the numbers you store will never contain decimals. For example, let’s say you are interested in the number of children in a sample of 10 families. This variable is a discrete variable (see a reminder on the variable types if you do…"
What libraries can load image in Python and what are their difference?,"When we face computer vision project, first of all we need to load the images before any preprocessing.

There are various libraries out there to perform imread() . Here I want to consolidate the popular libraries for loading image and their difference. This article will go through:

Libraries for loading image Colour channel Efficiency Cheatsheet!

Library for loading image

There are four libraries that are usually used for loading images.

Matplotlib — plt.imread()

OpenCV — cv2.imread()

Pillow — Image.open()

scikit-image — io.imread()

import matplotlib.pyplot as plt img = plt.imread(img_dir) import cv2 img = cv2.imread(img_dir)

from PIL import Image img = Image.open(img_dir)

from skimage import io img = io.imread(img_dir)



Colour channel

After loading the image, usually plt.imshow(img) will be used to plot the images. Let’s plot some doge!

You may spot that the OpenCV image above looks odd. It is because matplotlib, PIL and skimage represent image in RGB (Red, Green, Blue) order, while OpenCV is in reverse order! (BGR — Blue, Green, Red)

Easy Fix"
Predicting How Expensive A Healthcare Provider is for the Government,"As Metis Data Science Bootcamp students, we were tasked to build a linear regression model for our first individual project. Simple enough right? Just draw a line through data.

Not so fast!

As a healthcare professional, I was excited to use this algorithm to solve a problem in the medical field. But what I quickly found out was, before any modeling, a successful machine learning project starts with picking the right kind of data. Linear regression works best with continuous numerical data which excludes several sources of publicly available healthcare data. However, it turns out that Medicare payment data is a perfect fit.

But why care about Medicare? Medicare is a government-funded health insurance program currently covering 44 million people, or 1 in 8 individuals in the U.S. (1). That carries with it some huge public health and financial implications for the government. As soon as you turn 65, you’re eligible for it with a few exceptions such as younger people with disabilities or those suffering from end stage renal disease. And this population is only getting bigger. In fact, the Census Bureau estimates that by 2030, the elderly population alone will nearly double to 78 million people or 1 in 5 Americans (2).

On top of this, it seems likely that Medicare is going to be increasingly more important in the coming years due to its prevalence in political discussions around the country. If the United States were to adopt a single-payer system, such as the proposed Medicare-for-All, it would be essential for us to lower costs wherever possible, and the effective usage of data could help to achieve this.

One way to do this would be to look at healthcare provider costs. You can follow along with my code on my GitHub repository; I’ve organized it chronologically with this article for your convenience.

After searching the Center for Medicare and Medicaid Services website, I was able to acquire the most recent provider payment data which had over 1 million rows, each corresponding to a healthcare organization or individual provider, and 70 features.

Data Cleaning

But as expected, this data was quite a mess. So in order to glean the most reliable insight I could, I decided to narrow down what I was measuring to only individual providers in the U.S, excluding territories and military zones.

I then replaced all missing values in columns with count or percentage data with zeroes. It is likely that if a provider doesn’t have patients with a certain disease, the field is just left blank; this means it should have been relatively safe to impute nulls without losing much fidelity.

There was also some documentation provided for the meaning of each column name. I decided the target variable should be total_medicare_payment_amt, which is the total amount the government paid for all the provider’s services per patient after deductible and coinsurance amounts have been subtracted. Additionally, I removed all columns that were either unnecessary (like provider names) or that would lead to possible data leakage (like other price-based columns).

Finally I was left with a little over 990,000 clean rows with 38 features. Let’s begin!

Initial Model

Using the Statsmodels python library, I just threw all my data into the ordinary least squares (OLS) linear regression to see how it would perform initially with no modifications.

Univariate Linear Regression Example

As previously mentioned, linear regression tries to find a linear relationship between independent variables and a dependent variable. Above, you can see the simplest univariate form with only one independent variable or feature. It uses the equation y = mx + b to find the best fit with the data; m is the slope coefficient and b is the y intercept.

But obviously with 38 features, this linear regression problem is a lot more complex. In this case, there will be 38 “mx” terms added together, with each m term corresponding to the size and direction of the effect that specific variable is having on the dependent variable. In geometric terms, we will be fitting a 38th dimensional hyperplane to 39th dimensional space (instead of a line). If you find a way to visualize this, let me know!

Ok, so now we have some intuition for the model, but how do we determine how well the model is doing?

The metric that is commonly used here is called the coefficient of determination or R-squared. Essentially, it is the percentage of variance of the target variable that is predicted by the features. We want an R-squared close to 1 which indicates that the model is very predictive.

The Inner Workings of R-squared

But let’s dive a little deeper into the actual formula of R-squared because it helps us to understand how we are evaluating the model. The most naive method we could use to predict Medicare costs would be to just guess the average cost. That is the green ȳ (called y-bar) in the the diagram above. This will be our baseline.

But we can do a lot better than that by using linear regression or the red ŷ (called y-hat). Now we just find how off these two predictions are from the actual value and divide them by each other (SSE/SST). This will tell us the percentage of variance the model cannot explain. But what we really want to know is what percentage of variance this model does explain. Subtracting that value from 1 will get us there.

1 - (Error Sum of Squares/Total Sum of Squares) or 1 - (SSE/SST)

And after running the initial model, the R-squared was 0.619. That means that our model only accounts for about 62% of the variation in the data. That’s not too good.

Checking Our Assumptions

But wait! Linear regression has many assumptions and it is important to check if our data is actually working for this model.

Assumption #1: Is there a linear relationship between the target variable and the features?

Number of Services vs Total Medicare Cost Before Feature Engineering

For illustration purposes, if we use the feature of total medicare cost, it’s not entirely clear. To rectify this situation, we could do some feature engineering. One option is to do a logarithmic transformation of both the feature and the target.

Number of Services vs Total Medicare Cost After Feature Engineering

Wow! That’s a dramatic improvement. Anyone can draw a line through that! As you can see, it is often the case that we need to transform the data in specific ways to make it conform to the assumptions of the model we are using.

Note: Always remember to undo this transformation afterwards in order to return your value back to the original context. Because after all, what does the logarithm of Medicare costs even mean?

Assumption #2: Are the target and the features normally distributed?

Total Medicare Cost Before and After Feature Engineering

In the above figure, the left plot shows the target variable before using a logarithmic transformation; as you can see, it is terribly right skewed. The right plot, on the other hand, shows how applying this transformation results in a remarkably normal distribution.

Assumption #3: Is there little to no multicollinearity among the features?

Correlation Coefficient Heat Map of All Variables

Multicollinearity is when the features are highly correlated with each other. Above, we see a heat map where the darker colors indicate strong positive correlations. Ideally, we would see only light colors everywhere else except the diagonal line across the middle, as obviously a variable will be perfectly correlated with itself.

But in reality, we see darker colors popping up all over the place, which indicates we are violating this assumption. This can lead to imprecise regression coefficients or worse, changes in sign for the same features in different samples, which makes it difficult to reliably extract meaning out of those coefficients.

The way to go about solving this is to remove features until there is no longer any collinearity. As will be discussed later, regularization techniques do this for you by zeroing out coefficients of some of the features that are collinear with each other.

Assumption #4: Are the residuals correlated with themselves?

An autocorrelation happens when the residuals for a specific feature are not independent from each other. This is considered bad because it indicates the model is not extracting all the information possible from the data, and thus, we see it in the residuals.

This can be measured through the Durbin-Watson test. Values near 2 indicate no autocorrelation, while values near 0 or 4 indicate strong autocorrelations. Our initial model has a value of 1.998, indicating that the model is extracting as much information as possible and the assumption has been met.

Assumption #5: Is the data homoskedastic?

What we want to avoid here is heteroskedasticity, a big word with a simple explanation. This is when the variance of the residuals change across the range of values in a feature.

An Example of Heteroskedasticity (Source)

As you can see in this hypothetical example, it is very clear that the variance gets wider as age increase. This is not good as it means that our model will get worse at making predictions the older someone gets. What we really want is a consistent predictability and variance across the entire range of values, known as homoskedasticity. In other words, the two dotted red lines would be parallel to each other.

Predicted vs Residuals Plot Before Feature Engineering

Here we see the predicted values versus the residuals for our model on the Medicare data. This doesn’t look good at all. There is a harsh cut off in the negative residuals (due to government cost always being greater than or equal to 0) and the variance is completely inconsistent across the range of values.

Predicted vs Residuals Plot After Feature Engineering

But after applying the logarithmic transformation that we did before, the plot looks relatively homoskedastic now and we have met this assumption. Boom!

Secondary Modeling

So after checking the assumptions of all the features, I decided to apply a log transformation to 3 features and the target variable.

Now, I put this newly transformed data back into the model and after training, it produced an R-squared of 0.92. Fantastic! This is a solid result as the new model can explain 30% more of the variance in the data versus the baseline model. This demonstrates how important it is to transform your data to meet the assumptions of the model you have chosen.

But this was just an OLS model. We can apply the regularization techniques briefly mentioned before which should further strengthen our model. These add an extra term to the cost function, penalizing the model for complexity. This is a good idea because simpler models are typically better than complex ones as they tend to be less susceptible to overfitting.

In other words, complex models tend to fit training data super well but perform poorly on unseen data. I switched over to the scikit-learn library to do this regularization, along with adding in more rigor to the process with a test-train split and cross validation.

I experimented with both ridge and LASSO regression and did hyper parameter tuning of the alpha terms which determine how strong the regularization will be. Surprisingly, both models with optimized alphas performed basically exactly the same as the OLS model with an R-squared of 0.92, with ridge being insignificantly better than LASSO. This indicates that regularization did not significantly help the model.

The LASSO coefficients support this finding as well. LASSO typically zeroes out any redundant features, leaving only a few remaining. In contrast, the best LASSO model only zeroed out 1 out of the 38 features. This is a surprising result, indicating that most features contribute to the predictability of the model and thus stronger regularization would only hurt the model’s performance.

Feature Importance

Speaking of coefficients, we can determine the importance of each feature by looking at the sign and magnitude of the coefficients. This allows us to provide valuable business insights to our stakeholders, in this case the Center for Medicare and Medicaid Services.

Top 10 Features That Increase Medicare Costs (Positive Coefficients)

From the top 10 features, I found it interesting that the fourth most important feature was the number of white patients a provider has. This is alarming as the model actually seems to care about race in some meaningful way.

This could possibly be exposing an underlying fault of the system, indicating that the white population is over-represented and thus, makes up a significantly larger percentage of the Medicare cost in comparison to other races. Other research needs to be done to determine the root cause but it is very possible that lack of access for underserved populations may contribute to this.

This is a powerful result and a example of the value that data science has for society. I was amazed to find that not only could I use this algorithm to improve a stakeholder’s bottom line but to also reveal social disparities. This is one of the main reasons I love data science; it can have immensely powerful effects on our society.

Top 10 Most Expensive Specialties

Looking at the top most expensive specialties, surgery clearly stands out. And it makes sense; surgery is incredibly expensive. This means that the government would best spend their efforts reducing surgery costs in order to most significantly impact their bottom line.

Top 10 Most Expensive Medical Conditions

In terms of medical conditions, largely preventable chronic diseases take the cake. This finding is a double-edged sword and falls in line with what we already know. Sadly, it means that most of these Medicare patients are suffering from diseases that they didn’t ever have to suffer from if they had just had different lifestyle choices. From both a financial and an ethical perspective, this is absolutely terrible.

But on a more positive note, this means that the government can save incredible amounts of money while also reducing immense amounts of suffering by beginning to focus on preventive lifestyle medicine instead of reactive treatments, such as surgery.

Obviously, we didn’t need data science to tell us to eat better and move more. But this further supports what we all know is necessary for the well being of everyone in society.

And in the process, we built a model to accurately predict how expensive a healthcare provider is for the government. Saving money and saving lives, what more could we ask for?"
Game Theory — History & Overview. What Is Game Theory & Why Is It…,"Life is survival & survival is competition. Whenever two or more entities are faced with constraints, such as a fixed amount of resources, in a win/lose scenario, competition emerges. It’s inevitable & natural. It’s evolutionary as evident by food-chain balances in ecosystems — of course, this innate behavior extends equally to humans. From competition, in any of its many forms, stems strategy.

Game theory is the branch of applied math used to create an optimum strategy in order to succeed in competitive situations of uncertainty & incomplete knowledge (like most real-life scenarios). It’s the mathematical study of decision making & modeling in situations of conflict that are found in everyday life across all industries & disciplines.

Published On Setzeus

Countries go to war for territory, businesses compete for market share, animals fight for resources, & political parties vie for votes. In a world ruled by interdependent agents aiming to increase their “value” in dynamic systems, game theory is monumentally germane.

While the academic discipline & the field itself was only formally established in the 1950s, game theory-like insights are clearly visible in centuries-old history. Below, we’ll review both the background of the ancient days & the forward acceleration of the 1950s .

The Ancient Days

Clear in hindsight, the ideas underlying game theory appear multiple times throughout history. From the war-musings of Sun Tzu to the evolutionary discoveries of Charles Darwin, it has long existed as a driving force of human behavior. The basis of ancient game theory, however, is usually attributed to the popularity of the following three specific works:"
Natural Language Processing — Event Extraction,"Natural Language Processing — Event Extraction

The amount of text generated every day is mind-blowing. Millions of data feeds are published in the form of news articles, blogs, messages, manuscripts, and countless more, and the ability to automatically organize and handle them is becoming indispensable.

With improvements in neural network algorithms, significant computer power increase, and easy access to comprehensive frameworks, Natural Language Processing never seemed so appealing. One of its common applications is called Event Extraction, which is the process of gathering knowledge about periodical incidents found in texts, automatically identifying information about what happened and when it happened.

For example:

2018/10 — President Donald Trump’s government banned countries from importing Iranian oil with exemptions to seven countries. 2019/04 — US Secretary of State Mike Pompeo announced that his country would open no more exception after the deadline. 2019/05 — The United States ended with exemptions that allowed countries to import oil from Iran without suffering from US sanctions.

This ability to contextualize information allows us to connect time distributed events and assimilate their effects, and how a set of episodes unfolds through time. Those are valuable insights that drive organizations like EventRegistry and Primer.AI, which provide the technology to different market sectors.

In this article, we’re going to build a simple Event Extraction script that takes in news feeds and outputs the events.

Get the data

The first step is to gather the data. It could be any type of text content as long as it can be represented in a timeline. Here I chose to use newsapi, a simple news source API with a free developer plan of up to 500 requests a day. Following are the functions built to handle the requests:

That last function returns a list of approximately 2.000 articles, given a specific query. Our purpose is just to extract events, so, in order to simplify the process, we’re keeping only the titles (in theory, titles should comprise the core message behind the news).

That leaves us with a data frame like the one below, including dates, descriptions, and titles."
Hash Tables Explained,"Hash Tables Explained

I ntroduction

When given large datasets defined by pieces of information that are related to other pieces of information, what are some ways we can store and retrieve information efficiently? To manage large quantities of relational data, we need to have data structures that have the capability quickly manipulate it (i.e. Insert, Delete, and Search). Suppose we had “key” data that corresponded with “value” data, then, one way to relate two pieces of information is to use a dictionary which is composed of key/value relationships. There are a few different ways to implement a dictionary, including using balanced binary search trees and doubly linked lists. In this article, we will talk about using hash tables, which is, by far, the fastest method of the three dictionary implementations and can perform insert, delete and search operations more efficiently than the other two dictionary implementations.

What is the motivation for a hash function?

First, I’m going to explain why we need something other than the simplest solution to relating information, known as the Direct Address Table. This naive solution is basically an array of size m, where the number of keys are less than m, and the address of each array index corresponds to the key, which either holds the value or holds a pointer to the value. Example: We can store the value associated with key k in the kth slot. Addresses that don’t have corresponding keys are simply known as: nil. See relation below between keys and values, as defined by the Direct Address Table (T).

How a Direct Address Table works

Insertion, Deletion, and Search are all O(1) in this case as you can go directly to the key through the address, and access the value. However, the two assumptions that limit the usage of this data structure for storing relational information is that:

The size of the array, m, is not too large. No two elements have the same key.

The first is a problem because we don’t want the array to take up too much space with nil elements. The second is also a problem because it limits the types of keys we can use.

So instead, we use a hash function.

What is a hash function?

A hash function, h(k), is a function that maps all the keys to the slots of an array. Another way to think about it is: given a key and an array, the hash function can make a suggestion as to where the index of the key should be stored in the array.

How does a hash function work?

There are several hash functions that can be implemented for a Hash Table, but the most prevalent one is the Division Method, where h(k) = k mod m, for some value of m. Other hashing functions include: the Multiplication Method and the Folding Method.

What is a hash collision and how do I resolve a hash collision?

So this is all good and well. But what happens when a key is hashed into the same array slot as another key? Aha! THIS is known as a hash collision. There are a few different ways of dealing with a hash collision, the most popular two ways are Open Addressing and Closed Addressing.

Open addressing is when you place an item some where other than its calculated position. We do this in a calculated way, such as linear probing, where a linear search is used to find an available slot and finding an item also involves a linear search.

How the Linear Probe works as an example of open addressing

Here’s a code snippet of a linear probe in a hash table:

class HashEntry:

def __init__(self, key, value):

self.key = key

self.value = value

self.next = None class HashTable:

def __init__(self, size):

self.size = size

self.keys = [None] * self.size

self.values = [None] * self.size



def hash_function(self, key):

return hash(key) % self.size



def get_slot(self, key):

slot = self.hash_function(key)

while self.keys[slot] and self.keys[slot] != key:

slot = self.hash_function(slot + 1)

return slot



def set(self, key, value):

slot = self.get_slot(key)

self.keys[slot] = key

self.values[slot] = value



def get(self, key):

return self.values[self.get_slot(key)]

Another way of open addressing is using quadratic probing, where we square the number of the foiled attempts when deciding how far from the point of original collision to look next. Each time another foiled attempt is made, the distance from the original point of collision grows rapidly.

Closed Addressing is essentially using linked lists to chain together keys that have the same hash value. The look-up for this method is the same as searching through a linked list.

Chaining uses Linked Lists to resolve hash collisions

Here’s a code snippet of chaining in a hash table:

class HashEntry:

def __init__(self, key, value):

self.key = key

self.value = value

self.next = None class HashTable:

def __init__(self, size):

self.size = size

self.table = [None] * self.size def hashing_function(self, key):

return hash(key) % self.size def rehash(self, entry, key, value):

while entry and entry.key != key:

prev, entry = entry, entry.next

if entry:

entry.value = value

else:

prev.next = HashEntry(key, value) def set(self, key, value):

slot = self.hashing_function(key)

entry = self.table[slot]

if not entry:

self.table[slot] = HashEntry(key, value)

else:

self.rehash(entry, key, value)



def get(self, key):

hash = self.hashing_function(key)

if not self.table[hash]: raise KeyError

else:

entry = self.table[hash]

while entry and entry.key != key: entry = entry.next

return entry.value

That’s all for now! I hope that this tidbit of information about Hash Tables and their collisions have inspired you to learn more about them.

Resources and Citations

Hash Tables and Hash Functions: https://www.youtube.com/watch?v=KyUTuwz_b7Q

MIT OpenCourseWare: https://www.youtube.com/watch?v=0M_kIqhwbFo

Stanford CS 161 Notes on Hashing: https://web.stanford.edu/class/archive/cs/cs161/cs161.1168/lecture9.pdf

Coding Cheat Sheets: https://www.alispit.tel/coding-cheat-sheets/data_structures/hash_tables.html"
Influencer Marketing using Web Scraping,"Photo by Kaleidico on Unsplash

Influencer marketing is not a foreign concept to us anymore. In the US retail business, there are only 33% of the retailers haven’t used or do not intend to use influencer marketing of any kind. However, it is a great way to lift your brand awareness through word-of-mouth advertising. The tricky part is how to find the right person and/or channel to promote your brand.

If you Google for the biggest influencers on social media, you are running against a wall. Instead, you should start with your audience pool and then extend the connections. For example, obtaining your commenters’ information and contacting them about news is a good way to find your influencers. The best way to get those candidates is through web scraping amongst your audience. A lot of businesses don’t have the budget for expensive tools like data integration. However, there are plenty of cheap alternatives out there for you to gather valuable information, and one of them is web scraping software.

What is Web Scraping

Web scraping is a technique to automate the process of data extraction. It involves the process of parsing a website, and collect the snippet of data for your needs. I am going to show you how to use a web scraping tool and make these data available for digital marketing. There is no programming skill required to conduct web scraping. The tool that I used is Octoparse, and I will explain why it yields incredible value for marketing professionals.

Find your “Evangelists”.

According to Joshua, comments are the place where we can leverage to extend our marketing strategy. I am not saying we leave comments and clap hands for your commenters. When the user name of the commenters is clickable, it is possible to connect with them by extracting the profile information. Whenever we have a new piece of content, we reach out to these people. This way we turn commenters into evangelists. Besides, you can go a little more creative by snowballing this method and creating your evangelist’s pool to further your marketing process.

Twitter Evangelists:

Blindly sending messages to beg for tweets won’t work. A great start off is to use your audience pool. The idea is:"
