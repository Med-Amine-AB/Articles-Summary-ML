title,content
Machine Learning — Diagnosing faults on vehicle trackers with a CNN,"Problem description

First of all, to perform any machine learning solution, data is needed. For this case study, a Brazilian logistics company provided some data sent by many trackers, so all the data used is real.

The modules installed in the vehicles send data during their entire period of operation. The company provided 12586 registries. The company provided 12586 registries. A registry is composed of all the data transmitted by one module in one day period. On average, each module sends 1116.67 points per day. Each point has eight attributes:

Battery Voltage : Float value for voltage of the vehicle battery;

: Float value for voltage of the vehicle battery; Longitude : Float value for the vehicle longitude;

: Float value for the vehicle longitude; Latitude : Float value for the vehicle latitude;

: Float value for the vehicle latitude; Ignition : Boolean value indicating whether the ignition is

switched on;

: Boolean value indicating whether the ignition is switched on; Odometer : Float value for the vehicle odometer;

: Float value for the vehicle odometer; GPS Signal : Boolean value indicating whether the GPS signal

is valid;

: Boolean value indicating whether the GPS signal is valid; Velocity : Float value for the vehicle speed;

: Float value for the vehicle speed; Memory index: Integer value for the memory position that the

point is saved in the module.

Since the modules send data regularly and on different frequencies according to each module, each registry has a different size. This way, the size of each registry is the number of points sent multiplied by 8 (quantity of attributes).

The aim is identifying the faults that a registry can have or if it is working correctly. This way, this is a multi-class classification problem, where there are eight classes; one class for the registries without faults, and seven possible faults, listed below:

Fault 1 : Wrong pulse set up;

: Wrong pulse set up; Fault 2 : Odometer locked;

: Odometer locked; Fault 3 : GPS locked;

: GPS locked; Fault 4 : Ignition wire released;

: Ignition wire released; Fault 5 : Accelerometer defective;

: Accelerometer defective; Fault 6 : Module buffer with a problem;

: Module buffer with a problem; Fault 7: GPS with jumps in location;

Some of these faults have several occurrences too low. The flaws that had less than 3% were all labeled as “Others”. The quantity of data of each failure can be seen on the table below.

Development

I aim to detect and isolate the faults; however, while just detecting faults is a binary problem, detection and isolation is a multi-class problem. It might be much easier to detect the faults. Look by the company side, almost always, when a module present faults, they need to replace it. So it does not matter much what is the fault, anyway we need to replace the module.

We propose to models here, one to detect faults and another to detect and isolate the faults.

Data pre-processing

As we aim to make minor changes in the dataset, without using any specific knowledge about the system operation, the data pre-processing is reasonably straightforward.

We choose to keep all the data, not performing any outlier analysis. Mostly outlier is a strong indication that there is a fault, if we throw all the outliers away, a lot of faulty registries would be gone. This way, the only thing we do is a normalization, using the RobustScaler. A traditional normalization can cause the data to be suppressed by some high value, so RobustScaler is used. This scaler uses the first and third quartiles as constraints and not the maximum and minimum values, so this normalization is more robust and maintains the outliers.

Considering the registries have variable length, it was not possible to directly construct the ML models with this data set. In this way, it was necessary to set a default size for all retries. Knowing that the highest number of points present in a registry is 6410, the mean of the number of points is 1116.67, and the standard deviation is 1155.75, the registries with a smaller amount of data were completed with “0'’ until they have the same amount of data as the largest registry.

Different approaches to set the default size of a registry could be performed, such as truncating the amount of data from above-average registries, and completing with “0'’ those that are below average, or truncating the amount of data from all registries with more data than the smaller registry. However, these approaches were tested and considered unfeasible due to the divergence of the CNN’s.

CNN architecture

I made a model based on VGG-16.

Using 4 convolutional blocks. Each like Conv->Conv->Pooling. Then two fully connected layers.

The idea on the convolutions is to make the filters choose and modify the attributes. To do that, we disposed of each registry with (8, 6410, 1) shape. This way, the CNN threats the data like an image. To check more deeply the architecture, check my GitHub (the link is on the introduction and at the conclusion).

The used batch size was 1, learning rate 0.0001 and he_uniform initializer. The batch_size can be higher. However, due to hardware restrainments, three was higher for me.

Model Training

Two models were trained, one for fault detection and another to fault detection and isolation. Thus, two structures of the same dataset were defined. These structures are described as follows:

Structure 1 : was used in the experiments to detect if the system had faults or not. It contains 12586 records, of which 7480 are flawed, and 5106 are flawless;

: was used in the experiments to detect if the system had faults or not. It contains 12586 records, of which 7480 are flawed, and 5106 are flawless; Structure 2: was used in the experiments to detect and isolate faults. It contains 5106 records flawless, 2170 with failure 2, 1573 with failure 3, 1702 with failure 4, and 2035 with “Others” failure, totaling 12586 records.

All the models were written in Python 3.6 with Sk-learn 0.20.2 and ran on Ubuntu 18.04 with an Intel i7–7700HQ, 16Gb RAM, GTX 1050Ti.

To run the algorithms, the datasets were separated into a training and a test set. For every experiment, 20% of the registries were for testing and 80% for training.

Model Evaluation

The metrics need adequate interpretation. In the case study in question, it is crucial to minimize the number of false positives, as this would involve sending a technician to perform unnecessary maintenance. While a false positive implies receiving a complaint of some malfunctioning module. Then, from a financial point of view, the recall has greater relevance than precision.

With 88.42 Precision and 87.96 Recall, the confusion matrix for the fault detection can be seen below:

With 54.98 Precision and 52.57 Recall, the confusion matrix for the fault detection and isolation can be seen below:

For both models, there is high variance and high bias. The models could have better results if the architecture was deeper or changing some hyperparameters. Alternatively, even performing more extensive data pre-processing.

Conclusions

As proposed, these models can detect and isolate faults on vehicle fleet tracking modules. However, the evaluative metrics are not suitable to use the models. On my other article, using knowledge from the systems, the metrics were higher can be me implemented at the company to diagnose faults on their modules remotely.

The limitations of the employed methods include the inability of the models to discover faults that are not mapped, so any faults which have not been learned would be misclassified to be one of the known ones.

As in the article, there was not any code, feel free to check it out on my GitHub repository.

References

[1] D. van Schrick, “Remarks on terminology in the field of

supervision, fault detection and diagnosis,” IFAC Proceedings

Volumes, vol. 30, no. 18, pp. 959–964, 1997.

[2] D. Wang and W. T. Peter, “Prognostics of slurry pumps based

on a moving-average wear degradation index and a general

sequential monte carlo method,” Mechanical Systems and

Signal Processing, vol. 56, pp. 213–229, 2015."
Transfer Learning Intuition for Text Classification,"Transfer Learning Intuition for Text Classification

This post is the fourth post of the NLP Text classification series. To give you a recap, I started up with an NLP text classification competition on Kaggle called Quora Question insincerity challenge. So I thought to share the knowledge via a series of blog posts on text classification. The first post talked about the different preprocessing techniques that work with Deep learning models and increasing embeddings coverage. In the second post, I talked through some basic conventional models like TFIDF, Count Vectorizer, Hashing, etc. that have been used in text classification and tried to access their performance to create a baseline. In the third post, I delved deeper into Deep learning models and the various architectures we could use to solve the text Classification problem. In this post, I will try to use ULMFit model which is a transfer learning approach for NLP.

As a side note: if you want to know more about NLP, I would like to recommend this excellent course on Natural Language Processing in the Advanced machine learning specialization. You can start for free with the 7-day Free Trial. This course covers a wide range of tasks in Natural Language Processing from basic to advanced: sentiment analysis, summarization, dialogue state tracking, to name a few. You can start for free with the 7-day Free Trial.

Before introducing the notion of transfer learning to NLP applications, we will first need to understand a little bit about Language models.

Language Models And NLP Transfer Learning Intuition:

In very basic terms, the objective of the language model is to predict the next word given a stream of input words. In the past, many different approaches have been used to solve this particular problem. Probabilistic models using Markov assumption is one example of this sort of models.

In the recent era, people have been using RNNs/LSTMs to create such language models. They take as input a word embedding and at each time state return the probability distribution of next word probability over the dictionary words. An example of this is shown below in which the…"
From Pandas to PySpark with Koalas,"Photo by Ozgu Ozden on Unsplash

For those who are familiar with pandas DataFrames, switching to PySpark can be quite confusing. The API is not the same, and when switching to a distributed nature, some things are being done quite differently because of the restrictions imposed by that nature.

I recently stumbled upon Koalas from a very interesting Databricks presentation about Apache Spark 3.0, Delta Lake and Koalas, and thought that it would be nice to explore it.

The Koalas project makes data scientists more productive when interacting with big data, by implementing the pandas DataFrame API on top of Apache Spark. pandas is the de facto standard (single-node) DataFrame implementation in Python, while Spark is the de facto standard for big data processing. With this package, you can: - Be immediately productive with Spark, with no learning curve, if you are already familiar with pandas. - Have a single codebase that works both with pandas (tests, smaller datasets) and with Spark (distributed datasets).

source: https://koalas.readthedocs.io/en/latest/index.html"
Data Science Methodology 101,"Data Science Methodology 101

Every Data Scientist needs a methodology to solve data science’s problems. For example, let’s suppose that you are a Data Scientist and your first job is to increase sales for a company, they want to know what product they should sell on what period. You will need the correct methodology to organize your work, analyze different types of data, and solve their problem. Your customer doesn’t care about how you do your job; they only care if you will manage to do it in time.

What is Methodology in Data Science?

Methodology in Data Science is the best way to organize your work, doing it better, and without losing time. Data Science Methodology is composed of 10 parts:

In this article, there are five parts, each of which contains more steps:

From Problem to Approach From Requirements to Collection From Understanding to Preparation From Modeling to Evaluation From Deployment to Feedback

If we look at the chart in the last image, we see that it is highly iterative and never ends; that’s because in a real case study, we have to repeat some steps to improve the model.

From Problem to Approach

Every customer’s request starts with a problem, and Data Scientists’ job is first to understand it and approach this problem with statistical and machine learning techniques.

The Business Understanding stage is crucial because it helps to clarify the goal of the customer. In this stage, we have to ask a lot of questions to the customer about every single aspect of the problem; in this manner, we are sure that we will study data related, and at the end of this stage, we will have a list of business requirements .

stage is crucial because it helps to clarify the goal of the customer. In this stage, we have to ask a lot of questions to the customer about every single aspect of the problem; in this manner, we are sure that we will study data related, and at the end of this stage, we will have a list of . The next step is the Analytic Approach, where, once the business problem has been clearly stated, the data scientist can define the analytic approach to solve the problem. This step entails expressing the problem in the context of statistical and machine-learning techniques, and it is essential because it helps identify what type of patterns will be needed to address the question most effectively. If the issue is to determine the probabilities of something, then a predictive model might be used; if the question is to show relationships, a descriptive approach may be required, and if our problem requires counts, then statistical analysis is the best way to solve it. For each type of approach, we can use different algorithms.

From Requirements to Collection

Once we have found a way to solve our problem, we will need to discover the correct data for our model.

Data Requirements is the stage where we identify the necessary data content, formats, and sources for initial data collection, and we use this data inside the algorithm of the approach we chose.

is the stage where we identify the necessary data content, formats, and sources for initial data collection, and we use this data inside the algorithm of the approach we chose. In the Data Collection Stage, data scientists identify the available data resources relevant to the problem domain. To retrieve data, we can do web scraping on a related website, or we can use repository with premade datasets ready to use. Usually, premade datasets are CSV files or Excel; anyway, if we want to collect data from any website or repository, we should use Pandas, a useful tool to download, convert, and modify datasets. Here is an example of the data collection stage with pandas.

import pandas as pd # download library to read data into dataframe pd.set_option('display.max_column', None)

dataframe = pd.read_csv(""csv_file_url"")

print(""Data read into dataframe!"") dataframe.head() # show the first few rows

dataframe.shape # get the dimensions of the dataframe

From Understanding to Preparation

Now that the data collection stage is complete, data scientists use descriptive statistics and visualization techniques to understand data better. Data scientists, explore the dataset to understand its content, determine if additional data is necessary to fill any gaps but also to verify the quality of the data.

In the Data Understanding stage, data scientists try to understand more about the data collected before. We have to check the type of each data and to learn more about the attributes and their names.

# get all columns from a dataframe and put them into a list

attributes = list(dataframe.columns.values) # then we check if a column exist and what is its name.

print([match.group(0) for attributes in attributes for match in [(re.compile("".*(column_name_keyword).*"")).search(attributes)] if match])

In the Data Preparation stage, data scientists prepare data for modeling, which is one of the most crucial steps because the model has to be clean and without errors. In this stage, we have to be sure that the data are in the correct format for the machine learning algorithm we chose in the analytic approach stage. The dataframe has to have appropriate columns name, unified boolean value (yes, no or 1, 0). We have to pay attention to the name of each data because sometimes they might be written in different characters, but they are the same thing; for example (WaTeR, water), we can fix this making all the value of a column lowercase. Another improvement can be made by deleting data exceptions from the dataframe because of their irrelevance.

# replacing all 'yes' values with '1' and 'no' with '0'

dataframe = dataframe.replace(to_replace=""Yes"", value=1)

dataframe = dataframe.replace(to_replace=""No"", value=0) # making all the value of a column lowercase

dataframe[""column""] = dataframe[""column""].str.lower()

From Modeling to Evaluation

Once data are prepared for the chosen machine learning algorithm, we are ready for modeling.

In the Modeling stage, the data scientist has the chance to understand if his work is ready to go or if it needs review. Modeling focuses on developing models that are either descriptive or predictive, and these models are based on the analytic approach that was taken statistically or through machine learning. Descriptive modeling is a mathematical process that describes real-world events and the relationships between factors responsible for them, for example, a descriptive model might examine things like: if a person did this, then they’re likely to prefer that. Predictive modeling is a process that uses data mining and probability to forecast outcomes; for example, a predictive model might be used to determine whether an email is a spam or not. For predictive modeling, data scientists use a training set that is a set of historical data in which the outcomes are already known. This step can be repeated more times until the model understands the question and answer to it.

stage, the data scientist has the chance to understand if his work is ready to go or if it needs review. Modeling focuses on developing models that are either descriptive or predictive, and these models are based on the analytic approach that was taken statistically or through machine learning. is a mathematical process that describes real-world events and the relationships between factors responsible for them, for example, a descriptive model might examine things like: if a person did this, then they’re likely to prefer that. is a process that uses data mining and probability to forecast outcomes; for example, a predictive model might be used to determine whether an email is a spam or not. For predictive modeling, data scientists use a that is a set of historical data in which the outcomes are already known. This step can be repeated more times until the model understands the question and answer to it. In the Model Evaluation stage, data scientists can evaluate the model in two ways: Hold-Out and Cross-Validation. In the Hold-Out method, the dataset is divided into three subsets: a training set as we said in the modeling stage; a validation set that is a subset used to assess the performance of the model built in the training phase; a test set is a subset to evaluate the likely future performance of a model.

Here is an example of modeling and evaluation:

# select dataset and training field

data = pd.read_csv(""student-mat.csv"", sep="";"")

data = data[[""G1"", ""G2"", ""G3"", ""studytime"", ""failures"", ""absences""]]

predict = ""G3"" # select field to predict x = np.array(data.drop([predict], 1))

y = np.array(data[predict]) # split the dataset into training and test subsets

x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.1) linear = linear_model.LinearRegression() # create linear regression model linear.fit(x_train, y_train) # perform the training of the model

acc = linear.score(x_test, y_test) # calculate the accuracy

print(""Accuracy: "", acc) # print the accuracy of the model

From Deployment to Feedback

Data scientists have to make the stakeholders familiar with the tool produced in different scenarios, so once the model is evaluated and the data scientist is confident it will work, it is deployed and put to the ultimate test.

The Deployment stage depends on the purpose of the model, and it may be rolled out to a limited group of users or in a test environment. A real case study example can be for a model destined for the healthcare system; the model can be deployed for some patients with low-risk and after for high-risk patients too.

stage depends on the purpose of the model, and it may be rolled out to a limited group of users or in a test environment. A real case study example can be for a model destined for the healthcare system; the model can be deployed for some patients with low-risk and after for high-risk patients too. The Feedback stage is usually made the most from the customer. Customers after the deployment stage can say if the model works for their purposes or not. Data scientists take this feedback and decide if they should improve the model; that’s because the process from modeling to feedback is highly iterative.

When the model meets all the requirements of the customer, our data science project is complete."
Brand Marketing on Facebook using Statistical Analysis,"Tips and tricks to promote brands and gain insights from data

Suppose, some day you think of starting a company. In this age and time, one of the most important things is the reach of your company. If the company is formed on certain products which are used by people in day-to-day life, then it is likely that you already have a lot of competitors in the market. Now, what matters is to beat the heat of the competition. So, the places where your company becomes accessible to people is not only the advertisement hoardings these days. It’s the social media where people keep scrolling through their news feed around three times a day. On average, an American spends 705 hours on social media every year. The chances of failing are negligible, and even if you do fail, you have nothing to lose.

So, let’s assume your company is officially participating in brand marketing on Social Media. You’ve set up a Facebook or Instagram page as the brand that is supposed to be marketed is a cosmetic brand ( Not a big fan of cosmetics but the dataset used is that of a cosmetic brand). You respond to customer questions, follow fans, post important news, and thank your advocates for their support. Beyond that, are you taking enough actions to monitor and analyze the results of your work? If you’re engaging in social media, then you should certainly measure those activities. Is there any better way of knowing it? It’s easier done than said, sounds ironical, huh ?

Social media metrics are data and statistics that give you insights into your social media marketing performance. Some of these metrics are explained in the blog below.

To suffice the purpose of this blog, we’ll segment the metrics into four different categories:

Awareness : These metrics illuminate your current and potential audience.

: These metrics illuminate your current and potential audience. Engagement : These metrics show how audiences are interacting with your content.

: These metrics show how audiences are interacting with your content. Conversion : These metrics demonstrate the effectiveness of your social engagement.

: These metrics demonstrate the effectiveness of your social engagement. Consumer: These metrics reflect how active customers think and feel about your brand.

I’ll be covering all the categories in general without going into the specific details for each. So, following are some of the metrics that you need to know to get a better understanding of the whole thing:

Reach: The total number of distinct people or the users the post has reached.

Impressions: The number of times a post was seen by the users. A post reached to n number of people can be seen by them m times individually. So, the total impressions become m*n. The number of times a post is displayed in your news feed, irrespective of the post being clicked. People may see multiple impressions of the same post. For example, someone might see a Page update in News Feed once, and then a second time if their friend shares it.

Total number of page likes: Likes show your audience size on Facebook. Over time, that number should be growing. If you’re stuck with around the same number of likes for months, it means one of two things:

1) You’re losing the same number of Likes as you’re gaining.

2) New people aren’t following you

Engagement: It means the number of times the users have performed an action on the post. Engagement is one of the most important Facebook metrics you can track. Subjectively, engagement is a sign that people actually like the content you’re sharing. But another reason engagement is so valuable is it may give your posts more exposure to your audience. This includes liking, commenting, sharing and people who’ve viewed your video or clicked on your links and photos. And it also includes people who’ve clicked on a commenter’s name, liked a comment, clicked on your Page name and even gave negative feedback by reporting your post.

Consumptions: This metric is similar to Engagement but Consumption does not necessarily produce a story.

Consumptions = Link Clicks + Photo Views + Video Plays + Other Clicks (some of which do generate stories)

Total interactions: As we know, the main motive is to increase the number of people viewing the post and to increase the number of interactions (like, comment, share) with the post so that a story is created and it automatically appears to the viewer’s friends in their News Feed. The total interactions are calculated from the actions performed by Lifetime Post Engaged Users (Story is created) and Lifetime Post Consumers (Story not created). Do note these metrics, these will be used later.

Total interactions= Likes + Comments + Shares

Type of Posts: There are four types of post on Facebook: Video, Photo, Status, Links. A general human tendency is to look at images, read and share them. Statuses are generally longer and people are reluctant to read it. So, the natural biasing is towards looking and reacting to Videos, Pictures followed by statuses.

Last year (2018), Facebook admitted that it prioritizes video in its algorithm, with extra emphasis on live video. If you can create video content, you have a much better chance of getting to the top of news feeds. So, you’ll see the later visualizations are a little biased towards Video Posts.

Paid Likes: Number of people who have Liked your Page as a result of a Facebook Ad campaign.

The Facebook page analytics provides data and statistics related to these metrics. So, what I have done is I have a dataset with the name Morto et al, which gave the metrics for advertisement of particular cosmetic anonymized brand on Facebook. The dataset has a total of 500 records which has 19 columns with such metric values defined. Now, coming back to next part of this blog is getting started with R programming.

R is a statistical programming language. R possesses an extensive catalog of statistical and graphical methods. It includes machine learning algorithms, linear regression, time series, statistical inferences. There are various packages in R which make it easy to get the tasks in the data science domain done easily. For detailed information theoretically on R, visit https://en.wikipedia.org/wiki/R_(programming_language) and https://www.tutorialspoint.com/r/index.htm to get help for syntax of R programming language.

Going to the project wherein I plotted various data visualizations which will help any layman to make decisions as to what actually helps in bringing the advertisement close to the end user.

Note: The following observations and data analysis was done by intuition and then some changes were done to get the best results out of it.

Chiefly, The total page likes depend on following Facebook Post metrics:

Total Reach

Total Impressions

Type of Post (Photo, Status, Video, Link)

Weekday

Hour of the day

With the help of multiple regression analysis, we get the coefficients for all the above metrics which will help us determine the importance of each metric, which helps in increasing our page likes.

Why Multiple Regression? Multiple regression is an extension of linear regression into relationship between more than two variables. In simple linear relation we have one predictor and one response variable, but in multiple regression we have more than one predictor variable and one response variable.

We create the regression model using the lm() function in R. The model determines the value of the coefficients using the input data. Next we can predict the value of the response variable for a given set of predictor variables using these coefficients.

data<-read.csv(""Morto_Et_Al.csv"") input<-read.csv(file = ""facebook.csv"", sep = "","")[ ,c('PTL','LPTReach','LPTImpressions','Type','Weekday','Hour')] model<-lm(PTL~LPTReach+LPTImpressions+Type+Weekday+Hour,data=input) a <- coef(model)[1]

XLPTReach <- coef(model)[2]

XLPTImpressions <- coef(model)[3]

XTypePhoto <- coef(model)[4]

XTypeStatus <- coef(model)[5]

XTypeVideo <- coef(model)[6]

XWeekday <- coef(model)[7]

Xhour <- coef(model)[8]

2. Prediction analysis:

Here, using the coefficients obtained by multiple regression analysis are used to generate an equation wherein putting the values for the variables the equation predicts the number of likes depending on the type of post.

x1 <- readline(""What is the total reach?"")

x1 <- as.numeric(x1)

x2 <- readline(""What is the value for total impressions?"")

x2 <- as.numeric(x2)

x6 <- readline(""What is weekday?"")

x6 <- as.numeric(x6)

x7 <- readline(""Which is the hour of the day?"")

x7 <- as.numeric(x7) x<-c(""Photo"",""Status"",""Video"")

type<-readline(""What is the type of post?"") if(""Photo"" %in% type) {

Y = a + XLPTReach*x1+XLPTImpressions*x2+XTypePhoto*2.5+XWeekday*x6+Xhour*x7Z = a + XLPTReach*x1+XLPTImpressions*x2+XTypePhoto*3+XWeekday*x6+Xhour*x7 }

else if (""Status"" %in% x) {

Y = a + XLPTReach*x1+XLPTImpressions*x2+XTypeStatus*1.4+XWeekday*x6+Xhour*x7

Z = a + XLPTReach*x1+XLPTImpressions*x2+XTypeStatus*2+XWeekday*x6+Xhour*x7 }

else if (""Video"" %in% x) {

Y = a + XLPTReach*x1+XLPTImpressions*x2+XTypeVideo*4+XWeekday*x6+Xhour*x7

Z = a + XLPTReach*x1+XLPTImpressions*x2+XTypeVideo*5+XWeekday*x6+Xhour*x7

}

3. Post weekday v/s total reach, Post weekday v/s Total Impressions:

Here, I have plotted a line graph to show the relation, how a post weekday affects the total reach and impressions. Results show a post posted on weekday 3 gives the best results for reach, and the same post has the maximum number of impressions the following day. Makes sense, doesn’t it?"
Data Science: Self-Taught vs. College,"Data Science has been a hot field for a couple of years now, there’s no point in even discussing that. As a newcomer, you might be wondering what’s the best way to get into the field, and how quickly you could acquire some worthy knowledge.

Photo by Avi Richards on Unsplash

In today’s article, I want to discuss two of the most popular ways of getting into any tech-related field, and those are:

By being self-taught

By getting a degree

And furthermore, I want to explore if those two options are somewhat different for a data science jobs, opposite to a let’s say front-end developer job.

Before starting, let me share a little background with you. I’m not a self-taught data scientist, I decided to go the college route. But before enrolling in my master’s, I learned to code on my own, a little bit with Java and Android, and a little bit with Python and data in general.

Because I have tried both, this article shouldn’t be biased. However, if you are making the decision right now, it should be based only on this article. I strongly encourage you to do further research as some stuff will differ from person to person, from country to country, and from college to college.

As I’m on college right now, the college route will be the first one to cover. Without further ado, let’s dive in.

The College Route

Photo by Element5 Digital on Unsplash

The college route is the one most people make, and there’s a good reason why.

If you’re thinking about enrolling into a Data Science master’s degree as I did, here are the 3 big questions you should know the answer to:

1. Does your country even offer a Data Science degree?

This one is kind of obvious, but Data Science isn’t just yet well recognized and represented in most universities. Sure, maybe you’ll dive a bit into machine learning on general Computer Science degree, but that’s not a Data Science specialization in any way, shape or form. Developing countries maybe still don’t offer a Data Science degree, so…"
AWS Elastic MapReduce (EMR) — 6 Caveats You Shouldn’t Ignore,"If you are in data and analytics industry, you must have heard of the burgeoning trend “data-lake” which, on simpler notes, represents a storage strategy that allows organizations to store data from different sources and of different characteristics (size, format and velocity) in one place. Data-lake then becomes an enabler of a number of use-cases like advanced analytics or data warehousing to name a few and generally data is moved to a more specialized store e.g. MPP relational engines or NoSQL to better serve the requirements of a specific use-case. If the platform is being provisioned in a cloud environment like AWS, Azure or GCP then object stores (e.g. S3 for AWS, Azure Data Lake Store Gen2 for Azure) are usually the strongest candidates to provide the foundational physical layer of data-lakes. Once the data is in data-lake, it passes through series of zones/phases/layers to establish semantic consistency in the data and to conform it for optimal consumption.

Usually, as per the reference architecture of data-lake platforms, which is agnostic of which cloud provider you choose, Hadoop (specifically Spark) is employed as a processing engine/component to process the data in data-layer as it progresses through different layers. These processing frameworks are well-integrated with data-lake services, provide capabilities like horizontally scalability, in-memory computation and unstructured data processing which position them as viable options in this context. One generally has a number of choices to use Hadoop distributions in the cloud for instance one can proceed with provisioning IaaS based infrastructure (i.e. AWS EC2 or Azure Virtual machines and installing a Hadoop distribution e.g. vanilla Hadoop, Cloudera/Hortonworks). Alternatively, almost all the cloud providers are providing Hadoop as managed services natively (e.g. ElasticMapReduce (EMR) in AWS, HDInsight/Databricks in Azure, Cloud Dataproc in GCP). Each of the options have their pros and cons. For example with IaaS based implementation, the overhead of provisioning, configuring and maintaining the cluster by yourself becomes a strong concern for many. Also, the intrinsic propositions of Cloud like elasticity and scalability pose a challenge in IaaS based implementation."
"How to setup the Python and Spark environment for development, with good software engineering practices","How to setup the Python and Spark environment for development, with good software engineering practices Bogdan Cojocar · Follow Published in Towards Data Science · 6 min read · Mar 17, 2019 -- 4 Share

In this article we will discuss about how to set up our development environment in order to create good quality python code and how to automate some of the tedious tasks to speed up deployments.

We will go over the following steps:

setup our dependencies in a isolated virtual environment with pipenv

how to setup a project structure for multiple jobs

how to run a pyspark job

how to use a Makefile to automate development steps

to automate development steps how to test the quality of our code using flake8

how to run unit tests for PySpark apps using pytest-spark

running a test coverage, to see if we have created enough unit tests using pytest-cov

Step 1: setup a virtual environment

A virtual environment helps us to isolate the dependencies for a specific application from the overall dependencies of the system. This is great because we will not get into dependencies issues with the existing libraries, and it’s easier to install or uninstall them on a separate system, say a docker container or a server. For this task we will use pipenv.

To install it on a mac os system for example run:

brew install pipenv

To declare our dependencies (libraries) for the app we need to create a Pipfile in the route path of our project:



url = '

verify_ssl = true

name = 'pypi' [[source]]url = ' https://pypi.python.org/simple' verify_ssl = truename = 'pypi' [requires]

python_version = ""3.6"" [packages]

flake8 = ""*""

pytest-spark = "">=0.4.4""

pyspark = "">=2.4.0""

pytest-cov = ""*""

There are three components here. In the [[source]] tag we declare the url from where all the packages are downloaded, in [requires] we define the python version, and finally in [packages] the dependencies that we need. We can bound a dependency to a certain version, or just take the latest one using the “*”symbol.

To create the virtual environment and to activate it, we need to run two commands in the terminal:"
Getting Started with Google BigQuery’s Machine Learning — Titanic Dataset,"While still in Beta, BigQuery ML has been available since mid last year; however, I didn’t get around to working with this Google cloud-based Machine Learning offering until recently. As a non-data scientist, my first impression — what’s not to like? After all, the ability to run ML models from the comfort of web-based SQL editor is a dream come-true for any analyst out there. Not only this platform eliminates the need to learn a programming language, be it R , Python or SAS; it also streamlines data engineering process by leveraging existing BigQuery data sources, instead of having to bring external data into your model. Effectively, this product removes a number of barriers to entry into this coveted data science specialty and democratizes the field of ML by allowing any analyst with adequate knowledge of SQL to run linear and logistic regression models without having to invest in pricey hardware, such as multi-core GPUs usually needed to support a scalable ML project. Below image does a great job showcasing platform’s capabilities:

BigQuery ML demo from Google AI Blog

Many aspiring data science students turn to the trusted Titanic: Machine Learning from Disaster data set from one of the most popular Kaggle competitions to practice working with binary classification models. In fact, for a beginner, a binary classification model is a fairly easy concept to grasp: your task is to simply predict whether a certain event will occur or will not happen; or whether a certain condition will evaluate to be true or false. For this problem, anyone is able to wrap their head around the concept of predicting whether a particular Titanic ship passenger survives one of the most monumental ship wrecks of all times: there are only two possibilities here. To follow along you simply need to log in to existing Kaggle account or create a new one and download all three files provides. The irony of using Kaggle website (purchased by Google back in 2017) and BigQuery platform (another Google product) is not lost on me. Let’s dive into this problem using sample data set and a working BigQuery instance."
How to spot red flags in a data science job opportunity,"Data science career advice

How to spot red flags in a data science job opportunity

Here are a few ways I assess a job before deciding to accept the job offer Schaun Wheeler · Follow Published in Towards Data Science · 11 min read · Sep 17, 2019 -- 2 Listen Share

This job looks delicious.

A while ago, I got a question from someone who had read one of my previous posts on finding a data science job. He mentioned my prioritization of work-life balance, and asked:

“I definitely agree, but how do you assess this? Do you ask in an interview something like, ‘how often do you work weekends?’ I’ve always been afraid of coming off as unenthusiastic, I suppose, and have only tried to ferret out info on Glassdoor or similar sites. And even if you do ask, I wonder how reliable the answer would be. At any rate, I’d be interested to hear your own approach for appraising a company in this regard.”

That got me thinking more broadly about all the things I’ve wanted from a job when I was an applicant, and all the ways I tried (and often failed) find out those things before I actually accepted the position. I think every job acceptance is a gamble, just as every job offer is a gamble, but I do think there are ways a candidate can minimize unexpected downside — no matter how far the job falls short of how good I thought it could be, I can at least avoid many jobs that will far exceed my expectations of how bad they could possibly be. That’s something, even if it’s not as much certainty as I would like.

So this post is about major problems I’ve discovered after accepting an offer of employment, and some ideas about how I could have spotted those problems before becoming tied to the job.

Spotting work-life balance problems

Let’s start with the issues that started this train of thought. Work-life balance is hard to assess, so I usually don’t assess it until I’m reasonably sure both I and my prospective employer feel the job is a good mutual fit. If an employer is going to cut me loose oher reasons (and there are many, even for the most experienced applicant), there’s no point in talking about anything else. By waiting until near the end of the process, I ensure as good a relationship with the hiring managers as I can expect to have without actually being hired. If the conversation still feels very threatening at that point, that tells me the job probably isn’t a very good fit for me.

I usually start the conversation along these lines: “I’d like to talk about work-life balance. I’m used to putting in whatever time and effort it takes to get the job done, but I also know how important it is for me to protect myself from burnout. Can you tell me a little about expectations along those lines? How often can I expect to need to work nights or weekends? If I have a family issue where it’s going to be better for me to work from home on a particular day, will I have the freedom to do that?”

I’ve found the answers to these questions less important than the tone and body language I observe while they are being answered. I’ve experienced obvious defensiveness: the manager bristles and says of course that sort of accommodation has to happen sometimes and it’s not realistic to expect that it doesn’t, but they try to keep it to a minimum because really they need people to put in whatever work is needed. I’ve experienced patronizing non-answers: the manager leans back in their chair and says something meaningless like “you know, we work hard and play hard here” or otherwise makes it clear that they’re using this as a teaching moment to educate me about what it means to do grown-up work. I’ve also experienced general cluelessness: the manager makes it clear that they haven’t really thought much about the subject and can’t really tell mehow often that sort of thing happens.

Defensiveness means they know work-life balance is a problem at their company but don’t feel empowered to do anything about it. Patronization means they have no respect for their employees’ time and health and are just trying squeeze as much benefit as they can out of their workforce without giving any thought to long-term sustainability. Cluelessness is either faked, in which case it means that the situation is bad but they’re not willing to be honest enough to tell me about it — in other words, they lie — or it’s real and that means they’re so disconnected from their workforce that they honestly don’t know.

Spotting unrealistic expectations about data science

Everyone says they want data science. Few people are willing to put in the work it takes to maintain a full-scale data science capability. There are several ways to spot this. In my experience, a walled-garden engineering organization, typified by a waterfall approach to planning, is a pretty strong indicator that a company isn’t ready or willing support a data scientist. Likewise, a focus on developing reports or making dashboards as a major part of the job description indicates, in my mind, that they’re not really asking for a data scientist. They’re asking for an analyst — perhaps one with statistical expertise, but an analyst nonetheless. But those are symptoms, not the disease.

The real issue, in my opinion, is whether a company values productionization of data science work. Analytic reports are made to be throw away — it doesn’t matter if the report goes to an engineer or to an executive. Any outputs that only serve the purpose of informing someone do not last. Jobs that throw my work away are not satisfying jobs — and it indicates that the company doesn’t really know what to do with my work.

In my experience, I can tell how much a company values productionization by their comfort levels when asked about productionization. As with work-life balance, the responses can range from defensive to patronizing to clueless. I’ve often seen prospective employers try to skirt this issue by talking about how they manage projects. That’s a red flag.

A project manager is an administrative position. Whenever a team is working on anything from building software to planning an event, a bunch of logistical issues (deadlines, stakeholders, division of labor, etc.) need coordinating. The bigger the team and more complicated the project, the more it can help to have someone whose job is to keep an eye on all these moving pieces. Project management is not product management. Project management figures out what part of the overall plan needs code written right now, decides what to do when a new feature exposes flaws in existing features, and maintaining control over the definition of “good enough” (which is, when all is said and done, the only real acceptance criteria any data science project ever has), and other day-to-day issues. Product management focuses on what the team has capacity to work on next (as opposed to right now), what stakeholders are asking for and what meetings are needed to flesh that out, and other ways of tying the implementation work into the larger organization.

If a company can only talk about data science work in terms of projects, I know that I don’t want to work for them. If they can talk about data science in terms of durable product offerings, and how they decide what gets put in the product and what doesn’t, then I know I might want to work for them.

Spotting a wimpy manager

I’ve found Liz Ryan’s concept of a wimpy manager useful. It’s somewhat rare, in my experience, to find a genuinely mean or abusive manager, although I know there are certainly more of those types of managers than any of us would like. It’s comparatively more common to find managers who are so unsure of their own position or competency, that they try to make everyone around them (especially those who report to them) look incompetent while at the same time imposing arbitrary rules designed to make them look “strong” and “decisive”.

I’ve found that wimpy managers systematically show those who report to them that the manager’s time is much more important than anything the employee could want. They cancel on me. They talk constantly about all the important things they did and all the important people they talked to that prevented them from really looking at the task they asked me to do. They’re unable to talk about any idea that they didn’t originate. If I’m looking for these things, they often come out in the interview process.

While a manager who obviously doesn’t care about my time is someone to be avoided, I also don’t want a manager who is painfully accommodating: never intentionally disagreeing with me, caving on a disagreement as soon as I push back, seemingly eager to assure me that whatever I want is what I’ll get. In my experience, this either means that they themselves are so under the thumb of a wimpy manager that they’ve lost their ability to assert their own will, or it means they’re telling me whatever they think I want to hear. Either way, those kinds of managers don’t manage — they just react. A data scientist is usually an individual contributor, which means it is usually a data scientists job to react to needs in the business. When I’ve had to constantly react to someone else’s reactions, I’ve burnt out.

Spotting a lack of prioritization process

Data science (and, I believe, developing code in general) works well when a company has a separation of power. Separation of power means separation of ownership — someone who has authority over one part of the process doesn’t get to have authority over the other. I tend to divide data science work into three parts: priorities, product, and people.

In an Agile framework, priorities are normally owned by the product manager (or the department in which product managers live — which in many organizations is confusingly called “product management” or just “product”). This owner gets to define and prioritize organizational asks, and is responsible for understanding and documenting systems, and for managing stakeholders. This owner has final authority on what gets planned and promised — if they determine that a feature is important enough to consume all available resources, they can put other priorities on hold, even if other people disagree with them.

The product is typically owned by the product owner, or sometimes the implementation team itself (or the department in which product owners and implementers live — which in many organization is just called the “engineering department”). This owner organizes the day-to-day work of the implementers, scheduling work, following up on delivery commitments, prioritizing bug reports, etc. This owner has final authority on the integrity of the product — if they determine that a change will threaten the stability or future availability of something already built, they can prevent the change from taking place until appropriate planning, process development, and change management has taken place.

People are owned by people manager (or the department in which people managers live, which ultimately is HR, but which in practice usually sees individual people managers embedded in all departments). This owner works on career plans and personnel reviews, manages inter-personal conflicts including when people don’t meet expectations, and deals with all of the other human aspects of running a team. This owner has final hiring/firing authority, even though they often receive input on those topics from the other owners.

Individual engineers and analysts have ownership as well — over the specifics of their implementation — but their ownership isn’t separate from the other powers. An analyst might not feel that an analytic solution is sound, but a product manager can still decide that the solution is good enough to ship, and the product owner can still decide that the solution is maintainable, and the people manager can still decide that developing and maintaining that solution is necessary for the employee to meet expectations and keep his or her employment. Good organizations don’t consistently trample on the concerns of their implementers, but the authority to do so is always there and is sometimes necessary and good, since implementers often have way too high a standard for what is good enough to meet a customer need.

Unless a company is an extremely small startup that can’t afford anything better, not having these three powers separated will prevent most data scientists from work on anything meaningful or lasting. When priorities and product are owned by the same person, unrealistic features are approved on unrealistic timeframes. When either priorities or product are owned by the same person who owns people, then mis-prioritization or failure to protect the product are bolstered because anyone who could oppose them is afraid of being fired.

In my experience, the best way to figure out if a company has a prioritization process is to ask about some of the recent things that were built and ask how it was decided that they should be built. If the answer to that kind of question is “[insert executive or manager name here] said we needed it”, that’s a problem. The best indication that a business has a decent prioritization process is that people are able to talk in terms of symptoms, diagnoses, and prescriptions. Companies worth working for can talk about the pain the company felt, explain how they identified the cause of that pain, and tell how they built something to address the cause. Other companies will dance around the issue.

What I really look for

In every case, what I really look for is someone who can give me specifics without blushing. Let’s go back to the example of work-life balance:

In my current job, which I’ve held for around two years now, I’ve worked straight through the weekend (including practically not sleeping) twice. I did that because I was obsessed with a particular analytic problem, not because anyone asked me to or even implicitly suggested that they wanted me to. I can work from home pretty much whenever I want as long as not being in the office doesn’t interfere with anyone else’s work. If you were interviewing with me, I would be able to tell you all of this while looking you in the eye. In fact, it would be obvious that I would be a little excited to be able to tell you about it, because it’s an aspect of the job that highly and genuinely value.

Contrast that with my previous job: I worked straight through nights/weekends about four times a year, and stayed late at work at few times a month, and most of the time I did this because I was told I had to (or was told, with an cynical sort of chuckle, that sometimes “you just have to put in the time it takes to get the job done”, thereby making it known what the expectation was without actually telling me to work around the clock). Work-from-home was not something management had much sympathy for although our department was a little more permissive on that issue than the rest of the company, because I thought it was an important freedom and I was the director of that department. Short deadlines were common and the people who set those deadlines didn’t have much patience for personal issues. If you had interviewed with me for a position at this company, I would have come across as a little defensive. I’d try not to give a patronizing response, but because my managers often adopted the patronizing approach, some of that probably would have bled through. You would have been able to tell that answering the question made me feel uncomfortable.

If something is important to me, it shouldn’t make a prospective employer uncomfortable to talk to me about it. If it does clearly make them uncomfortable, I will probably be disappointed with the situation if I take the job. That holds for all concerns I could potentially have about a job. It’s not a foolproof method, but it’s the best I’ve been able to figure out."
What if AI model understanding were easy?,"Irreverent Demystifiers

What if AI model understanding were easy?

Let’s talk about the What-If Tool, as in “What if getting a look at your model performance and data during ML/AI development weren’t such a royal pain in the butt?” (Or ignore my chatter and scroll straight to the walkthrough screenshots below!)

Why bother with analytics for AI?

Being able to get a grip on your progress is the key to speedy iteration towards an awesome ML/AI solution, so good tools designed for analysts working in the machine learning space help them help you meet ambitious targets and catch problems like AI bias before it hurts your users.

The What-If Tool is for speedy machine learning analytics, built to accelerate iterative model development and training.

Analytics is not about proving anything, so this tool won’t help with that. Instead, it’ll help you discover the unknown unknowns in your data faster. Learn more about explainable AI (XAI) and its limitations here.

About the What-If Tool

The What-If Tool is for speedy machine learning analytics, built to accelerate iterative model development and training. The first version (released in late 2018) was pretty, but you couldn’t use it unless you were all-in on TensorFlow. As someone who appreciates the expediency of tools like Scikit Learn, I’m delighted that What-If Tool is now geared at all analysts working with models in Python.

No more TensorFlow exclusivity!

We’ve been incorporating feedback from internal and external users to make the tool awesome for data scientists, researchers, and corporate megateams alike. Learn more about our UX journey here. Without further ado, let me make myself a hexpresso and play with the most current version to give you my take on what’s awesome and what’s awful.

What’s awesome about the What-If Tool?

Easy to use and versatile

In the current version of What-If Tool, we expanded access to the magic beyond TensorFlow afficionados. Yes, that’s right — no more TF-exclusivity! This is model understanding and quick data exploration for feature selection/preprocessing insights even if you’re allergic to TensorFlow. Want to compare models made in Scikit Learn or PyTorch? Step right up! Does it work with standard Jupyter notebooks? You bet! It also works with Colaboratory because we know you prefer to choose your weapon. The tool is designed to reduce the amount of code you need to write to get your eyes on your data, so it’s built for ease of use.

In this screenshot, we’re using the tool to compare two classifiers (deep neural network on the x-axis, linear model on the y-axis) trained on the UCI Census Income Dataset to predict whether someone will earn more than $50,000 a year. Numbers closer to 1 indicate that a model is giving a stronger YES vote. The scatterplot shows the votes of one model versus the other. See the notebook here and play with it yourself if you’re feeling curious — no install required.

As expected, there’s a positive correlation but the models don’t give identical results. (Working as intended! They’re different models, after all.) If I’m curious about how the model votes are related to, say marital status, it’s very easy to find out — simply select that feature from the dropdown menu.

Voilà! Most of our dataset shows civil marriages and we see an interesting preponderance of other statuses where the models disagree with one another or both vote a strong no. Remember, this is analytics, so don’t jump to conclusions beyond our current dataset!

The What-If Tool is not going to give you every slice of every view of every way that you might want to explore your data. But it’s great at what it’s designed for: a first start with low effort. It also works on a subsample, which means you get a quick look quickly without having to pay the memory cost to ingest and process all your data if you don’t want to. Huzzah for speed!

It’s great at what it’s designed for: a first look with low effort.

Fighting AI bias

The What-If Tool is also your secret weapon for fighting AI bias. To understand why, check out my discussion of AI bias here. Its bias-catching features are not an accident — a large fraction of the project’s core team hails from Google Brain’s PAIR initiative aimed at human-centered research and design to make AI partnerships productive, enjoyable, and fair.

In the fairness tab, we can play with all kinds of uncomfortable questions. For example, we can find out where we’d have to set our classification thresholds (the ones you’d naively want to put at 50%) for males vs females in our test set to achieve demographic parity between them. Uh-oh.

Smarter ML/AI iteration

The What-If Tool incorporates the Facets tool, which tackles the data analytics piece without the model understanding component.

In the features tab, I can get a quick look at histograms to show me how my features are distributed. Oh my goodness, capital loss is a super imbalanced feature with only ~4% nonzero values. I’m already itching to try dropping it and rerunning both models. If you’ve been around the block a few times (or studied the math) you’ll know that putting something like that in a linear model is bad news indeed. I see similar trouble with capital gains. (If you insist on using ’em, how about doing some light feature engineering to combine them? Minuses are awesome.) Ah, and here’s a question for the more advanced analysts among you: can you see why optimizing for accuracy should make us very nervous?

What-If puts both together to help you iterate smartly. Think of it like this: to figure out what to do next in the kitchen, you want a handy way to compare the tastiness of several potential recipes (with model understanding) and also getting a handle on what’s in your grocery bags (with data analytics) so you don’t accidentally use rotten tomatoes. Facets gave you eyes on your ingredients, while the What-If Tool goes a step further to deliver that plus the recipe comparison. If you’ve been cooking blindly, you’ll love this tool for iterative model development and training.

Exploring counterfactuals

Never underestimate the power of being able to ask your own what-if questions, like “What if we raise this person’s work hours and change their gender? How does the model react?” The What-If Tool is purpose-built to give you more of a grip on guided what-if/counterfactual questions. The tool makes it easy to see how the prediction changes if you vary a variable (finally!) over its domain and shows you whether there’s some value where the prediction behaves in a suspicious way and letting you see exactly where the classification flips from, say, NO to YES. Try playing with the counterfactual options to find a datapoint’s most similar counterpart in a different predicted class. It’s a great way to see the effects of subtle differences on your model’s output.

Back to our first tab. That red point I’ve selected is one where the models are having an argument: neural network says nah, but linear model says a gentle yes to high income. What-If… I want to do a quick deep dive into that point off the diagonal? I simply click on it and there’s the info. Turns out the linear model is right, this is a high income earner. Moreover, it’s a married woman who works 10 hours per week. I love how quickly I could see that.

What’s this “visualize” thing on the left? Let’s see what happens if we try toggling the “counterfactual” setting.

Aha! Here’s the nearest buddy where neural network changes its mind and correctly predicts a large salary. And it is a buddy indeed: this is a male executive who works 45 hours a week. What-If… we do a deep dive and see which of these differences the models are most sensitive to?

Looking at the partial dependence plots, we can see that the neural network (blue) seems to expect pay to go up with hours worked, while the linear model (orange) slopes down. Curious. The statistician in me is shouting at all of us not to get excited — they’re probably both wrong in their own way, so we shouldn’t learn anything grand about the universe, but seeing how models react to inputs is very valuable for picking approaches to try next. Our mystery candidate’s lower hours worked look more compelling to the linear model (yeah, quiet down friends, obviously the economist in me is just as suspicious as the statistician). I bet we also want to take a quick look at other features here — how about gender…?

Interestingly, the linear model (orange) is not getting itself too excited about gender, but the neural network (blue) seems more reactive to it. How about our mystery woman’s question-mark of an occupation? Could that be contributing to her lower score by the neural network?

Whoa, while the linear model (orange) is stoic again, the neural network (blue) gives execs a pretty big prediction boost relative to those with missing occupation information. Now isn’t the time to say that snarky thing about linear models versus neural networks, is it? Well, maybe I’ll restrain myself… the whole point of the tool is to give you eyes on your data so you can iterate wisely, not let biases take you by surprise, and create a more awesome model faster. We’re not done yet! (But I sure have a few ideas I’m inspired to try next.)

Learn more about our two model types here.

What’s annoying about the What-If Tool?

Work in progress

The tool isn’t perfect yet. For example, you’ll occasionally stumble onto something guaranteed to earn a scowl from Tufte fans — for example, the screenshot below had me ranting in a meeting recently. (If you can’t see why, it’s a good opportunity for a little data viz lesson: Why are the text labels “Young” and “Not Young” the only visual cues? Why not shape? Because we’re working on making it better in this way and in others too, but perfection takes time. As part of the collaboration, I rant on your behalf to help these issues should dissipate rapidly.)

Also… how about them axis labels?

Unguided exploration

The tool will go where your curiosity takes it, but what do you do if you’re not feeling creative? Perhaps you wish the tool were more prescriptive, guiding your eye towards what’s important? Your feedback is on our radar and we’re working on it, but for those who think something beautiful might be lost if your exploration gets hemmed in, never fear! We believe in options and understand that not everyone wants the prescriptive side of things, just as not everyone wants to play video games with a fixed storyline as opposed to an open world.

Limited customization

You want every customization under the sun, which is such a data-sciency thing to say. I’ve said things like that too — I remember the first question I asked in a mandatory SAS training for stats PhD students: “How do I write these functions myself so they do exactly what I want?”.

So when you ask the same thing about the What-If Tool, I’ll tell you what my profs told me that day: that’s what raw Python and R are for! (Or, heaven help us, C/C++ if you’re going that far down into the weeds.) Visualization tools like the What-If Tool aren’t replacements, they’re accelerators. They give you a first look with minimal effort so you know where to dig, but once you’ve picked your spot, you’re probably going to want to write your own code to dig exactly the way you like to. If you’re an expert analyst with your own awesome way of doing things, our goal is to help you narrow your search so there’s less code to write later, not to replace your entire approach.

TensorFlow-ish terminology

Another thing that irritates me (and the rest of statistician-kind, I’m sure) is the terminology compromises we had to make for the sake of our TensorFlow user group, keeping some of the TensorFlow legacy lingo that makes traditional data scientists want to punch something. Yeah, that “inference” isn’t inference. TensorFlow is a hilarious bucket of words appropriated and promptly misused — fellow nerds, don’t even get me started on its use of “experiment”, “validation”, “estimator”, or the batch vs minibatch thing… Just let this be a lesson about thinking carefully about what you’re calling things when it’s just you and your buddies bouncing some ideas around in a garage. What if the project is a success and everyone will have to live with your choices? Sigh.

Verdict

All in all, these grumbles are on the petty side. Overall, I really like the What-If Tool and I hope you will too.

See it in action!

While the What-If Tool is not designed for novices (you need to know your way around the basics and it’s best if this isn’t your first rodeo with Python or notebooks), it’s an awesome accelerant for the practicing analyst and ML engineer.

If you’re eager to see the What-If Tool in action, you don’t have to install anything — just go here. We’ve got dazzling demos and docs aplenty. If you want to start using it for realsies, you don’t even need to install TensorFlow. Simply pip install witwidget.

If you’re a fan of Google Cloud Platform, you might be excited by a new integration that just got announced. Now you can connect your AI Platform model to the What-If Tool with just one method call! Check out how here.

Thanks for reading! How about an AI course?

If you had fun here and you’re looking for an applied AI course designed to be fun for beginners and experts alike, here’s one I made for your amusement:

Enjoy the entire course playlist here: bit.ly/machinefriend

Liked the author? Connect with Cassie Kozyrkov

Let’s be friends! You can find me on Twitter, YouTube, Substack, and LinkedIn. Interested in having me speak at your event? Use this form to get in touch."
Creating A Voice Recognition Calculator Android App,"Photo by Andres Urena on Unsplash

Automatic Speech Recognition is one of the most famous topics in Machine Learning nowadays, with a lot of newcomers every day investing their time and expertise into it. In this post, we will build a simple end-to-end voice-activated calculator app that takes speech as input and returns speech as output. Inputs include integers and basic mathematical operators, while outputs are the result of the operation uttered by the user.

What This Tutorial isn’t

A theoretical tutorial on how speech recognition works.

A tutorial on Cloud-based speech APIs like Amazon Polly or Google Cloud Speech.

A tutorial on Java syntax and principles.

A tutorial on how the Java-XML communication works on Android.

A tutorial on Android Material Design.

Prerequisites

Basic Android Development knowledge.

Android Studio (you can use other IDEs like Eclipse, but I will be covering Android Studio in this tutorial).

Desired: Basic API knowledge (with no authentication required).

Tutorial Outline

Creating the Android project

Creating the speech-to-text functionality

Dealing with input values (integers and operators)

Calculating the result

Creating the text-to-speech functionality

Creating The Android Project

Let’s start off by opening Android Studio and starting a new project. If you already have a project open, go to File -> Close Project, then follow the figure below:

Choose Empty Activity then click Next:

Choose the project details you want, or keep everything set to default, then click Finish:"
A Dummies’ guide to building a Kubeflow Pipeline,"A Dummies’ guide to building a Kubeflow Pipeline Prafful Mishra · Follow 5 min read · Nov 1, 2019 -- 3 Listen Share

Kubeflow provides a layer of abstraction over Kubernetes handling things in a better way for Data Science & ML pipelines. It allows ML pipelines to become production-ready and to be delivered at scale through the resilient framework for distributed computing(i.e Kubernetes).

We will dive into the details required on how to make a machine learning pipeline from multiple components throughout an ML solution lifecycle.

Before we delve deeper into the implementation of components and stitching of pipelines, let’s have a look at how things are organized in Kubeflow.

Individual components of a pipeline can be generated using functions or docker images with a wrapper (build using ‘kfp’) packaging these components as a stitched pipeline.

1. Building pipeline through functions

Let’s start with building the pipeline using simple Python functions. This is also known as creating pipelines through lightweight or non-reusable components.

Let’s take a simple example of the computation of an average i.e.

avg = add(x1,x2,x3,x4.....xn)/n

We will compute the above formula with each mathematical operation as a component of a pipeline. A basic Python function to do so can be written as:

# function 1

def add(numbers : list) -> int:

return sum(numbers)

# function 2

def div (sum : int, num : list) -> float:

return (sum/len(numbers))

num = [1,2,3]

avg = div(add(num),num)

Just to set the context, the above two components would be running on separate machines (pods) in the underlying Kubernetes cluster. Hence, we simply cannot pass variables across functions.

Context String

However, to make this stateless execution sudo-stateful we will ask both functions to fetch the required variables from a centrally accessible location(this can be Azure storage or a Google cloud or anything for that matter).

Where to look for the required data is passed as a context string which would look like this:

context = {

""numbers"" : ""example.com/address/of/numbers"",

""total"" : ""example.com/address/of/numbers"",

""auth"" : ""auth_token_to_access_data""

}

context = json.dumps(context)

Modified Functions

The functions we declared need to be modified as :

Provision to install necessary packages if required (as every function will run on a new pod which may or may not have the necessary packages)

Provision to download and upload data from the shared memory according to the context string

# component 1

def add(ctx : str) -> str:

""""""This function calculates the sum of all the elements of a

list stored in a shared memory and uploads the result""""""

#loading context string

context = json.loads(ctx)

#defining the install function

import subprocess

def install(name):

subprocess.call(['pip', 'install', name])

#install packages (installing numpy for the sake of demo)

install('numpy')

#getting the auth token from context

auth_token = context[""auth""]



#downloading the data required

numbers = download(context[""numbers""], auth_token)



#uploading the intermediate result

upload(sum(numbers),""example.com/address/of/sum"",auth_token)

#adding the address to intermediate result to context string

context[""sum""] = ""example.com/address/of/sum""

return context



# component 2

def div (ctx : str) -> str:

""""""This function calculates the average from sum and total

number of elements stored in a shared memory and uploads the

result""""""

#loading context string

context = json.loads(ctx)

#defining the install function

import subprocess

def install(name):

subprocess.call(['pip', 'install', name])

#install packages (installing numpy for the sake of demo)

install('numpy')

#getting the auth token from context

auth_token = context[""auth""]



#downloading the data required

sum = download(context[""sum""], auth_token)

numbers = download(context[""numbers""], auth_token)



#uploading the final result

upload(sum/len(numbers),""example.com/address/of/avg"",auth_token)

#adding the address to result to context string

context[""avg""] = ""example.com/address/of/avg""

return context

Once we are ready with our new function definitions, we need to stitch the functions together to form a pipeline from these de-coupled lightweight components. This means, defining the execution flow of all of the components and the sharing of context strings between them across pods.

Stitching functions to form a pipeline

Before we move forward, it is suggested that you have a basic understanding of docker (maybe read “Building small Python Docker images, How to?” by Islam Shehata).

We will use KFP (after you’ve installed it using pip) to stitch the pipeline after converting the functions we defined to a container operation :

from kfp.components import func_to_container_op

#converting functions to container operation

add_operation = func_to_container_op(add, base_image = <base_image>)

div_operation = func_to_container_op(div, base_image = <base_image>)

Let’s stitch the pipeline for the above-created containers:

#importing KFP pipeline

from kfp.dsl import pipeline

# defining pipeline meta

@pipeline(

name='Calculate Average',

description='This pipeline calculates average'

)

# stitch the steps

def average_calculation_pipeline(context: str=context):

#passing context to step 1

step_1= add_operation(context)

#passing output of step 1 to step 2

step_2 = div_operation(step_1.output)

2. Building pipelines through docker images

NOTE : Understanding docker and building docker images is important before proceeding further.

Making changes to the functions

To build a pipeline through heavy-weight (reusable) containers, the functions need two modifications:

accept context string as a sys argv

write the result in to file

no need to have subprocess installations inside the functions, as ‘requirements.txt’ used for building the docker image would take care of that

The function should look something like this:

import sys

add(sys.argv[1])

def add (ctx : str)



#getting the system argument

context = json.loads(ctx)

.

.

.



with open('output.txt','w') as out_file:

out_file.write(json.dumps(context))

Similar changes to be done to the div function and separate images to be built for individual functions (obviously)

Building Docker Images

Docker images need to be created now in the usual manner with no changes to Dockerfile as the arguments to be passed at ENTRYPOINT will be taken care of by KFP.

Uploading Created Images

Once the images are built we need to upload the images to a location accessible by the pods in the cluster (i.e private or public docker registry).

NOTE: relevant image pull auth needs to be taken care of while setting the cluster for private docker registry (refer : Pull image from private registry)

Stitching the pipeline

Once the images have been pushed we are ready to stitch the pipeline once again.

Unlike lightweight containers, we don’t need to convert functions to container operations and directly proceed to stitch the pipeline together.

#importing KFP pipeline

from kfp.dsl import pipeline

# defining pipeline meta

@pipeline(

name='Calculate Average',

description='This pipeline calculates average'

)

#importing container operation

from kfp.dsl import Containerop

# stitch the steps

def average_calculation_pipeline(context: str=context):

step_1 = ContainerOp(

name = 'add', # name of the operation

image = 'docker.io/avg/add', #docker location in registry

arguments = [context], # passing context as argument

file_outputs = {

'context': '/output.txt' #name of the file with result

}

)

step_2 = ContainerOp(

name = 'div', # name of operation

image = 'docker.io/avg/add', #docker location in registry

arguments = [step_1], # passing step_1.output as argument

file_outputs = {

'context': '/output.txt' #name of the file with result

}

)

3. Compiling Kubeflow Pipeline

#importing KFP compiler

from kfp.compiler import Compiler

#compiling the created pipeline

Compiler().compile(average_calculation_pipeline, 'pipeline.zip')

4. Initialising Pipeline Run through Script

This zip file produced after the compilation can either be uploaded to create a kubeflow pipeline through the Kubeflow UI route or can be created using the following script.

#importing KFP client

from kfp import client

#initialising client instance

client = kfp.Client()

#creating experiment

experiment = client.create_experiment(

name = ""Average Experiment"",

description = ""This is the average experiment""

)

#Define a run name

run_name = ""This is test run: 01""

#Submit a pipeline run

run_result = client.run_pipeline(

experiment.id,

run_name,

pipeline_filename,

params = {}

)

print(run_result)

5. Conclusion

The above article describes the most basic explanation for beginners on how to create a Kubeflow pipeline to be able to deliver Machine Learning at scale."
The Hundred-Page Machine Learning Book Book Review,"Should you buy the book?

Yes. But you don’t have to. You can read it first. But you should buy a copy, hold it, read it, sit it on your coffee table. Then when your friends ask, ‘What is machine learning?’, you’ll be able to tell them.

Who is the book for?

Maybe you’re studying data science. Or you’ve heard of machine learning being everywhere and you want to understand what it can do. Or you’re familiar with applying the tools of machine learning but you want to make sure you’re not missing any.

I’ve been studying and practising machine learning for the past two-years. I built my own AI Masters Degree, it led to being a machine learning engineer. This book is part of my curriculum now but if it was out when I started, it would’ve been on there from the beginning.

What previous knowledge do I need for the Hundred-Page Machine Learning book?

Having a little knowledge about math, probability and statistics would be helpful but The Hundred-Page Machine Learning Book has been written a way that you’ll get most of these as you go.

So the answer to this question remains open. I read it from the perspective of a machine learning engineer, I knew some things but learned many more."
The Trolley Problem Isn’t Theoretical Anymore,"You are the conductor of a run-a-way trolley that is hurtling down its track at 85 miles an hour, heading straight for a group of young boys playing on the tracks, blissfully unaware of their impending doom. You realize that you can pull a lever to switch the trolley to an alternate track, saving the lives of these boys. Before you pull the lever though, you see there is a young girl who is playing on the tracks of the alternate route. Pulling this lever would mean ending her life. You have ten seconds until it is too late to decide…

What do you do?

The Trolley problem was a thought experiment first introduced by Philippa Foot in 1967. In 1984, this problem was reintroduced in an academic paper by Dr. JJ Thomson. It has been cited over 1300 times.

The good news is that discussions about ethics are becoming more common in computer science classrooms at universities. Engineers are finally beginning to discuss problems about values and fairness when it comes to digital systems and algorithms. What aren’t as highly discussed though, are the consequences — intended or not — of discriminatory systems and biased algorithms that are already in effect and being used by humans every day.

The trolley problem is already being played out by companies like Tesla, Google, Uber, Lyft, Argo, Embark, and General Motors. The problem goes like this :

If a self driving car finds itself in a situation where it has to swerve to save its driver, but swerving left means hitting a child crossing the street, and swerving right means hitting two elderly women crossing the road — which direction should it swerve?

Previously, Google chose the values of deontology: always hit the smallest object no matter what (there was no difference between a trashcan and a baby in a stroller)*. Tesla opted out of accountability; crowd-source human driving data and mimic human driving behaviors. This includes speeding, swerving, and (sometimes) breaking the law.

Why are CS classrooms discussing algorithms and AI theoretically? The technology is here. It isn’t theoretical anymore. It is time to assess the algorithms that already exist in the growing digital landscape. The ones that make decisions that could negatively or positively impact…"
Using Natural Language Processing to Predict Topics of Articles (now in Chinese!),"Using Natural Language Processing to Predict Topics of Articles (now in Chinese!)

I have a complicated relationship with the Chinese media. I’ve done work for CCTV in the past, and most of my translation clients have some sort of government ties. Overall, I think that the daily news (新闻联播) that gets presented on CCTV is generally objective when it comes to international news. However, when it comes to domestic Chinese news, there can be more of an agenda. Having been a regular consumer of 新闻联播 before, I’m well aware of how the program will often shape a story to be very pro-China, and will often do fluff pieces on important figures.

But as a Data Scientist, I wanted to perform an exploration of this, and see if there were any recognizable trends in how many fluff pieces (for Xi Jinping in particular) are produced over time.

The Dataset

The dataset I’m using was found on Kaggle, and features every written news article published for use on the 新闻联播 program between January 1, 2016 to October 15, 2018. It has 4 columns: Date, Tag, Headline, and Content.

The Tag column differentiates the rows in the dataset between ““详细全文” some relatively long news, “国内“ domestic short news, “国际“ international short news.”



Project Goals:

Explore whether there is any sort of trend in positive news/fluff pieces about Xi Jinping (especially surrounding the time of the constitutional change). Come up with a way to predict whether a piece will be about Xi Jinping. Create a filter for fluff pieces about Xi Jinping (sometimes it can feel like you are being bombarded by propaganda)(future work )

Data Exploration and Code

To get started, here’s a list of all the imports that were used in this project:

Most of these are fairly standard for NLP, but one package that some might be unfamiliar with is jieba. Jieba is a popular tokenizer specifically for Chinese characters, and typically does a great job parsing which should be compound characters and which should not. It also allows you to add custom tokenization rules pretty easily, if something isn’t parsing quite right.

And this is typically what the data looked like:

It had 20,738 rows with only 100 or so stories that repeated. Since these were all stories that did actually air, I decided to leave them in.

Non-NLP Feature Engineering

Before getting to the NLP aspect of the project, I wanted to see if there was perhaps an increase in Xi Jinping-friendly stories surrounding times when there may typically be negative public sentiment about him (specifically in March, when he changed to constitution to remove presidential term limits). This required me to “manually” determine if each news story was about Xi, an arduous task with over 20,000 rows of data.



So I counted the ratio of mentions of Xi Jinping (or Chairman Xi, Secretary Xi, etc.) to the character count of an article, getting the percentage he was mentioned, and cross-referenced articles with high “Xi counts” with whether or not he is mentioned in the title.

This was typically a good enough measure for whether an article was truly about Xi or not, which allowed me to tag it (for later use by NLP classifiers). I did do a manual count of about 2,000 rows, and found 33 false positives. Logically, I didn’t see how an article could be about Xi Jinping without actually mentioning him, so wasn’t worried about false negatives in articles with a Xi count of 0.0, and 33/2000 false positives is a good enough error that the rule seems alright.

Results

We can see that the number of articles about Xi Jinping varies throughout the year, but has generally been on the rise in 2018. However, the rise for the time period around March specifically could just be random variation. If we had another year of data, it would be interesting to see if this rise continued, or if it was simply a slightly larger increase than the normal variation we see throughout the seasons of the rest of the years. 2.5 years isn’t really a long enough time to observe definite trends in something as abstract as whether the media is trying to promote more positive pieces about the President.

Predictions with NLP Classifiers

The dictionary keeping track of all the values was rather long, so I cut it down to only record words/phrases that appeared 20 times or more. Some of the top words we see throughout the dataset are ‘发展’ (development), ‘中国’ (China), ‘习近平’ (Xi Jinping), ‘合作’ (cooperation), etc. This isn’t very surprising for a dataset of Chinese mainstream news articles.

Methodology



I was able to train NLTK's basic Naive Bayes classifier using all 12,000+ characters as a feature set, but ran into memory issues when training other classifiers, so ended up using just the top 5,000 character combinations as features. All 20,738 of the articles were shuffled using random.shuffle, then the first 15,000 were used to train, resulting in roughly a 72/28 split for training and testing.

NLP Results

The first pass with all 12,000 characters as a featureset is interesting to examine because of the changes that occurred when less characters were used. Most notably, some of the figures that strongly indicate an article is about Xi Jinping are different, and both 检阅台 (review/inspection platform) and 挥手致意 (waving to express regards/respect) are absent in 5k-character featureset’s list of most informative features. Both of these terms (highlighted in red) are almost exclusively used during a review of the troops (typically done by Xi Jinping).

And the most informative people (in order) were:

Ding Xuexiang 丁薛祥 (General Secretary of the CPC)

Yang Jiechi 杨洁篪 (Director of the Office of Foreign Affairs)

Fang Fenghui 房峰辉 (Former Chief of Joint Staff)

Ma Xiaotian 马晓天 (Former Deputy Chief of Joint Staff)

Zhao Keshi 赵克石 (Former Nanjing Commander)

In the second pass, with a 5,000-character featureset, we see that 丁薛祥 (Ding Xuexiang, Xi Jinping’s aide) and 杨洁篪 (Yang Jiechi, a key leader of China’s foreign policy) are found in both featuresets. Interestingly, it appears that Xi Jinping himself (highlighted in green) appears throughout the dataset so much that he is 45 times more likely to appear in articles that aren’t actually about him. The 中央办公厅 (the General Office of the CPC, highlighted in red) is also present in this featureset’s 25 most important features, but was absent in the previous version’s most important features.

And the most informative people (in order) this time were:

Ding Xuexiang 丁薛祥 (General Secretary of the CPC)

Yang Jiechi 杨洁篪 (Director of the Office of Foreign Affairs)

Liu He 刘鹤 (Vice Premier of the People’s Republic of China)

Wang Huning 王沪宁 (Chinese political leader)

Purely by looking at how likely these people/features were to indicate whether a story was about Xi Jinping, we can see the importance of certain politicians that might not often appear in the public eye.

Models

The baseline of this dataset was 89.787%, which is relatively high, but it turns out most of the articles in the dataset were not about Xi Jinping.

Here’s how each of the classifiers I used performed:

1. Multinomial Naive Bayes: 92.646%

2. Bernoulli Naive Bayes: 85.953%

3. Logistic Regression: 95.904%

4. Support Vector Clustering (SVC): 93.883%

Most of these classifiers actually did improve upon the baseline, except for the Bernoulli NB classifier, but I imagine with proper tweaking it might also be able to get better.

Final Thoughts

Overall, this was a very fun project, and first foray into Natural Language Processing. I learned a lot more than mentioned in this post, and will write about unsupervised LDA models for parsing Chinese once I have performed some more exploration. As for this project, the next step could be to turn it into a filter of Xi Jinping-related news, or apply the techniques used here to filter out any topic one could want. It could also be used to draw a map of relationships in the CPC based on the proximity of certain political figures to Xi Jinping. It is also one step along the way of defining that vague concept of what it means for an article to be “about” someone or something, and handling it in a way that computers can understand.

All code and visualizations used in this article is available here: GitHub

Please feel free to reach out to me via LinkedIn if you have any questions about wrangling/analyzing data in Chinese!"
Quantum Neural Networks for FinTech,"Variational Quantum Circuits

Certain quantum circuits behave very much like the neural networks we know and love from machine learning. We can develop “variational quantum circuits” that mimic the behavior of the deep neural networks that have become so popular and effective in deep learning. The gates in these circuits behave like layers of a neural network, and we can train them in various ways to perform all the same tasks as the neural networks you might use in TensorFlow or Pytorch.

In fact, in many cases, quantum methods provide an extreme improvement in computational power and accuracy. Even simulating such methods classically has shown up to 100x improvement as seen in applications of TensorNetwork, a library built by Google which uses TensorFlow as its backend and allows users to replace layers of a neural network with a tensor network. If you’re not familiar with what tensor networks are, check out my other post on how they can be used to simulate quantum circuits, and how they are being used in machine learning by Google and others.

Okay, so quantum circuits can be trained like neural networks and can perform tasks that neural networks can. Then what? Well, then you can run it on an actual quantum computer! I know, most of the news you’ve read says quantum computing is still several years out and that practical applications are still unclear. That information is horribly outdated and inaccurate. Don’t believe me? Try out IBMQ Quantum Experience and see for yourself. IBM currently allows users to sign up for a free account and develop code in Qiskit, which can then be run on one of the IBM quantum computers. In fact, I have a Github tutorial repo showing you how to use open source software PennyLane to perform various quantum machine learning tasks! Let’s have a look at one of those examples."
A ConvNet that works well with 20 samples: Wavelet Scattering,"Often in data-constrained scenarios, scene comprehension has to occur with few time series observations - whether that’s audio, visual, or even radar. We do this using a surprisingly underrated technique called wavelet scattering.

Wavelet scattering (or scatter transform) generates a representation that’s invariant to data rotation/translation and stable to deformations of your data. Uninformative variations in your data are discarded — e.g. an audio sample time-shifted by various amounts. Information for downstream tasks like classification is preserved. Wavelet scattering requires no training and works great with low data.

Its main computation is convolution, making it fast and applicable to images and 1D signals. We focus on signals in this article. We will retrace findings by the signal processing community and relate it to modern machine learning concepts. I show that, yes, we can do great without learning, using 20 samples. Recreate experiments and illustrations in this article with the colab notebook in this link.

Wavelets

A wavelet can be convolved with the signal in the same sense that filters can. I think of convolution as the continuous analog to inner products, where large activation (commonly said in ML) or wavelet coefficient is caused by similarity between the continuous objects. By convolving elements from a dictionary to the signal under inspection, we capture local, spatial dependencies.

Convolution is a pivotal computation in the emergence of deep learning — it is extremely fast. The wavelet scattering implementation used by this article calls a deep learning backend solely for the efficient convolution! Kymatio is a great Python package built by passionate researchers that implement wavelet scattering, leveraging the PyTorch framework.

Real and imaginary components of the Morlet Wavelet from M. Adamczyk et al., Automatic Sleep Spindle Detection and Genetic Influence Estimation Using Continuous Wavelet Transform (2015)

The basic building block of wavelet scattering is the Morlet wavelet. It is a Gaussian windowed sinusoid with deep connections to mammal hearing and vision. By convolving wavelets ψᵥ indexed by different frequency locations v, the wavelet transform of x is the set of scatter coefficients

{ x ∗ ψᵥ }ᵥ

When the wavelet’s sine component has room to dilate (sine wave ‘slowing’ its oscillation), it decomposes the signal at decorrelated scales. That’s good for revealing the signal’s frequency structure, but doing so over the course of a longer time range. The consequence is that a wider Gaussian window trades temporal resolution for increased frequency resolution ( itself a consequence of Heisenberg’s Uncertainty Principle). In practice, the width of the Gaussian window that tapers the sine wave is an important parameter [M. Cohen 2018].

Wavelet Scattering

The historic context of wavelet scattering starts with Fourier transform, the canonical signal processing technique. The shortcoming of Fourier representation includes its instability to signal deformations at high frequency. For a signal x perturbed slightly by a high frequency deformation into x̃, their spectrogram representations look different ( large ‖ FFT(x) -FFT(x̃) ‖ ) even if they remain similar signals to the human eye. This instability is due to sine wave’s inability to localize frequency information, since sine itself has non-localized support.

November, Golden Gardens credit: u/purebredcrab at reddit.com/r/analog

Wavelet transform fixes this by decomposing the signal with a family of wavelets, with various dilation, where every wavelet has localized support (flattening out eventually like the Morlet wavelet). The resulting wavelet representation localizes high frequency components of the signal. Yet because the wavelet operator commutes with translations, the resulting representation becomes translation covariant — shifting a signal also shifts its wavelet coefficients. This makes comparison between translated signals difficult, and translation invariance is key to tasks like classification. How do we achieve a signal representation Φ(x) that is translation invariant, stable under deformations, and offers good structural information at all frequencies?

Wavelet scattering builds a signal representation Φ(x) with a redundant dictionary of Morlet wavelets. While the space of signals X can be really high dimensional, the transform forms a kernel metric over the space of signals, inducing a lower dimensional manifold. Watch Stéphane Mallat discuss the manifold interpretation with visualization.

Readers have probably trained a neural convolution network to encode an image into a latent manifold Z, whose code/latent representation is used for classification or structure discovery — and that’s what is happening in analogy. Wavelet scattering encodes the dataset X where uninformative variability in X: translation, rotation, and scaling — the action of groups — are discarded in the process.

The key benefits of transforming a signal by Φ [J. Bruna and S. Mallat, 2013] is that

Φ is invariant to signal translation.

Denote by xₜ a signal identical to x, except translated in time, then Φ(x) = Φ(xₜ).

Φ is stable under signal deformations.

I.e. Φ is Lipschitz continuous to deformations — difference between scatter representations of a signal with its deformed version is linear. A deformation can be some local displacement/distortion (or a ridiculous amount of distortion, as a later example shows). For Lipschitz constant C>0 and a displacement field τ(u) causing deformations to create x̃,

‖Φ( x ) - Φ( x̃ )‖ ≤ C‖x‖ supᵤ|∇τ(u)|

Where ‖x‖= ∫ ‖x(u)‖²du and supᵤ|∇τ(u)| is the global deformation amplitude.

Φ does not require learning.

The priors introduced by wavelet scattering are nice enough that its performance often makes learning redundant; plus it comes with interpretable features and outputs. In data-constrained scenarios, if comparable data is publicly available, a nice plan is to pipe your small dataset through a pretrained model. But in the difficult situation that your dataset is small and unique, consider wavelet scattering as an initialization for ConvNets and other models. I suspect the future of ‘data constrained learning’ will be in synergizing predefined filters alongside learned filters.

Figure below illustrates stability under deformations. Left we applied scatter transform to the voice of a speaker saying ‘zero’. The scatter representation consists of the coefficients derived from averaging/low pass filter, order 1 wavelets, and order 2 wavelets. Right After applying a displacement field that has mostly masked the structure of the original signal with a sine wave, Φ( x̃ ) is barely affected; the deformation’s effect has been linearized by Φ’s transformation."
Do You Know Credible Interval,"If you apply for a position that involves AB testing, SQL or any type of user experience research, chances are your interviewer’s favorite question is: “Here is a confidence interval from the XYZ experiment. What does it mean?” There are two ways to answer the questions:

“There is 95% probability/plausibility/likelihood that the population parameter lies in the interval.” “If we repeat the experiment infinitely many times, 95% of the experiments will capture the population parameter in their confidence intervals.”

If you answer the first way, unfortunately, you’ve pretty much failed the interview. Most people can’t tell the difference between the two statements, and can get away with it. But as a data expert, you cannot. The first statement is interpreting Bayesian credible interval. The second, frequentist confidence interval.

Why So Confusing

If you have never taken a statistics inference class, you may think the second statement is just a paraphrase of the first. In fact, the difference is profound. This boils down to two opposite ideologies about probability: frequentist and Bayesian.

The Bayesian defines probability as a “belief.” A belief can be strong or weak, and is modified continuously as new evidence emerges. The belief itself is described by a probability distribution. For example, my belief about average user checkout time can be something like this: “There is 3.5% probability that average check out time is between 44~46s; 34% probability that average checkout time is between 11~24s.” Notice that I’m not making a statement about what the average check out time is precisely, I’m making statement about different ranges, and the probability of the ranges.

The frequentist defines probability by “frequent trials.” A frequentist believes that a population parameter is fixed (for example, diabetes rate, conversion rate, average checkout time), and the only way to find it is through many experiments. Because real world experiment is often expensive, we use central limit theorem to derive from a single experiment the same sampling distribution we would get by conducting many experiments."
Cleaning Web-Scraped Data With Pandas and Regex! (Part I),"Member-only story Cleaning Web-Scraped Data With Pandas and Regex! (Part I)

|| I || Introduction

Now that I’ve started programming on a daily basis, I decided to invest in a laptop that would allow me to multi-task smoothly and run all my desired applications the way I’m used to.

I own a Unix-based Macbook, but its M3 processor just doesn’t cut it when I’m working on projects that require processing a lot of images or videos. I also have a Windows notebook from work with a decent i5, but after using it, I’ve realized that my needs are much higher.

However, I’m not about to just drop $2000 any machine with good specs. I actually want to understand how the components/specs of a laptop (Size, Storage, RAM, Processor, etc.) contribute to form an overall representation of the Lapton in the market at a certain price. For example, I want to derive:

The relationship between different components of the laptop and its price.

The relationship between product ratings and specific components.

A comparison of different brands and their similar products.

There are a number of insights I could derive from scraping product data, and I will eventually develop an algorithm to find the right Laptop given a number of inputs.

|| II || Web Scraping

Thanks to a friend, I found out about the “Web Scraper” extension on Google Chrome. You can find it here.

The extension helped me collect the required data for Laptops on Amazon.

This web-scraper does what most others do: it collects information we want from the page source. Websites don’t always make it easy for you to extract data from their pages, therefore you would need to clean the extracted data before you can use it for any kind of analysis.

Photo by The Creative Exchange on Unsplash

So what do I mean by “cleaning” the data?

Often, the data will have some impurities, such as NaN (empty) values…"
gRPC in Golang,"gRPC in Golang

For a long period of time, REST API has dominated the web programming world until gRPC came and disrupted the industry. There are numerous posts online discussing the advantages of gRPC and comparing it with REST, so I am not going to make redundant comments on this point. My understanding is that gRPC inherits the functionality of REST and extended it with faster, lighter and more flexible service. In this post, let’s take a glance of gRPC and implement a simple service using Golang.

Prerequisites

Install Go: https://golang.org/doc/install

Install protoc : gRPC uses protobuf to communicate, in order to generate relevant files, you will need to install protoc :

brew install protobuf // For MacOS

install protoc-gen-go : as we use go in this post, you will need the go-support for protobuf

go get -u github.com/golang/protobuf/protoc-gen-go

install grpc : the grpc package for go

go get -u google.golang.org/grpc

(Note: make sure your GOPATH is correctly set up so that your packages and project will be under GOPATH/src )

gRPC Overview

Comparing to traditional REST API, where the client communicates with the server by specifying a bunch of constraints, like sending to a specific url — localhost/api/v1/hello , localhost/api/v1/doSomething and specifying what kind of operations like PUT, POST, GET, … I think gRPC in a way abstracts the idea and defines the communication by simply calling functions in which messages are defined in protobuf format.

With gRPC, client can directly call a function in the server, as you will see later, they actually share the same protobuf file. And a huge advantage from the image above is that server and client written in different languages can communicate with each other easily all based on that they share one protobuf file.

If you are a bit confused so far about gRPC and protobuf, let’s continue and implement a service and see how protobuf plays the role in the communication. In this post, we are going to implement a simple unary service, that is sending a request and…"
The Future of Machine Learning,"The Future of Machine Learning

Photo by Arseny Togulev on Unsplash

Machine learning is a trendy topic in this age of Artificial Intelligence. The fields of computer vision and Natural Language Processing (NLP) are making breakthroughs that no one could’ve predicted. We see both of them in our lives more and more, facial recognition in your smartphones, language translation software, self-driving cars and so on. What might seem sci-fi is becoming a reality, and it is only a matter of time before we attain Artificial General Intelligence.

In this article, I will be covering Jeff Dean’s keynote on the advancements of computer vision and language models and how ML will progress towards the future from the perspective of model building.

Computer vision

Photo by Alex Knight on Unsplash

The field of Machine learning is experiencing exponential growth today, especially in the subject of computer vision. Today, the error rate in humans is only 3% in computer vision. This means computers are already better at recognizing and analyzing images than humans. What an amazing feat! Decades ago, computers were hunks of machinery the size of a room; today, they can perceive the world around us in ways that we never thought possible.

The progress we’ve made from 26% error in 2011 to 3% error in 2016 is hugely impactful. The way I like to think is, computers have now evolved eyes that work. — Jeff Dean

Now this achievement — made possible with advancements in machine learning — isn’t just a celebration for computer geeks and AI experts, it has real-world applications that save lives and make the world a better place. Before I blab about a life-saving application of computer vision, let me illustrate to you the power of computer vision.

Let’s say I give you 10,000 pictures of dogs and I ask you to classify them into their respective species, are you able to do that? Well, you can, but you have to be a dog expert and it’ll take days by the time you’re done. But for a computer…"
Generative Graphic Design: Will Algorithm-Driven Design Change our Approach in Designing?,"On the other hand, Audience-based input design involves the audience for completion by inserting their set of constraints, which adds more experiential value. An example of this is ‘Phase’, a generative type tool created by type designer Elias Hanzer, developed by Florian Zia that reacts to a manual slider that manipulates its form in real-time or sounds through voice input.

My input in creating my custom type. Visit: https://www.eliashanzer.com/phase/

The experimental nature that is in generative design, in general, allows us creators to go in deeper when it comes to experimenting when possibilities are now endless. It might be liberating to some, now that this generative tool, not only does it generate a boundless amount of iterations to our design, but it also buys us time. To some extent, it probably does sound intimidating now that this kind of technology available today can create more iterations than we ever could, realizing limitless potentials that we could ever imagine. So what does this mean for us?

Modern Problem Requires Modern Solution

The question “will AI take my job?” sounds quite outdated for designers now that it has been answered with a “No” since creative endeavor is something that can’t be hard-wired on a human-level experience (at least for now), and computational creativity is still a growing area of research. But, there’s a catch.

With the ever-growing workplace demands and what feels like an intensified competition for some, the need for deeper and wider experimentation to seek for a difference that will stand out from trends and similarities also intensifies as mediocrity becomes a foul net to stay away from. But this doesn’t have to be scary. The tool’s potential to, for example, nail a dynamic brand identity or to create a real-time data visualization is immense and impactful that it could possibly be a game-changer in the future, which comes with its pitfalls too. The reality is that while its ability to flawlessly generate possibilities may replace designers of today, it certainly will not replace designers of tomorrow. Therefore, with this powerful tool in our hands today, how do we survive and thrive for tomorrow?

Associate professor of computer science at Georgetown University Cal Newport explains in his book ‘Deep Work’ that the two groups that he claims are poised to thrive in the new economy (a result of unprecedented growth and impact of technology that are massively restructuring our current economy) are those who can work creatively with intelligent machines and those who are stars in their field.

The power that we have as designers is actually immeasurable: to make and break rules at the same time. The decision-makers that are constantly reimagining the definition of beauty in what we use, what we see, and what we experience every day. Our future creative endeavor in design could perhaps be an open collaboration between man & highly-intelligent machines as much it is today with our devices and software. The idea may sound ludicrous now but it may become the new norm in the future.

Cal Newport also generously points out the two core abilities to thrive in the new economy in ‘Deep Work’: The ability to quickly master hard things and the ability to produce at an elite level, in terms of both quality and speed. The ability to quickly master hard things allows you to adapt to changes in the new economy easier. The accelerating rate of the ever-evolving technology might frighten those who are unable to keep up and have bigger chances to be left out as demands themselves will adapt and evolve to what’s available in the market. The spare time to learn new a skill or software you haven’t had the chance to delve into might be worth investing now for the long run. Excelling at what you do so differently that no one does it like you do sounds like a solid investment too. Technology does not and will not stop anytime soon, and for the millions of years that we’ve evolved as a species, this time around is no different. Here, I’m not nor will I ever intend to go against traditional, hands-on medium such as prints. The main objective is to fully utilize and adapt to technological advancement.

To loosely quote from a design talk I attended this year, “We upgrade our devices every day, so why not ourselves & our workflow?”"
Advanced candlesticks for machine learning (ii): volume and dollar bars,"Advanced candlesticks for machine learning (ii): volume and dollar bars

In this article we will learn how to build volume and dollar bars and we will explore what advantages they offer in respect to traditional time-based candlesticks and tick-bars. Finally, we will analyze two of their statistical properties — autocorrelation and normality of returns — in a large dataset of 16 cryptocurrency trading pairs Gerard Martínez · Follow Published in Towards Data Science · 9 min read · May 2, 2019 -- 4 Listen Share

Introduction

In a previous post we learned how to build tick bars and assessed their particular ability to self-regulate the sampling rate based on a higher or lower activity in the market. Similar to tick bars, volume and dollar bars also allow the synchronization of the sampling rate with the activity of the market, but each of them understands the concept of activity in a different way. In the case of tick bars, market activity is defined as number of trades taken place in the exchanged. Volume bars define activity as the number of assets traded in the exchange — for instance, number of Bitcoins exchanged. For dollar bars, activity is defined as fiat value exchanged — for instance, sample a bar every time 1000$ in assets are exchanged — which can be measured in dollars but also in Euro, Yens, etc.

Therefore, each bar type understands and synchronizes to market activity in a different way and this differential understanding brings its advantages and disadvantages. Let’s dig into them.

Advantages and disadvantages of tick, volume and dollar bars

With the tick bars we found a way to scan the trade history of an exchange and sample more bars simply when more trades where executed in the exchange. While a strong correlation between number of trades placed and information arrival may exist, the correlation is not guaranteed. For instance, a well acquainted algorithm or trader may automatically place very small, repetitive orders to influence the sentiment of the market (by turning the trade history “green”), to hide the total amount of volume (also known as iceberg orders) or simply to disorient other trading bots by falsifying information arrival.

A potential solution to this scenario is to use volume bars instead. Volume bars do not care about the sequence or number of trades, they just care about the total volume of these trades. For them, information arrival is increased volume traded between the exchange users. This way, volume bars are able to bypass misleading interpretations of the number of trades being executed at the cost of loosing any information that could lie hidden in the actual sequence of trades.

Another interesting feature about volume bars, which may sound obvious but is important to notice, is that market volume information is intrinsically coded on the bar themselves: each volume bar is a bucket of a predefined volume. Again, this may sound obvious, but for long time, and still today, lots of researchers in the financial space are clue-less about how to include volume information in their predictive models. Volume bars yield volume information out-of-the-box.

Now, the problem with the volume bars is that volume exchanged may be very correlated with the actual value of the asset being exchanged. For instance, with 10,000 dollars you could buy around 10 bitcoins in early 2017, but by the end of the 2017 you could only buy half a bitcoin with it. That massive fluctuation of the underlying value greatly undermines the power of volume bars because a volume size that is relevant at some point in time may not be relevant in a near future due to the revaluation of the asset. A way to correct for this fluctuation is by, instead of counting the number of assets exchanged (volume bars), counting the quantity of fiat value exchanged (dollar bars), which happens to be in dollars for the BTC-USD pair, but could also be in Euros for the ETH-EUR pair, etc.

Building volume and dollar bars

Now that we have seen the strengths and weaknesses of each bar type, let’s look at how can we actually build them.

Here’s a fast Python implementation to build volume bars:

Snippet 1. Volume bar implementation

And here an implementation for dollar bars, which only includes few small modifications to the previous function — let’s see if you can spot them:

Snippet 2. Dollar bar implementation

Finally, here’s how they look compared to traditional time-based candlesticks:

Figure 1. Trade, time-based, volume and dollar bars depiction. Asterisks are plotted every time a candle is sampled

Volume and dollar-based candles are similar to the tick bars in the sense that, while is true that they look chaotic and overlapped when compared to the harmonic time-based ones, they do their job well at sampling whenever there is a change in market activity.

Statistical analysis of volume bars

Let’s now look at their statistical properties. We’ll be looking at serial correlation of returns by performing the Pearson auto-correlation test and the Durbin-Watson test. Finally, we will also look at the normality of results by performing the Jarque-Bera and the Shapiro-Wilk tests. Refer to the old article about tick bars to learn more about these statistical tests.

In the case of volume bars, we will be showing results on 16 trading pairs and for 7 volume sizes each (labeled as tier1 to tier7). Unlike tick bars, the volumes sizes are cryptocurrency-specific since they depend on multiple factors such as total amount of coins in circulation, underlying asset value, etc. The way we selected the volume sizes for each cryptocurrency was by calculating the mean volume exchanged per day and then by dividing the daily mean volume by the same ratios as 5min, 15min, 30min, 1h, 4h, 12h correspond to 1d, and rounded to the nearest 10. Here are the automatically chosen volumes per cryptocurrency pair:

Table 1. CryptoDatum.io volume bar sizes

Figure 2. Pearson auto-correlation for volume bars

Figure 3. Durban-Watson statistic for volume bars

Results are in line with what we saw with tick bars. Volume bars show slightly less auto-correlation in comparison to time-based candlesticks (Figure 2 and 3) and the null hypothesis of normality is rejected in most cases in both normality tests (Figure 4) so we have certainty that returns do not follow a Gaussian distribution."
An Executive’s Guide to Implementing AI and Machine Learning,"An Executive’s Guide to Implementing AI and Machine Learning

As a Chief Analytics Officer, I’ve had to bridge the gap between business needs and data scientists. How that gap is bridged is, in my experience, the difference between how well the value and promise of artificial intelligence (AI) and machine learning is realized. Here are a few things I’ve learned.

AI = machine learning (at least in 2019)

Machine learning is a path to get to AI. At least as of 2019, it is the only known viable path that I’m aware of. In the coming years, there may be other approaches. The two terms are not interchangeable, but for our purposes I will focus on machine learning.

Machine learning is a category of tools and approaches where a computer is given a large training set of data that includes an “answer key”. The machine then learns how to derive the answer key from combinations of the inputs. The model is then tested against a different testing data set to determine its accuracy.

Machine learning as a category can include basic statistical tools (e.g. linear regression) that fit this approach. It also includes neural networks, decision trees, and several other tools.

Is machine learning the right tool for the problem you’re trying to solve?

This one has tripped me up in the past.

For example, recently I had a data set with a lot of data collected from hospitals which had, for each employee, fifty measurements (for example, whether they showed up for work on time or whether they were consistently the only experienced person on their shift) and an indicator of whether they resigned in the weeks and months following. The question was: given this data set, could we create a model to predict employees who would resign before they did so, allowing hospitals to intervene early?

We spent months reviewing the data set and had used basic data visualization approaches to determine a set of rules. For example, employees who were just hired were twice as likely to resign than employees who had already worked at the hospital for ten years. Employees in certain…"
How can AI make Construction better?,"How can AI make Construction better?

Construction lags other sectors in innovation and productivity. This post explores applications where AI can help bridge the gap. Shuvashish Chatterjee · Follow Published in Towards Data Science · 8 min read · Sep 17, 2019 -- 1 Listen Share

Digital assistant in construction (Illustration by author)

The world will need to spend $57 trillion in infrastructure and housing by 2030 to make room for the rural to urban migrants. The infrastructure and construction industry, which employs 7% of the world’s working-age population, will shoulder the bulk of this responsibility. However, the construction sector has an intractable productivity hurdle. Large projects typically take 20 percent longer to finish than scheduled and are up to 80 percent over budget. In the last two decades, the labor productivity of the construction industry has stagnated at 1%. Financial returns for contractors are often relatively low and volatile. Equally worrying is the proportion of worker fatalities (highest of all other sectors).

Traditionally, the construction industry has been making incremental improvements. Each project is unique, because of which it is not possible to scale up new ideas. Adopting new technologies is impractical. In this post, we will look at a few applications where artificial intelligence can help the sector leapfrog across.

Making construction project management predictable.

There are far too many variables that can throw an execution out of control in the Engineering Procurement and Contracts sector. Labor shortages, inclement weather, supply outages, and regulatory clearances are some parameters that contribute to the inherent variability in project management. Each project is considered unique. This lack of standardization has made the process digitization slow. Daily reports, drawings, contracts, continue to rely heavily on paper-based communications. Information flow between front-line contractors and project planners is erratic and slow-paced. Frontline contractor’s planning horizon is often limited to the next 7 days. Most project management is reactive. The mid-senior level staff is engaged in routine firefighting exigencies.

We can use machine learning to implement day- and week-ahead forecasts. Every site gets a list of activities that are predicted to start on that date. A real-time dashboard lists high-risk activities for cost and time overruns. Operational data sources within construction is limited. However, most sites log inventory levels for project costing. Similarly, project progress is monitored on scheduling tools. AI can weave together such data streams and combine them with weather and historical performance to predict future outcomes. It will enable contractors and site managers to take a realistic view and prioritize interventions."
Intermediate Streamlit,"Intermediate Streamlit

It’s never too early to start scaffolding your app. Image by Michael Gaida from Pixabay.

Streamlit is a great tool to give your data science work an interface. I’ve been using it as a lightweight dashboard tool to display simple visualizations, wrapping python packages in a UI, and exploring model evaluations for NLP models (live examples 1 and 2). It’s allowed me to better understand my data and created an interface that can help translate between code/data/analysis and communication with stakeholders and subject matter experts.

However, today’s prototypes become tomorrow’s production apps. There’s a risk in making things too easy — before you know it you’ve set up expectations that the next iteration will happen just as quickly, or that your app is robust and well tested, or that deploying it company-wide is around the corner, or that the new intern can take it from here to save some money.

As your app grows you’ll need to manage the growing pains. In this post I’ll provide some tips and ideas that I’ve found helpful as my streamit apps evolved. I’ll start with some simple tips to build better apps and end with some ideas for making apps modular and testable.

(As a functional note, any time you see a gist you should be able to run it as a streamlit app and explore yourself. If you’re willing to do a pip install streamlit pandas altair vega_datasets , you can run any of the gists below with streamlit run <gist_url> ).

Display Clean Variable Names

The variable names in a DataFrame might be snake cased or formatted in a way not appropriate for end users, e.g. pointless_metric_my_boss_requested_and_i_reluctantly_included . Most of the streamlit widgets contain a format_func parameter which takes function that applies formatting for display to the option values you provide the widget. As a simple example, you could title case each of the variable names.

You can also use this functionality, combined with a dictionary, to explicitly handle the formatting of your values. The example below cleans up the column names from the birdstrikes dataset for use as a dropdown to describe each column.

Passing `dict.get` as an argument to `format_func` allows you to explicitly control the display of widget values

Use Caching (But Benchmark It First)

It can be tempting to throw that handy @st.cache decorator on everything and hope for the best. However, mindlessly applying caching means that we're missing a great opportunity to get meta and use streamlit to understand where caching helps the most.

Rather than decorating every function, create two versions of each function: one with the decorator and one without. Then do some basic benchmarking of how long it takes to execute both the cached and uncached versions of that function.

In the example below, we simulate loading a large dataset by concatenating 100 copies of the airports dataset, then dynamically selecting the first n rows and describing them.

Don’t blindly apply `@st.cache` — benchmark and apply where appropriate

Since each step (data load, select rows, describe selection) of this is timed, we can see where caching provides a speedup. From my experience with this example, my heuristics for caching are:

Always cache loading the dataset

Probably cache functions that take longer than a half second

Benchmark everything else

I think caching is one of streamlit’s killer features and I know they’re focusing on it and improving it. Caching intelligently is also complex problem, so it’s a good idea to lean more towards benchmarking and validating that the caching functionality is acting as expected.

Create Dynamic Widgets

Many examples focus on creating dynamic visualizations, but don’t forget you can also program dynamic widgets. The simplest example of this need is when two columns in a dataset have a nested relationship and there are two widgets to select values from those two columns. When building an app to filter data, the the dropdown for the first column should change the options available in the second dropdown.

Linking behavior of two dropdowns is a common use case. The example below builds a scatterplot with the cars dataset. We need a dynamic dropdown here because the variable we select for the x-axis doesn't need to be available for selection in the y-axis.

We can also go beyond this basic dynamic functionality: what if we sorted the available y-axis options by their correlation with the selected x variable? We can calculate the correlations and combining this with the widget’s format_func to display variables and their correlations in sorted order.

Dynamic widgets are a powerful way to add more context around your app’s functionality

Make Heavy Use of f-strings & Markdown

In the above example, we used python’s f-strings to interpolate the variable names and their correlation values. In building an interface around the analysis, much of it requires creating or manipulating strings in variable names, widget values, axis labels, widget labels, or narrative description.

If we want to display some analysis in narrative form and there’s a few particular variables we want to highlight, f-strings and markdown can help us out. Beyond an easy way to fill strings with specific variable values, it’s also an easy way to format them inline. For example, we might use something like this to display basic info about a column in a dataset and highlight them in a markdown string.

mean = df[""values""].mean()

n_rows = len(df) md_results = f""The mean is **{mean:.2f}** and there are **{n_rows:,}**."" st.markdown(md_results)

We’ve used two formats here: .2f to round a float to two decimal places and , to use a comma as a thousands separator. We've also used markdown syntax to bold the values so that they're visually prominent in the text.

Consider Switching to Altair for Visualizations

If you’ve been prototyping visualizations with another library, consider switching to Altair to build your visualizations. In my experience, I think there are three key reasons a switch could be beneficial:

Altair is probably faster (unless we’re plotting a lot of data) It operates directly on pandas DataFrames Interactive visualizations are easy to create

On the first point about speed, we can see a drastic speedup if we prototyped using matplotlib. Most of that speedup is just the fact that it takes more time to render a static image and place it in the app compared to rendering a javascript visualization. This is demonstrated in the example app below, which generates a scatterplot for some generated data and outputs the timing for the creation and rendering for each part of the visualization.

Altair can be faster than matplotlib if you’re plotting less than a few thousand pionts.

Working directly with DataFrames provides another benefit. It can ease the debugging process: if there’s an issue with the input data, we can use st.write(df) to display the DataFrame in a streamlit app and inspect it. This makes the feedback loop for debugging data issues much shorter. The second benefit is that it reduces the amount of transformational glue code sometimes required to create specific visualizations. For basic plots, we could use a DataFrame's plotting methods, but more complex visualizations might require us to restructure our dataset in a way that makes sense with the visualization API. This additional code between the dataset and visualization can be the source of additional complexity and can be a pain point as the app grows. Since Altair uses the Vega-Lite visualization grammar, the functions available in the transforms API can be used to make any visualization appropriate transformations.

Finally, interactive visualizations with Altair are easy. While an app might start by using streamlit widgets to filter and select data, an app could also use a visualization could as the selection mechanism. Rather than communicating information as a string in a widget or narrative, interactive visualizations allow visual communication of aspects of the data within a visualization.

Don’t Neglect Refactoring, Writing Modular Code, and Testing

It’s easy to spend a few hours with streamlit and have a 500 line app.py file that nobody but you understands. If you're handing off your code, deploying your app, or adding a some new functionality it's now possible that you'll be spending a significant amount of time trying to remember how your code works because you've neglected good code hygiene.

If an app is beyond 100 lines of code, it can probably benefit from a refactor. A good first step is to create functions from the code and put those functions in a separate helpers.py file. This also makes it easier to test and benchmark caching on these functions.

There’s no specific right way on how exactly to refactor code, but I’ve developed an exercise that can help when starting an app refactor.

Refactoring Exercise

In the app.py, try to:

only import streamlit and helper functions (don’t forget to benchmark @st.cache on these helper functions)

on these helper functions) never create a variable that isn’t input into a streamlit object, i.e. visualization or widget, in the next line of code (with the exception of the data loading function)

These aren’t hard and fast rules to always abide by: you could follow them specifically and have a poorly organized app because you’ve got large, complex functions that do too much. However, they are good objectives to start with when moving from everything in app.py to a more modular structure.

The example below highlights an app before and after going through this exercise.

Functions are great and you should use them.

Another benefit of reorganizing code in this way is that the functions in the helpers file are now easier to write tests for. Sometimes I struggle with coming up with ideas of what to test, but I’ve found that now it’s really easy to come up with tests for my apps because I’m more quickly discovering bugs and edge cases now that I’m interacting more closely with the data and code. Now, any time my app displays a traceback, I fix the function that caused it and write a test to make sure the new behavior is what I expect.

Wrap Up

I’ve been enjoying my time with streamlit — it’s a fantastic tool that’s addressed a clear need in the data science workflow. However, today’s prototypes are tomorrow’s production apps, and it’s easy for a simple app to become an unmaintainable nightmare for a data science team. The ideas covered in this article have helped me manage the pains associated with moving my apps beyond a prototype and I hope they do the same for you."
Virtual banking race in Hong Kong — from hiring perspective,"When we try to put related business functions together in the radar diagram, it reveals the relative expansion focus of each company —

Standard Chartered

A lot of emphasis on expanding Technology related functions (Engineering, Project Management, Security)

related functions (Engineering, Project Management, Security) Risk/Compliance

Marketing (especially online/social media channels)

ZhongAn

Top priority on strengthening core business functions (Operations, Risk, Finance)

(Operations, Risk, Finance) IT and Admin

WeLab

Similar to ZhongAn, top priority on strengthening core business functions. While ZhongAn put more emphasis on expanding the Operations team, WeLab current focus is to expand the Finance team.

team. Establishing various Marketing/Relations functions, which is not seen in the other 2 companies — expect more marketing campaigns?

functions, which is not seen in the other 2 companies — expect more marketing campaigns? Engineering

Hiring Intern

Keywords in Job Descriptions

Other than high level insights like expansion focus, there are other interesting findings if we drill deeper into the job descriptions.

A job description usually consist of at least 2 parts — (i) the responsibilities of the role, and(ii) the requirements of the role. Responsibilities is more about the role (the tasks), while requirements is more about the person (the needed skills). I separated these 2 parts from each other and apply keyterms extraction (top 5 keywords) on each of them.

We use the SGRank algorithm provided by a package called textacy, which provides a lot of handy NLP toolkits on top of spacy. The top 5 keywords extracted by the SGRank algorithm is pretty good, even without any fine tuning.

For example, for the Executive Assistant role in WeLab, the responsibilities keywords are [“ground transportation”, “travel booking”, “business meetings”, “meeting schedules”, “level executives”], and the requirements keywords are [“communication skills”, “ms word”, “excellent interpersonal”, “high flexibility”, “good english”]. These make a pretty good summary of the role nature and the skills needed.

Clustering roles into groups with word embedding and t-SNE

spacy provides us with noun_chunks and word embeddings which are very useful when we are trying to cluster individual datapoints into groups. spacy built-in part-of-speech annotator does a pretty good job in extracting noun_chunks from a given text. These nouns are good representation of the role nature.

If you get the .vector property of a noun chunk, it will return the word embeddings (vector representation) of the noun chunk. Word embeddings allow us to quantify relationship between words (how similar 2 words/phrases are). It even make arithmetics on words possible!

In our case, I take an average of the noun_chunks word embeddings of the Requirements column for each role, and applied a method called t-SNE to reduce the 300-dimension vectors into (x, y) coordinates. The results are visualised in an interactive plot created by bokeh.

If we hover on the adjacent points, we can see some structure among these points. The lower part of the plots are mostly engineering roles, and fullstack developer is between frontend developer and backend developer. On the left, admin roles like Administrative Assistant, Executive Assistant, Receptionist etc are close to each other, and similar structure exists in other parts of the plot. This technique could be useful when HR managers/Recruiters are trying to analyse the job openings by competitors in the market.

Closing

There is a lot information we can tell from the job openings of a company, especially when the company is actively hiring. This may explain why “secretive” industries like hedge fund tend to hire via agents.

At the same time, the job openings could serve as a signal to the candidates/investors/general public about the “stage” or “taste” of your company — what kind of roles you are hiring the most may tell story about the “stage”; and the techstack/skills listed in the job description tells a lot about the “taste” of your company.

The virtual banking scene will only be going forward, given the advantages enjoyed by this sub-sector. In 2018, Standard Chartered spent 790M HKD[1] on rent/building related matters, which contributes 6.78% to the 11,647M total operating expenses. And the 7,074M staff cost could be utilised more efficiently when banks can go branchless. The retail banking industry could be very different in a few years from now.

[1] https://av.sc.com/hk/zh/content/docs/hk-c-ar2018-101.pdf p.256

Appendix: Data used in this analysis"
Fables of Data Science — Anscombe’s quartet,"Fables of Data Science — Anscombe’s quartet

A fable is a short story that teaches a lesson or conveys a moral. Here we explore Anscombe’s quartet and see what horror it warns us against. dearC · Follow Published in Towards Data Science · 5 min read · Jul 18, 2019 -- Share

Once upon a time..

..in a land far far away there used to live a man called Francis John “Frank” Anscombe. He was a statistician of great repute.

He wandered far and wide meeting many fellow practitioners and sat in the Citadel training young apprentices in the sacred art of statistics. The more he talked and trained people he saw a dangerous trend. People tended to ignore visualizations in favor of summary statistics. Everyone said it was too much effort to plot the data.

A worried Frank is a Red Frank

This was a dangerous, Frank was very worried. This plague was spreading was spreading fast, he did not have the time counter it. He had to do something, something that would help stop it.

So he rushed to talk to the council of olders but every one just laughed at him.

“When you have the data’s mean,

What else do you want to examine?

Standard deviation is just sublime,

Plotting is a big waste of time!” — Master PoorPoetrix

Frank had never anticipated that the Council too was poisoned, he had to open their eyes.

So he traveled, traveled northwards into the Lands of Always Winter, to the peak of the Frostfang mountains. There he sat, deep in meditation. And after 5 minutes he realized that it is shit cold out in the mountains and he should have packed more woolen clothes.

Shivering in cold he went around looking for a cave to settle in and as luck would have it he found one.

There Frank meditated and he mediated deep,

though some claimed he just fell asleep.

In his dreams he saw numbers, numbers and more numbers and it was then that he realized that the numbers were the key to his solution. Day after day he pondered on those numbers and finally he solved their mystery."
Why We Need To Rethink Central Authority In The Age of AI,"Deposit Photos

This post was developed in collaboration with Dr. George Tomko, Inventor, Biometric Encryption and Smart Data Agents.

We live in an age of increasing centralization that pervades all aspects of our culture. In today’s world, centralization equates to control; centralization equates to power. Centralization gave rise to bureaucratic institutions where decisions, borne by a few, ran through a hierarchical structure. This ensured a system where one authority determined how systems were run and how objectives were met. This is symbolic of how authoritarian governments operate. These governments have unlimited power but their effective size is much smaller, run by one or a few persons who impose order. If a constitution does exist within this type of system, it is essentially ignored if it promotes limiting powers of the state versus giving more voice to the people. Although leaders in many of these states are elected, this is wrapped in a shroud of whitewash where leaders do not govern based on the consent of the people.

Originally free societies evolved within a decentralized system, as exemplified by many of western economies. There were checks and balances based on a distributed system designed to serve the interests of the people and provide its citizens with the services they need. This kept society functioning. There are, however, aspects to western economies that have become increasingly centralized. Central banks have been given responsibility as lender to the government to ensure price stability through monetary policy, but in effect have achieved the opposite. One of their functions, for example, printing money, flows initially to the powerful, and by the time it reaches the pocketbooks of the people, it has inflated the cost of goods, thus widening the gap between the rich and poor. Banks, trusts and credit issuers function within a centralized financial system that controls the flow of money designed to mitigate fraud, so processes are implemented to censor transactions, and mandate their flow through centralized systems to maintain this control. A functioning society built on meeting the needs of its inhabitants has bequeathed these institutions with certain powers formed from a foundation of trust."
An Overview of Japanese NLP Libraries,"An Overview of Japanese NLP Libraries

nagisa-tutorial-pycon2019

PyCon JP 2019 is held in 2019/9/16~ 2019/9/17 for two days. I will publish some posts about the talks I am interested in.

As an NLP engineer, I am happy to find a talk related to NLP. This post is a brief English summary of the nagisa talk from Taishi Ikeda. You can find the slides here and the tutorial here in Japanese.

Two libraries

There are tons of Japanese NLP libraries but how to choose a good one for use needs some research. Thanks to Taishi Ikeda who save time for us. The figure above collect many Japanese NLP libraries and make a pretty detail comparison among them. If you don’t know Japanese, no need to worry. I just recommend two tools, Juman++ and nagisa.

A simple criterion to determine the performance is that whether a library has provided a neural-based model for prediction. In other words, do the maintainers update the library along with the technologies development? According to Taishi Ikeda, Juman++ and nagisa are the only two libraries that provide the neural-based model.

nagisa

Because Taishi Ikeda’s talk is mainly about nagisa I will briefly introduce nagisa. The model used by nagisa is the Bi-LSTM-CRF. The CRF layer is ignored because he don’t want to confuse those people who is not familiar with NLP.

The corpus that nagisa trained on is KWDLC. And nagisa performs well especially on the emoji symbols

Taishi Ikeda is very kind to provide the Colab notebook for quick play.

Reference"
Predicting Heart Disease Mortality,"Predicting Heart Disease Mortality

According to the Center for Disease Control, “About 610,000 people die of heart disease in the United States every year–that’s 1 in every 4 deaths.” It is unlikely anyone reading this hasn’t been affected by this disease in some way. I, myself, lost a family member to the disease at just 57 years old earlier this year. The causes are well documented and understood, yet it remains the leading cause of death in the United States. Is it possible that changes in public policy can help save lives in this regard?

By building a machine learning model to predict heart disease mortality rates across states, we should be able to identify states that have effectively reduced those rates. If so, the hope is that we can extrapolate and export those policy principles to other states.

Historical Data

The CDC publishes data on heart mortality and other leading causes of death every year going back to 1999. By tracking the number of deaths relative to the population of each state, we can see that mortality rates have been trending downward in the last 20 years.

As we can see, the trend seems to level out in 2011, likely due to an aging population.

Leading Causes of Heart Disease

After conducting some general research, four primary factors were identified that have a substantial impact on heart disease:

By running a simple linear regression with each predictor variable with the target, we can start to get a sense of which factors have the most significant impact.

From here, we can see that population demographics seem to have the largest impact…"
TensorFlow 2.0 — Here Is What I’ve Learned From A.I. Day At Google,"TensorFlow 2.0 — Here Is What I’ve Learned From A.I. Day At Google

Believe it or not, there are a lots of different meetups talking about data science and AI here in Singapore.

But my favourite meetups are still TensorFlow and Deep Learning Singapore and DataScience SG.

When I realized that TensorFlow and Deep Learning Singapore was going to organize this one day conference — A.I. Day that talked and showed the latest and greatest technology which is being used in developing and creating real-world AI products… Words simply couldn’t describe how excited I was!

It’s one thing to see people talk about the emergence of these technologies and another to see industry experts break down how some of these products are being made with tips and tricks of the trade.

All the speakers are first-hand practitioners working in the field rather than marketing and sales people.

What’s more exciting is that for the first time in Singapore, this conference invited 4 Google Developer Experts in Machine Learning and 3 speakers from Google Brain — all on the same platform.

If you’re someone who just started or has been playing around with TensorFlow 2.0, I’m sure you’ve watched some of the videos by Paige Bailey— currently the product manager for TensorFlow core as well as Swift for TensorFlow (who was also a developer advocate for TensorFlow core).

Therefore, being able to meet her in person during the conference was beyond my excitement!

And this is exactly what I like about being a part of the data science and open source community — sharing with each other and learning at the same time."
Reasoning With Probability and Pyro — Is My Model Good Enough?,"Check accuracy again, but only on 100 test observations

>>> Accuracy is 86.00%

We got a slightly lower number, but still comparable.

That said, while the numbers are close, having faith in the model based on 100 examples is harder than having faith based on 14,000 examples.

Intuitively, our confidence in the test set’s score increases as the size of the test set increases. Pyro allows us to numerically estimate our confidence and how much room for error we have.

The Pyro Model

The Pyro workflow is very unique and requires three components

A model function which simulates our underlying model (not the decision tree, but the process which gives us correct or incorrect labels). This function starts with a prior distribution A kernel which measures the likelihood of the observed examples as they are produced by the model function A sampler which builds the posterior distribution

First we need to define our observations for Pyro. In our case, our observations are the cases where we see correct and incorrect classification. Let’s consider both the small test set (100 observations) and the large test set (14,000 observations).

Now we have to define the model function. The function accepts our observations and tries to sample from a prior distribution and calculate the likelihood of the prior based on our observations. The kernel will then update the prior to a more likely posterior distribution.

It doesn’t look like much, but in this model two important events happen.

First, we have used the `pyro.sample` function to register a parameter named ”p” as a learnable value for Pyro. The use of either `pyro.sample` or `pyro.param` registers the resulting value with Pyro’s internal store (a special dictionary-like object) as learnable values. In our case we’ve said ”p” is a learnable distribution.

We’ve also registered every observation as a learnable value which should comply with the observation we provide to it.

The kernel we will register this model with (Hamiltonian Monte Carlo kernel) will look at all learnable values defined in this model and will attempt to adjust the learnable distributions such that they increase the likelihood of provided observations.

The sampler we will use (Markov Chain Monte Carlo) will run the HMC kernel to find a posterior distribution.

>>> Sample: 100%|█████████████████████████████████████████| 200/200 [03:46, 1.13s/it, step size=1.51e+00, acc. prob=0.916]

Now we’ve run our sampler, we can do several things with our resulting posterior probability. First, we may want to visualize the probability ”p” we’ve defined. We can use do so by sampling from our sampler.

The posterior distribution for our accuracy score given 100 examples

>>> mean std median 2.5% 97.5% n_eff r_hat

>>> p 0.86 0.04 0.86 0.79 0.93 185.83 1.00



>>> Number of divergences: 0

Not such a great result… 100 observations is not really enough to settle on a good outcome. The distribution does not seem to center around a particular value, and when we ask for the 95% credibility interval our true value can lie anywhere between 79% and 93%. This may or may not be accurate enough for our purposes.

Let’s see how confident we can be in our model when we use all 14,000 observations

Modifying Our Model

If we run all 14,000 observations through the same model, it would take a very long time to run.

This is because we cycle through each observation in our code.

Pyro contains a more convenient, vectorized, method of approaching our model.

First, we redefine our model function such that it accepts NO observations, but rather it returns its own observations

Now, we defined a second function that takes as input a model function, and observations, and utilizes `pyro.poutine` to run the model function in a conditioned environment. It’s important our observations here have the same name (“obs”) as they do in the model function.

Finally, we re-run the MCMC sampler, but now with our conditioned model, and we send our model function, as well as our observations, as arguments

>>> Sample: 100%|█████████████████████████████████████████| 200/200 [01:47, 1.86it/s, step size=7.87e-01, acc. prob=0.984]

The posterior distribution for our accuracy score given 14,000 examples

>>> mean std median 2.5% 97.5% n_eff r_hat

>>> p2 0.87 0.00 0.87 0.87 0.87 9.88 1.07



>>> Number of divergences: 0

Now we can get a much tighter fit around the 87% mark. Just to compare the two distributions, we could plot them together.

The two posterior distributions for our accuracy based on the number of support examples

I hope you all enjoyed reading this brief intro to Pyro and its capabilities. I certainly look forward to trying new things and working with probabilistic programming more!"
How to Use ggplot2 in Python,"Introduction

Thanks to its strict implementation of the grammar of graphics, ggplot2 provides an extremely intuitive and consistent way of plotting your data. Not only does ggplot2’s approach to plotting ensure that each plot comprises certain basic elements but it also simplifies the readability of your code to a great extent.

However, if you are a frequent user of Python, then implementing the grammar of graphics can be extremely challenging due to the lack of standardized syntax in popular plotting libraries, such as matplotlib or seaborn. Should you still want to make use of the grammar of graphics, then the Python package plotnine provides an alternative for you.

The Grammar of Graphics

In case you should be unfamiliar with the grammar of graphics, here is a quick overview:

Main Components of the Grammar of Graphics

As you can see, there are several components that make up the grammar of graphics, starting with your data. After identifying the data you would like to visualize, you have to specify the variables you are interested in. For instance, you might want to display one variable on the x-axis and another on the y-axis. Third, you have to define what type of geometric object (geom for short) you would like to utilize. This could be anything from a bar plot to a scatter plot or any of the other existing plot types.

These first three components are compulsory. Without data, there is nothing to plot. Without axis definitions, there is nothing to plot either. And finally, without defining a geometric object, you will only see an empty coordinate system. The remaining components making up the grammar of graphics are optional and can be implemented to improve visualizations. Facets refer to specifications of subplots, that is, plotting several variables within your data next to one another in separate plots. Statistical transformations mainly refer to the inclusion of summary statistics in your plot, such as the median or percentiles. Coordinates describe the different coordinate systems available to you. The most used and default coordinate system is the Cartesian coordinate system. Depending on the structure of the data you would like to plot, lesser used coordinate systems, such as the Polar coordinate system, might provide a better way of visualizing your data. Finally, themes provide a variety of options to design all non-data elements of your plot, such as the legend, background, or annotations.

While there are many ways of visualizing the grammar of graphics, I particularly like the one I created above because it implies the additivity of these layers as well as the fact that they are building upon one another. If you have ever used ggplot2, you are familiar with the ‘+’ in its syntax that symbolizes the same idea described above.

plotnine

plotnine is a Python package allowing you to use ggplot2-like code that is implementing the grammar of graphics. By doing so, just as in ggplot2, you are able to specifically map data to visual objects that make up the visualization. This enables you to improve both the readability as well as the structure of your code. While you could set matplotlib’s style to ggplot, you cannot implement the grammar of graphics in matplotlib the same way you can in ggplot2.

Installation

Before getting started, you have to install plotnine. As always, there are two main options for doing so: pip and conda.

Plotting

Having installed plotnine, you can get started plotting using the grammar of graphics. Let us begin by building a very simple plot only using the three requisite components: data, aesthetics, and geometric objects.

Building a plot using the grammar of graphics

As you can see, the syntax is very similar to ggplot2. First, we specify the data source. In our case, the data we are using is the classic mpg data set. Next, we define that the variable ‘class’ is going to be displayed on the x-axis. Lastly, we say that we would like to use a bar plot with bars of size 20 to visualize our data. Let us look at the complete code and the resulting plot:

The code above will yield the following output:

While this is a good start, it is not very nice to look at yet. Let us use other components of the grammar of graphics to beautify our plot.

For instance, we could flip the axes using coord_flip() and customize the plot and axes titles with labs() to improve our plot. Using the code chunk above, our plot would look like this:

Plotting Multidimensional Data

Besides basic plots, you can do almost everything you could otherwise do in ggplot2, such as plotting multidimensional data. If you would like to visualize the relationships between three variables you could add aesthetics to an otherwise two-dimensional plot:

Adding color to the aesthetics will prompt plotnine to display a two-dimensional plot using displ (engine displacement, in liters) on its x- and hwy (highway miles per gallon) on its y-axis and color the data according to the variable class. We have also switched the geometric object to geom_point(), which will give us a scatter instead of a bar plot. Let us take a look at what that would look like:

Conclusion

As you can see, plotnine provides you with the ability to utilize the grammar of graphics within Python. This increases the readability of your code and allows you to specifically map parts of your data to visual objects. If you are already familiar with ggplot2, then you won’t have to learn anything new to master plotnine. If not, here is a link to the ggplot2 website on which you can find out plenty more about the grammar of graphics and all types of geometric objects available to you."
The Impact of Marijuana Legalization on Traffic Deaths: A Synthetic Control Approach,"Image by Jess Liotta on flickr

With the rise of marijuana legalization, there has also been rising questions regarding the potential impact on traffic deaths and collisions. Here, we will perform an analysis using Synthetic Control Method (SCM) to determine the impact marijuana legalization had on driving fatalities.

Selected Sample

So far, 11 states have voted to legalize recreational marijuana. Some of the states are still awaiting full legislation to be implemented or legalized too recently to collect adequate data on. With this in mind, we have several states that can be analyzed:

Alaska Colorado Oregon Washington

For this analysis, we will be using the fatality rate used by the NHTSA: fatalities per 100 million miles driven. To start, we can see the rate of fatalities per 100 million miles traveled for all 4 states since legalization of marijuana was enacted.

Fatality rates increased in three out of four states — the exception being Alaska. However, it is more difficult to identify if there is a causal relationship between these variables. To determine that will take a more rigorous analysis. For simplicity, I will only analyze Colorado and Washington (the first two states to legalize recreational marijuana). Both of these states legalized marijuana in early 2014 and provide us with the longest period to analyze.

Building A Synthetic Control

The method of establishing causality here is the Synthetic Control Method. Before we proceed any further, we should clear up some terminology that will be used. An “intervention” or “treatment” is the event to which we are studying the impact of (in this case, recreational marijuana legalization). A “treated group” refers to the group which underwent the intervention. For this analysis, it is the states of Colorado and Washington. Predictor variables are variables that are able to impact the dependent variable."
Pandas from basic to advanced for Data Scientists,"Grouping

Suppose if you want to Manipulate on a particular group of data. In this case, let us get only the rows that belong to new york. With group object, you can get a summary of the sum, mean, median of all groups at a time.

Group by City

city_group = df.groupby(‘city’)

A group object was created and if you want to see specific group data, just need to get the group.

city_group.get_group(‘new york’)

Output

new york group

Aggregations

In the above section, we just grouped the data by the city but what if I would like to see the average temperature and average wind speed ???. We will use aggregations here.

Group by and aggregate

df.groupby(‘city’).agg({‘temperature’:’mean’, ‘windspeed’:’mean’})

Output

Mean temperature and wind speed

Merging

In the above sections, we dealt with having a single data frame. If there are two data frames and you would like to analyze them together !!!. In this scenario, the merge plays a key role and simplifies the join of two data frames.

create two data frames

df1 = pd.DataFrame({

“city”: [“new york”,”florida”,”mumbai”],

“temperature”: [22,37,35],

})

df2 = pd.DataFrame({

“city”: [“chicago”,”new york”,”florida”],

“humidity”: [65,68,75],

})

Simple Merge: This gives you the matching rows in both data frames

pd.merge(df1,df2,on=’city’)

Output

Matching rows of two data frames

Outer: Get all rows from both data frames. Add a new parameter (how).

pd.merge(df1,df2,on=”city”,how=”outer”)

Output

Outer join

In similar, we can get all the matching rows along with the left data frame (left join) and right data frame (right join). By specifying parameter how with values left/right.

Crosstab

Suppose if you want to see the frequency count of the event type ( rainy/sunny) in each city. Cross tab makes these things easier.

pd.crosstab(df.city,df.event)

Output

Frequency count of the event by city

Note: We can get any aggregation mean, median, etc. Just we need to pass an extra parameter to the function.

Reshape with melt

If you want to get the columns as rows along with values, suppose for each city I would like to have temperature and wind speed in a separate value column. In this case temperature, windspeed hold a single column and their values hold another column.

pd.melt(df,id_vars=[‘day’,’city’,’event’],var_name=’attribute’)

Output

Reshaped data

References

code basics, https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ"
Microsoft Introduction to AI — Part 1,"Image used under licence from Getty Images.

Microsoft Introduction to AI — Part 1

Are you a bit like me and have wanted to learn about Artificial Intelligence although felt a little intimidated by the maths involved? Maybe you thought the concepts were too difficult to understand and you would be out of your depth. I recently completed the Microsoft Introduction to AI course and wrote course notes to help me retain the knowledge that I have learned. I have tried to write these notes in a basic way to make them easy to consume. I’ve recently become an aunt and have bought a few children’s books related to technology and space, I really love how the authors and illustrators have managed to simplify complicated topics. So, I’ve been inspired to treat these topics in a similar way by simplifying them to make them a lot more accessible.

*If you would like to know a little background info behind the course notes and other notes related to tech and product design you can find out more here.*

Summary

The Microsoft Introduction to AI course provides an overview of AI and explores machine learning principles that provide the foundation for AI. From the course you can discover the fundamental techniques that you can use to integrate AI capabilities into your apps. Learn how software can be used to process, analyse and extract meaning from natural language. Find out how software processes images and video to understand the world the way humans do. Learn about how to build intelligent bots that enable conversations between humans and AI systems.

Image created by the author. Microsoft Introduction to Artificial Intelligence Course

The course takes approximately 1 month to complete so 1 medium article I write contains 1 week's worth of content. This means that it would only take you approximately 18 minutes to read 1 week worth of content which is a fast way of learning. The course is free without a certificate however, if you’d like a certificate as proof of completion there is a fee. There are labs associated with this course that I won’t include in the notes as I believe the best way to learn is to actually do the labs. However, these notes are useful if you’d like to know about the fundamental theory behind AI and would like to learn it in a way that might be a lot simpler than other resources. I’ve tried to write it in layman terms and have included visuals to help illustrate the ideas. These notes are useful if you don’t have time to do the course, it’s a quick way to skim through the core concepts. Alternatively, if you have done the course like me you can use these notes to retain what you have learned.

Instructor: Graeme Malcolm — Senior Content Developer at Microsoft Learning Experiences.

Syllabus

The course is broken into the four parts which include:

1. Machine Learning (*this medium article will focus on just this section)

Learn about the fundamentals about AI and machine learning.

Learn how software can be used to process, analyse and extract meaning from natural language.

Learn how software can be used to process images and video to understand the world the way that we do.

Find out how to build intelligent bots that enable conversational communication between humans and AI systems.

Image created by the author.

Machine Learning

The ‘Machine Learning’ part of the course will tackle the following topics:

· What is Artificial Intelligence? · What is Machine Learning? · Supervised · Unsupervised · Regression · Classification · Clustering

Illustration by Michael Korfhage for HR Magazine SHRM.

What is Artificial Intelligence?

Artificial Intelligence (AI) is a way to enable people to accomplish more by collaborating with smart software. Think of it as putting a more human face on technology. AI is technology that can learn from vast amounts of data that is available in the modern world. Learning from this data it can understand our human kind of language and can respond in a similar kind of way. It’s technology that can see and interpret the world the way that we humans do.

Illustration by Justin Middendorp.

What is Machine Learning?

Machine learning (ML) provides the foundation for artificial intelligence.

So what is it?

Machine learning gives computers the ability to learn and make predictions or decisions based on data without explicitly programming that in. Well as the name suggests it’s a technique in which we train a software model using data. A model is a mathematical representation of a real-world process. The model learns from the training cases (training situations or examples) and then we can use the trained model to make predictions for new data cases. The key to this is to understand that fundamentally computers are very good at one thing and that is performing calculations. To have a computer make intelligent predictions from the data, we just need a way to train it to perform the correct calculations.

We start with a dataset that contains historical records which we often call ‘cases’ or ‘observations’. Each observation includes numeric features. Numeric features are basically characteristics of the item we’re working with and they have a numeric value attached to each characteristic.

Illustration by Vecteezy.

Let’s call the numeric feature X.

In general, we also have some value that we’re trying to predict which we’ll call that Y. We use our training cases to train a machine learning model so that it can calculate a value for Y from the features in X. So in very simplistic terms, we’re creating a function that operates on a set of features ‘X’, to produce predictions ‘Y’. Don’t worry if this is confusing it will make more sense in the next sections where we start to apply real world examples.

Now generally speaking, there are two broad types of machine learning and they are called supervised and unsupervised.

Supervised

In supervised learning scenarios, we start with the observations that include known values for the variable that we want to predict. We call these ‘labels’. Since we started with data that includes the label we’re trying to predict, we can train the model using only some of the data and withhold the rest of the data which we can use to evaluate the performance of the model. We then use a machine learning algorithm to train a model that fits the features to the known label.

Since we started with the known label value we can validate the model by comparing the value predicted by the function to the actual label value that we knew. Then when we’re happy that the model works well, we can use it with new observations for which the label is unknown and generate new predicted values.

In this example we know the value of both X (numeric feature) and Y (variable we want to predict). Since we know X and Y we can use this algorithm to train our model. Once the model has been trained and we are happy that it works well we can use this model to calculate Y for when X is unknown. Illustration by Vecteezy.

Unsupervised

Unsupervised learning is different from supervised learning, in that this time we don’t have the known label values in the training dataset. We train the model by finding similarities between the observations. After the model is trained, each new observation is assigned to the cluster of observations with the most similar characteristics.

In this example the value Y is unknown and so the way we train the model is through finding similarities between the observations. The observations are categorised in clusters that have similar characteristics. Once we train the model based on these clusters we can use it to predict the value of Y by assigning a new observation to a cluster. Illustration by Vecteezy.

Regression

Okay, let’s start with a supervised learning technique called ‘Regression’. Imagine we have some historic data about some health trials participants. We have information such as the exercise they have done, the number of calories they have spent and a lot more stats and info about them. In this case we could use machine learning to predict how many calories any new participants might be expected to burn while engaging in some exercises. When we need to predict a numeric value, like for example an amount of money or a temperature or the number of calories then what we use is a supervised learning technique called regression.

For example, let’s suppose Rosy here is a participant in our health study. Here she is doing some weight exercises. We gather some data about Rosy when she first signed up for the study. We also gather data while she’s exercising and capture data using a fitness monitor smart watch.

Now what we want do is model the calories burned using the features we have for Rosy’s exercise. These numeric features (X) are her age, weight, heart rate, duration, and so on. In this case we know all of the features and we know the label value (Y) of 231 calories. So we need an algorithm to learn the function that operates all of Rosy’s exercise features to give us a result of 231.

Illustration by Vecteezy.

Now of course a sample of only one person isn’t likely to give us a function that generalises well. So what we need to do is gather the same sort of data from lots of diverse participants and train our model based on this larger set of data.

Illustration by Vecteezy.

After we’ve trained the model and we have a generalised function that can be used to calculate our label Y, we can then plot the values of Y calculated for specific features of X values on a chart like this.

Image created by the author.

We can then interpolate any new values of X to predict an unknown Y.

Image created by the author.

Now because we started with data that includes the label we are trying to predict, we can train the model using only some of the data and withhold the rest of the data for evaluating model performance.

Then we can use the model to predict (F(X)) for evaluation data and compare the predictions or scored labels to the actual labels that we know to be true. The difference between the predicted and actual levels are what we call the ‘residuals’. Residuals can tell us something about the level of error in the model.

Image created by the author.

Now there are a few ways we can measure the error in the model and these include root-mean-square error (RMSE) and mean absolute (MAE). Now both of these are absolute measures of error in the model.

Image created by the author.

For example an RMSE value of 5 would mean that the standard deviation of error from our test error is 5 calories. Of course absolute values can vary wildly depending on what you are predicting. An error of 5 calories would seem to indicate a reasonably good model. But if we were predicting how long an exercise session takes an error of 5 hours would indicate a very bad model.

So you might want to evaluate the model using relative metrics to indicate a more general level of error as a relative value between 0 and 1. Relative absolute error (RAE) and relative squared error (RSE) produce a metric where the closer to 0 the error, the better the model.

Image created by the author.

The coefficient of determination (CoD(R2)) which we sometimes call R squared is another relative metric but this time a value closer to 1 indicates a good fit for the model.

Image created by the author.

Classification

So we’ve seen how to train a regression model to predict a numeric value. Now it’s time to look at another kind of supervised learning called classification. Classification is a technique that we can use to predict which class or category that something belongs to. The simplest variant of this is binary classification (ones and zeros) where we predict whether an entity belongs to one of two classes. It’s often used to determine if something is true or false about the entity.

For example, suppose we take a number of patients in our health clinic and we gather some personal details. We run some tests and we can identify which patients are diabetic and which are not. We can learn a function that can be applied to these patient features and give the result 1 for patients that are diabetic and 0 for patients that aren’t.

Illustration by Vecteezy.

More generally, a binary classifier is a function that can be applied to features X to produce a Y value of 1 or 0.

Illustration by Vecteezy.

Now the function won’t actually calculate an absolute value of 1 or 0, it will calculate a value between 1 and 0. We will use a threshold value (dotted line in diagram) to decide whether the result should be counted as a 1 or a 0.

The threshold is represented as the dotted line. Image created by the author.

When you use the model to predict values, the resulting value is classed as a 1 or a 0 depending on which side of the threshold line it falls.

Image created by the author.

Because classification is a supervised learning technique we withhold some of the test data to validate the model using known labels.

Image created by the author.

Cases where the model predicts a 1 for a test observation that actually has a label value of 1 these are considered true positives (TP).

Image created by the author.

Similarly cases where the model predicts 0 and the actual label is 0 these are true negatives (TN).

Image created by the author.

Now the choice of threshold determines how predictions are assigned to classes. In some cases a predicted value might be very close to the threshold but is still misclassified. You can move the threshold to control how the predicted values are classified. In the case of the diabetes model it might be better to have more false positives (FP) but reduce the number of false negatives (FN) so that more people who are at risk of diabetes get identified.

Image created by the author.

The number of true positives (TP), false positives (FP), true negatives (TN), and false negative (FN) produced by your model is crucial in evaluating its effectiveness.

Image created by the author.

The grouping of these are often shown in what’s called a confusion matrix shown below. This provides the basis for calculating performance metrics for the classifier. The simplest metric is accuracy which is just the number of correctly classified cases divided by the total number of cases.

Image created by the author.

In this case there are 5 true positives (TP) and 4 true negatives (TN). There are also 2 false positives (FP) and no false negatives (FN). That gives us 9 correct predictions out of a total of 11 which is an accuracy of 0.82 or 82%.

Image created by the author.

Now that might seem like a really good result but perhaps surprisingly accuracy actually isn’t all that useful as a measure of a model’s performance. Suppose that only 3% of the population is diabetic. I can create a model that simply always predicts zero and it would be 97% accurate but it’s completely useless for identifying potential diabetics.

A more useful metric might be the fraction of cases classified as positive that are actually positive. This metric’s known as precision. In other words out of all the cases classified as positives which ones are true and not false alarms.

Image created by the author.

In this case there are 5 true positives, and 2 false positives. So our precision is 5 / (7) which is 0.71 or 71% of our cases identified as positive are really diabetic and 29% are false alarms.

Image created by the author.

In some situations we might want a metric that’s sensitive to the fraction of positive cases we correctly identify. We call this metric recall. Recall is calculated as the number of true positives divided by the combined true positives and false negatives. In other words, what fraction of positive cases are correctly identified?

Image created by the author.

In this case, there are 5true positives and no false negatives. So our recall is 5 out of 5 which is of course is 1 or 100%. So in this case we’re correctly identifying all patients with diabetes. Now recall actually has another name sometimes it’s known as the True Positive Rate.

Image created by the author.

There’s an equivalent rate for false positives compared to the actual number of negatives. In this case we have 2 false positives and 4 true negatives. So our false positive rate is 2/(6) which is 0.33.

Image created by the author.

Now you may remember that the metrics we got were based on a threshold (blue dotted line) of around 0.3 and we can plot the true positive rate against the false positive rate for that threshold like this.

Image created by the author.

If we were to move the threshold back to 0.5 our true positive rate becomes 4 out of 5 or 0.8. Our false positive rate is 1 out of 6 or 0.16 which we can plot here.

Image created by the author.

Moving the threshold further to say 0.7 gives us a true positive rate of 2 out of 5 or a 0.4 and a false positive rate of 0 out of 6 or 0.

Image created by the author.

If we plotted this for every possible threshold rate we would end up with a curved line as shown in the diagram below. Now this is known as a receiver operator characteristic, a ROC chart. Now the area under the curve (AUC) is an indication of how well the model predicts. Generally, you want to see a large AUC with a curve staying as close as possible to the top left corner of the chart. A perfect classifier would go straight up the left and then along the top giving an AUC of one. Now, you can always compare with a diagonal line and that represents how well the model would perform if you simply made a 50–50 guess. It’s an AUC of 0.5. So you’re just simply random guessing 50% of the time true, 50% false. In this case, our model has an AUC of 0.9 which means that our model is definitely outperforming guessing.

The area under the curve (AUC) is an indication of how well the model predicts. Generally, you want to see a large AUC with a curve staying as close as possible to the top left corner of the chart. What is shown in the blue graph is a good example of a model that is outperforming a 50–50 guess. Image created by the author.

Clustering

Well, we’ve seen some examples of supervised learning specifically regression and classification but what about unsupervised learning? Now with unsupervised learning techniques you don’t have a known label with which to train the model. But you can still use an algorithm that finds similarities in data observations in order to group them into clusters.

Suppose for example our health clinic has a website that contains links to articles and medical and healthy lifestyle publications. Now I might want to automatically group similar articles together.

Illustration by Vecteezy.

Or maybe I want to segment our study participants and we can categorise them based on similar characteristics.

Illustration by Vecteezy.

There are a number of ways we can create a clustering model and we’re going to look at one of the most popular clustering techniques which is called k-means clustering.

Image created by the author.

Now the key to understanding k-means is to remember that our data consists of rows of data and each row has multiple features. Now if we assume that each feature is a numeric value then we can plot them as coordinates. Now here we’re plotting two features on a two dimensional grid. But in reality, multiple features would be plotted in n-dimensional space.

We then decide how many clusters we want to create which we call k. We plot k points at random locations that represent the center points of our clusters.

k points are represented as the stars in the diagram. Image created by the author.

In this case, k is 3 so we’re creating 3 clusters. Next, we identify which of the three centroids each point is closest to and assign the points to clusters accordingly.

Image created by the author.

Then we move each centroid to the true center of the points and its cluster.

Image created by the author.

We then reallocate the points in the cluster based on their nearest centroid.

Image created by the author.

We just repeat that process until we have nicely separated clusters.

Image created by the author.

So what do I mean by nicely separated? Well, we want a set of clusters that separate data by the greatest extent possible. To measure this we can compare the average distance between the cluster centers.

Image created by the author.

In addition, the average distance between the points in the cluster and their centers.

Image created by the author.

Clusters that maximise this ratio have the greatest separation. We can also use the ratio of the average distance between clusters and the maximum distance between the points and the centroid of the cluster.

Image created by the author.

Now another way we could evaluate the results of a clustering algorithm is to use a method called principal component analysis (PCA). In this method we decompose the points in a cluster into directions. We represent the first two components of the PCA decomposition as an ellipse.

Image created by the author.

The first principal component is along the direction of the maximum variance or major axis of the ellipse and the second PCA is along the minor axis of the ellipse. A cluster that is perfectly separate from the first cluster shows up as an ellipse with the major axis of the ellipse perpendicular to the ellipse of the first cluster.

Image created by the author.

Every second cluster is reasonably well separated but not perfectly separated.

Image created by the author.

Then it will have a major axis that is not quite perpendicular to the first ellipse. If the second cluster is quite poorly separated from the first then the major axis of both ellipses will be nearly collinear.

Image created by the author.

So the ellipse may be more like a circle because the second cluster is less well defined.

Final Word

Thanks for reading this article, Part 1 of the Microsoft Introduction to Artificial Intelligence course. If you found this helpful then check out all 4 parts on my Medium account or in Towards Data Science. If you had some trouble with some of the concepts in this article (don’t worry it took me awhile for the information to sink in) and you need a bit more info, then enrol for free in the Microsoft Introduction to AI course. It’s helpful to watch the course videos alongside with these notes.

*If you would like to know a little background info behind the course notes and other notes related to tech and product design you can find out more through here.*

A little background

Hi, I’m Christine :) I’m a product designer who’s been in the digital field for quite some time and have worked at many different companies; from large companies (as large as 84,000 employees), to mid size and to very small startups still making a name for themselves. Despite having a lot of experience I’m a product designer who has a fear of suffering from the dunning-kruger effect and so I’m continuously trying to educate myself and I’m always searching for more light. I believe to be a great designer you need to constantly hone your skills especially if you are working in the digital space which is constantly in motion."
How to be less wrong,"How to be less wrong

How much longer will the Berlin Wall stand? This is what went through J. Richard Gott’s head when he visited Germany in 1969.

If you think about it, this is a tough question, as there is not much data available about lifetimes of walls in Germany (in fact, the Berlin Wall is a single datapoint). How then to approach a question like that, where we are almost completely in the dark?

Remains of the Berlin Wall. (Source: Pixabay)

Gott, today a professor in Astrophysics at Princeton University, thought about it like this: there was nothing special about him seeing the Wall on that particular day in that particular year. If you divide the Wall’s entire lifetime into four equal-length segments, then there is a 50% probability that he would have arrived within the middle two segments. This in turn translates into an estimate for how much longer the Wall should remain: between a third of its lifetime so far (if he happened to visit at end of the two middle segments), and three times its lifetime so far (if he happened to visit at the beginning of the two middle segments).

Given that the Wall at the time was 8 years old in 1969, he concluded that with 50% confidence it would last another 3 to 24 years. The Wall lasted another 20 years.

There is a 50% chance that Gott’s visit falls within the middle two segments of the Wall’s existence. (Source)

The Copernican Principle: we are not special

Gott’s reasoning that he did not visit the Wall at any special moment in time is really an application of the much broader Copernican principle, which states that we occupy neither a special place, nor exist in a special time in the Universe’s history. Earth is not a special place, and neither is our Solar System or Milky Way Galaxy. 2019 is not a special time to be alive.

Gott’s calculation based on the Copernican principle can help us arrive at least at an estimate of the timing of an event when we are otherwise completely in the dark. It can be a fun exercise to estimate various world events with it:"
"Cleaning, Analyzing, and Visualizing Survey Data in Python","Cleaning, Analyzing, and Visualizing Survey Data in Python

A tutorial using pandas , matplotlib , and seaborn to produce digestible insights from dirty data Charlene Chambliss · Follow Published in Towards Data Science · 10 min read · Mar 30, 2019 -- 6 Listen Share

If you work in data at a D2C startup, there’s a good chance you will be asked to look at survey data at least once. And since SurveyMonkey is one of the most popular survey platforms out there, there’s a good chance it’ll be SurveyMonkey data.

The way SurveyMonkey exports data is not necessarily ready for analysis right out of the box, but it’s pretty close. Here I’ll demonstrate a few examples of questions you might want to ask of your survey data, and how to extract those answers quickly. We’ll even write a few functions to make our lives easier when plotting future questions.

We’ll be using pandas , matplotlib , and seaborn to make sense of our data. I used Mockaroo to generate this data; specifically, for the survey question fields, I used ""Custom List"" and entered in the appropriate fields. You could achieve the same effect by using random.choice in the random module, but I found it easier to let Mockaroo create the whole thing for me. I then tweaked the data in Excel so that it mirrored the structure of a SurveyMonkey export.

Oh boy…here we go

Your first reaction to this might be “Ugh. It’s horrible.” I mean, the column names didn’t read in properly, there are a ton of NaNs, instead of numerical representations like 0/1 or 1/2/3/4/5 we have the actual text answers in each cell…And should we actually be reading this in with a MultiIndex?

But don’t worry, it’s not as bad as you might think. And we’re going to ignore MultiIndexes in this post. (Nobody really likes working with them anyway.) The team needs those insights ASAP — so we’ll come up with some hacky solutions.

First order of business: we’ve been asked to find how the answers to these questions vary by age group. But age is just an age--we don't have a column for age groups! Well, luckily for us, we can pretty easily define a function to create one.

But if we try to run it like this, we’ll get an error! That’s because we have that first row, and its value for age is the word “age” instead of a number. Since the first step is to convert each age to an int , this will fail.

We need to remove that row from the DataFrame, but it’ll be useful for us later when we rename columns, so we’ll save it as a separate variable.

You will notice that, since removing headers , we've now lost some information when looking at the survey data by itself. Ideally, you will have a list of the questions and their options that were asked in the survey, provided to you by whoever wants the analysis. If not, you should keep a separate way to reference this info in a document or note that you can look at while working.

OK, now let’s apply the age_group function to get our age_group column.

Great. Next, let’s subset the data to focus on just the first question. How do the answers to this first question vary by age group?

Great. We have the answers in a variable now. But when we go to plot this data, it’s not going to look very good, because of the misnamed columns. Let’s write up a quick function to make renaming the columns simple:

Remember headers from earlier? We can use it to create our new_names_list for renaming.

It’s already an array, so we can just pass it right in, or we can rename it first for readability.

Isn’t that so much nicer to look at? Don’t worry, we’re almost to the part where we get some insights.

Notice how groupby and other aggregation functions ignore NaNs automatically. That makes our lives significantly easier.

Let’s say we also don’t really care about analyzing under-30 customers right now, so we’ll plot only the other age groups.

OK, this is all well and good, but the 60+ group has more people in it than the other groups, and so it’s hard to make a fair comparison. What do we do? We can plot each age group in a separate plot, and then compare the distributions.

“But wait,” you might think. “I don’t really want to write the code for 4 different plots.”

Well of course not! Who has time for that? Let’s write another function to do it for us.

I believe it was Jenny Bryan, in her wonderful talk “Code Smells and Feels,” who first tipped me off to the following:

If you find yourself copying and pasting code and just changing a few values, you really ought to just write a function.

This has been a great guide for me in deciding when it is and isn’t worth it to write a function for something. A rule of thumb I like to use is that if I would be copying and pasting more than 3 times, I write a function.

There are also benefits other than convenience to this approach, such as that it:

reduces the possibility for error (when copying and pasting, it’s easy to accidentally forget to change a value)

makes for more readable code

builds up your personal toolbox of functions

forces you to think at a higher level of abstraction

(All of which improve your programming skills and make the people who need to read your code happier!)

Hooray, laziness!

This is, of course, generated data from a uniform distribution, and we would thus not expect to see any significant differences between groups. Hopefully your own survey data will be more interesting.

Next, let’s address another format of question. In this one, we need to see how interested each age group is in a given benefit. Happily, these questions are actually easier to deal with than the former type. Let’s take a look:

And look, since this is a small DataFrame, age_group is appended already and we won't have to add it.

Cool. Now we have the subsetted data, but we can’t just aggregate it by count this time like we could with the other question — the last question had NaNs that would be excluded to give the true count for that response, but with this one, we would just get the number of responses for each age group overall:

This is definitely not what we want! The point of the question is to understand how interested the different age groups are, and we need to preserve that information. All this tells us is how many people in each age group responded to the question.

So what do we do? One way to go would be to re-encode these responses numerically. But what if we want to preserve the relationship on an even more granular level? If we encode numerically, we can take the median and average of each age group’s level of interest. But what if what we’re really interested in is the specific percentage of people per age group who chose each interest level? It’d be easier to convey that info in a barplot, with the text preserved.

That’s what we’re going to do next. And — you guessed it — it’s time to write another function.

Quick note to new learners: Most people won’t say this explicitly, but let me be clear on how visualizations are often made. Generally speaking, it is a highly iterative process. Even the most experienced data scientists don’t just write up a plot with all of these specifications off the top of their head.

Generally, you start with .plot(kind='bar') , or similar depending on the plot you want, and then you change size, color maps, get the groups properly sorted using order= , specify whether the labels should be rotated, and set x- or y-axis labels invisible, and more, depending on what you think is best for whoever will be using the visualizations.

So don’t be intimidated by the long blocks of code you see when people are making plots. They’re usually created over a span of minutes while testing out different specifications, not by writing perfect code from scratch in one go."
Spatial data science for the uninitiated,"The most intuitive example is geospatial data, which carries information about where things happen on Earth. Geospatial data can describe natural or human subjects like topography, political boundaries, urban systems, weather and climate patterns, road networks, distributions of species, consumption habits, shipping and logistics systems, demographics and so on.

The spatial dimensions usually are measurements of latitude (the y-coordinate) and longitude (the x-coordinate), and sometimes altitude (the z-coordinate), which can place a point precisely on, above or below the Earth’s surface. GIS skills and tools (geographic information systems / science) are often used by spatial data scientists to manipulate, analyze and visualize geospatial data.

It’s worth noting that many spatial analysis techniques are actually agnostic to the scale — so we could apply the same algorithms to a map of the Earth as we could to a map of a cell, or a map of the universe. And spatial data science techniques can be applied to more abstract problems with a spatial element — analyzing how closely associated words are, for example, by working out how often they are found together.

Raster and Vector

Spatial data typically falls into two categories: raster and vector. Both are ways to describe space and represent features, but they work quite differently.

Raster Data

A raster is a “grid of regularly sized pixels”. By assigning each cell in the grid a value — or a few values — images can be described numerically, as multidimensional arrays.

For example, take a 3x3 grid that looked like this:

A 3x3 raster grid.

If 1 means black and 0 means white, we could represent it numerically like this:

img = [[ 1, 0, 1 ],

[ 0, 1, 0 ],

[ 1, 0, 1 ]]

The numbers in raster cells can mean lots of things — the altitude of the land or depth of the sea at that specific point, the amount of ice or snow on that point, the number of people living within that pixel, and so on. Further, just about any color in the visible spectrum can be described by a combination of three numbers representing the intensity of Red, Green and Blue (RGB) — satellite images are raster data structures. GeoTiff, jpg, png and bitmap files contain raster data.

A raster image of population in Africa, from http://www.ncgia.ucsb.edu/pubs/gdp/pop.html.

Vector Data

Vector data is a bit more abstract. In a vector dataset, features are individual units in the dataset, and each feature typically represents a point, line or polygon. These features are represented mathematically, usually by numbers that signify either the coordinates of the point, or the vertices (corners) of the geometry.

Vector features from Saylor Academy.

Points, Lines, Polygons

As a quick example, here’s a bare-bones numerical representation of each of these types of features:

point = [ 45.841616, 6.212074 ] line = [[ -0.131838, 51.52241 ],

[ -3.142085, 51.50190 ],

[ -3.175046, 55.96150 ]] polygon = [[ -43.06640, 17.47643 ],

[ -46.40625, 10.83330 ],

[ -37.26562, 11.52308 ],

[ -43.06640, 17.47643 ]]

// ^^ The first and last coordinate are the same

Vector features will often have some metadata included that describes the feature — the name of a road, say, or the population of a state. These extra, non-spatial metadata of a feature are usually called “attributes”, and are often represented in an “attribute table”. Very often spatial data scientists will combine the spatial dimensions (coordinates — for points, or coordinate arrays — for lines and polygons) with non-spatial dimensions in their analysis. GeoJSON and .shp files commonly contain vector data.

Why is this different from regular data science?

The short answer is that it isn’t: spatial data science is a discipline within data science. However, spatial data have some characteristics that make them need special treatment. This is true in the way the data is stored and handled from a programming / database perspective, and how it is analyzed from an algorithmic perspective. That means spatial data scientists have to learn some concepts — mainly from geometry, like representing 3D shapes on flat surfaces — that other data scientists might never have to deal with.

Tools for Spatial Data Science

Spatial data scientists try to make sense of these spatial datasets to better understand the system or phenomenon they’re studying. Some incredible (and often free) software tools make this possible. Most programming languages like Python, R and Javascript have amazing spatial analysis libraries like geopandas and turf.js, and desktop programs like QGIS make visualizing and analyzing spatial data accessible to less technical people. There are also powerful online tools like Mapbox, Carto and Google BigQuery to help with these analysis and visualization challenges. JavaScript libraries like Leaflet and Mapbox GL JS enable web developers to create interactive maps in the browser.

A few examples

Spatial data scientists might have the task of analyzing spatial distribution — to see if points cluster together, are spread out, or are randomly placed — to, say, work out the best place to build a new airport or retail center, or understand patterns of violence or crime.

Clustering ACLED conflict events in Yemen from a project I did at UCL.

It could entail analyzing trends through time — by seeing how a certain district’s voting outcomes evolved, or how sentiment toward some issue changed in different parts of a country.

Maybe analysts are analyzing satellite imagery to map unmapped areas to help deliver emergency services more efficiently, or work out how shady or sunny a new potential building site or bike route is. It might mean calculating the most efficient route to get from A to B given current traffic conditions. For as niche as the field is, spatial data science has a breadth of applications across just about every sector and field."
The proper way of handling mixed-type data. State-of-the-art distance metrics.,"Fun fact: Scikit-Learn doesn’t have any distance metrics which can handle both categorical and continuous data! How can we then use clustering algorithms, e.g. k-NN, if we have a dataset with mixed-type variables?

Photo by Fredy Jacob on Unsplash

Update (27/07/19) — The package has been released at PyPI as Distython. I have published an article to explain how it works.

The big problem that I have faced during my summer internship in the IT Innovation Centre was the lack of existing implementations of distance metrics which could handle both mixed-type data and missing values. It has started my long search for algorithms which can satisfy those requirements. Several research papers later, I have discovered quite interesting distance metrics which can help to improve the accuracy of your machine learning model when dealing with mixed-type data, missing values, or both. I have implemented them in my spare time and published their code implementation on the Github so you can use it easily with Scikit-Learn. But how? I will explain that in this tutorial!

What I love about Towards Data Science, is that it attracts many like-minded people who are passionate about AI and Data Science. That’s why I would like to connect with you on Linkedin! You can also leave any feedback and questions via my personal website.

Overview of Heterogenous Distance Metrics

Photo by Annie Spratt on Unsplash

Before we start, I would like to recommend to look at this paper if you want to get a more in-depth understanding of the algorithms I will talk about. My main goal here is to provide you with an intuitive understanding of those algorithms so you can use my article as a quick reference sheet. You can find the practical part with a code at the end of the article. Let’s get started!

Distance Metrics

But wait… What are actually the distance metrics? The distance metrics measure the distance between two instances in the dataset. They measure the similarity between instances based on their features. For example, imagine patients of a…"
BottleNet: Learnable Feature Compression for Accelerated Edge Intelligence,"BottleNet: Learnable Feature Compression for Accelerated Edge Intelligence

Neural networks are either deployed on the local devices or the cloud servers. If it is very large then the only option will be the cloud server. The downside of using cloud services is the communication cost that you need to upload a relatively large input. The privacy is also compromised by giving the raw valuable data to the cloud owners. So the question is why not offloading the features instead of the raw input? Features are often sparse, and we may achieve higher compression ratios in the feature space. BottleNet is a paper presenting a method for feature compression which only requires to offload 316 bytes of data to the cloud servers for inference on ImageNet dataset.

BottleNet adds two extra units into an existing neural network. 1. A convolutional layer for reducing the channel/spatial dimensions 2. A lossy compressor (e.g. JPEG). Photo by author.

Given a deep neural network (DNN), we insert a convolutional layer which reduces the spatial and channel dimensions. Then we pass the reduced features of the convolution to JPEG compression. This leads to an average size of 316 bytes needs to be uploaded to the cloud for the rest of inference! Much less than a JPEG compressed 224x224 image (26667 bytes on average)! A convolutional layer for restoring the original feature dimensions followed by a JPEG decompression is used on the cloud.

Learnable dimension reduction and restoration units along the (a) channel and (b) spatial dimension of features. Photo by author.

But how to train a neural network with a non-differentiable layer (JPEG) in the middle? Approximation! Because A pair of compressor and decompressor can be approximated by an identity function, we set its derivative simply to one.

Embedding non-differentiable compression (e.g., JPEG) in DNN architecture. We approximate the pair of JPEG compressor and decompressor units by identity function to make the model differentiable in backpropagation. Photo by author.

So, in summary, we add a set of layers (Bottleneck Unit) in a neural network to reduce the communication costs of transferring the raw input image to the cloud servers. If we have an edge device, it would be preferable to insert the bottleneck unit in the initial layers to avoid the high computation costs of edge devices:

Bottleneck Unit — all the reduction, compression, decompression, restorations units altogether. Photo by author.

What happens if we simply apply a JPEG compression on the intermediate features and offload it the cloud for the rest of the computations of downstream layers? Huge accuracy loss! But if the neural network is trained while being aware of the presence of a JPEG compression unit in the middle, then the accuracy loss will become smaller. The following figure shows the accuracy gap between these two approaches:

Accuracy loss will be much lower if the network is aware of the presence of JPEG compression on its features. RB1 in this figure refers to the first residual block of the ResNet-50 model. Photo by author.

Find more about BottleNet on https://arxiv.org/abs/1902.01000.

This work has been published in the International Symposium on Low Power Electronics and Design (ISLPED), 2019. https://ieeexplore.ieee.org/document/8824955/"
Training on batch: how do you split the data?,"Three ways to split your data into batches compared for time & memory efficiency and code quality.

Introduction

With increasing volumes of the data, a common approach to train machine-learning models is to apply the so-called training on batch. This approach involves splitting a dataset into a series of smaller data chunks that are handed to the model one at a time.

In this post, we will present three ideas to split the dataset for batches:

creating a “big” tensor,

loading partial data with HDF5,

python generators.

For illustration purposes, we will pretend that the model is a sound-based detector, but the analysis presented in this post is generic. Despite the example is framed as a particular case, the steps discussed here are essentially splitting, preprocessing and iterating over the data. It conforms to a common procedure. Regardless of the data comes in for image files, table derived from a SQL query or an HTTP response, it is the procedure that is our main concern.

Specifically, we will compare our methods by looking into the following aspects:

code quality,

memory footprint,

time efficiency.

What is a batch?

Formally, a batch is understood as an input-output pair (X[i], y[i]) , being a subset of the data. Since our model is a sound-based detector, it expects a processed audio sequence as input and returns the probability of occurrence of a certain event. Naturally, in our case, the batch is consisted of:

X[t] - a matrix representing processed audio track sampled within a time-window, and

- a matrix representing processed audio track sampled within a time-window, and y[t] - a binary label denoting the presence of the event,"
Speeding up data wrangling with dtplyr,"Generating the dataset

We generate an artificial dataset. The first thing that came to my mind is an order registry, in which we store:

id of the client

of the client name of the product

of the product date of purchase

of purchase amount of product purchased

of product purchased the unit price of a certain product

As this is only a toy example, we do not dive deeply into the logic behind the dataset. We can agree that it vaguely resembles a real-life scenario. For testing the performance of different approaches, we generate 10 million rows of data."
We are All Baby Shark (in Data Tracking),"Ocearch tracks whales, seals, sharks, and many other types of sea life as they make their way back and forth across the ocean. The tracking is available on their site and you can even sign up to follow certain sharks as they send in their telemetry data to the center.

Oh, does it bother you to be tagged?

All of this comes together as a ton of data including, location, speed, depth, heart rate and other metrics in the hope that we can better understand these animals in their own environment. By tracking and storing all of this data, environmentalists and biologists hope to analyze and improve the understanding of these magnificent creatures, without disturbing them.

The process of tagging an animal certainly has some momentary pain for the animal, and then they pretty much go about their lives and begin to transmit tons of data.

Fitness Trackers Are Telemetry Tags for Humans

So adorably simple.

In her book, The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power, Shoshana Zuboff does an excellent write up on telemetry and its origins and similarities to human tracking. Telemetry in the wild requires us to capture and forcibly attach a device to the animal. In the human realm, this happens in retail stores and on Amazon when we buy our fitness trackers. Sure, it’s a bit painful to pay the $600 for a top-notch tracker, but heck, I want to improve my fitness, so let’s go for it.

About ten years ago, this was my Timex watch. It was before fitness tracking really took off. There were a few Garmin and Polar trackers back then, but they were like strapping a microwave to your wrist, so only the most die-hard triathletes used them. Most of us just used this type of stopwatch to keep track of how long we worked out. This Timex was a higher-end model because it actually stored something like the last 20 workouts, which amounted to exactly 40 data points total. Start. Stop. Start. Stop. Start. Stop.

Truly high-tech.

SWOLF you.

Today, I have this incredible Garmin Fenix watch. This watch tracks literally everything. Time, location, waypoints, direction, speed, cadence, pace, heart rate, elevation, VO2 Max (oxygen velocity), steps, weight, swim speed, and even your SWOLF. Never heard of SWOLF? It’s what you get if swimming and golf have a baby.

To think how far these devices have come in such a brief period is amazing. I imagine they learned many of their algorithmic tricks from the classic use of telemetry in other fields of biology.

Tracking all of this data is at first amazing to a data geek such as myself. However, after a few months of inspecting and analyzing the data, I came to understand two painful realizations.

First, I’m No Healthier

Gathering data is not the same as having an answer or an action plan. The reality is that data is helpful but it must be interpreted and turned into action by individuals. As it turns out, my findings personally are not that dissimilar to others, in that fitness tracking doesn’t have a drastic difference in the outcome.

The reams and reams of data available from Garmin are impressive but what I realized is that I would analyze that data for a pretty decent amount of time each week. At the end of each workout and at least a few times a week, I’d review the data for anywhere from 5 minutes to 10 minutes. Add that up, and I could have actually used that time to work out, resulting in roughly one additional 45-minute workout a week. Doesn’t sound like much, but to give credit to the Timex stopwatch, finding the most important data points and focusing on those is sometimes more important than tracking everything.

Second, I’m Producing a Ton of Personal Data

Being the geek that I am, I took the output from Garmin from one 8-mile run while visiting the Jersey Shore and began to analyze it. Garmin provides these beautiful dashboard views of the data it gathers workout by workout. You can view some combination of screens on your phone or desktop through their App."
A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model,"1. Introduction of Word2vec

Word2vec is one of the most popular technique to learn word embeddings using a two-layer neural network. Its input is a text corpus and its output is a set of vectors. Word embedding via word2vec can make natural language computer-readable, then further implementation of mathematical operations on words can be used to detect their similarities. A well-trained set of word vectors will place similar words close to each other in that space. For instance, the words women, men, and human might cluster in one corner, while yellow, red and blue cluster together in another.

There are two main training algorithms for word2vec, one is the continuous bag of words(CBOW), another is called skip-gram. The major difference between these two methods is that CBOW is using context to predict a target word while skip-gram is using a word to predict a target context. Generally, the skip-gram method can have a better performance compared with CBOW method, for it can capture two semantics for a single word. For instance, it will have two vector representations for Apple, one for the company and another for the fruit. For more details about the word2vec algorithm, please check here.

2. Gensim Python Library Introduction

Gensim is an open source python library for natural language processing and it was developed and is maintained by the Czech natural language processing researcher Radim Řehůřek. Gensim library will enable us to develop word embeddings by training our own word2vec models on a custom corpus either with CBOW of skip-grams algorithms.

At first, we need to install the genism package. Gensim runs on Linux, Windows and Mac OS X, and should run on any other platform that supports Python 2.7+ and NumPy. Gensim depends on the following software:

Python >= 2.7 (tested with versions 2.7, 3.5 and 3.6)

>= 2.7 (tested with versions 2.7, 3.5 and 3.6) NumPy >= 1.11.3

>= 1.11.3 SciPy >= 0.18.1

>= 0.18.1 Six >= 1.5.0

>= 1.5.0 smart_open >= 1.2.1

There are two ways for installation. We could run the following code in our terminal to install genism package.

pip install --upgrade gensim

Or, alternatively for Conda environments:

conda install -c conda-forge gensim

3. Implementation of word Embedding with Gensim Word2Vec Model

In this tutorial, I will show how to generate word embedding with genism using a concrete example. The dataset I used for this tutorial is from Kaggle Dataset.

This vehicle dataset includes features such as make, model, year, engine, and other properties of the car. We will use these features to generate the word embeddings for each make model and then compare the similarities between different make model. The full python tutorial can be found here.

>>> df = pd.read_csv('data.csv')

>>> df.head()

3.1 Data Preprocessing:

Since the purpose of this tutorial is to learn how to generate word embeddings using genism library, we will not do the EDA and feature selection for the word2vec model for the sake of simplicity.

Genism word2vec requires that a format of ‘list of lists’ for training where every document is contained in a list and every list contains lists of tokens of that document. At first, we need to generate a format of ‘list of lists’ for training the make model word embedding. To be more specific, each make model is contained in a list and every list contains lists of features of that make model.

To achieve this, we need to do the following things :

a. Create a new column for Make Model

>>> df['Maker_Model']= df['Make']+ "" "" + df['Model']

b. Generate a format of ‘ list of lists’ for each Make Model with the following features: Engine Fuel Type, Transmission Type, Driven_Wheels, Market Category, Vehicle Size, Vehicle Style.

# Select features from original dataset to form a new dataframe

>>> df1 = df[['Engine Fuel Type','Transmission Type','Driven_Wheels','Market Category','Vehicle Size', 'Vehicle Style', 'Maker_Model']] # For each row, combine all the columns into one column

>>> df2 = df1.apply(lambda x: ','.join(x.astype(str)), axis=1) # Store them in a pandas dataframe

>>> df_clean = pd.DataFrame({'clean': df2}) # Create the list of list format of the custom corpus for gensim modeling

>>> sent = [row.split(',') for row in df_clean['clean']] # show the example of list of list format of the custom corpus for gensim modeling

>>> sent[:2]

[['premium unleaded (required)',

'MANUAL',

'rear wheel drive',

'Factory Tuner',

'Luxury',

'High-Performance',

'Compact',

'Coupe',

'BMW 1 Series M'],

['premium unleaded (required)',

'MANUAL',

'rear wheel drive',

'Luxury',

'Performance',

'Compact',

'Convertible',

'BMW 1 Series']]

3.2. Genism word2vec Model Training

We can train the genism word2vec model with our own custom corpus as following:

>>> model = Word2Vec(sent, min_count=1,size= 50,workers=3, window =3, sg = 1)

Let’s try to understand the hyperparameters of this model.

size: The number of dimensions of the embeddings and the default is 100.

window: The maximum distance between a target word and words around the target word. The default window is 5.

min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.

workers: The number of partitions during training and the default workers is 3.

sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.

After training the word2vec model, we can obtain the word embedding directly from the training model as following.

>>> model['Toyota Camry'] array([-0.11884457, 0.03035539, -0.0248678 , -0.06297892, -0.01703234,

-0.03832747, -0.0825972 , -0.00268112, -0.09192555, -0.08458661,

-0.07199778, 0.05235871, 0.21303181, 0.15767808, -0.1883737 ,

0.01938575, -0.24431638, 0.04261152, 0.11865819, 0.09881561,

-0.04580643, -0.08342388, -0.01355413, -0.07892415, -0.08467747,

-0.0040625 , 0.16796461, 0.14578669, 0.04187112, -0.01436194,

-0.25554284, 0.25494182, 0.05522631, 0.19295982, 0.14461821,

0.14022525, -0.2065216 , -0.05020927, -0.08133671, 0.18031682,

0.35042757, 0.0245426 , 0.15938364, -0.05617865, 0.00297452,

0.15442047, -0.01286271, 0.13923576, 0.085941 , 0.18811756],

dtype=float32)

4. Compute Similarities

Now we could even use Word2vec to compute the similarity between two Make Models in the vocabulary by invoking the model.similarity( ) and passing in the relevant words. For instance, model.similarity(‘Porsche 718 Cayman’, ‘Nissan Van’) This will give us the Euclidian similarity between Porsche 718 Cayman and Nissan Van.

>>> model.similarity('Porsche 718 Cayman', 'Nissan Van')

0.822824584626184 >>> model.similarity('Porsche 718 Cayman', 'Mercedes-Benz SLK-Class')

0.961089779453727

From the above examples, we can tell that Porsche 718 Cayman is more similar to Mercedes-Benz SLK-Class than Nissan Van. We also can use the built-in function model.most_similar() to get a set of the most similar make models for a given make model based on the Euclidean distance.

>>> model1.most_similar('Mercedes-Benz SLK-Class')[:5] [('BMW M4', 0.9959905743598938),

('Maserati Coupe', 0.9949707984924316),

('Porsche Cayman', 0.9945154190063477),

('Mercedes-Benz SLS AMG GT', 0.9944609999656677),

('Maserati Spyder', 0.9942780137062073)]

However, Euclidian similarity cannot work well for the high-dimensional word vectors. This is because Euclidian similarity will increase as the number of dimensions increases, even if the word embedding stands for different meanings. Alternatively, we can use cosine similarity to measure the similarity between two vectors. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. The cosine similarity captures the angle of the word vectors and not the magnitude. Under cosine similarity, no similarity is expressed as a 90-degree angle while the total similarity of 1 is at a 0-degree angle.

The following function shows how can we generate the most similar make model based on cosine similarity.

def cosine_distance (model, word,target_list , num) :

cosine_dict ={}

word_list = []

a = model[word]

for item in target_list :

if item != word :

b = model [item]

cos_sim = dot(a, b)/(norm(a)*norm(b))

cosine_dict[item] = cos_sim

dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order

for item in dist_sort:

word_list.append((item[0], item[1]))

return word_list[0:num] # only get the unique Maker_Model

>>> Maker_Model = list(df.Maker_Model.unique()) # Show the most similar Mercedes-Benz SLK-Class by cosine distance

>>> cosine_distance (model,'Mercedes-Benz SLK-Class',Maker_Model,5) [('Mercedes-Benz CLK-Class', 0.99737006),

('Aston Martin DB9', 0.99593246),

('Maserati Spyder', 0.99571854),

('Ferrari 458 Italia', 0.9952333),

('Maserati GranTurismo Convertible', 0.994994)]

5. T-SNE Visualizations

It’s hard to visualize the word embedding directly, for they usually have more than 3 dimensions. T-SNE is a useful tool to visualize high-dimensional data by dimension reduction while keeping relative pairwise distance between points. It can be said that T-SNE looking for a new data representation where the neighborhood relations are preserved. The following code shows how to plot the word embedding with T-SNE plot.

def display_closestwords_tsnescatterplot(model, word, size):



arr = np.empty((0,size), dtype='f')

word_labels = [word] close_words = model.similar_by_word(word) arr = np.append(arr, np.array([model[word]]), axis=0)

for wrd_score in close_words:

wrd_vector = model[wrd_score[0]]

word_labels.append(wrd_score[0])

arr = np.append(arr, np.array([wrd_vector]), axis=0)



tsne = TSNE(n_components=2, random_state=0)

np.set_printoptions(suppress=True)

Y = tsne.fit_transform(arr) x_coords = Y[:, 0]

y_coords = Y[:, 1]

plt.scatter(x_coords, y_coords) for label, x, y in zip(word_labels, x_coords, y_coords):

plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')

plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)

plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)

plt.show() >>> display_closestwords_tsnescatterplot(model, 'Porsche 718 Cayman', 50)

This T-SNE plot shows the top 10 similar vehicles to the Porsche 718 Cayman in two-dimensional space.

About Me

I am a master student in Data Science at the University of San Francisco. I am passionate about using Machine Learning to solve business challenges. You can also find me through Linkedin."
Making a DotA2 Bot Using ML,"Making a DotA2 Bot Using ML

Designing a resource-efficient machine-learning algorithm Musashi Schroeder · Follow 18 min read · May 30, 2019 -- 1 Listen Share

The bot roster

Problem

In December of 2018, the creators of AI Sports gave a presentation and introduced the DotA2 AI Competition to the school. DotA (Defense of the Ancients) is a game played by two teams, each consisting of five players who can choose from over one hundred different heroes. The goal of the game is to destroy the opponents base, while defending your own. Each hero has access to at least four unique abilities, and are able to purchase items that also have different abilities. Items are purchased with gold earned from destroying opponent structures, or defeating players or creeps, NPC (non-player character) units that spawn to help defend and attack bases.

The complexity of the game comes from not only the roster of characters, but the ever changing state of the map. Full information games such as chess or go leave no information withheld from players, allowing them to see every possible action on the board at any given time. The DotA map includes a “fog of war” which hides any part of the map not seen by a player or their teammates. Each hero’s abilities also have “cooldowns” — after a player uses an ability, they cannot use it again for a set amount of time — and use mana as a resource. While a player has access to this information about their allies, they do not have this information about the opponent, and must take it into consideration when engaging in fights.

The rules for the competition were to program a full team of five bots to play in Captain’s Mode. Captain’s Mode sets one member of each team as a captain, giving them the ability to choose the heroes for the rest of the team, and also “banning”, or selecting heroes that the opponent’s team cannot use. In order to avoid having all of our characters banned, we needed to program at least sixteen of them. Our limitation set by Maurice,42 Silicon Valley staff, was that we could not use the built in “desires,” a system to provide default behaviors for the bot to execute, provided by Valve. Instead of using the default bot behaviors, we were tasked with writing the code from the bottom up. The API for DotA2 is written in Lua and allows players to create their own bots. The competition was originally designed to use a C++ API written by

Overview of DotA battlefield with labels, image from https://dota2.gamepedia.com/Map

the AI Sports creators, but due to “complications,” our team used Lua instead.

How Was it Solved

Learning Lua and the API

In order to create the bot, we first read through the API and looked for other examples that users had created. The DotA API was made available in early 2016, though hasn’t received any meaningful updates since approximately October of 2017. The first resource we used was a guide on getting started, written by RuoyuSon. RuoyuSon explained where to find other resources and how to start games, as well as useful console commands for the testing process. Valve also provides small examples of bot scripts in the games directory that can be used to potentially get started. With the API and other examples, we naively believed we could create a bot and have a crude, working version of the code within a week.

The first challenge came in selecting the heroes we wanted to use and starting the game. What we didn’t know at the time was that if the code for hero selection has an error, the entire game will crash without displaying anything. The example provided by Valve can be used to quickly create hero selection code for All Pick Mode, but is unusable for Captain’s Mode. In order to select heroes, we read through other examples of the code. Although our current iteration of the bot allows for human players to play against and alongside it, the original version was only meant to play against another bot in Captain’s Mode. Finally getting a simple version of the hero selection working took a little over one week, but since then has been modified to support All Pick Mode and human players.

After getting the game to start, we began experimenting with making heroes walk to locations on the map. We quickly learned not knowing the Lua language made writing and understanding other examples of code difficult. While we were able to make bots walk to certain locations or purchase items, we frequently made syntax errors and finding bugs in code took considerable time. After a frustrating two weeks, we took time to learn the language before engaging with the API again.

While the tournament drew near, we were still figuring out Lua and fighting to understand the API. Our heroes moved to the correct locations and they were able to fight enemy minions and opponents, albeit poorly, but they would never retreat, resulting in death after death. Even against the easiest default bot difficulty, Passive, we were unable to win. We implemented a crude retreat function — simply telling the bots to run back to their base if they took too much damage — that helped, but left a lot to be desired. We were able to consistently win against the Passive bot, but usually ended the game with close to 100 deaths per game on our side, and we were lucky to see two deaths on the opponents.

The next step, now that the groundwork had been laid for bot behaviors, was to begin to individualize each bot so that they could use their abilities. Each bot uses their skills based on conditions, allowing them to fight the enemy. At this point our lack of DotA experience began to show — although the bots were able to use skills, they didn’t use them optimally simply because we didn’t know what optimal was. We frequently asked people with more experience for tips and slowly made the bot stronger. Finally, we were able to defeat the Passive bot with a positive score. We attempted to beat the Easy difficulty, but struggled. The difference between the two was significant and we needed to implement more behaviors in order to win.

State Machine

Up to this point, all code had been written as predefined actions for each bot to execute. The complexity of DotA gradually made it more and more difficult to separate what actions to take and when. Should we fight in this instance, or run away? When should we focus on hitting creeps? While we were able to defeat the Passive difficulty consistently, we realized that Easy would be a significant hurdle. We began to discuss possible options, and landed on the State Machine.

Example of State Machine

When modifying bot behaviors, it became impossible to cleanly separate when it would perform actions. They were so closely intertwined that adjusting one would affect the performance of the other, and none of the behaviors worked particularly well. We were also unable to include more behaviors neatly without disrupting other parts of the code. By creating the State Machine, we were able to separate each behavior, using weighted averages to decide which would be the most optimal in any instance of the game. The code for each bot is run every frame, allowing for constant calculations and giving each behavior a value as a weight. Assuming we programmed the bot well, it could now decide for itself what it should do based on the game state.

At this point we were able to separate each behavior into its own distinct code, broken down into components and conditions. Components are pieces of information that are always necessary to calculate a behavior, while conditions would add or subtract from the weight only under specific circumstances. Separating the code allowed us to make each behavior perform better — previously, each behavior was reliant on another, but by using the State Machine, we would only execute the parts of the code we needed to and only when we needed to.

While some of us set up the State Machine, we also continued to improve the non-State Machine version, to the point that we were able to defeat the Easy difficulty. We were once again seeing 100 deaths on the scoreboard, but would get more kills on our side and eke out wins. The code from the non-State Machine version easily slotted into our new bot, allowing us to continue working without any significant delays.

One of the benefits of the State Machine was the modularity of the system. Prior to this, the bot’s generic behaviors were made up of two files that had necessary comments written throughout in order to understand which part of the code was being looked at — the new version had separate files for each weight and the behaviors were separated so they did not interact with one another. The modularity allowed multiple people to work on different parts of the project without affecting what someone else might be working on, improving clarity, simplicity, and the team’s workflow.

We were also preparing for our first bot vs bot match against another team in the competition, but the State Machine was untested and not ready to implement. This gave us one last chance to see how the previous version held up. Before we started our scrimmage, we decided to test and make sure both teams’ code ran properly. When both bots had a random mix of the opponent’s heroes and their own, the teams realized we had made an error in the picking phase. Both teams were able to fix the issue, but it was another instance of fighting with the API, something that would persist throughout the entire process. At this time, we were also notified by Maurice that the tournament would postponed for a month, giving us a chance to continue to improve our bots.

During testing against Valve’s proprietary bots, we would frequently have to restart games because of the compatibility issues with their bot and Captain’s Mode. We decided to make our own picking mode for two teams in order to speed up the process, and cut down on the unnecessary restarts. We gave our opponent’s bots a random team of five and used this team for much of our testing. What we didn’t know at the time was that this would come back to bite us later.

Our team continued to work with the State Machine, adding more behaviors that we were unable to implement before. As the behaviors increased, we also started to see improvements in our matches against Valve’s bot. After defeating Easy, within 24 hours we were able to beat Medium, and the next day we beat Hard and Unfair back to back. We were ecstatic, not expecting to beat Unfair much later down the line, but as we decided to watch the opponent’s bots closer, our jaws dropped. Two of our opponent’s bots didn’t buy items, and one didn’t use any abilities. Although we were able to win, still a feat in and of itself, it wasn’t a real victory against the Unfair bot.

What we didn’t know was that Valve only implemented specific skill usage and item purchasing on 46 of the bots. We changed the opponent’s lineup to five of those bots, and while we could put up a good fight against Hard and win about forty percent of the time, we rarely won against Unfair. We began to have more discussions about what we could do to increase our win rate, resulting in our first roster change. After looking at the heroes we had implemented, at the time only five, we decided to switch out heroes that would hopefully fit our overall game plan better. Immediately we saw an increase and, while we had become attached to the heroes we chose to use, we began to consider swapping heroes as an option as we continued to program.

Data Gathering

We continued to implement more behaviors into the State Machine, and added more features and, as we did, saw a slow but steady increase in performance in our matches. In order to see how well we did when including something new, we had to watch a full game to observe the specific behavior and to see whether or not we won the match. All of the bot’s weights were hand-tuned, and any tweaks we made might not be visible within a single game. Even sped up, a game would take between ten and fifteen minutes. In order to gather any meaningful data, we could spend over hours just watching. To speed up this process and make sure that any change we added was meaningful, using Python, the Go programming language , and Docker, we began to create a way of gathering data over hundreds of games.

Maurice gave us access to fifteen computers which we could use to run games on and gather data. At this point, we had researched a “headless” mode for DotA; we were able to run games graphicless which would speed up the games themselves, and allow us to run multiple instances of the game without using the GPU. Using Docker, we set up a client to server connection that allowed us to use virtual machines on fourteen of those computers. We calculated that we could run up to four games per computer optimally, so ran four virtual machines at six times speed. Altogether, we were able to run games approximately 300 times faster than with originally.

Each game could range between fifteen and eighty minutes. Docker Swarm distributed the total number of games requested evenly to all of our worker computers. If we were running less than 56 games this solution would be fine, but anything more would be suboptimal. We initially attempted to deploy using Docker Swarm, but it made more sense for us to create our own solution. It would need to be customizable, work well on a distributed network, and have support for simple concurrency. We decided to use Go because it filled our criteria and was easy to build and deploy. Finally, Python was used to graph and illustrate our data results as histograms and line graphs.

Data showing wins and losses over time

Using this setup, we were able to run 500 games over an hour, giving us meaningful data. While it was still necessary for us to watch games to observe and confirm behaviors worked properly, we could now test them and gather data in order to confirm whether or not a change was beneficial or detrimental to the bot.

As we went into the final weeks, we played with the idea of incorporating a genetic algorithm. The State Machine weights were all hand tuned and based on our observations. Specifically our Farm, Hunt, and Retreat weight were so closely tied together that by changing the values of one, we would see dramatic differences in the way they played and their win rates would generally decrease. We knew they were at a good point, but were sure they weren’t optimal, especially considering different characters played differently and using the same weights made them all play more or less the same. Using a genetic algorithm would use machine learning to tune each weight, giving us the most ideal numbers to defeat the default bots, and hopefully our opponents in the tournament. An ambitious goal was to create different genes for each character, thereby giving them each their own unique play style, but we knew that without more time or more computing power, we would have to make do with the hand-tuned weights we had.

A week before the competition, we strayed away from adding major features, only including small changes that our data decisively proved would increase the win rate. By the end, with the State Machine we were able to achieve a consistent win rate above 98% against the Valve bots. Ready for the competition, Maurice messaged us, informing us that once again the competition had been extended for another month.

Genetic Algorithm

With the month long extension to the tournament, we began to discuss how we could create a genetic algorithm. In the end, we decided to use Go once again because our data gathering programs had already been written in it, therefore making it easier to tie the programs together.

Genetic algorithm flowchart, from arwn

In order to get the genetic algorithm to work, we needed to run multiple iterations of our bot. From those iterations, we would grab the top five heroes genes and “breed” them by shuffling, averaging, and splicing them together. The next generation would be made up of slightly modified versions (using a 10% mutation probability to choose which genes to change, and 10% mutation rate to change each gene by the respective amount) which we would then gather data on, repeating the process until the beginning of the competition. Our plan was to replace the current hand-tuned genes with our new machine learned ones.

Our first step was making sure we could run the genetic algorithm using Go and Docker, and modify the Lua script at the same time. Each bot’s gene was a Lua file containing the values we wanted to mutate using the genetic algorithm. We used Go to read in the gene file, mutated the values, and output the new gene using a gene template. The new genes were then used for the subsequent iterations.

Having successfully created a way to read and write to our new gene files, instead of making one generic genetic algorithm as we had originally planned, we created genes for each hero we were using. In order to make it work, each file had to include the name of the hero we were writing to. Unfortunately we could only train five heroes at a time, so we opted to train our starting lineup and use our hand tuned genes for the rest of the heroes we had implemented.

Finishing the genetic algorithm ended up taking longer than planned. We hoped to have it running and training within a week, but needed a few more days to iron out bugs. We had each made separate parts of the genetic algorithm, and piecing each together took some time.

Finally, the genetic algorithm worked, but as we began to run the first generations, we ran into multiple issues. At this point, we had continued to have some issues with our Docker containers not running games, but had chosen to ignore it for the time being because while it had been slower in collecting data, it wasn’t a significant time difference. If one computer malfunctioned and dropped off the network the server would hang, waiting for data to come in from the downed machine. When we decided to use the genetic algorithm, we needed it to run non-stop and continue working through each generation. If a worker failed to respond, the server could never move onto the next generation because it was waiting for the remaining games to come in. It made little sense for us to monitor the computers in shifts all day, so we added in a way of timing out if we did not get a response from the container after a period of time.

In the end, after about four days of starting and stopping the genetic algorithm, we finally had it working. While running the genetic algorithm and confirming that it worked, we decided to change our team lineup in favor of one we thought could raise our win rate. When we began running the genetic algorithm and set up the genes we wanted to manipulate, as a team, we went through them and adjusted them to numbers we believed made sense for the genetic algorithm to start on. At that time, we decided to manipulate approximately 25 components and conditions, the “genes,” from our Farm, Hunt, and Retreat weights. This change combined with a new hero selection we used for the opposing team dropped our win rate from 98% to 80%. While the genetic algorithm was slowly raising the win rate, we spoke as a team and decided that if we could boost it by switching or adding heroes early on, it could be worth testing. After the switch, the initial 80% rose closer to 90%.

While we observed the bot, we knew that time was beginning to run out and it wasn’t growing fast enough. Although it was a risky decision that could result in a potentially drastic decrease in win rate, we decided to adjust the rate of change from 10% mutation probability and 10% mutation rate to 15% and 25% respectively. We calculated that in the most ideal situation, in order to get rid of a gene that was not useful, it would take at least thirty generations, or at least one week. We wanted to reduce that number and figured if we doubled it, we would see a higher rate of change, for better or for worse. After days of observing the results, our risk paid off and the bot saw a faster and more consistent increase in win rate.

Fitness growth over time

Once we were sure of the outcome, we began to add in more genes to manipulate from other State Machine weights. Another problem we ran into throughout the project that we had been unable to solve was how to play early on in the game, and how to play near the end. In DotA, the play styles between the two are drastically different. The behaviors that are important early on are less important as the game goes on for longer, and vice versa. Our strategy up to this point had been to trade a slightly weaker start to the game for a more powerful finish. We had tried to tweak the weights multiple times, but even if they played better at the beginning, the manipulated weights would fail in the end dropping the overall win rate. Now that we had our working genetic algorithm, we added in various multipliers to health for it to adjust, but also decided to add in multipliers based on how powerful the hero is. Heroes go from level 1 to 25 and get stronger as they gain levels. By hand we were never able to manipulate the weights in an effective way that would allow for early and late game adjustments. With the genetic algorithm, we could now leave it up to the computer to decide when to play differently.

After one more hero change, we settled on our final roster and continued to let the genetic algorithm work. A few days before the beginning of the tournament, we saw that the bot had finally reached a 99% win rate for one generation, but this dropped the next generation to 96%. While our rapid manipulation of genes had created a powerful bot, once it got closer and closer to it’s theoretical peak, the 25% mutation rate would change to much at once and dropped the win rate. We decided that in order to preserve our win rate, we would need to slow down the mutation. The mutation probability was dropped to 7% and the mutation rate was dropped to 15%.

As we changed our genetic algorithm once again, we decided to take another risk. Up to this point we had been taking the top five genes from each hero as parents, breeding them and using the offspring for the next generation. While this had worked for us, classically it was not how we should have been using a genetic algorithm. In a genetic algorithm, all genes should have a chance of being picked, but we were actively selecting which to use. The importance of using the lower win rate bots is diversity. While in the generation it may not have performed as well, in a future generation its genes may be an important part towards increasing the win rate. In order to make sure those lower win rate genes had a chance at being selected, we mapped all of the genes, allowing a higher probability of being picked to the higher win rates, while still giving the lower win rates a chance, albeit smaller.

We also discussed a change in strategy. Taking the genes from each individual bot was necessary, but we considered the importance of taking all of the genes from one “team” of bots. A bot at first glance may have looked like it had less potential, but as part of a team it’s genes could have been an important key to victory. We thought about the benefits of switching from the individual bots to only breeding teams, but couldn’t justify losing out on more powerful heroes genes. As we came to the conclusion that we should continue using the same strategy of selecting the individual bots, a thought popped into our heads. What if we do both? Taking the individual genes was undeniably important, but by breeding bots from the same team with the stronger individual bots, we believed we could unlock the potential of both worlds.

Conclusion

The genetic algorithm seems to have improved the bot, although its play style is considerably different from the original non-genetic version. While we were much more aggressive earlier, we now play more conservatively and aim to win mostly by destroying structures and winning the occasional team fights. The older bot would group together more often as a team, forcing opponent bots to react and resulting in more fights. If we continued to work on the project, I believe the next step would be having the bot begin to fight itself and the older, non-genetic version of the bot.

Through this project, I’ve been able to learn multiple programming languages, as well as familiarize myself with Docker, and the importance of documentation when working as a team. The reason I decided to work on this project was less an interest in DotA2, but more trying to understand machine learning. I’d heard the term multiple times, had read about it, but didn’t have a real understanding of what it entailed and how to actually program it. Participating on this project gave me a unique opportunity to work with machine learning, and has increased my understanding of programming as a whole."
Are AI ‘Thinking Machines’ Really Thinking?,"Since the development of the first universal computers scientists have postulated the existence of an artificial consciousness; a constructed system that can mirror the complex interactions that take place within the human brain. While some public figures are openly terrified about the coming cyborg apocalypse, for most people artificial intelligence these days refers to tools and applications that can help us get our work done faster, rather than androids and artificial people. AI is now predominantly considered as a narrow use of a particular type of technology, distinct from artificial general intelligence (AGI), a much broader concept that encompasses synthetic consciousness.

Elon Musk: right to be afraid?

Considering the growth in the field of AI over the past decade or so, and the massive ongoing investment, it is worth exploring just how far along the path we have travelled towards Terminators, replicants and R2-D2, and the problems that have presented themselves. Many scientists and thinkers believe that AGI is a scientific inevitability based on the concept of universality, while others suggest that there are ontological physical limitations that prevent the recreation of consciousness. The disagreement is effectively a philosophical one; there is no empirical evidence that comprehensively backs either hypothesis. What is clear is that scientists have been extremely effective at recreating, and even improving upon certain human skills, and entirely unsuccessful at reproducing others.

‘Artoo’ even had a sense of humour.

The idea of artificial, synthetic consciousness that could resemble a human-like intelligence raises mind-boggling ethical and moral questions. This is a massive and fascinating topic that I will not address here. Instead, I will consider the practical barriers to the development of such an entity, and their philosophical implications.

Artificial Intelligence is one of the leading development trends in tech research today, to the extent that it infiltrates almost all other technologies. AI will continue to redefine how businesses operate as advanced analytics and automation become more efficient and reliable, meaning that companies that fail to adapt will risk getting left behind. New AI technologies like those found in autonomous cars, or generative adversarial networks that can construct entirely new original novelties, could lead to the development of previously unimaginable applications and ideas.

These advancements are based on the core idea of ‘thinking machines’; software that can replicate certain cognitive functions of the human brain. There is no single definition of AI (even the term ‘intelligence’ is subjective), but it is most often understood to refer to applications that can perceive their environments to accomplish their programmable objectives. Machines that can learn, i.e. develop understanding beyond what has been hard-coded are amongst the largest sub-set of developments in AI. Machine learning or deep learning algorithms are often based on artificial neural networks. These are computing systems that are specifically modelled on how the human brain works.

We refer to these as ‘thinking machines’ even though they don’t think in the way that humans do. They perceive their environments, but they are not aware of them. Computers are furnished with memory, just like conscious beings are, and modern AI systems can anticipate or predict based on informational input (this is one of the ways an AI can construct a predictive model, for example for business or in healthcare). These capabilities are all thought to be necessary aspects of consciousness, but a machine is only capable of implementing them in extremely narrow forms. AI is inflexible and incapable of anticipating or remembering outside of its definite, limited programming. For example, a highly advanced machine-learning algorithm designed to make predictions in road-traffic patterns cannot repurpose its intelligence to have a conversation or play a game.

Machines can be programmed to learn how to play chess but would be stumped if presented with your accounts.

Facilitating flexibility in AI in this way appears to represent a significant challenge. However, it may not be the most challenging aspect of consciousness to recreate. The concept of subjective experience, that is, internal and often unexplainable mental states and reactions, is often thought of as the ‘big question’ of consciousness by both psychotherapists and philosophers. Thomas Nagel wrote:

“…an organism has conscious mental states if and only if there is something that it is like to be that organism — something it is like for that organism.”

In other words, it is not enough for a machine to think — it must know that it is thinking and have a sense of its existence apart from its thoughts. Descartes famously said “I think, therefore I am”, to illustrate that he had a mind, as distinct from a physical thinking brain. This idea is often linked with the concept of qualia — the subjective interpretation of sensations, neither explainable nor predictable. Philosophers often might describe the ‘ouchiness’ of the sensation of pain or the innate ‘redness’ that we experience when we perceive the colour red. We can describe scientifically what happens when light rays make contact with the cones in our eyes, and we can compare it to other similar colours we’ve seen, but there is no way for two people to objectively compare their personal experience of red. This concept is inherently problematic for scientists, and they mostly tend to ignore it. It is, however, one of many intangible, indefinable abstractions that undoubtedly exist, both within the human mind and exclusive of it, that cannot be defined scientifically."
How data management practice enables the successful implementation of a single customer view?,"What is a single view of the customer?

A single customer view is a centralized location that consolidates all the knowable data and information of your customers within your organization and enables you to view, find, and understand every aspect of your customers. Having a single view of customers within your environment helps you understand what and how your customers purchase. These intelligence servers as fuel to your sales and marketing team’s formulation and development of strategy.

Insight into the difficulties?

Extraction, consolidation, and integration are what you need on the face of it. The implementation and the solution of it are not straightforward at all, though. Customer interaction with your services or/and products can be long, stretching across multiple online and offline channels and touchpoints. Managing, governing, consolidating, and transforming valuable data and information with a tremendous variety across different channels and touchpoints is very much challenging.

Data management practice helps address the challenges

Reviewing the end-to-end process and mechanics of building a single customer view reveals what elements of data management practice help solve the challenges.

Step 1 Extraction: extraction of all the knowable data of your customers from different systems by similar column names, by documentation from business analysts, and by knowledge from system analysts. But

A. what knowable customer data do you have in your organization?

B. how do you know which system(s) to extract the customer data that you search for?

Elements: Data Dictionary, Enterprise Data Flow, Data Lineage

Step 2 Data Cleansing: removal of all the noises of your customer data by rules provided by IT. But"
